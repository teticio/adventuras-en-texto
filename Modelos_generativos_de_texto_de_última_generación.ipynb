{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modelos generativos de texto de última generación",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teticio/aventuras-en-texto/blob/master/Modelos_generativos_de_texto_de_%C3%BAltima_generaci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T25DUNWU0R7t",
        "colab_type": "text"
      },
      "source": [
        "# Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZq-8euV0Q-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "df61098e-43af-4111-8e50-4c504a60278c"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import selectors\n",
        "import subprocess as sp\n",
        "from IPython.display import clear_output\n",
        "from IPython.core.display import display, HTML\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import get_file\n",
        "\n",
        "sess = tf.Session()\n",
        "checkpoint_dir = os.getcwd().replace('\\\\', '/') + '/checkpoints'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4eQ2KGyPgUx",
        "colab_type": "text"
      },
      "source": [
        "# XLNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EItLO1rwgd9Y",
        "colab_type": "text"
      },
      "source": [
        "### Instalar modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg2zUAnoZ_IK",
        "colab_type": "code",
        "outputId": "92fd654d-e072-4549-c9eb-0a753a0ab719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!test -d XLnet-gen || git clone --quiet https://github.com/rusiaaman/XLnet-gen.git\n",
        "!pip install -q -r XLnet-gen/requirements.txt\n",
        "!test -e cased_L-24_H-1024_A-16.zip || wget -q https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip\n",
        "!test -d xlnet_cased_L-24_H-1024_A-16 || unzip -q cased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.0MB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 19.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq_iYW7ogjw7",
        "colab_type": "text"
      },
      "source": [
        "### Generar texto\n",
        "\n",
        "Requiere `pip install tensorflow==1.14.0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JBzCdVv6Q-z",
        "colab_type": "code",
        "outputId": "0787d36c-b9a0-4e19-c551-f8b826052f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "ejemplo = 'Sadly, however, before she could get to a phone to tell anyone- about it, a terribly stupid catastrophe occurred, and the idea was lost forever.' #@param {type : 'string'}\n",
        "with open('test', 'wt') as file:\n",
        "    file.write(ejemplo)\n",
        "command = ['python', 'XLnet-gen/language_generation.py',\n",
        "           '--model_config_path=xlnet_cased_L-24_H-1024_A-16/xlnet_config.json',\n",
        "           '--init_checkpoint=xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt',\n",
        "           '--spiece_model_file=xlnet_cased_L-24_H-1024_A-16/spiece.model',\n",
        "           '--input_file=test',\n",
        "           '--max_mem_length=512',\n",
        "           '--num_toks_pred=512',\n",
        "           '--num_samples=1',\n",
        "           '--top_p=0.9']\n",
        "p = sp.Popen(command, stderr=sp.PIPE)\n",
        "if p.returncode != 0:\n",
        "    print(p.communicate()[1].decode())\n",
        "with open('test.xlnet', 'rt') as file:\n",
        "    for line in file.readlines():\n",
        "        display(HTML('<p>' + line + '</p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======Example 0=================\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>Sadly, however, before she could get to a phone to tell anyone- about it, a terribly stupid catastrophe occurred, and the idea was lost forever.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======Example 0 SAMPLE 0======\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>Now she was stuck with the dreadful notion of seeing herself impure and not even knowing what she had imagined. Sure, it was only a whiff of impure air she had been breathing for that whole time, but what could she have actually imagined? She had no idea how to think of anything.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>After everyone had spoken, she turned to take the stairs down to her apartment and watched the people leave. The crowd were already in a hurry to get out of the restaurant. Since she was alone, she thought she would try to a lie. With each successive thing she wanted to tell people, she found it hard to think of a lie.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>A big pro at lying like her father was, she needed to think of something else that would have an easy target in the eyes of those around her. After all, this was her last chance to tell her father about her friend and everyone else about her boyfriend and the wedding plans they had on the horizon. At the moment, it was more important to avoid things than face her fears.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>For that reason, she started to think about something else. Possibly something that would involve going to a dungeon, torture, or some other horrible horrible thing that would hurt her. It would also need to be intimate with people, which her father would probably be very uncomfortable with. When she thought about the possibility, she was appalled by how terribly destructive and scary it all could be.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>Next, she thought about another rather horrible idea that was \"on the thoughts\" of the other people in the restaurant. These men were already together. She decided it would be easy for them to discover that there were two women in the party, and take their chances with them. She knew she could prevent this. In fact, she was preparing herself mentally to use her abilities to prevent it. If anything, it might also make things even more difficult for her dad.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>She thought about how hard it was not to see herself in the same light as the woman who had been in the photo. In fact, it was hard to imagine the woman as the person who had been in the photo, because she had no idea what it would be like. It seemed much more difficult than she had thought it would be.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>She had to have some kind of defense for herself. If anyone did discover that she was a female, it would mean a lot to her father. The thought of that made her want to scream, but the words didn't come easily to her. They didn't come\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>==================================\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw1_5HFpPHXA",
        "colab_type": "text"
      },
      "source": [
        "# GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-gN6fB1Rr7h",
        "colab_type": "text"
      },
      "source": [
        "### Instalar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqpWEas7zw7P",
        "colab_type": "code",
        "outputId": "c1de14f6-1189-4721-fba2-614407c26358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!test -d gpt-2 || git clone --quiet https://github.com/nshepperd/gpt-2.git\n",
        "!pip install -q -r gpt-2/requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 604kB 7.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 16.8MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Iij49z25SpD",
        "colab_type": "text"
      },
      "source": [
        "### Importar los módulos de GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMdg--TD5DDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "c8d0f76c-9986-49dc-e822-614a245a86ef"
      },
      "source": [
        "if not 'gpt-2' in sys.path: # hack\n",
        "    sys.path += ['gpt-2']\n",
        "if not 'gpt-2/src' in sys.path: # hack\n",
        "    sys.path += ['gpt-2/src']\n",
        "sys.argv = ['train.py', '--dataset=../train.txt', '--model_name=345M']    \n",
        "import train\n",
        "import model, sample, encoder"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0725 17:19:35.405254 139781391767424 deprecation_wrapper.py:119] From gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0725 17:19:35.428422 139781391767424 deprecation_wrapper.py:119] From gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkpW4YIFRnWf",
        "colab_type": "text"
      },
      "source": [
        "### Descargar los pesos del modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8bVBQsB_oBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('gpt-2/models/345M'):\n",
        "    sp.call(['python', 'download_model.py', '345M'], cwd='gpt-2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2dTNhrkROWv",
        "colab_type": "text"
      },
      "source": [
        "### Definir la función para generar textos\n",
        "\n",
        "Basado en https://github.com/openai/gpt-2/blob/master/src/interactive_conditional_samples.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJsfjW3fASYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interact_model(\n",
        "    prompt,\n",
        "    model_name='117M',\n",
        "    seed=None,\n",
        "    nsamples=1,\n",
        "    batch_size=1,\n",
        "    length=None,\n",
        "    temperature=1,\n",
        "    top_k=0,\n",
        "    top_p=0.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Interactively run the model\n",
        "    :model_name=117M : String, which model to use\n",
        "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
        "     results\n",
        "    :nsamples=1 : Number of samples to return total\n",
        "    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
        "    :length=None : Number of tokens in generated text, if None (default), is\n",
        "     determined by model hyperparameters\n",
        "    :temperature=1 : Float value controlling randomness in boltzmann\n",
        "     distribution. Lower temperature results in less random completions. As the\n",
        "     temperature approaches zero, the model will become deterministic and\n",
        "     repetitive. Higher temperature results in more random completions.\n",
        "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "     considered for each step (token), resulting in deterministic completions,\n",
        "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "     special setting meaning no restrictions. 40 generally is a good value.\n",
        "    :top_p=0.0 : Float value controlling diversity. Implements nucleus sampling,\n",
        "     overriding top_k if set to a value > 0. A good setting is 0.9.\n",
        "    \"\"\"\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    cwd = os.getcwd()\n",
        "    os.chdir(cwd + '/gpt-2') # hack\n",
        "    raw_text = prompt\n",
        "    texts = []\n",
        "    \n",
        "    try:\n",
        "        enc = encoder.get_encoder(model_name)\n",
        "        hparams = model.default_hparams()\n",
        "        with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
        "            hparams.override_from_dict(json.load(f))\n",
        "\n",
        "        if length is None:\n",
        "            length = hparams.n_ctx // 2\n",
        "        elif length > hparams.n_ctx:\n",
        "            raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k, top_p=top_p\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        context_tokens = enc.encode(raw_text)\n",
        "        for _ in range(nsamples // batch_size):\n",
        "            out = sess.run(output, feed_dict={\n",
        "                context: [context_tokens for _ in range(batch_size)]\n",
        "            })[:, len(context_tokens):]\n",
        "            for i in range(batch_size):\n",
        "                texts += [enc.decode(out[i])]\n",
        "                \n",
        "    except:\n",
        "        os.chdir(cwd) # hack\n",
        "        raise\n",
        "    os.chdir(cwd)\n",
        "    return texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6yskrmEFWNIl"
      },
      "source": [
        "### Especificar los checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a8428b7e-1b92-4c97-f53a-49fd1979b420",
        "id": "4D3vkm1iWK4k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# modelo pre-entrenado\n",
        "checkpoint_num = ''\n",
        "model_checkpoint_path = 'model_checkpoint_path: \"model.ckpt\"'\n",
        "\n",
        "with open('gpt-2/models/345M/checkpoint', \"wt\") as file:\n",
        "    print(model_checkpoint_path)\n",
        "    file.write(model_checkpoint_path)\n",
        "with open('gpt-2/models/345M/counter', \"wt\") as file:\n",
        "    file.write(f'{checkpoint_num}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_checkpoint_path: \"model.ckpt\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDVL323ARG34",
        "colab_type": "text"
      },
      "source": [
        "### Generar muestras con el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq6s-tH1ljXu",
        "colab_type": "code",
        "outputId": "8ff49c27-4314-413f-ed7e-c43b5f77bd6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ejemplo = \"You're in a desert, walking along in the sand, when all of a sudden you look down and see a tortoise, Leon. It's crawling toward you. You reach down, you flip the tortoise over on its back. The tortoise lays on its back, its belly baking in the hot sun, beating its legs trying to turn itself over, but it can\\u2019t, not without your help. But you\\u2019re not helping. Why is that?\" #@param {type : 'string'}\n",
        "numero_de_muestras = 3 #@param {type : 'number'}\n",
        "temperature=1 #@param {type : 'number'}\n",
        "#@markdown La temperatura controla el grado de aleatoriedad (0 = determinista)\n",
        "top_k=40 #@param {type : 'integer'}\n",
        "#@markdown Número de candidatos considerados en el beam search (0 = \"greedy\", funciona bien con 40)\n",
        "top_p=0.9 #@param {type : 'number'}\n",
        "#@markdown Controla la diversidad. (0 = valor por defecto, funciona bien con 0.9)\n",
        "texts = interact_model(prompt=ejemplo,\n",
        "                       model_name='345M',\n",
        "                       nsamples=numero_de_muestras,\n",
        "                       temperature=temperature,\n",
        "                       top_k=top_k,\n",
        "                       top_p=top_p)\n",
        "for i, text in enumerate(texts):\n",
        "    display(HTML('<p>' + \"=\" * 40 + \" SAMPLE \" + str(i+1) + \" \" + \"=\" * 40 + '</p>'))\n",
        "    display(HTML('<p>' + ('<b><i>' + ejemplo + '</b></i>' + text).replace('\\n', '<br>') + '</p>'))\n",
        "    display(HTML('<p>' + \"=\" * 80 + '</p>'))\n",
        "    display(HTML('<p><br></p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======================================== SAMPLE 1 ========================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>You're in a desert, walking along in the sand, when all of a sudden you look down and see a tortoise, Leon. It's crawling toward you. You reach down, you flip the tortoise over on its back. The tortoise lays on its back, its belly baking in the hot sun, beating its legs trying to turn itself over, but it can’t, not without your help. But you’re not helping. Why is that?</b></i>’ The tortoise is silent. It stares up at you.<br><br><br>For this, you apologize. You tell it to move on. You apologize again. The tortoise no longer can sit still. It moves on, dragging its leg.<br><br><br>It looks up. The tortoise looks up at you with a bright grin.<br><br><br>***<br><br><br>For days, the last images from one of my least favorite movies from my childhood are burned in my mind. I can't remember the title of the movie, as I could think of it at any moment. But the trailer comes out, and the whole world swells with shame. The movie exists, in my mind. Everything in the movie is an expression of my experiences. People I've known ask me why I thought those movies, and I'd tell them there's nothing wrong with them. Then, once they've done so, they can't remember what they were expecting.<br><br><br>I didn't even know for sure what was supposed to happen with the kids at the end. Many of my memories from that movie overlap in ways that make sense. But no matter what kids, students, or the \"real\" world watches, or doesn't see, they'll understand something about the movie.<br><br>Twenty years ago, the Academy of Motion Picture Arts and Sciences approved Shame, my first feature film. The parents were giving me a lot of grief for casting two white actors in blackface as human beggars. But for me, it was an honor and an adventure, a delight in the chase, ever closer to seeing people really feel for one another.<br><br><br>About the orgy-loving moon girl It is no accident that the film, screened for a student audience, was nominated for an Oscar. She is naked, she is shameless, but she is not only an attractive person. As she peels a pack of lemons and sacrifices them to insects like bats and scorpions, a detail that was only intended to provoke laughs, I recognized with joy that its tragic content was beautiful, even uplifting. \"The End of the World as We Know It\" is also a moment of transcendence. It is no accident that the movie, screened for a student audience, was nominated for an Oscar. She is naked, she is shameless, but she is not only an attractive person. As she peels a pack of lemons and sacrifices them to insects like bats and scorpions, a detail that was only intended to provoke laughs, I recognized with joy that its tragic</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>================================================================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><br></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======================================== SAMPLE 2 ========================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>You're in a desert, walking along in the sand, when all of a sudden you look down and see a tortoise, Leon. It's crawling toward you. You reach down, you flip the tortoise over on its back. The tortoise lays on its back, its belly baking in the hot sun, beating its legs trying to turn itself over, but it can’t, not without your help. But you’re not helping. Why is that?</b></i> Is there something wrong’with you? Liger A You’are telling the truth. Hard enough that you have to take every breath out of it. Leon A It's your turn. Harder than you think. Harder than you're willing to go. You hold it. Liger A You. Hurry! Hurry, hurry’do not stop. Behind you’a beaver, a burrowing lizard, spots you. The burrowing lizard just as it was always kind enough to curl up in a ball next to you and stretch out an arm to shake your hand. Then it climbs up on its hind legs. Liger A It raises its right arm. It opens a small door over your shoulder. She turns the knob on the door to the room below. Three dead animals are scattered around, running through a ravine. Liger A More dead bodies, crawling through. There's blood everywhere. Then, like a train, something drops to the ground from above, throws back some body parts, and spills them all out over the place. Another little kid with a shirtless and stupid pose. The burrowing lizard. The burrowing lizard raises its right arm, takes out its fist, and goes for the butt. But Leon D turns around. Come back. Leon D Come back, coward! Liger A Do not try to beat them over the head, look down, and tell me you are fighting them, leaving them to die, it is better that than walking around surrounded by them? Big smile. \"Good idea\" makes you dizzy. Big smile, large slow smile, friend the burrowing lizard gets a drag and takes a bite out of his own butthole. Liger A Yes, it has laid down some zombie bodies and laid them out with all that dying away. He has killed quite a number of them, but that is more to attract the carnivorous insects that once ate their own. Big smile. \"I will teach you to live\", they say from beneath you. Leon D Oughta fuckin' bother’move out’or else! Liger A Do not. Leon D You are an alien. ’You’will’be going to another planet with monsters. So you understand not to bother, because it could just as well take you with it. I assume you know how far away this planet is, maybe a million kilometers. Liger A You've got a course? That's this planet's orbit. More than</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>================================================================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><br></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======================================== SAMPLE 3 ========================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>You're in a desert, walking along in the sand, when all of a sudden you look down and see a tortoise, Leon. It's crawling toward you. You reach down, you flip the tortoise over on its back. The tortoise lays on its back, its belly baking in the hot sun, beating its legs trying to turn itself over, but it can’t, not without your help. But you’re not helping. Why is that?</b></i> Because ’I don't give a shit about a tortoise, I've been around for forty years and I've fought wars’ But, though Leon is wounded, you don't give a shit because ’I'm a coward, we need help, they're coming. They're crawling up the side of a hill, and out in the distance you see a red and blue and white patch’ And suddenly, you start running with Leon. I've seen tanks burn before, but this is hot’ But again, you run after them, running against the never ending dangers of the desert. You see a red, yellow and white rectangle ’this is the Death Star’ and suddenly, something terrible happens. A giant cylindrical object is burning through the sky. As you watch it burn, an explosion’ the blob of smoke ’clears the horizon as it floats up to us, a huge, blazing white globe’ charging towards us. And you watch it right behind you on that gigantic globe. You rush toward it, aim, blast. But not, because you don't care. You're watching the Death Star do one amazing thing, and now, the heroes’and the galaxy’are going down’<|endoftext|>People change, jobs change, progress stagnates. The topic of video game marketing, the platform that supposedly gave rise to entire forms of entertainment, seems and always has been relegated to the back of main headlines and unreported blogs. And if you're looking to game makers in particular, this is becoming a problem because there's still a bit of quality control to this idea that a publication that hopes to expand a market, is going to put their whole company at risk for another outlet to copy and paste to their liking, instead of producing something unique to their audience.<br><br>It's easy to get into this territory. Why should an animated RPG be your exclusive lifestyle knowledge, compared to the world of Minecraft or Portal? I've heard it's because game makers don't always know how to sell their product well enough to encourage the hardcore gamers that make the title popular, and are sometimes unable to convince people to spend a month or more just for that core multiplayer experience.<br><br><br>It's not that all game designers are nukers. Many are commercial designers of diverse styles and language tones who want to sell their designs across platforms, including still-useable games like Minecraft. If you want to compare your game design to a 5 year old game you did on two</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>================================================================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><br></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3PU6QOz0Pva",
        "colab_type": "text"
      },
      "source": [
        "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45QUnzzAnDbT",
        "colab_type": "text"
      },
      "source": [
        "### Contestar una pregunta sobre un texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DsXfTZBra-M",
        "colab_type": "code",
        "outputId": "625d7ae1-9cca-45bd-c138-16a12d992d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        }
      },
      "source": [
        "contexto = 'The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer \\n\\\n",
        "Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in \\n\\\n",
        "Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried \\n\\\n",
        "the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started \\n\\\n",
        "ahead of the 1936 Summer Olympics. \\n\\\n",
        "\\n\\\n",
        "After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the \\n\\\n",
        "Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was \\n\\\n",
        "following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing \\n\\\n",
        "ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of \\n\\\n",
        "Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the \\n\\\n",
        "event. \\n\\\n",
        "\\n\\\n",
        "Q: What was the theme \\n\\\n",
        "A: “one world, one dream”. \\n\\\n",
        "Q: What was the length of the race? \\n\\\n",
        "A: 137,000 km \\n\\\n",
        "Q: Was it larger than previous ones? \\n\\\n",
        "A: No \\n\\\n",
        "Q: Where did the race begin? \\n\\\n",
        "A: Olympia, Greece \\n\\\n",
        "Q: Is there anything notable about that place? \\n\\\n",
        "A: birthplace of Olympic Games \\n\\\n",
        "Q: Where did they go after? \\n\\\n",
        "A: Athens \\n\\\n",
        "Q: How many days was the race? \\n\\\n",
        "A: seven \\n\\\n",
        "Q: Did they visit any notable landmarks? \\n\\\n",
        "A: Panathinaiko Stadium \\n'\n",
        "\n",
        "pregunta = 'Q: And did they climb any mountains? \\n\\\n",
        "A:'\n",
        "\n",
        "display(HTML('<p>' + (contexto + '<b><i>' + pregunta + '</b></i>').replace('\\n', '<br>') + '</p>'))\n",
        "texts = interact_model(prompt=contexto + pregunta,model_name='345M', temperature=0.001)\n",
        "answer = texts[0][:texts[0].find('Q')]\n",
        "display(HTML('<p>' + answer + '</p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer <br>Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in <br>Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried <br>the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started <br>ahead of the 1936 Summer Olympics. <br><br>After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the <br>Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was <br>following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing <br>ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of <br>Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the <br>event. <br><br>Q: What was the theme <br>A: “one world, one dream”. <br>Q: What was the length of the race? <br>A: 137,000 km <br>Q: Was it larger than previous ones? <br>A: No <br>Q: Where did the race begin? <br>A: Olympia, Greece <br>Q: Is there anything notable about that place? <br>A: birthplace of Olympic Games <br>Q: Where did they go after? <br>A: Athens <br>Q: How many days was the race? <br>A: seven <br>Q: Did they visit any notable landmarks? <br>A: Panathinaiko Stadium <br><b><i>Q: And did they climb any mountains? <br>A:</b></i></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p> Mount Everest \n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alhg-bLiKQbD",
        "colab_type": "code",
        "outputId": "127f2876-7c26-4cd3-8855-bcf1a8e67d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "contexto = 'Tom goes everywhere with Catherine Green, a 54-year-old secretary. He moves around her office at work and goes \\n\\\n",
        "shopping with her. ”Most people don’t seem to mind Tom,” says Catherine, who thinks he is wonderful. ”He’s my \\n\\\n",
        "fourth child,” she says. She may think of him and treat him that way as her son. He moves around buying his food, \\n\\\n",
        "paying his health bills and his taxes, but in fact Tom is a dog. \\n\\\n",
        "\\n\\\n",
        "Catherine and Tom live in Sweden, a country where everyone is expected to lead an orderly life according to rules \\n\\\n",
        "laid down by the government, which also provides a high level of care for its people. This level of care \\n\\\n",
        "costs money. \\n\\\n",
        "\\n\\\n",
        "People in Sweden pay taxes on everything, so aren’t surprised to find that owning a dog means more \\n\\\n",
        "taxes. Some people are paying as much as 500 Swedish kronor in taxes a year for the right to keep their dog, which \\n\\\n",
        "is spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. However, most \\n\\\n",
        "such treatment is expensive, so owners often decide to offer health and even life for their dog. \\n\\\n",
        "\\n\\\n",
        "In Sweden dog owners must pay for any damage their dog does. A Swedish Kennel Club official explains what this means: \\n\\\n",
        "if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay \\n\\\n",
        "for any damage done to the car, even if your dog has been killed in the accident. \\n\\\n",
        "\\n\\\n",
        "Q: How old is Catherine? \\n\\\n",
        "A: 54 \\n'\n",
        "\n",
        "pregunta = 'Q: where does she live? \\n\\\n",
        "A:'\n",
        "\n",
        "display(HTML('<p>' + (contexto + '<b><i>' + pregunta + '</b></i>').replace('\\n', '<br>') + '</p>'))\n",
        "texts = interact_model(prompt=contexto + pregunta, model_name='345M', temperature=0.001)\n",
        "answer = texts[0][:texts[0].find('Q')]\n",
        "display(HTML('<p>' + answer + '</p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>Tom goes everywhere with Catherine Green, a 54-year-old secretary. He moves around her office at work and goes <br>shopping with her. ”Most people don’t seem to mind Tom,” says Catherine, who thinks he is wonderful. ”He’s my <br>fourth child,” she says. She may think of him and treat him that way as her son. He moves around buying his food, <br>paying his health bills and his taxes, but in fact Tom is a dog. <br><br>Catherine and Tom live in Sweden, a country where everyone is expected to lead an orderly life according to rules <br>laid down by the government, which also provides a high level of care for its people. This level of care <br>costs money. <br><br>People in Sweden pay taxes on everything, so aren’t surprised to find that owning a dog means more <br>taxes. Some people are paying as much as 500 Swedish kronor in taxes a year for the right to keep their dog, which <br>is spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. However, most <br>such treatment is expensive, so owners often decide to offer health and even life for their dog. <br><br>In Sweden dog owners must pay for any damage their dog does. A Swedish Kennel Club official explains what this means: <br>if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay <br>for any damage done to the car, even if your dog has been killed in the accident. <br><br>Q: How old is Catherine? <br>A: 54 <br><b><i>Q: where does she live? <br>A:</b></i></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p> in Sweden. She lives in a small apartment in a small town in the north of Sweden. She is a single mother of two children. She is a member of the Swedish Kennel Club.\n",
              "\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76OckKscm3W5",
        "colab_type": "text"
      },
      "source": [
        "### Resumir un texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPtPeYo9iGWn",
        "colab_type": "code",
        "outputId": "45b7f823-cc2b-41a9-8237-aae8fc974f11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "contexto ='1. Introduction \\n\\\n",
        "\\n\\\n",
        "Machine learning systems now excel (in expectation) at \\n\\\n",
        "tasks they are trained for by using a combination of large \\n\\\n",
        "datasets, high-capacity models, and supervised learning \\n\\\n",
        "(Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei \\n\\\n",
        "et al., 2016). Yet these systems are brittle and sensitive to \\n\\\n",
        "slight changes in the data distribution (Recht et al., 2018) \\n\\\n",
        "and task specification (Kirkpatrick et al., 2017). Current \\n\\\n",
        "systems are better characterized as narrow experts rather than \\n\\\n",
        "competent generalists. We would like to move towards more \\n\\\n",
        "general systems which can perform many tasks – eventually \\n\\\n",
        "without the need to manually create and label a training \\n\\\n",
        "dataset for each one. \\n\\\n",
        "\\n\\\n",
        "The dominant approach to creating ML systems is to collect a \\n\\\n",
        "dataset of training examples demonstrating correct \\n\\\n",
        "behavior for a desired task, train a system to imitate these \\n\\\n",
        "behaviors, and then test its performance on independent \\n\\\n",
        "and identically distributed (IID) held-out examples. This \\n\\\n",
        "has served well to make progress on narrow experts. But \\n\\\n",
        "the often erratic behavior of captioning models (Lake et al., \\n\\\n",
        "2017), reading comprehension systems (Jia & Liang, 2017), \\n\\\n",
        "and image classifiers (Alcorn et al., 2018) on the diversity \\n\\\n",
        "and variety of possible inputs highlights some of the shortcomings \\n\\\n",
        "of this approach. \\n\\\n",
        "\\n\\\n",
        "Our suspicion is that the prevalence of single task training \\n\\\n",
        "on single domain datasets is a major contributor to the lack \\n\\\n",
        "of generalization observed in current systems. Progress \\n\\\n",
        "towards robust systems with current architectures is likely \\n\\\n",
        "to require training and measuring performance on a wide \\n\\\n",
        "range of domains and tasks. Recently, several benchmarks \\n\\\n",
        "have been proposed such as GLUE (Wang et al., 2018) and \\n\\\n",
        "decaNLP (McCann et al., 2018) to begin studying this. \\n\\\n",
        "\\n\\\n",
        "Multitask learning (Caruana, 1997) is a promising framework for \\n\\\n",
        "improving general performance. However, multitask training in NLP \\n\\\n",
        "is still nascent. Recent work reports modest performance \\n\\\n",
        "improvements (Yogatama et al., \\n\\\n",
        "2019) and the two most ambitious efforts to date have \\n\\\n",
        "trained on a total of 10 and 17 (dataset, objective) \\n\\\n",
        "pairs respectively (McCann et al., 2018) (Bowman et al., \\n\\\n",
        "2018). From a meta-learning perspective, each (dataset, \\n\\\n",
        "objective) pair is a single training example sampled \\n\\\n",
        "from the distribution of datasets and objectives. Current \\n\\\n",
        "ML systems need hundreds to thousands of examples to \\n\\\n",
        "induce functions which generalize well. This suggests that \\n\\\n",
        "multitask training many need just as many effective training \\n\\\n",
        "pairs to realize its promise with current approaches. It will \\n\\\n",
        "be very difficult to continue to scale the creation of datasets \\n\\\n",
        "and the design of objectives to the degree that may be required  \\n\\\n",
        "to brute force our way there with current techniques. \\n\\\n",
        "This motivates exploring additional setups for performing \\n\\\n",
        "multitask learning. \\n'\n",
        "\n",
        "pregunta = 'TL;DR:' # Too Long; Didn't Read (¡resúmelo por favor!)\n",
        "\n",
        "display(HTML('<p>' + (contexto + '<b><i>' + pregunta + '</b></i>').replace('\\n', '<br>') + '</p>'))\n",
        "texts = interact_model(prompt=contexto + pregunta, model_name='345M', temperature=0.001)\n",
        "answer = texts[0][:texts[0].find('.')+1]\n",
        "display(HTML('<p>' + answer + '</p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>1. Introduction <br><br>Machine learning systems now excel (in expectation) at <br>tasks they are trained for by using a combination of large <br>datasets, high-capacity models, and supervised learning <br>(Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei <br>et al., 2016). Yet these systems are brittle and sensitive to <br>slight changes in the data distribution (Recht et al., 2018) <br>and task specification (Kirkpatrick et al., 2017). Current <br>systems are better characterized as narrow experts rather than <br>competent generalists. We would like to move towards more <br>general systems which can perform many tasks – eventually <br>without the need to manually create and label a training <br>dataset for each one. <br><br>The dominant approach to creating ML systems is to collect a <br>dataset of training examples demonstrating correct <br>behavior for a desired task, train a system to imitate these <br>behaviors, and then test its performance on independent <br>and identically distributed (IID) held-out examples. This <br>has served well to make progress on narrow experts. But <br>the often erratic behavior of captioning models (Lake et al., <br>2017), reading comprehension systems (Jia & Liang, 2017), <br>and image classifiers (Alcorn et al., 2018) on the diversity <br>and variety of possible inputs highlights some of the shortcomings <br>of this approach. <br><br>Our suspicion is that the prevalence of single task training <br>on single domain datasets is a major contributor to the lack <br>of generalization observed in current systems. Progress <br>towards robust systems with current architectures is likely <br>to require training and measuring performance on a wide <br>range of domains and tasks. Recently, several benchmarks <br>have been proposed such as GLUE (Wang et al., 2018) and <br>decaNLP (McCann et al., 2018) to begin studying this. <br><br>Multitask learning (Caruana, 1997) is a promising framework for <br>improving general performance. However, multitask training in NLP <br>is still nascent. Recent work reports modest performance <br>improvements (Yogatama et al., <br>2019) and the two most ambitious efforts to date have <br>trained on a total of 10 and 17 (dataset, objective) <br>pairs respectively (McCann et al., 2018) (Bowman et al., <br>2018). From a meta-learning perspective, each (dataset, <br>objective) pair is a single training example sampled <br>from the distribution of datasets and objectives. Current <br>ML systems need hundreds to thousands of examples to <br>induce functions which generalize well. This suggests that <br>multitask training many need just as many effective training <br>pairs to realize its promise with current approaches. It will <br>be very difficult to continue to scale the creation of datasets <br>and the design of objectives to the degree that may be required  <br>to brute force our way there with current techniques. <br>This motivates exploring additional setups for performing <br>multitask learning. <br><b><i>TL;DR:</b></i></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p> We propose a novel approach to training ML systems which \n",
              "allows for the creation of large datasets and large \n",
              "objectives.</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI9Ys9-gQKbh",
        "colab_type": "text"
      },
      "source": [
        "### Descargar un texto para fine-tunear el modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGi73ipH0lMn",
        "colab_type": "code",
        "outputId": "ef4e5501-6d78-41c8-b677-07e1178ef2cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "if os.path.exists('./train.txt'):\n",
        "    os.remove('./train.txt')\n",
        "\n",
        "# The Hitchhikers Guide to the Galaxy\n",
        "#get_file(os.getcwd() + '/train.txt', origin='https://docs.google.com/uc?export=download&id=1yQgoz5QmvbxQzN2TZWWjvniPqDhGoC8W')\n",
        "\n",
        "# Lord of the Rings : The Fellowship of the Ring\n",
        "#get_file(os.getcwd() + '/train.txt', origin='https://docs.google.com/uc?export=download&id=1kZOHpaPV2ml8rT9l_bbWPt06H6qz-kcm')\n",
        "\n",
        "# Game of Thrones: A Song of Fire and Ice\n",
        "get_file(os.getcwd() + '/train.txt', origin='https://docs.google.com/uc?export=download&id=1saZmZA07QAqkG-afxlHDPEXNhDNV8Qka')\n",
        "\n",
        "# Cien Años de Soledad (no funciona muy bien al no haber sido entrenado en español el modelo...)\n",
        "#get_file(os.getcwd() + '/train.txt', origin='https://docs.google.com/uc?export=download&id=1XDJmGP9MtOLQSujO5Voicp1wyekHchpm')\n",
        "\n",
        "# Harry Potter and the Goblet of Fire\n",
        "#get_file(os.getcwd() + '/train.txt', origin='https://docs.google.com/uc?export=download&id=10OhbIQHNJrtBiKer8tP_LbxjASqItNzZ')\n",
        "\n",
        "encoding = 'cp1252' if os.name == 'nt' else 'utf-8'\n",
        "raw_text = ''\n",
        "with open('train.txt', 'r', encoding=encoding, errors='backslashreplace') as f:\n",
        "    raw_text += f.read()\n",
        "with open('train.txt', 'w', encoding=encoding, errors='backslashreplace') as f:\n",
        "    f.write(raw_text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://docs.google.com/uc?export=download&id=1saZmZA07QAqkG-afxlHDPEXNhDNV8Qka\n",
            "10338304/Unknown - 2s 0us/step"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joro1L73Qc8i",
        "colab_type": "text"
      },
      "source": [
        "### Crear directorio para guardar los checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyyVsOx0lHhx",
        "colab_type": "code",
        "outputId": "4ebefbfc-d6bb-4da0-b4a6-989a5c7a1925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "try: # estamos en Google Colab?\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    checkpoint_dir = '/content/drive/My Drive/Colab Notebooks/checkpoints'\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAC8hmQbQiqt",
        "colab_type": "text"
      },
      "source": [
        "### Entrenar el modelo\n",
        "\n",
        "https://github.com/nshepperd/gpt-2.git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPFHeGYW6sDk",
        "colab_type": "code",
        "outputId": "2ae3cc83-9dc6-46e1-d5c8-b7b194c29101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "train.CHECKPOINT_DIR = checkpoint_dir\n",
        "tf.reset_default_graph()\n",
        "cwd = os.getcwd()\n",
        "os.chdir(cwd + '/gpt-2') # hack\n",
        "try:\n",
        "    train.main()\n",
        "except:\n",
        "    os.chdir(cwd) # hack\n",
        "    raise\n",
        "os.chdir(cwd) # hack     "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint models/345M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:16<00:00, 16.34s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 2920877 tokens\n",
            "Training...\n",
            "[1 | 15.77] loss=3.52 avg=3.52\n",
            "[2 | 18.73] loss=3.03 avg=3.27\n",
            "[3 | 21.70] loss=3.06 avg=3.20\n",
            "[4 | 24.67] loss=2.15 avg=2.93\n",
            "[5 | 27.64] loss=3.11 avg=2.97\n",
            "[6 | 30.62] loss=2.79 avg=2.94\n",
            "[7 | 33.59] loss=1.60 avg=2.74\n",
            "[8 | 36.55] loss=3.15 avg=2.79\n",
            "[9 | 39.52] loss=2.86 avg=2.80\n",
            "[10 | 42.48] loss=3.17 avg=2.84\n",
            "[11 | 45.44] loss=2.96 avg=2.85\n",
            "[12 | 48.40] loss=2.79 avg=2.85\n",
            "[13 | 51.36] loss=2.84 avg=2.85\n",
            "[14 | 54.32] loss=3.30 avg=2.88\n",
            "[15 | 57.28] loss=2.72 avg=2.87\n",
            "[16 | 60.26] loss=2.79 avg=2.86\n",
            "[17 | 63.23] loss=3.53 avg=2.91\n",
            "[18 | 66.21] loss=3.34 avg=2.93\n",
            "[19 | 69.20] loss=3.03 avg=2.94\n",
            "interrupted\n",
            "Saving /content/checkpoints/run1/model-20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWqHS0LAQ3ET",
        "colab_type": "text"
      },
      "source": [
        "### Especificar los checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkZcMgDsy2V3",
        "colab_type": "code",
        "outputId": "5ddc77a3-d7c4-485d-c576-1fd2b19af8aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# modelo tuneado con Harry Potter and the Goblet of Fire\n",
        "checkpoint = checkpoint_dir + '/run1_Harry_Potter'\n",
        "checkpoint_num = '16000'\n",
        "model_checkpoint_path = 'model_checkpoint_path: \"' + checkpoint + '/model-' + checkpoint_num + '\"'\n",
        "\n",
        "# modelo tuneado con Game of Thrones: A Song of Fire and Ice\n",
        "#checkpoint = checkpoint_dir + '/run1_GOT'\n",
        "#checkpoint_num = '18754'\n",
        "#model_checkpoint_path = 'model_checkpoint_path: \"' + checkpoint + '/model-' + checkpoint_num + '\"'\n",
        "\n",
        "with open('gpt-2/models/345M/checkpoint', \"wt\") as file:\n",
        "    print(model_checkpoint_path)\n",
        "    file.write(model_checkpoint_path)\n",
        "with open('gpt-2/models/345M/counter', \"wt\") as file:\n",
        "    file.write(f'{checkpoint_num}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_checkpoint_path: \"/content/drive/My Drive/Colab Notebooks/checkpoints/run1_Harry_Potter/model-16000\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL0Ve7HdRA1f",
        "colab_type": "text"
      },
      "source": [
        "### Generar muestras con el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1c73df8e-5276-42d6-b311-b84e0f60b7d2",
        "id": "zONNL240PcYv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ejemplo = 'CHAPTER 1 - The Return of Voldermort' #@param {type : 'string'}\n",
        "numero_de_muestras = 3 #@param {type : 'number'}\n",
        "temperature=1 #@param {type : 'number'}\n",
        "#@markdown La temperatura controla el grado de aleatoriedad (0 = determinista)\n",
        "top_k=40 #@param {type : 'integer'}\n",
        "#@markdown Número de candidatos considerados en el beam search (0 = \"greedy\", funciona bien con 40)\n",
        "top_p=0.9 #@param {type : 'number'}\n",
        "#@markdown Controla la diversidad. (0 = valor por defecto, funciona bien con 0.9)\n",
        "texts = interact_model(prompt=ejemplo,\n",
        "                       model_name='345M',\n",
        "                       nsamples=numero_de_muestras,\n",
        "                       temperature=temperature,\n",
        "                       top_k=top_k,\n",
        "                       top_p=top_p)\n",
        "for i, text in enumerate(texts):\n",
        "    display(HTML('<p>' + \"=\" * 40 + \" SAMPLE \" + str(i+1) + \" \" + \"=\" * 40 + '</p>'))\n",
        "    display(HTML('<p>' + ('<b><i>' + ejemplo + '</b></i>' + text).replace('\\n', '<br>') + '</p>'))\n",
        "    display(HTML('<p>' + \"=\" * 80 + '</p>'))\n",
        "    display(HTML('<p><br></p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======================================== SAMPLE 1 ========================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>CHAPTER 1 - The Return of Voldermort</b></i><br><br>The thunderous clunks of Hagrid’s heavy wooden dining-room door as it swung closed were a constant source of amusement to Harry. It was due to this, that he’s been using the excuse of a valuable lesson to get to bed early, and sneakogle away from Ron and Hermione for the last time.<br><br>Alarmed as he was at the idea of having to share a room with Ron and Hermione, Harry still didn’t think they were cruel people; on the contrary, both of them seemed quite capable of putting up with a lotlier than they did. Harry had knocked over his cauldron and spilled a considerable amount of its contents upon the floor, but he had yet to move an inch, while Hermione was being choked, bound, gagged and raped with her hands, repeatedly.<br><br>It was an appalling sight, seeing Hermione gagged, bound and raped with a Goblin Lord, seated behind her. The whole house was watching her, as though she were a demon possessed.<br><br>One by one, the ranks of spectators swelled. Witches and wizards from all over the country were coming to watch the most amazing scene of sorcery and murder Harry had ever witnessed.<br><br>Then a wizard just like Hermione appeared just behind Hermione. She was very short and round-faced, with a large orange nose and almond-shaped brown eyes. She was talking to Ludo Bagman, the Head of the International League of Zemouregal Wizards, in a soothing and even voice.<br><br>‘Ludo, could I have a sec … could I have a sec … could I?’<br><br>She saw, with a pinch of panic, that Bagman was leading a short, stout man into the room.<br><br>‘Ah, it’s Ludo!’ says Mr Bagman, grinning. ‘Ah, I see … aye, I see … a fine pair of chaps … you?’<br><br>‘I am,’ said Ludo, ‘yeh are.’<br><br>‘How long have you been here?’ says Bagman.<br><br>‘One year,’ says Ludo.<br><br>‘Ah, yes, I was just starting – soye hurry up!’<br><br>Bagman hurried along the table, and Harry saw Ludo Bagman putting his initials to the voter’s names.<br><br>‘Very interesting</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>================================================================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><br></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======================================== SAMPLE 2 ========================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>CHAPTER 1 - The Return of Voldermort</b></i><br><br>When Harry awoke next morning, he knew he would not like to admit it; he was very angry with Voldermort for what Harry had said about Wormtail, and rightly so. The episode had shaken Harry so much that he had been unable to go to the library before the end of the term. He had been feeling very blue all through school, and by the time he got back to the castle after lunch, he was already wishing he could just keep going, throw the book under the kettle, and watch The Lord of the Rings …<br><br>But that wasn’t why he was writing. He was writing because he had been so wrong about Wormtail. It was time to walk the walk, and Harry was going to show Voldemort exactly how wrong he was.<br><br><br><br><br><br>— CHAPTER ONE —<br><br><br>The Russian Blood<br><br>Harry had walked around the edges of the dungeon for nearly four hours. He saw no sign of Voldemort’s ghost. Tired and lonely, he lay, apart from Ron and Hermione, listening to the wind outside outside. Hot, dry clothes felt slightly thin and stubbly under his fingers, and the breeze that blew across his face was somehow so gratifying to his ear that he had to suppress a pained sort of grumble.<br><br>He looked up at Harry.<br><br>‘You OK, Harry?’ he said.<br><br>‘Yeah,’ said Harry.<br><br>‘You needed to go to the hospital wing, it’s down the hill.’<br><br>‘Got a fever,’ said Harry.<br><br>‘Got a dragon?’ said Hermione. ‘Well, I’ve got a dragon,’ she added, and she was pointing at the unicorn shank that was stirring in the pan. The shank was moving, a curious sort of humming, as it aged.<br><br>‘I’ve got to go and see Professor Moody,’ Harry said, getting to his feet. ‘He’s been – he’s in a bad way. He kept disappearing … it’s probably back to the Dursleys wanting to get him back.’<br><br>‘Must be nice,’ said Hermione, pulling her scarf back over her head. ‘To have someone who’s so powerful try and take you from him.’<br><br>‘Yeah, it’s</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>================================================================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><br></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======================================== SAMPLE 3 ========================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>CHAPTER 1 - The Return of Voldermort</b></i><br><br><br><br>TWENTY-THREE<br><br>The Department of Mysteries<br><br><br><br>TWENTY-FOUR<br><br>Beyond the Veil<br><br><br><br>TWENTY-FIVE<br><br>The Madness of Mr Crouch<br><br><br><br>TWENTY-SIX<br><br>Something Wicked This Way Comes<br><br><br><br>TWENTY-SEVEN<br><br>The Dream<br><br><br><br>TWENTY-EIGHT<br><br>The Second Task<br><br><br><br>TWENTY-NINE<br><br>Veritaserum<br><br><br><br>TWENTY-TEN<br><br>Veritaserum<br><br><br><br>TWENTY-THIRTEEN<br><br>Veritasections<br><br><br><br>TWENTY-FOURTEEN<br><br>Veritisation<br><br><br><br>TWENTY-FIVETEEN<br><br>Veritisation<br><br><br><br>TWENTY-SIXTEEN<br><br>Veritisation<br><br><br><br>TWENTY-SEVENTEEN<br><br>Veritisation<br><br><br><br>TWENTY-EIGHTTEEN<br><br>Veritisation<br><br><br><br>TWENTY-NINETEEN<br><br>Veritisation<br><br><br><br>TWENTY-TENTEEN<br><br>Veritisation<br><br><br><br>TWENTY-TWELVE<br><br>Veritisation<br><br><br><br>TWENTY-THIRTEEN<br><br>Veritisation<br><br><br><br>TWENTY-FOURTEEN<br><br>Veritisation<br><br><br><br>TWENTY-FIVETEEN<br><br>Veritisation<br><br><br><br>TWENTY-SIXTEEN<br><br>Veritisation<br><br><br><br>TWENTY-SEVENTEEN<br><br>Veritisation<br><br><br><br>TWENTY-EIGHTTEEN<br><br>Veritisation<br><br><br><br>TWENTY-NINE<br><br>Veritisation<br><br><br><br>TWENTY-TEN<br><br>Veritisation<br><br><br><br>TWENTY-TWELVE<br><br>Veritisation<br><br><br><br>TWENTY-THIRTY<br><br>The Third Task<br><br><br><br>TWENTY-FOURTEEN<br><br>Veritisation<br><br><br><br>TWENTY-FIVE<br><br>The Fourth Task<br><br><br><br>TWENTY-SIXTEEN<br><br>Veritisation<br><br><br><br>TWENTY-SEVEN<br><br>The Seventh Task<br><br><br><br>TWENTY-EIGHT<br><br>Veritisation<br><br><br><br>TWENTY-NINE<br><br>Veritisation<br><br><br><br>TWENTY-</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>================================================================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><br></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxtpZl4Ltf01",
        "colab_type": "text"
      },
      "source": [
        "### Traducir una frase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urw1IW82tH95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# puedes encontrar otros texto paralelos aquí\n",
        "# cuidado porque se actualizan periodicamente\n",
        "# https://www.manythings.org/anki/\n",
        "\n",
        "path_to_zip = get_file('spa-eng.zip',\n",
        "                       origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "                       extract=True)\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
        "\n",
        "frases = []\n",
        "with open(path_to_file, 'rt') as file:\n",
        "    for line in file.readlines():\n",
        "        tab = line.find('\\t')\n",
        "        frases += [line[:tab] + ' = ' + line[tab+1:]]\n",
        "with open('train.txt', 'wt') as file:\n",
        "    for _ in np.random.choice(range(len(frases)), len(frases), replace=False):\n",
        "        file.write(frases[_])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uUYLJ3GreJ5-"
      },
      "source": [
        "### Especificar los checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a8428b7e-1b92-4c97-f53a-49fd1979b420",
        "id": "3-bsl8feeJ6H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# modelo pre-entrenado\n",
        "checkpoint_num = ''\n",
        "model_checkpoint_path = 'model_checkpoint_path: \"model.ckpt\"'\n",
        "\n",
        "with open('gpt-2/models/345M/checkpoint', \"wt\") as file:\n",
        "    print(model_checkpoint_path)\n",
        "    file.write(model_checkpoint_path)\n",
        "with open('gpt-2/models/345M/counter', \"wt\") as file:\n",
        "    file.write(f'{checkpoint_num}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_checkpoint_path: \"model.ckpt\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NbgqHJTea8k",
        "colab_type": "text"
      },
      "source": [
        "### Entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0fc7ff96-cb9a-4176-f9c2-c98b247979b8",
        "id": "CvfqdED_AHJ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train.CHECKPOINT_DIR = checkpoint_dir\n",
        "tf.reset_default_graph()\n",
        "cwd = os.getcwd()\n",
        "os.chdir(cwd + '/gpt-2') # hack\n",
        "try:\n",
        "    train.main()\n",
        "except:\n",
        "    os.chdir(cwd) # hack\n",
        "    raise\n",
        "os.chdir(cwd) # hack        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-18000\n",
            "Loading dataset...\n",
            "dataset has 2787399 tokens\n",
            "Training...\n",
            "[18001 | 16.33] loss=1.01 avg=1.01\n",
            "[18002 | 19.37] loss=0.84 avg=0.93\n",
            "[18003 | 22.42] loss=0.88 avg=0.91\n",
            "[18004 | 25.47] loss=1.01 avg=0.93\n",
            "[18005 | 28.53] loss=0.80 avg=0.91\n",
            "[18006 | 31.60] loss=0.93 avg=0.91\n",
            "[18007 | 34.66] loss=0.88 avg=0.91\n",
            "[18008 | 37.72] loss=0.90 avg=0.90\n",
            "[18009 | 40.78] loss=0.98 avg=0.91\n",
            "[18010 | 43.84] loss=0.92 avg=0.91\n",
            "[18011 | 46.91] loss=0.84 avg=0.91\n",
            "[18012 | 49.97] loss=0.88 avg=0.90\n",
            "[18013 | 53.03] loss=0.90 avg=0.90\n",
            "[18014 | 56.09] loss=0.84 avg=0.90\n",
            "[18015 | 59.16] loss=0.99 avg=0.91\n",
            "[18016 | 62.22] loss=0.91 avg=0.91\n",
            "[18017 | 65.28] loss=0.90 avg=0.91\n",
            "[18018 | 68.34] loss=0.88 avg=0.90\n",
            "[18019 | 71.40] loss=0.99 avg=0.91\n",
            "[18020 | 74.47] loss=0.88 avg=0.91\n",
            "[18021 | 77.53] loss=0.84 avg=0.90\n",
            "[18022 | 80.59] loss=0.87 avg=0.90\n",
            "[18023 | 83.66] loss=0.93 avg=0.90\n",
            "[18024 | 86.73] loss=0.90 avg=0.90\n",
            "[18025 | 89.79] loss=0.94 avg=0.90\n",
            "[18026 | 92.86] loss=0.87 avg=0.90\n",
            "[18027 | 95.93] loss=0.99 avg=0.91\n",
            "[18028 | 98.99] loss=0.84 avg=0.90\n",
            "[18029 | 102.06] loss=0.87 avg=0.90\n",
            "[18030 | 105.13] loss=0.86 avg=0.90\n",
            "[18031 | 108.22] loss=0.82 avg=0.90\n",
            "[18032 | 111.29] loss=0.85 avg=0.90\n",
            "[18033 | 114.35] loss=0.88 avg=0.90\n",
            "[18034 | 117.43] loss=0.94 avg=0.90\n",
            "[18035 | 120.49] loss=0.87 avg=0.90\n",
            "[18036 | 123.56] loss=0.79 avg=0.89\n",
            "[18037 | 126.65] loss=0.82 avg=0.89\n",
            "[18038 | 129.71] loss=0.87 avg=0.89\n",
            "[18039 | 132.77] loss=0.91 avg=0.89\n",
            "[18040 | 135.81] loss=0.88 avg=0.89\n",
            "[18041 | 138.88] loss=0.88 avg=0.89\n",
            "[18042 | 141.95] loss=0.90 avg=0.89\n",
            "[18043 | 145.01] loss=0.86 avg=0.89\n",
            "[18044 | 148.08] loss=0.85 avg=0.89\n",
            "[18045 | 151.16] loss=0.97 avg=0.89\n",
            "[18046 | 154.26] loss=0.86 avg=0.89\n",
            "[18047 | 157.35] loss=0.84 avg=0.89\n",
            "[18048 | 160.42] loss=0.85 avg=0.89\n",
            "[18049 | 163.44] loss=0.85 avg=0.89\n",
            "[18050 | 166.49] loss=0.92 avg=0.89\n",
            "[18051 | 169.56] loss=0.95 avg=0.89\n",
            "[18052 | 172.62] loss=0.85 avg=0.89\n",
            "[18053 | 175.68] loss=0.83 avg=0.89\n",
            "[18054 | 178.75] loss=0.80 avg=0.88\n",
            "[18055 | 181.81] loss=0.83 avg=0.88\n",
            "[18056 | 184.87] loss=0.84 avg=0.88\n",
            "[18057 | 187.93] loss=0.84 avg=0.88\n",
            "[18058 | 191.00] loss=0.85 avg=0.88\n",
            "[18059 | 194.06] loss=0.90 avg=0.88\n",
            "[18060 | 197.12] loss=0.85 avg=0.88\n",
            "[18061 | 200.18] loss=0.89 avg=0.88\n",
            "[18062 | 203.24] loss=0.86 avg=0.88\n",
            "[18063 | 206.30] loss=0.93 avg=0.88\n",
            "[18064 | 209.35] loss=0.84 avg=0.88\n",
            "[18065 | 212.42] loss=0.92 avg=0.88\n",
            "[18066 | 215.48] loss=0.93 avg=0.88\n",
            "[18067 | 218.54] loss=0.81 avg=0.88\n",
            "[18068 | 221.60] loss=0.95 avg=0.88\n",
            "[18069 | 224.67] loss=0.76 avg=0.88\n",
            "[18070 | 227.73] loss=0.89 avg=0.88\n",
            "[18071 | 230.81] loss=0.83 avg=0.88\n",
            "[18072 | 233.89] loss=0.92 avg=0.88\n",
            "[18073 | 236.97] loss=0.86 avg=0.88\n",
            "[18074 | 240.06] loss=0.90 avg=0.88\n",
            "[18075 | 243.13] loss=0.80 avg=0.88\n",
            "[18076 | 246.20] loss=0.81 avg=0.88\n",
            "[18077 | 249.26] loss=0.92 avg=0.88\n",
            "[18078 | 252.32] loss=0.85 avg=0.88\n",
            "[18079 | 255.38] loss=0.85 avg=0.88\n",
            "[18080 | 258.45] loss=0.96 avg=0.88\n",
            "[18081 | 261.55] loss=0.77 avg=0.88\n",
            "[18082 | 264.65] loss=0.90 avg=0.88\n",
            "[18083 | 267.72] loss=0.93 avg=0.88\n",
            "[18084 | 270.80] loss=0.91 avg=0.88\n",
            "[18085 | 273.88] loss=0.87 avg=0.88\n",
            "[18086 | 276.95] loss=0.95 avg=0.88\n",
            "[18087 | 280.02] loss=0.83 avg=0.88\n",
            "[18088 | 283.09] loss=0.69 avg=0.87\n",
            "[18089 | 286.14] loss=0.94 avg=0.88\n",
            "[18090 | 289.20] loss=0.90 avg=0.88\n",
            "[18091 | 292.27] loss=0.86 avg=0.88\n",
            "[18092 | 295.35] loss=0.89 avg=0.88\n",
            "[18093 | 298.41] loss=0.93 avg=0.88\n",
            "[18094 | 301.48] loss=0.86 avg=0.88\n",
            "[18095 | 304.55] loss=0.91 avg=0.88\n",
            "[18096 | 307.62] loss=0.93 avg=0.88\n",
            "[18097 | 310.69] loss=0.96 avg=0.88\n",
            "[18098 | 313.74] loss=0.91 avg=0.88\n",
            "[18099 | 316.81] loss=0.89 avg=0.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " you to stop doing things that aren't working. = Te puedo siendo dejar de hacer cosas que no están funcionando.\n",
            "How far did you go? = ¿Hasta dónde has visto?\n",
            "I am thinking about it. = Estoy pensando.\n",
            "He loves his son, not his own. = A él le encantan a su hijo, no a su propio.\n",
            "I am reading a short story. = Estoy leyendo una pequeña cuena.\n",
            "I do not agree. = No estoy de acuerdo.\n",
            "I don't want to hurt you. = No quiero lastimaros.\n",
            "What a waste of money! = ¡Qué derroche de dinero!\n",
            "Tom did not want to talk to Mary. = Tomás no quería hablar con Mary.\n",
            "The boy said he was born on February the fifth, 1900. = El niño dijo que nació el decimose del marido de 1900.\n",
            "I don't get why you don't like Tom anymore. = Ya no entiendo por qué no le gustas a Tom.\n",
            "I'm always late for school. = Siempre llego tarde a la escuela.\n",
            "I have a little money. = Tengo un poco de dinero.\n",
            "Tom was killed by a knife-wielding maniac. = Tom murió por un único nutriculoso.\n",
            "He's been dead for three years. = Lleva muerto tres años.\n",
            "Tom doesn't understand what you want. = Tom no entiende lo que quieres.\n",
            "I'm pretty sure Tom is right now. = Estoy bastante seguro de que Tom va riérdate.\n",
            "We don't want to spend any more time like this. = No queremos pasar más tiempo así.\n",
            "I think Tom still has a lot to learn. = Creo que Tom aún tiene muchas cosas que aprender.\n",
            "He's not a teacher. = Él no es profesor.\n",
            "Tom did the honors. = Tom hizo los honranos.\n",
            "\"Do you know how many people died in the restaurant yesterday?\" \"No, I'm just trying to find the nearest bank.\" = \"¿Sabes cuántas personas murieron en el restaurante ayer?\" \"No, solo trato de encontrar al banco.\"\n",
            "Are you familiar with the law of the jungle? = ¿Conoce usted el ley del juguete?\n",
            "This isn't going to end well. = Esto no va a corporar bien.\n",
            "I have to talk to Tom. = Tengo que hablar con Tom.\n",
            "Tell me it's not true. = Decime que no es verdad.\n",
            "Tom hasn't changed much. = Tom no ha cambiado mucho.\n",
            "The bus has already left. = El autobús ya se fue.\n",
            "He got on the bus at Kenilworth. = Él se subió al bus en Kenilworth.\n",
            "I'm starting to feel much better. = Estoy empezando a sentirme mucho mejor.\n",
            "Is the police here? = ¿Está aquí la policía?\n",
            "He told the children a joke. = Él le entregó un chiste a los niños.\n",
            "The old man told me his secret. = El anciano me contó su secreto.\n",
            "Why do dolphins care about us? = ¿Por qué necesitamos por nosotros del dedo?\n",
            "I'm not a student. = Yo no soy estudiante.\n",
            "Tom had a stroke. = Tom tuvo un golpe.\n",
            "They're mine. = Ellas son mías.\n",
            "Where's Tom talking? = ¿Dónde está hablando Tom?\n",
            "My brother's going to kill me. = Mi hermano va a matarme.\n",
            "They're good kids. = Son buenos niños.\n",
            "I love ice cream. = Me encanta el helado.\n",
            "You know it's me. = Sabes que es yo.\n",
            "They won't give up! = ¡No se va a rendir.\n",
            "She advised him not to go out and drink a little beer. = Ella le aconsejó que no saliera y no beber un poco de comer.\n",
            "I want to buy a bottle of wine. = Quiero comprar una botella de vino.\n",
            "What you wrote doesn't have any grammatical errors. = Lo que usted escribi\n",
            "\n",
            "[18100 | 360.61] loss=0.94 avg=0.88\n",
            "[18101 | 363.67] loss=0.92 avg=0.88\n",
            "[18102 | 366.73] loss=0.90 avg=0.88\n",
            "[18103 | 369.79] loss=0.87 avg=0.88\n",
            "[18104 | 372.85] loss=0.72 avg=0.88\n",
            "[18105 | 375.89] loss=0.98 avg=0.88\n",
            "[18106 | 378.96] loss=0.90 avg=0.88\n",
            "[18107 | 382.01] loss=0.81 avg=0.88\n",
            "[18108 | 385.08] loss=0.81 avg=0.88\n",
            "[18109 | 388.14] loss=0.83 avg=0.88\n",
            "[18110 | 391.21] loss=0.82 avg=0.88\n",
            "[18111 | 394.28] loss=0.88 avg=0.88\n",
            "[18112 | 397.34] loss=0.92 avg=0.88\n",
            "[18113 | 400.39] loss=0.93 avg=0.88\n",
            "[18114 | 403.45] loss=0.81 avg=0.88\n",
            "[18115 | 406.51] loss=0.88 avg=0.88\n",
            "[18116 | 409.58] loss=0.95 avg=0.88\n",
            "[18117 | 412.64] loss=0.84 avg=0.88\n",
            "[18118 | 415.70] loss=0.99 avg=0.88\n",
            "[18119 | 418.76] loss=0.98 avg=0.88\n",
            "[18120 | 421.83] loss=0.84 avg=0.88\n",
            "[18121 | 424.88] loss=0.88 avg=0.88\n",
            "[18122 | 427.95] loss=0.88 avg=0.88\n",
            "[18123 | 431.01] loss=0.85 avg=0.88\n",
            "[18124 | 434.08] loss=0.97 avg=0.88\n",
            "[18125 | 437.16] loss=0.93 avg=0.88\n",
            "[18126 | 440.24] loss=0.81 avg=0.88\n",
            "[18127 | 443.31] loss=0.91 avg=0.88\n",
            "[18128 | 446.40] loss=0.87 avg=0.88\n",
            "[18129 | 449.47] loss=0.91 avg=0.88\n",
            "[18130 | 452.56] loss=0.88 avg=0.88\n",
            "[18131 | 455.65] loss=0.90 avg=0.88\n",
            "[18132 | 458.72] loss=0.88 avg=0.88\n",
            "[18133 | 461.79] loss=0.84 avg=0.88\n",
            "[18134 | 464.87] loss=0.96 avg=0.88\n",
            "[18135 | 467.95] loss=0.91 avg=0.88\n",
            "[18136 | 471.04] loss=0.87 avg=0.88\n",
            "[18137 | 474.12] loss=0.93 avg=0.88\n",
            "[18138 | 477.19] loss=0.93 avg=0.88\n",
            "[18139 | 480.25] loss=0.92 avg=0.88\n",
            "[18140 | 483.32] loss=0.83 avg=0.88\n",
            "[18141 | 486.41] loss=0.95 avg=0.88\n",
            "[18142 | 489.49] loss=0.98 avg=0.89\n",
            "[18143 | 492.58] loss=0.86 avg=0.89\n",
            "[18144 | 495.65] loss=0.87 avg=0.89\n",
            "[18145 | 498.73] loss=0.91 avg=0.89\n",
            "[18146 | 501.81] loss=0.86 avg=0.89\n",
            "[18147 | 504.89] loss=0.86 avg=0.88\n",
            "[18148 | 507.98] loss=0.91 avg=0.89\n",
            "[18149 | 511.05] loss=0.92 avg=0.89\n",
            "[18150 | 514.11] loss=0.90 avg=0.89\n",
            "[18151 | 517.20] loss=0.80 avg=0.88\n",
            "[18152 | 520.28] loss=0.94 avg=0.89\n",
            "[18153 | 523.38] loss=0.85 avg=0.88\n",
            "[18154 | 526.46] loss=0.88 avg=0.88\n",
            "[18155 | 529.55] loss=0.89 avg=0.88\n",
            "[18156 | 532.64] loss=0.87 avg=0.88\n",
            "[18157 | 535.73] loss=0.95 avg=0.89\n",
            "[18158 | 538.80] loss=0.91 avg=0.89\n",
            "[18159 | 541.89] loss=0.78 avg=0.88\n",
            "[18160 | 544.98] loss=0.81 avg=0.88\n",
            "[18161 | 548.06] loss=0.72 avg=0.88\n",
            "[18162 | 551.14] loss=0.95 avg=0.88\n",
            "[18163 | 554.24] loss=0.91 avg=0.88\n",
            "[18164 | 557.33] loss=0.73 avg=0.88\n",
            "[18165 | 560.43] loss=0.90 avg=0.88\n",
            "[18166 | 563.54] loss=0.78 avg=0.88\n",
            "[18167 | 566.62] loss=0.92 avg=0.88\n",
            "[18168 | 569.72] loss=0.71 avg=0.88\n",
            "[18169 | 572.79] loss=0.90 avg=0.88\n",
            "[18170 | 575.89] loss=0.85 avg=0.88\n",
            "[18171 | 578.96] loss=0.82 avg=0.88\n",
            "[18172 | 582.05] loss=0.93 avg=0.88\n",
            "[18173 | 585.13] loss=0.92 avg=0.88\n",
            "[18174 | 588.19] loss=0.82 avg=0.88\n",
            "[18175 | 591.25] loss=0.65 avg=0.88\n",
            "[18176 | 594.35] loss=0.83 avg=0.87\n",
            "[18177 | 597.43] loss=0.90 avg=0.87\n",
            "[18178 | 600.52] loss=0.93 avg=0.88\n",
            "[18179 | 603.61] loss=0.73 avg=0.87\n",
            "[18180 | 606.71] loss=0.86 avg=0.87\n",
            "[18181 | 609.81] loss=0.94 avg=0.87\n",
            "[18182 | 612.90] loss=0.93 avg=0.88\n",
            "[18183 | 615.95] loss=0.80 avg=0.87\n",
            "[18184 | 619.00] loss=0.86 avg=0.87\n",
            "[18185 | 622.08] loss=0.91 avg=0.87\n",
            "[18186 | 625.16] loss=0.87 avg=0.87\n",
            "[18187 | 628.23] loss=0.82 avg=0.87\n",
            "[18188 | 631.33] loss=0.88 avg=0.87\n",
            "[18189 | 634.42] loss=0.99 avg=0.88\n",
            "[18190 | 637.50] loss=0.95 avg=0.88\n",
            "[18191 | 640.60] loss=0.79 avg=0.87\n",
            "[18192 | 643.69] loss=0.88 avg=0.88\n",
            "[18193 | 646.78] loss=0.78 avg=0.87\n",
            "[18194 | 649.88] loss=0.80 avg=0.87\n",
            "[18195 | 652.94] loss=0.89 avg=0.87\n",
            "[18196 | 656.01] loss=0.90 avg=0.87\n",
            "[18197 | 659.08] loss=0.88 avg=0.87\n",
            "[18198 | 662.18] loss=1.00 avg=0.88\n",
            "[18199 | 665.27] loss=0.85 avg=0.87\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " me here. = Ella vuelve tarde.\n",
            "What do you know about my family? = ¿Qué sabes acerca de mi familia?\n",
            "The train is running twenty minutes late. = El tren va corriente veinte minutos.\n",
            "It is cold in this room. = Hace frío en esta habitación.\n",
            "Some days I don't think about him. = Other¹ siempre pienso en él.\n",
            "There is no need for formal education. = Es posible de la educación de estudiar estudiar fuerte.\n",
            "It didn't rain at all. = No nieva en absoluto.\n",
            "Tom went back to Boston. = Tom volvió a Boston.\n",
            "Tom is looking out for himself. = Tom está mirando por sí mismo.\n",
            "Tom thinks he's good at tennis. = Tom cree que se le dan bien el tenis.\n",
            "This is so stupid. = Esto es tan estúpido.\n",
            "Tom didn't want to go out in the rain. = Tom no quería salir en la lluvia.\n",
            "All of a sudden, they began to laugh. = De repente, se pusieron a reírse.\n",
            "I can't stay long. = No puedo quedarme un rato tiempo.\n",
            "Tom has to go to the bookstore today. = Tom tiene que ir al libro hoy en día.\n",
            "The dog was biting the boy. = El perro estaba mordiendo al niño.\n",
            "Who do you plan to spend Christmas with? = ¿Con quién piensas pasar la navidad?\n",
            "I'm going to buy a new saxophone. = Voy a comprarme un saxo nuevo.\n",
            "I will do my homework after I watch television. = Haré mi estudio después de ver la tele.\n",
            "I know that you'll be able to do it. = Sé que usted será lo que podrá hacer.\n",
            "You're not good enough. = No eres lo suficientemente bueno.\n",
            "No one came. = Nadie vino.\n",
            "It is not my place to tell you what you need to do. = No es mi lugar para decirte lo que hay que hacerse.\n",
            "I wish that I had been there. = Ojalá hubiera estado allí.\n",
            "I'll tell you what that means. = Te diré lo que eso significa.\n",
            "I had my brother carry this bag. = Hice que mi hermano llevara esta maleta.\n",
            "What time is the train arriving? = ¿A qué hora llega el trajeta?\n",
            "We need more time. = Necesitamos más tiempo.\n",
            "A large crowd gathered outside the hotel. = Una gran multitud se reunió afuera del hotel.\n",
            "I love sports. = Me encantan los deportes.\n",
            "I've never forgotten you. = Nunca te he olvidado.\n",
            "They didn't want to do it. = No quiso hacerlo.\n",
            "I'm really not in a mood to talk right now. = Ahora mismo no estoy de humor para hablar.\n",
            "I don't want anybody to hear my dirty work. = No quiero que nadie se oye mi trabajo deportivo.\n",
            "I know this feeling. = Lo sé esta sensación.\n",
            "I'd like a hamburger. = Quisiera una hamburguesa.\n",
            "Tom is my only son. = Tom es mi único hijo.\n",
            "The dog wants some water. = El perro quiere algo de agua.\n",
            "I have to clean the bathroom. = Tengo que limpiar el cuarto de la cuartada.\n",
            "She looked as if she had been ill. = Ella parecía como si hubiera estado enferma.\n",
            "The two countries reached an agreement yesterday. = La dos países llegaron un acuerdo ayer.\n",
            "Would you take care of our dog? = ¿Cuidarías a nuestro perro?\n",
            "I don't know if I can do this. = No sé si puedo hacer esto.\n",
            "Tom asked me if I knew where Mary lived. = Tom me preguntó si acaso sabía dónde vivía María.\n",
            "Is that enough for you? = ¿Es eso suficiente para tú?\n",
            "It could happen to you. = Podría ocurriros a vosot\n",
            "\n",
            "[18200 | 705.88] loss=0.86 avg=0.87\n",
            "[18201 | 708.97] loss=0.95 avg=0.88\n",
            "[18202 | 712.06] loss=0.90 avg=0.88\n",
            "[18203 | 715.14] loss=0.97 avg=0.88\n",
            "[18204 | 718.21] loss=0.89 avg=0.88\n",
            "[18205 | 721.28] loss=0.79 avg=0.88\n",
            "[18206 | 724.38] loss=0.92 avg=0.88\n",
            "[18207 | 727.46] loss=0.74 avg=0.87\n",
            "[18208 | 730.55] loss=0.73 avg=0.87\n",
            "[18209 | 733.62] loss=0.91 avg=0.87\n",
            "[18210 | 736.69] loss=0.91 avg=0.87\n",
            "[18211 | 739.78] loss=0.90 avg=0.87\n",
            "[18212 | 742.87] loss=0.86 avg=0.87\n",
            "[18213 | 745.95] loss=0.87 avg=0.87\n",
            "[18214 | 749.03] loss=0.95 avg=0.88\n",
            "[18215 | 752.12] loss=0.88 avg=0.88\n",
            "[18216 | 755.21] loss=0.86 avg=0.87\n",
            "[18217 | 758.30] loss=0.88 avg=0.88\n",
            "[18218 | 761.38] loss=0.90 avg=0.88\n",
            "[18219 | 764.46] loss=0.86 avg=0.88\n",
            "[18220 | 767.54] loss=0.87 avg=0.88\n",
            "[18221 | 770.62] loss=0.84 avg=0.87\n",
            "[18222 | 773.71] loss=0.85 avg=0.87\n",
            "[18223 | 776.80] loss=0.87 avg=0.87\n",
            "[18224 | 779.88] loss=0.70 avg=0.87\n",
            "[18225 | 782.97] loss=0.99 avg=0.87\n",
            "[18226 | 786.04] loss=0.94 avg=0.87\n",
            "[18227 | 789.12] loss=0.84 avg=0.87\n",
            "[18228 | 792.21] loss=0.91 avg=0.87\n",
            "[18229 | 795.30] loss=0.92 avg=0.88\n",
            "[18230 | 798.39] loss=0.81 avg=0.87\n",
            "[18231 | 801.48] loss=0.89 avg=0.87\n",
            "[18232 | 804.57] loss=0.91 avg=0.88\n",
            "[18233 | 807.66] loss=0.92 avg=0.88\n",
            "[18234 | 810.74] loss=0.95 avg=0.88\n",
            "[18235 | 813.80] loss=0.95 avg=0.88\n",
            "[18236 | 816.86] loss=0.98 avg=0.88\n",
            "[18237 | 819.95] loss=0.93 avg=0.88\n",
            "[18238 | 823.02] loss=0.84 avg=0.88\n",
            "[18239 | 826.10] loss=0.93 avg=0.88\n",
            "[18240 | 829.20] loss=0.81 avg=0.88\n",
            "[18241 | 832.28] loss=0.90 avg=0.88\n",
            "[18242 | 835.37] loss=0.78 avg=0.88\n",
            "[18243 | 838.46] loss=0.91 avg=0.88\n",
            "[18244 | 841.55] loss=0.88 avg=0.88\n",
            "[18245 | 844.63] loss=0.89 avg=0.88\n",
            "[18246 | 847.71] loss=0.94 avg=0.88\n",
            "[18247 | 850.79] loss=0.90 avg=0.88\n",
            "[18248 | 853.88] loss=0.82 avg=0.88\n",
            "[18249 | 856.95] loss=0.89 avg=0.88\n",
            "[18250 | 860.03] loss=0.87 avg=0.88\n",
            "[18251 | 863.11] loss=0.91 avg=0.88\n",
            "[18252 | 866.18] loss=0.82 avg=0.88\n",
            "[18253 | 869.26] loss=0.86 avg=0.88\n",
            "[18254 | 872.35] loss=0.91 avg=0.88\n",
            "[18255 | 875.43] loss=0.85 avg=0.88\n",
            "[18256 | 878.51] loss=0.88 avg=0.88\n",
            "[18257 | 881.60] loss=0.87 avg=0.88\n",
            "[18258 | 884.68] loss=0.88 avg=0.88\n",
            "[18259 | 887.76] loss=0.91 avg=0.88\n",
            "[18260 | 890.83] loss=0.94 avg=0.88\n",
            "[18261 | 893.91] loss=0.87 avg=0.88\n",
            "[18262 | 896.99] loss=0.84 avg=0.88\n",
            "[18263 | 900.08] loss=0.92 avg=0.88\n",
            "[18264 | 903.16] loss=0.88 avg=0.88\n",
            "[18265 | 906.24] loss=0.87 avg=0.88\n",
            "[18266 | 909.33] loss=0.89 avg=0.88\n",
            "[18267 | 912.42] loss=0.96 avg=0.88\n",
            "[18268 | 915.51] loss=0.84 avg=0.88\n",
            "[18269 | 918.60] loss=0.97 avg=0.88\n",
            "[18270 | 921.68] loss=0.87 avg=0.88\n",
            "[18271 | 924.78] loss=0.86 avg=0.88\n",
            "[18272 | 927.87] loss=0.99 avg=0.88\n",
            "[18273 | 930.95] loss=0.87 avg=0.88\n",
            "[18274 | 934.04] loss=0.86 avg=0.88\n",
            "[18275 | 937.13] loss=0.90 avg=0.88\n",
            "[18276 | 940.22] loss=0.82 avg=0.88\n",
            "[18277 | 943.32] loss=0.91 avg=0.88\n",
            "[18278 | 946.41] loss=0.88 avg=0.88\n",
            "[18279 | 949.49] loss=0.86 avg=0.88\n",
            "[18280 | 952.58] loss=1.02 avg=0.88\n",
            "[18281 | 955.66] loss=0.86 avg=0.88\n",
            "[18282 | 958.76] loss=0.90 avg=0.88\n",
            "[18283 | 961.85] loss=0.82 avg=0.88\n",
            "[18284 | 964.94] loss=0.80 avg=0.88\n",
            "[18285 | 968.02] loss=0.91 avg=0.88\n",
            "[18286 | 971.12] loss=0.82 avg=0.88\n",
            "[18287 | 974.20] loss=0.89 avg=0.88\n",
            "[18288 | 977.28] loss=0.88 avg=0.88\n",
            "[18289 | 980.37] loss=0.94 avg=0.88\n",
            "[18290 | 983.46] loss=0.90 avg=0.88\n",
            "[18291 | 986.55] loss=0.77 avg=0.88\n",
            "[18292 | 989.64] loss=0.92 avg=0.88\n",
            "[18293 | 992.73] loss=0.93 avg=0.88\n",
            "[18294 | 995.81] loss=0.88 avg=0.88\n",
            "[18295 | 998.90] loss=0.90 avg=0.88\n",
            "[18296 | 1001.98] loss=0.97 avg=0.88\n",
            "[18297 | 1005.06] loss=0.92 avg=0.88\n",
            "[18298 | 1008.15] loss=0.83 avg=0.88\n",
            "[18299 | 1011.24] loss=1.05 avg=0.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "íblico todos al día.\n",
            "We must not leave the port until tomorrow. = No debemos salir de la porte hasta mañana.\n",
            "He's always complaining about his health. = Él siempre está quejando de su salud.\n",
            "I didn't understand anything Tom said. = No entendí nada de lo que dijo Tom.\n",
            "The teacher warned us at the beginning that there wouldn't be much study. = El profesor nos advirtió a principio que no saliera mucho estudiar.\n",
            "You're as human as any other human being. = Eres tan humano como cualquiera humano.\n",
            "Mary gave her doll a pink dress. = Mary le dio a su vieja ala un traje rosa.\n",
            "She was wearing a black hat. = Ella llevaba puesto un sombrero negro.\n",
            "My aunt has died. = Mi tía ha muerto.\n",
            "Tom isn't sure where to go. = Tom no está seguro de adónde ir.\n",
            "Tom was wearing a white jacket. = Tom llevaba puesto una campera blanca.\n",
            "Why didn't somebody tell me? = ¿Por qué no me dijo nadie?\n",
            "Tom is on his own. = Tom está sola.\n",
            "Tom was taken hostage. = Tom fue tomado entradnada.\n",
            "That isn't what I bought. = Eso no es lo que compré.\n",
            "Tom got lost. = Tom se perdió.\n",
            "Is it warm out? = ¿Está caliente hacia sacar?\n",
            "I was told that I could learn to play the trombone. = Se dijeron que yo podía aprender al tocionero.\n",
            "Tom was my first boyfriend. = Tom fue mi primer novio.\n",
            "He didn't deny that he had stolen the money. = Él no negó que él había robado el dinero.\n",
            "Could you come back later? = ¿Podrías volver más tarde?\n",
            "Tom wants to see you, Mary. = Tom quiere verte, Mary.\n",
            "This box has a very tight fit. = Esta caja tiene un cajón sullido con gusto.\n",
            "I will keep my word. = Mantendré mi palabra.\n",
            "The car has been standing still since the taxi stopped. = El auto está quedando quedado quedado allá.\n",
            "He was the last one to leave. = Él fue el último en irse.\n",
            "Tom isn't sure how long he'll be staying in Boston. = Tom no está seguro de cuánto te permaneá en Boston.\n",
            "I don't have any choice. = No tengo elección.\n",
            "Tom didn't really want to go. = Tom en realidad no quería ir.\n",
            "There is no point in doing that. = No tiene caso en hacerlo.\n",
            "Do you have any idea who wrote this book? = ¿Tiene idea de quién escribió ese libro?\n",
            "It's so hot in this room! = ¡Hala calor en esta habitación!\n",
            "That was a good question. = Eso fue un buen pregunta.\n",
            "The water came up to my knees. = El agua me llegaba hasta las rodillas.\n",
            "I have no regrets. = No me arrepiento nada.\n",
            "They want me. = Me quieren a mí.\n",
            "They are always talking about Tom. = Siempre están hablando acerca de Tom.\n",
            "Tom knows when he's being watched. = Tom sabe cuando está siendo vigilado.\n",
            "Tom is the person who caught Mary. = Tom es la persona que atrapó a Mary.\n",
            "Tom is an alcoholic. = Tom es un alcohólico.\n",
            "Would you put the dishes in the sink? = ¿Pones los platos en el fregadero?\n",
            "We were on the train for three hours. = Hicimos tres horas en el tren.\n",
            "Tom was my friend when I was younger. = Tom era mi amigo cuando yo era más joven.\n",
            "Tom has lost sight of his kids. = Tom perdió de vista a sus niños.\n",
            "My brother is an only child. = Mi hermano es hijo único.\n",
            "I love my girlfriend. = Amo a mi novia.\n",
            "That was the only thing we had to do. = Era lo único que teníamos\n",
            "\n",
            "[18300 | 1051.79] loss=0.99 avg=0.88\n",
            "[18301 | 1054.87] loss=0.87 avg=0.88\n",
            "[18302 | 1057.95] loss=0.89 avg=0.88\n",
            "[18303 | 1061.03] loss=0.89 avg=0.88\n",
            "[18304 | 1064.11] loss=0.83 avg=0.88\n",
            "[18305 | 1067.21] loss=0.90 avg=0.88\n",
            "[18306 | 1070.28] loss=0.72 avg=0.88\n",
            "[18307 | 1073.36] loss=0.67 avg=0.88\n",
            "[18308 | 1076.44] loss=0.90 avg=0.88\n",
            "[18309 | 1079.52] loss=0.91 avg=0.88\n",
            "[18310 | 1082.58] loss=0.96 avg=0.88\n",
            "[18311 | 1085.67] loss=0.92 avg=0.88\n",
            "[18312 | 1088.74] loss=0.89 avg=0.88\n",
            "[18313 | 1091.83] loss=0.91 avg=0.88\n",
            "[18314 | 1094.90] loss=0.89 avg=0.88\n",
            "[18315 | 1097.97] loss=0.88 avg=0.88\n",
            "[18316 | 1101.04] loss=0.82 avg=0.88\n",
            "[18317 | 1104.11] loss=0.85 avg=0.88\n",
            "[18318 | 1107.19] loss=0.93 avg=0.88\n",
            "[18319 | 1110.28] loss=0.99 avg=0.88\n",
            "[18320 | 1113.36] loss=0.93 avg=0.88\n",
            "[18321 | 1116.45] loss=0.99 avg=0.88\n",
            "[18322 | 1119.55] loss=0.89 avg=0.88\n",
            "[18323 | 1122.62] loss=0.91 avg=0.89\n",
            "[18324 | 1125.69] loss=0.86 avg=0.89\n",
            "[18325 | 1128.78] loss=0.89 avg=0.89\n",
            "[18326 | 1131.86] loss=0.88 avg=0.89\n",
            "[18327 | 1134.95] loss=0.92 avg=0.89\n",
            "[18328 | 1138.03] loss=0.77 avg=0.88\n",
            "[18329 | 1141.11] loss=0.93 avg=0.88\n",
            "[18330 | 1144.18] loss=0.93 avg=0.89\n",
            "[18331 | 1147.26] loss=0.83 avg=0.88\n",
            "[18332 | 1150.34] loss=0.94 avg=0.89\n",
            "[18333 | 1153.41] loss=0.86 avg=0.88\n",
            "[18334 | 1156.49] loss=0.84 avg=0.88\n",
            "[18335 | 1159.58] loss=0.84 avg=0.88\n",
            "[18336 | 1162.66] loss=0.85 avg=0.88\n",
            "[18337 | 1165.75] loss=0.84 avg=0.88\n",
            "[18338 | 1168.85] loss=0.96 avg=0.88\n",
            "[18339 | 1171.94] loss=0.90 avg=0.88\n",
            "[18340 | 1175.03] loss=0.88 avg=0.88\n",
            "[18341 | 1178.11] loss=1.02 avg=0.89\n",
            "[18342 | 1181.19] loss=0.69 avg=0.88\n",
            "[18343 | 1184.28] loss=0.90 avg=0.88\n",
            "[18344 | 1187.37] loss=0.87 avg=0.88\n",
            "[18345 | 1190.44] loss=0.84 avg=0.88\n",
            "[18346 | 1193.52] loss=0.86 avg=0.88\n",
            "[18347 | 1196.62] loss=0.79 avg=0.88\n",
            "[18348 | 1199.69] loss=0.90 avg=0.88\n",
            "[18349 | 1202.77] loss=0.76 avg=0.88\n",
            "[18350 | 1205.86] loss=0.91 avg=0.88\n",
            "[18351 | 1208.94] loss=0.75 avg=0.88\n",
            "[18352 | 1212.00] loss=0.94 avg=0.88\n",
            "[18353 | 1215.05] loss=0.79 avg=0.88\n",
            "[18354 | 1218.08] loss=0.88 avg=0.88\n",
            "[18355 | 1221.17] loss=0.85 avg=0.88\n",
            "[18356 | 1224.25] loss=0.82 avg=0.88\n",
            "[18357 | 1227.34] loss=0.78 avg=0.88\n",
            "[18358 | 1230.43] loss=0.91 avg=0.88\n",
            "[18359 | 1233.51] loss=0.85 avg=0.88\n",
            "[18360 | 1236.59] loss=0.96 avg=0.88\n",
            "[18361 | 1239.69] loss=0.84 avg=0.88\n",
            "[18362 | 1242.76] loss=0.92 avg=0.88\n",
            "[18363 | 1245.84] loss=0.84 avg=0.88\n",
            "[18364 | 1248.92] loss=0.90 avg=0.88\n",
            "[18365 | 1252.00] loss=0.86 avg=0.88\n",
            "[18366 | 1255.10] loss=0.79 avg=0.88\n",
            "[18367 | 1258.19] loss=0.86 avg=0.88\n",
            "[18368 | 1261.28] loss=0.85 avg=0.88\n",
            "[18369 | 1264.36] loss=0.92 avg=0.88\n",
            "[18370 | 1267.45] loss=0.98 avg=0.88\n",
            "[18371 | 1270.53] loss=0.85 avg=0.88\n",
            "[18372 | 1273.62] loss=0.85 avg=0.88\n",
            "[18373 | 1276.71] loss=0.75 avg=0.88\n",
            "[18374 | 1279.79] loss=0.91 avg=0.88\n",
            "[18375 | 1282.87] loss=0.82 avg=0.88\n",
            "[18376 | 1285.96] loss=0.98 avg=0.88\n",
            "[18377 | 1289.06] loss=0.86 avg=0.88\n",
            "[18378 | 1292.15] loss=0.82 avg=0.88\n",
            "[18379 | 1295.24] loss=0.89 avg=0.88\n",
            "[18380 | 1298.33] loss=0.93 avg=0.88\n",
            "[18381 | 1301.41] loss=0.91 avg=0.88\n",
            "[18382 | 1304.50] loss=0.96 avg=0.88\n",
            "[18383 | 1307.60] loss=0.92 avg=0.88\n",
            "[18384 | 1310.70] loss=0.85 avg=0.88\n",
            "[18385 | 1313.78] loss=0.86 avg=0.88\n",
            "[18386 | 1316.87] loss=0.94 avg=0.88\n",
            "[18387 | 1319.95] loss=0.85 avg=0.88\n",
            "[18388 | 1323.04] loss=0.88 avg=0.88\n",
            "[18389 | 1326.13] loss=0.92 avg=0.88\n",
            "[18390 | 1329.23] loss=0.86 avg=0.88\n",
            "[18391 | 1332.31] loss=0.95 avg=0.88\n",
            "[18392 | 1335.39] loss=0.89 avg=0.88\n",
            "[18393 | 1338.47] loss=0.97 avg=0.88\n",
            "[18394 | 1341.55] loss=0.81 avg=0.88\n",
            "[18395 | 1344.64] loss=0.87 avg=0.88\n",
            "[18396 | 1347.73] loss=0.84 avg=0.88\n",
            "[18397 | 1350.81] loss=0.96 avg=0.88\n",
            "[18398 | 1353.89] loss=0.80 avg=0.88\n",
            "[18399 | 1356.97] loss=0.96 avg=0.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". = No aceptará.\n",
            "He is a good liar. = Él es un buen mentiroso.\n",
            "I've been to England several times. = He estado en Inglaterra varias veces.\n",
            "I will be glad when she gives birth to a daughter. = A daré encantarme cuando daña a una niña.\n",
            "My mother has been sick for a week. = Mi mamá está enferma durante una semana.\n",
            "Tom asked Mary if she was all right. = Tom le preguntó a Mary si estaba bien.\n",
            "Would you like to dance? = ¿Te gustaría bailar?\n",
            "She looks very pretty today. = Ella se ve muy guapa hoy.\n",
            "That book seems useful. = Ese libro parece siendo útil.\n",
            "Do not open the windows. = No abras la ventana.\n",
            "Why don't you tell me what happened to Tom? = ¿Por qué no me contás qué le sucedió a Tom?\n",
            "What's the best way to keep sharks at bay? = ¿Cuál es la mejor forma de evitar que a las sharkas se engañen?\n",
            "Mary asked me if I really needed an abortion. = Mary me preguntó si solo necesitaba una abortion.\n",
            "I can't tell you what Tom said. = No puedo contarte lo que dijo Tomás.\n",
            "Somebody must've left the door open. = Debe de haber dejado la puerta abierta.\n",
            "It is very difficult. = Es muy difícil.\n",
            "I don't want to give up. = No quiero rendirme.\n",
            "He loves to read. = A él le encanta leer.\n",
            "Did you find out something? = ¿Has descubierto algo?\n",
            "I will stay here until it stops raining. = Me quedaré aquí hasta que parara.\n",
            "Do you want to eat dinner? = ¿Querés cenar?\n",
            "Tom has three times as much money as Mary does. = Tom tienes tres veces más dinero que Mary.\n",
            "Tom is a man of few words. = Tom es un hombre de pocos palabras.\n",
            "Is his story true? = ¿Es ciertamente ciertamente ciertamente?\n",
            "It's not a good place to start a relationship. = No es un buen lugar para empezar una reláfera.\n",
            "He is very patient. = Él es muy paciente.\n",
            "Do you want to talk after class? = ¿Quieres hablar después de la clase?\n",
            "I couldn't have written this story. = No podría haberlo escrito este escrito.\n",
            "I had an engagement, so I had to take the day off. = Tenía un compromiso, así que tuviera tomar el día libre.\n",
            "It has never happened to me before. = Nunca me ha pasado eso antes.\n",
            "Tom didn't go there today. = Tom no fue allá hoy.\n",
            "I can see a castle from my bedroom window. = Puedo ver casa desde la ventana de mi habitación.\n",
            "What happened at the party last night? = ¿Qué pasó anoche en la fiesta?\n",
            "His behavior made my father very angry. = Su conducta lo hace muy enégal de mi papá.\n",
            "I'm so busy I can't help you with anything. = Estoy tan ocupado que no puedo ayudarte con nada.\n",
            "Let's go into the jungle together. = Entramos juntos a la jungla.\n",
            "This is a very special day. = Este es un día muy especial.\n",
            "She took good care of him after his death. = Ella le ocupó bien de él después de su muerte.\n",
            "Do you speak Filipino? = ¿Habláis filipino?\n",
            "It wasn't necessary to take such a risk. = No era necesario que tomar riesgos así.\n",
            "He knows who I am and what I'm trying to do. = Él sabe quién soy y lo que intento hacer.\n",
            "Tom didn't say how many times he had been there. = Tom no dijo cuántas veces había estado allí.\n",
            "What did you learn? = ¿Qué aprendiste?\n",
            "The situation is really scary. = La\n",
            "\n",
            "[18400 | 1397.51] loss=0.93 avg=0.88\n",
            "[18401 | 1400.59] loss=0.84 avg=0.88\n",
            "[18402 | 1403.67] loss=1.00 avg=0.88\n",
            "[18403 | 1406.75] loss=0.93 avg=0.88\n",
            "[18404 | 1409.84] loss=0.90 avg=0.88\n",
            "[18405 | 1412.92] loss=0.93 avg=0.88\n",
            "[18406 | 1415.99] loss=0.79 avg=0.88\n",
            "[18407 | 1419.07] loss=0.89 avg=0.88\n",
            "[18408 | 1422.16] loss=0.82 avg=0.88\n",
            "[18409 | 1425.24] loss=0.83 avg=0.88\n",
            "[18410 | 1428.33] loss=0.91 avg=0.88\n",
            "[18411 | 1431.41] loss=0.85 avg=0.88\n",
            "[18412 | 1434.49] loss=0.82 avg=0.88\n",
            "[18413 | 1437.57] loss=0.85 avg=0.88\n",
            "[18414 | 1440.66] loss=0.96 avg=0.88\n",
            "[18415 | 1443.74] loss=0.86 avg=0.88\n",
            "[18416 | 1446.83] loss=0.82 avg=0.88\n",
            "[18417 | 1449.92] loss=0.94 avg=0.88\n",
            "[18418 | 1453.01] loss=0.91 avg=0.88\n",
            "[18419 | 1456.09] loss=0.90 avg=0.88\n",
            "[18420 | 1459.17] loss=0.87 avg=0.88\n",
            "[18421 | 1462.26] loss=0.92 avg=0.88\n",
            "[18422 | 1465.35] loss=0.91 avg=0.88\n",
            "[18423 | 1468.44] loss=0.82 avg=0.88\n",
            "[18424 | 1471.53] loss=0.90 avg=0.88\n",
            "[18425 | 1474.61] loss=0.83 avg=0.88\n",
            "[18426 | 1477.70] loss=0.86 avg=0.88\n",
            "[18427 | 1480.78] loss=0.81 avg=0.88\n",
            "[18428 | 1483.88] loss=0.90 avg=0.88\n",
            "[18429 | 1486.97] loss=0.77 avg=0.88\n",
            "[18430 | 1490.05] loss=0.85 avg=0.88\n",
            "[18431 | 1493.13] loss=0.69 avg=0.88\n",
            "[18432 | 1496.21] loss=0.84 avg=0.88\n",
            "[18433 | 1499.30] loss=0.89 avg=0.88\n",
            "[18434 | 1502.40] loss=0.87 avg=0.88\n",
            "[18435 | 1505.49] loss=0.91 avg=0.88\n",
            "[18436 | 1508.57] loss=0.76 avg=0.88\n",
            "[18437 | 1511.65] loss=0.94 avg=0.88\n",
            "[18438 | 1514.74] loss=0.73 avg=0.87\n",
            "[18439 | 1517.82] loss=0.94 avg=0.88\n",
            "[18440 | 1520.91] loss=0.74 avg=0.87\n",
            "[18441 | 1524.01] loss=0.87 avg=0.87\n",
            "[18442 | 1527.09] loss=0.88 avg=0.87\n",
            "[18443 | 1530.16] loss=0.90 avg=0.87\n",
            "[18444 | 1533.25] loss=0.88 avg=0.87\n",
            "[18445 | 1536.34] loss=0.96 avg=0.87\n",
            "[18446 | 1539.43] loss=0.85 avg=0.87\n",
            "[18447 | 1542.52] loss=0.80 avg=0.87\n",
            "[18448 | 1545.60] loss=0.84 avg=0.87\n",
            "[18449 | 1548.69] loss=0.92 avg=0.87\n",
            "[18450 | 1551.78] loss=0.89 avg=0.87\n",
            "[18451 | 1554.87] loss=0.97 avg=0.88\n",
            "[18452 | 1557.95] loss=0.79 avg=0.87\n",
            "[18453 | 1561.02] loss=0.89 avg=0.87\n",
            "[18454 | 1564.09] loss=0.82 avg=0.87\n",
            "[18455 | 1567.18] loss=0.92 avg=0.87\n",
            "[18456 | 1570.26] loss=0.84 avg=0.87\n",
            "[18457 | 1573.35] loss=0.92 avg=0.87\n",
            "[18458 | 1576.42] loss=0.88 avg=0.87\n",
            "[18459 | 1579.50] loss=0.92 avg=0.88\n",
            "[18460 | 1582.60] loss=0.92 avg=0.88\n",
            "[18461 | 1585.67] loss=0.92 avg=0.88\n",
            "[18462 | 1588.76] loss=0.82 avg=0.88\n",
            "[18463 | 1591.84] loss=0.63 avg=0.87\n",
            "[18464 | 1594.93] loss=0.93 avg=0.87\n",
            "[18465 | 1598.02] loss=0.98 avg=0.87\n",
            "[18466 | 1601.11] loss=0.81 avg=0.87\n",
            "[18467 | 1604.19] loss=0.87 avg=0.87\n",
            "[18468 | 1607.28] loss=0.91 avg=0.87\n",
            "[18469 | 1610.36] loss=1.00 avg=0.88\n",
            "[18470 | 1613.45] loss=0.93 avg=0.88\n",
            "[18471 | 1616.54] loss=0.82 avg=0.88\n",
            "[18472 | 1619.64] loss=0.89 avg=0.88\n",
            "[18473 | 1622.72] loss=0.85 avg=0.88\n",
            "[18474 | 1625.81] loss=0.94 avg=0.88\n",
            "[18475 | 1628.90] loss=0.81 avg=0.88\n",
            "[18476 | 1631.98] loss=0.86 avg=0.88\n",
            "[18477 | 1635.07] loss=0.91 avg=0.88\n",
            "[18478 | 1638.15] loss=0.80 avg=0.87\n",
            "[18479 | 1641.23] loss=0.96 avg=0.88\n",
            "[18480 | 1644.31] loss=0.88 avg=0.88\n",
            "[18481 | 1647.40] loss=0.88 avg=0.88\n",
            "[18482 | 1650.48] loss=0.86 avg=0.88\n",
            "[18483 | 1653.56] loss=0.94 avg=0.88\n",
            "[18484 | 1656.65] loss=0.95 avg=0.88\n",
            "[18485 | 1659.74] loss=0.88 avg=0.88\n",
            "[18486 | 1662.84] loss=0.84 avg=0.88\n",
            "[18487 | 1665.93] loss=0.92 avg=0.88\n",
            "[18488 | 1669.02] loss=0.85 avg=0.88\n",
            "[18489 | 1672.09] loss=0.88 avg=0.88\n",
            "[18490 | 1675.17] loss=0.91 avg=0.88\n",
            "[18491 | 1678.25] loss=0.94 avg=0.88\n",
            "[18492 | 1681.34] loss=0.84 avg=0.88\n",
            "[18493 | 1684.43] loss=0.90 avg=0.88\n",
            "[18494 | 1687.52] loss=0.75 avg=0.88\n",
            "[18495 | 1690.60] loss=0.93 avg=0.88\n",
            "[18496 | 1693.68] loss=0.92 avg=0.88\n",
            "[18497 | 1696.77] loss=0.87 avg=0.88\n",
            "[18498 | 1699.86] loss=0.84 avg=0.88\n",
            "[18499 | 1702.94] loss=0.93 avg=0.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " el uno.\n",
            "I've never liked him. = Nunca me ha gustado.\n",
            "Where's Tom? = ¿Dónde está Tom?\n",
            "A person like Tom, who gets up late every day, is odd. = Una persona como Tom, que se levanta todos los días se levantado, es raro.\n",
            "He's a man you can rely on. = Él es un hombre en el que puedes confiar.\n",
            "Tom isn't a scientist, but a painter. = Tom no es científico, sino pintaduro.\n",
            "I'm only trying to protect you. = Solo estoy tratando de protegerte.\n",
            "He went to see her four times a week. = Fue a ver caminar a vuestras veces a la semana.\n",
            "Do you think I should go alone? = ¿Tú crees que debería ir solo?\n",
            "I'll call you when I'm free. = Os llamo cuando tengo libre.\n",
            "I know that you are in love with me. = Sé que tú está enamorado de mí.\n",
            "I wish I had understood it then. = Lo deseo tener explicado entonces.\n",
            "The story was a best seller. = La historia era un best seller.\n",
            "It's not good to be a woman. = No es bueno ser una mujer.\n",
            "Tom is a man we can rely on. = Tom es un hombre en el que podemos confiar.\n",
            "Tom was lying on the beach, enjoying the fresh air. = Tom estaba tumbado encima de la playa, disfrutando el aire fresco.\n",
            "I'm an optimist. = Soy optimista.\n",
            "She had a very difficult examination. = Ella tuvo un examen muy difícil.\n",
            "There's no sense in going out today. = Hace falta si salimos hoy.\n",
            "Tom needs us. = Tom nos necesita.\n",
            "Are you coming in or not? = ¿Insensate y no pasa?\n",
            "Tom is waiting. = Tom está esperando.\n",
            "I didn't know that happened here. = No sabía que pasó aquí.\n",
            "Tom was in the middle of a long discussion with Mary. = Tom estaba en medio de unatuelas con Mary.\n",
            "This is so exciting. = Esto es tan emocionante.\n",
            "It looks like you're having a good time. = Parece que estás pasando bien.\n",
            "I don't know why you don't want to talk to me. = No sé por qué no quieres hablar conmigo.\n",
            "Who's that woman standing over there? = ¿Quién es esa mujer parada por allá?\n",
            "Please don't touch me with your foot. = No me toquen usted, por favor.\n",
            "I need to be here. = Necesito estar aquí.\n",
            "I like reading. = Me gusta leer.\n",
            "We'll stay here for a while. = Nos vamos a permanecer un rato tiempo.\n",
            "Tom asked the waitress for a menu. = Tom le pidió a la pequeña con un menú.\n",
            "I was very tired today. = Hoy estaba muy cansada.\n",
            "Can we get started before midnight? = ¿Podemos comenzar a principios antes de la medianoche?\n",
            "I don't know for certain yet. = Aún no sé saber con seguridad.\n",
            "I want to learn to speak Japanese, too. = Yo también quiero aprender a hablar japonés.\n",
            "The police are not going to solve this mystery. = La policía no va a resolver esta misteriosa.\n",
            "When will they arrive? = ¿Cuándo llegarán?\n",
            "She was too afraid to enter his room. = Ella tenía demasiado miedo para entrar a su habitación.\n",
            "Don't call me up after ten o'clock. = No me llame después de las diez.\n",
            "She said that she loved him. = Dijo que lo amaba.\n",
            "Tom ate the whole pizza. = Tom se comió todo el pizza.\n",
            "You can't be too careful when driving. = No puedes ser muy cuidadoso cuando conduces.\n",
            "No one laughed at Tom. = Nadie se reía de Tom.\n",
            "He asked a question that I had not heard him answer. = Le hizo una pre\n",
            "\n",
            "[18500 | 1744.42] loss=1.05 avg=0.88\n",
            "[18501 | 1747.50] loss=0.93 avg=0.88\n",
            "[18502 | 1750.58] loss=0.85 avg=0.88\n",
            "[18503 | 1753.66] loss=0.89 avg=0.88\n",
            "[18504 | 1756.76] loss=0.94 avg=0.88\n",
            "[18505 | 1759.83] loss=0.82 avg=0.88\n",
            "[18506 | 1762.92] loss=0.91 avg=0.88\n",
            "[18507 | 1766.01] loss=0.76 avg=0.88\n",
            "[18508 | 1769.08] loss=0.96 avg=0.88\n",
            "[18509 | 1772.16] loss=0.93 avg=0.88\n",
            "[18510 | 1775.25] loss=0.81 avg=0.88\n",
            "[18511 | 1778.34] loss=0.91 avg=0.88\n",
            "[18512 | 1781.41] loss=0.87 avg=0.88\n",
            "[18513 | 1784.49] loss=1.03 avg=0.88\n",
            "[18514 | 1787.57] loss=0.85 avg=0.88\n",
            "[18515 | 1790.65] loss=0.85 avg=0.88\n",
            "[18516 | 1793.73] loss=0.95 avg=0.88\n",
            "[18517 | 1796.81] loss=0.67 avg=0.88\n",
            "[18518 | 1799.90] loss=0.91 avg=0.88\n",
            "[18519 | 1802.97] loss=0.94 avg=0.88\n",
            "[18520 | 1806.06] loss=0.82 avg=0.88\n",
            "[18521 | 1809.14] loss=0.86 avg=0.88\n",
            "[18522 | 1812.22] loss=0.83 avg=0.88\n",
            "[18523 | 1815.28] loss=1.00 avg=0.88\n",
            "[18524 | 1818.33] loss=0.88 avg=0.88\n",
            "[18525 | 1821.42] loss=0.93 avg=0.88\n",
            "[18526 | 1824.50] loss=0.83 avg=0.88\n",
            "[18527 | 1827.58] loss=0.93 avg=0.88\n",
            "[18528 | 1830.67] loss=0.88 avg=0.88\n",
            "[18529 | 1833.76] loss=0.83 avg=0.88\n",
            "[18530 | 1836.84] loss=0.85 avg=0.88\n",
            "[18531 | 1839.93] loss=0.95 avg=0.88\n",
            "[18532 | 1842.99] loss=0.85 avg=0.88\n",
            "[18533 | 1846.07] loss=0.81 avg=0.88\n",
            "[18534 | 1849.16] loss=0.91 avg=0.88\n",
            "[18535 | 1852.23] loss=0.92 avg=0.88\n",
            "[18536 | 1855.34] loss=0.90 avg=0.88\n",
            "[18537 | 1858.42] loss=0.87 avg=0.88\n",
            "[18538 | 1861.51] loss=0.90 avg=0.88\n",
            "[18539 | 1864.57] loss=0.95 avg=0.88\n",
            "[18540 | 1867.67] loss=0.78 avg=0.88\n",
            "[18541 | 1870.78] loss=0.91 avg=0.88\n",
            "[18542 | 1873.86] loss=0.92 avg=0.88\n",
            "[18543 | 1876.95] loss=0.78 avg=0.88\n",
            "[18544 | 1880.03] loss=0.61 avg=0.88\n",
            "[18545 | 1883.13] loss=0.85 avg=0.88\n",
            "[18546 | 1886.22] loss=0.90 avg=0.88\n",
            "[18547 | 1889.31] loss=0.94 avg=0.88\n",
            "[18548 | 1892.40] loss=0.85 avg=0.88\n",
            "[18549 | 1895.48] loss=0.79 avg=0.88\n",
            "[18550 | 1898.57] loss=0.71 avg=0.87\n",
            "[18551 | 1901.66] loss=0.89 avg=0.87\n",
            "[18552 | 1904.73] loss=0.91 avg=0.87\n",
            "[18553 | 1907.83] loss=0.93 avg=0.88\n",
            "[18554 | 1910.91] loss=0.90 avg=0.88\n",
            "[18555 | 1913.99] loss=0.81 avg=0.88\n",
            "[18556 | 1917.08] loss=0.88 avg=0.88\n",
            "[18557 | 1920.16] loss=0.86 avg=0.87\n",
            "[18558 | 1923.25] loss=0.84 avg=0.87\n",
            "[18559 | 1926.34] loss=0.65 avg=0.87\n",
            "[18560 | 1929.41] loss=0.89 avg=0.87\n",
            "[18561 | 1932.50] loss=0.77 avg=0.87\n",
            "[18562 | 1935.57] loss=0.81 avg=0.87\n",
            "[18563 | 1938.66] loss=0.92 avg=0.87\n",
            "[18564 | 1941.76] loss=0.94 avg=0.87\n",
            "[18565 | 1944.86] loss=0.98 avg=0.87\n",
            "[18566 | 1947.94] loss=0.89 avg=0.87\n",
            "[18567 | 1951.04] loss=0.87 avg=0.87\n",
            "[18568 | 1954.13] loss=0.87 avg=0.87\n",
            "[18569 | 1957.22] loss=0.88 avg=0.87\n",
            "[18570 | 1960.32] loss=0.94 avg=0.87\n",
            "[18571 | 1963.42] loss=0.94 avg=0.87\n",
            "[18572 | 1966.50] loss=0.91 avg=0.88\n",
            "[18573 | 1969.59] loss=0.93 avg=0.88\n",
            "[18574 | 1972.68] loss=0.88 avg=0.88\n",
            "[18575 | 1975.78] loss=0.91 avg=0.88\n",
            "[18576 | 1978.88] loss=0.90 avg=0.88\n",
            "[18577 | 1981.98] loss=0.97 avg=0.88\n",
            "[18578 | 1985.06] loss=0.93 avg=0.88\n",
            "[18579 | 1988.16] loss=0.91 avg=0.88\n",
            "[18580 | 1991.25] loss=0.89 avg=0.88\n",
            "[18581 | 1994.33] loss=0.89 avg=0.88\n",
            "[18582 | 1997.43] loss=0.86 avg=0.88\n",
            "[18583 | 2000.50] loss=0.87 avg=0.88\n",
            "[18584 | 2003.60] loss=0.87 avg=0.88\n",
            "[18585 | 2006.68] loss=0.87 avg=0.88\n",
            "[18586 | 2009.78] loss=0.93 avg=0.88\n",
            "[18587 | 2012.88] loss=0.87 avg=0.88\n",
            "[18588 | 2015.97] loss=0.89 avg=0.88\n",
            "[18589 | 2019.05] loss=0.80 avg=0.88\n",
            "[18590 | 2022.14] loss=0.79 avg=0.88\n",
            "[18591 | 2025.22] loss=0.83 avg=0.88\n",
            "[18592 | 2028.29] loss=0.74 avg=0.87\n",
            "[18593 | 2031.39] loss=0.85 avg=0.87\n",
            "[18594 | 2034.47] loss=0.98 avg=0.88\n",
            "[18595 | 2037.54] loss=0.72 avg=0.87\n",
            "[18596 | 2040.62] loss=0.90 avg=0.87\n",
            "[18597 | 2043.70] loss=0.96 avg=0.88\n",
            "[18598 | 2046.78] loss=0.92 avg=0.88\n",
            "[18599 | 2049.87] loss=0.84 avg=0.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". = Una manera la puede contar con más tímo del tiempo.\n",
            "His house is near the park. = Su casa queda cerca del parque.\n",
            "We're tired. = Estamos cansados.\n",
            "They went swimming. = Ellos fueron a nadar.\n",
            "They are working hard to get their reward. = Están trabajando duro para conseguir su recompensa.\n",
            "You need to talk to me. = Necesitas hablar conmigo.\n",
            "You should've listened to me. = Deberías haberme escuchado.\n",
            "You can't change your username. = No puedes cambiar tu nombre de usuaria.\n",
            "He has a long nose. = Él tiene una nariz barata.\n",
            "I don't want to do this when I'm with you. = No quiero hacer esto cuando estoy contigo.\n",
            "What an interesting book this is! = ¡Qué libro más interesante!\n",
            "You can't take it with you when you die. = No te puedes llevar con ella cuando te mueras.\n",
            "That is all I know about him. = Eso es todo lo que sé sobre él.\n",
            "He is kind to her. = Él es amable con ella.\n",
            "I've got nothing to do with the case. = No tengo nada que ver con el caso.\n",
            "I need you to look at this. = Necesito que mirara esto.\n",
            "My father's plane landed on my birthday. = El avión de mi padre acolló en mi cumpleaños.\n",
            "We found an old man's pocket book in the chest. = Encontramos el libro de un anciano del tarjeta debajo del erecto.\n",
            "You can do it. You can! = ¡Usted puede hacerlo! ¡Sí!\n",
            "He was elected by the students. = Él fue elegido por los estudiantes.\n",
            "This coffee is too strong for me. = Este café está demasiado fuerte para mí.\n",
            "It's a good opportunity. = Es un buen ocasión.\n",
            "Tom doesn't know a thing about cooking. = Tom no sabe nada acocar sobre cocina.\n",
            "What does Tom buy for Mary on his birthday? = ¿Qué compra Tom para Mary de su cumpleaños?\n",
            "That would be embarrassing. = Eso sería avergonzado.\n",
            "Please don't argue. = Por favor, no discutan.\n",
            "Can you tell Tom apart from his brother? = ¿Usted puede distinguir a Tom de su hermano?\n",
            "What a pleasant surprise! = ¡Qué sorpresa agradable!\n",
            "I don't like the heat wave. = No me gusta el televisor centenario.\n",
            "Tom is very sensitive to cold. = Tom es muy sensible al frío.\n",
            "You're very brave, aren't you? = Eres muy valiente, ¿verdad?\n",
            "He had never had a bad experience. = Él nunca tuvo una mala experiencia.\n",
            "The situation is very complicated. = La situación es muy complicada.\n",
            "Tom doesn't have a car. = Tom no tiene coche.\n",
            "I need to know what I can do for you. = Necesito saber qué puedo hacer por ti.\n",
            "I'm not going to get up. = No voy a levantarme.\n",
            "The plane is bound for Chicago. = El avión va con destino a Chicago.\n",
            "What's the most annoying sound you've ever heard? = ¿Qué son las sinceras son que has oído alguna vez?\n",
            "Tom is very busy, isn't he? = Tom está muy ocupado, ¿no?\n",
            "I know nothing about her. = No sé nada acerca de ella.\n",
            "Tom didn't like Mary at first. = A Tom no le gustaba Mary al principio.\n",
            "Tom likes me. = Él me quiere.\n",
            "I love my family. = Amo a mi familia.\n",
            "Don't be rude. = No seas cortés.\n",
            "Tom didn't say how many times he'd been injured, but he had several thousand more than Mary. = Tom no dijo cuántas veces éste había sido herido, pero tuvo pasados más de más que Mary.\n",
            "Tom\n",
            "\n",
            "[18600 | 2090.43] loss=0.90 avg=0.88\n",
            "[18601 | 2093.52] loss=0.85 avg=0.88\n",
            "[18602 | 2096.61] loss=0.73 avg=0.87\n",
            "[18603 | 2099.70] loss=0.87 avg=0.87\n",
            "[18604 | 2102.78] loss=0.87 avg=0.87\n",
            "[18605 | 2105.85] loss=0.83 avg=0.87\n",
            "[18606 | 2108.94] loss=0.93 avg=0.87\n",
            "[18607 | 2112.03] loss=0.87 avg=0.87\n",
            "[18608 | 2115.10] loss=0.94 avg=0.87\n",
            "[18609 | 2118.20] loss=0.88 avg=0.87\n",
            "[18610 | 2121.29] loss=0.90 avg=0.87\n",
            "[18611 | 2124.38] loss=0.82 avg=0.87\n",
            "[18612 | 2127.46] loss=0.94 avg=0.87\n",
            "[18613 | 2130.55] loss=0.85 avg=0.87\n",
            "[18614 | 2133.65] loss=0.87 avg=0.87\n",
            "[18615 | 2136.74] loss=0.95 avg=0.88\n",
            "[18616 | 2139.82] loss=0.76 avg=0.87\n",
            "[18617 | 2142.89] loss=0.82 avg=0.87\n",
            "[18618 | 2145.99] loss=0.89 avg=0.87\n",
            "[18619 | 2149.07] loss=0.93 avg=0.87\n",
            "[18620 | 2152.16] loss=0.91 avg=0.87\n",
            "[18621 | 2155.25] loss=0.96 avg=0.88\n",
            "[18622 | 2158.33] loss=0.87 avg=0.88\n",
            "[18623 | 2161.39] loss=0.88 avg=0.88\n",
            "[18624 | 2164.47] loss=0.90 avg=0.88\n",
            "[18625 | 2167.57] loss=0.88 avg=0.88\n",
            "[18626 | 2170.66] loss=0.86 avg=0.88\n",
            "[18627 | 2173.75] loss=0.91 avg=0.88\n",
            "[18628 | 2176.82] loss=0.89 avg=0.88\n",
            "[18629 | 2179.90] loss=0.82 avg=0.88\n",
            "[18630 | 2182.98] loss=0.85 avg=0.88\n",
            "[18631 | 2186.08] loss=0.93 avg=0.88\n",
            "[18632 | 2189.17] loss=0.93 avg=0.88\n",
            "[18633 | 2192.26] loss=0.78 avg=0.88\n",
            "[18634 | 2195.34] loss=1.00 avg=0.88\n",
            "[18635 | 2198.42] loss=0.74 avg=0.88\n",
            "[18636 | 2201.52] loss=0.77 avg=0.87\n",
            "[18637 | 2204.61] loss=0.88 avg=0.87\n",
            "[18638 | 2207.69] loss=0.90 avg=0.87\n",
            "[18639 | 2210.79] loss=0.90 avg=0.87\n",
            "[18640 | 2213.86] loss=0.67 avg=0.87\n",
            "[18641 | 2216.95] loss=0.80 avg=0.87\n",
            "[18642 | 2220.02] loss=0.67 avg=0.87\n",
            "[18643 | 2223.12] loss=0.97 avg=0.87\n",
            "[18644 | 2226.20] loss=0.84 avg=0.87\n",
            "[18645 | 2229.30] loss=0.92 avg=0.87\n",
            "[18646 | 2232.37] loss=0.70 avg=0.87\n",
            "[18647 | 2235.44] loss=0.86 avg=0.87\n",
            "[18648 | 2238.53] loss=0.87 avg=0.87\n",
            "[18649 | 2241.61] loss=0.80 avg=0.87\n",
            "[18650 | 2244.71] loss=0.95 avg=0.87\n",
            "[18651 | 2247.80] loss=1.00 avg=0.87\n",
            "[18652 | 2250.90] loss=0.85 avg=0.87\n",
            "[18653 | 2253.97] loss=0.89 avg=0.87\n",
            "[18654 | 2257.06] loss=0.82 avg=0.87\n",
            "[18655 | 2260.15] loss=0.89 avg=0.87\n",
            "[18656 | 2263.24] loss=0.97 avg=0.87\n",
            "[18657 | 2266.33] loss=0.83 avg=0.87\n",
            "[18658 | 2269.41] loss=0.74 avg=0.87\n",
            "[18659 | 2272.49] loss=0.91 avg=0.87\n",
            "[18660 | 2275.58] loss=0.83 avg=0.87\n",
            "[18661 | 2278.67] loss=0.86 avg=0.87\n",
            "[18662 | 2281.76] loss=0.80 avg=0.87\n",
            "[18663 | 2284.85] loss=0.91 avg=0.87\n",
            "[18664 | 2287.93] loss=0.86 avg=0.87\n",
            "[18665 | 2291.01] loss=0.91 avg=0.87\n",
            "[18666 | 2294.09] loss=0.90 avg=0.87\n",
            "[18667 | 2297.16] loss=0.76 avg=0.87\n",
            "[18668 | 2300.26] loss=0.69 avg=0.87\n",
            "[18669 | 2303.36] loss=0.88 avg=0.87\n",
            "[18670 | 2306.44] loss=0.96 avg=0.87\n",
            "[18671 | 2309.53] loss=0.78 avg=0.87\n",
            "[18672 | 2312.61] loss=0.79 avg=0.87\n",
            "[18673 | 2315.69] loss=0.71 avg=0.86\n",
            "[18674 | 2318.78] loss=0.93 avg=0.87\n",
            "[18675 | 2321.88] loss=0.86 avg=0.87\n",
            "[18676 | 2324.97] loss=0.91 avg=0.87\n",
            "[18677 | 2328.06] loss=0.69 avg=0.86\n",
            "[18678 | 2331.14] loss=0.84 avg=0.86\n",
            "[18679 | 2334.23] loss=0.91 avg=0.86\n",
            "[18680 | 2337.31] loss=0.92 avg=0.86\n",
            "[18681 | 2340.41] loss=0.93 avg=0.87\n",
            "[18682 | 2343.49] loss=0.98 avg=0.87\n",
            "[18683 | 2346.58] loss=0.90 avg=0.87\n",
            "[18684 | 2349.65] loss=0.96 avg=0.87\n",
            "[18685 | 2352.73] loss=0.85 avg=0.87\n",
            "[18686 | 2355.81] loss=0.79 avg=0.87\n",
            "[18687 | 2358.90] loss=0.91 avg=0.87\n",
            "[18688 | 2361.98] loss=0.85 avg=0.87\n",
            "[18689 | 2365.06] loss=0.84 avg=0.87\n",
            "[18690 | 2368.16] loss=0.69 avg=0.87\n",
            "[18691 | 2371.23] loss=0.86 avg=0.87\n",
            "[18692 | 2374.32] loss=0.99 avg=0.87\n",
            "[18693 | 2377.41] loss=0.87 avg=0.87\n",
            "[18694 | 2380.49] loss=0.85 avg=0.87\n",
            "[18695 | 2383.58] loss=0.83 avg=0.87\n",
            "[18696 | 2386.68] loss=0.82 avg=0.87\n",
            "[18697 | 2389.75] loss=0.88 avg=0.87\n",
            "[18698 | 2392.83] loss=0.86 avg=0.87\n",
            "[18699 | 2395.92] loss=0.76 avg=0.86\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "From them? = ¿Los informes?\n",
            "This tie goes well with your shirt. = Esta corbata va bien con tu camisa.\n",
            "Tom was sentenced to four months in jail. = A Tom le condenaron a cuatro meses en prisión.\n",
            "What's the date today? = ¿Qué fecha es hoy?\n",
            "Tom has a daughter whose name is Mary, but she is not his. = Tom tiene una hija que no es María pero no es su nombre.\n",
            "It would be nice to see this movie again. = Estaría bien ver la película otra vez.\n",
            "Is it true that you came here alone? = ¿Es verdad que viniste aquí solo?\n",
            "He gave up smoking three years ago. = Él dejó el tabaco hace tres años.\n",
            "Tom isn't old enough to drink. = Tom no tiene edad suficiente para beber.\n",
            "Let me speak to Tom about it. = Déjame hablar a Tom acerca de eso.\n",
            "He was the first man to cross the Pacific Ocean. = Él fue el primer hombre en cruzar el Océano Pacífico.\n",
            "Tom is out of town and cannot be reached. = Tom está fuera de la ciudad, no puede llegar por ella.\n",
            "My brother looks just like me. = Mi hermano se parece exactamente a mí.\n",
            "I need you here in case something happens. = Te necesito aquí por si pasa algo.\n",
            "He was wounded in a burglary. = Él fue herido en un robo.\n",
            "Did you find anything? = ¿Encontraste algo?\n",
            "I didn't want to be discovered. = No me quería ser vigilado.\n",
            "Does Tom like to be alone? = ¿A Tom le gusta estar solo?\n",
            "Tom says he doesn't have enough money. = Tom dice que no tiene suficiente dinero.\n",
            "I'm sure Tom misses you. = Estoy seguro que Tom te extraña.\n",
            "A little work will do. = Un poco de trabajo lleva a un poco.\n",
            "He took the book to the station. = Llevó el libro a la estación.\n",
            "I thought you came here yesterday. = Pensé que llegaste ayer aquí.\n",
            "I went to visit my father for the first time in ten years. = Avisé a mi padre por primera vez en diez años.\n",
            "I think he'll like this. = Creo que a él le gustará esto.\n",
            "You're on the wrong train. = Estás en el tren equivocado.\n",
            "I think this is the most interesting book that I've ever read. = Creo que esta es la verdad libro que he leía hasta ahora.\n",
            "I'm just a boy who makes mistakes. = Sólo soy un niño que comete errores.\n",
            "He was angry at everyone. = Él estaba satisfecho con todo el mundo.\n",
            "Let Tom finish. = Deja que Tom termina.\n",
            "I want a picture of my family. = Desea una fotografía de mi familia.\n",
            "You're the new guy. = Sos el nuevo chico.\n",
            "He often speaks to his son from behind. = Voy a menudo con su padre a desviar tras un grupo.\n",
            "I'll have two hot dogs with spicy sauce for dinner. = Trata dos panchos con salsa secundaria para la cena.\n",
            "I'm very tired. = Estoy muy cansado.\n",
            "It's too early to say. = Es demasiado temprano para decirlo.\n",
            "This book was written in French. = Este libro fue escrito en francés.\n",
            "The police would not give us more information. = La Policía no nos darían más información.\n",
            "She told him that she knew the secret. = Ella le dijo que sabía el secreto.\n",
            "Tom is not a very good kisser. = A Tom no le cantara muy bien besar.\n",
            "There's a lot to be learned about this town. = Hay mucho que esto aprender sobre esta ciudad.\n",
            "He said that he had written the letter himself. = Él dijo que él se había escrito la carta.\n",
            "What made you want to come to my party? = ¿Qué te hizo querer para ven\n",
            "\n",
            "[18700 | 2436.38] loss=0.82 avg=0.86\n",
            "[18701 | 2439.47] loss=0.89 avg=0.86\n",
            "[18702 | 2442.56] loss=0.68 avg=0.86\n",
            "[18703 | 2445.65] loss=0.81 avg=0.86\n",
            "[18704 | 2448.72] loss=0.94 avg=0.86\n",
            "[18705 | 2451.81] loss=0.70 avg=0.86\n",
            "[18706 | 2454.88] loss=0.99 avg=0.86\n",
            "[18707 | 2457.96] loss=0.92 avg=0.86\n",
            "[18708 | 2461.05] loss=0.85 avg=0.86\n",
            "[18709 | 2464.12] loss=0.77 avg=0.86\n",
            "[18710 | 2467.20] loss=0.89 avg=0.86\n",
            "[18711 | 2470.28] loss=0.78 avg=0.86\n",
            "[18712 | 2473.36] loss=0.82 avg=0.86\n",
            "[18713 | 2476.44] loss=0.98 avg=0.86\n",
            "[18714 | 2479.52] loss=0.87 avg=0.86\n",
            "[18715 | 2482.61] loss=0.88 avg=0.86\n",
            "[18716 | 2485.69] loss=0.75 avg=0.86\n",
            "[18717 | 2488.77] loss=0.73 avg=0.86\n",
            "[18718 | 2491.85] loss=0.89 avg=0.86\n",
            "[18719 | 2494.93] loss=0.92 avg=0.86\n",
            "[18720 | 2498.02] loss=0.85 avg=0.86\n",
            "[18721 | 2501.11] loss=0.97 avg=0.86\n",
            "[18722 | 2504.20] loss=0.85 avg=0.86\n",
            "[18723 | 2507.27] loss=0.86 avg=0.86\n",
            "[18724 | 2510.37] loss=0.91 avg=0.86\n",
            "[18725 | 2513.45] loss=0.81 avg=0.86\n",
            "[18726 | 2516.55] loss=0.92 avg=0.86\n",
            "[18727 | 2519.65] loss=0.86 avg=0.86\n",
            "[18728 | 2522.72] loss=0.87 avg=0.86\n",
            "[18729 | 2525.79] loss=0.86 avg=0.86\n",
            "[18730 | 2528.88] loss=0.78 avg=0.86\n",
            "[18731 | 2531.95] loss=0.90 avg=0.86\n",
            "[18732 | 2535.03] loss=0.82 avg=0.86\n",
            "[18733 | 2538.09] loss=0.90 avg=0.86\n",
            "[18734 | 2541.18] loss=0.92 avg=0.86\n",
            "[18735 | 2544.27] loss=0.88 avg=0.86\n",
            "[18736 | 2547.35] loss=0.94 avg=0.86\n",
            "[18737 | 2550.43] loss=0.88 avg=0.86\n",
            "[18738 | 2553.53] loss=0.94 avg=0.86\n",
            "[18739 | 2556.62] loss=0.90 avg=0.86\n",
            "[18740 | 2559.71] loss=0.93 avg=0.86\n",
            "[18741 | 2562.78] loss=0.83 avg=0.86\n",
            "[18742 | 2565.86] loss=0.77 avg=0.86\n",
            "[18743 | 2568.95] loss=0.85 avg=0.86\n",
            "[18744 | 2572.04] loss=0.83 avg=0.86\n",
            "[18745 | 2575.13] loss=0.80 avg=0.86\n",
            "[18746 | 2578.22] loss=0.81 avg=0.86\n",
            "[18747 | 2581.31] loss=0.98 avg=0.86\n",
            "[18748 | 2584.40] loss=0.89 avg=0.86\n",
            "[18749 | 2587.50] loss=0.88 avg=0.86\n",
            "[18750 | 2590.57] loss=0.76 avg=0.86\n",
            "[18751 | 2593.65] loss=0.84 avg=0.86\n",
            "[18752 | 2596.75] loss=0.86 avg=0.86\n",
            "[18753 | 2599.84] loss=0.96 avg=0.86\n",
            "[18754 | 2602.92] loss=1.01 avg=0.86\n",
            "[18755 | 2606.02] loss=0.93 avg=0.87\n",
            "[18756 | 2609.10] loss=0.87 avg=0.87\n",
            "[18757 | 2612.18] loss=0.84 avg=0.87\n",
            "[18758 | 2615.26] loss=0.88 avg=0.87\n",
            "[18759 | 2618.34] loss=0.90 avg=0.87\n",
            "[18760 | 2621.42] loss=0.95 avg=0.87\n",
            "[18761 | 2624.50] loss=0.91 avg=0.87\n",
            "[18762 | 2627.59] loss=1.01 avg=0.87\n",
            "[18763 | 2630.68] loss=0.85 avg=0.87\n",
            "[18764 | 2633.77] loss=0.91 avg=0.87\n",
            "[18765 | 2636.85] loss=0.90 avg=0.87\n",
            "[18766 | 2639.94] loss=0.84 avg=0.87\n",
            "[18767 | 2643.02] loss=0.87 avg=0.87\n",
            "[18768 | 2646.11] loss=0.74 avg=0.87\n",
            "[18769 | 2649.18] loss=0.73 avg=0.87\n",
            "[18770 | 2652.29] loss=0.81 avg=0.87\n",
            "[18771 | 2655.36] loss=0.88 avg=0.87\n",
            "[18772 | 2658.45] loss=0.92 avg=0.87\n",
            "[18773 | 2661.53] loss=0.92 avg=0.87\n",
            "[18774 | 2664.62] loss=0.70 avg=0.87\n",
            "[18775 | 2667.72] loss=0.93 avg=0.87\n",
            "[18776 | 2670.83] loss=0.68 avg=0.86\n",
            "[18777 | 2673.92] loss=0.78 avg=0.86\n",
            "[18778 | 2677.01] loss=0.87 avg=0.86\n",
            "[18779 | 2680.10] loss=0.87 avg=0.86\n",
            "[18780 | 2683.19] loss=0.81 avg=0.86\n",
            "[18781 | 2686.27] loss=0.83 avg=0.86\n",
            "[18782 | 2689.38] loss=0.96 avg=0.86\n",
            "[18783 | 2692.47] loss=0.72 avg=0.86\n",
            "[18784 | 2695.56] loss=0.93 avg=0.86\n",
            "[18785 | 2698.65] loss=0.87 avg=0.86\n",
            "[18786 | 2701.74] loss=0.93 avg=0.86\n",
            "[18787 | 2704.83] loss=0.83 avg=0.86\n",
            "[18788 | 2707.91] loss=0.91 avg=0.86\n",
            "[18789 | 2711.00] loss=0.97 avg=0.86\n",
            "[18790 | 2714.08] loss=0.95 avg=0.87\n",
            "[18791 | 2717.17] loss=0.84 avg=0.87\n",
            "[18792 | 2720.26] loss=0.87 avg=0.87\n",
            "[18793 | 2723.35] loss=0.94 avg=0.87\n",
            "[18794 | 2726.43] loss=0.83 avg=0.87\n",
            "[18795 | 2729.51] loss=0.92 avg=0.87\n",
            "[18796 | 2732.58] loss=0.87 avg=0.87\n",
            "[18797 | 2735.67] loss=0.86 avg=0.87\n",
            "[18798 | 2738.76] loss=0.90 avg=0.87\n",
            "[18799 | 2741.85] loss=0.89 avg=0.87\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " = Tom cómodo la cabeza.\n",
            "I would like to watch TV tonight. = Me gustaría ver la televisión esta noche.\n",
            "How many hours a day does he work? = ¿Cuántas horas trabaja?\n",
            "She is on the verge of starvation. = Ella está al bote de hambre.\n",
            "I'd say no if I were you. = Si yo fuera tú, no dicho.\n",
            "Do you want me to be there or not? = ¿Quieres que sea allí o no?\n",
            "Is that a gift for me? = ¿Es esa un regalo para mí?\n",
            "I need the key. = Necesito la llave.\n",
            "Tom is an honest boy. = Tom es un chico honesto.\n",
            "There is no hurry. = No hay apuro.\n",
            "Do you know how far Australia is from New Zealand? = ¿Sabes a qué distancia está Australia de Nueva Zelanda?\n",
            "I think I'd like to live in the country. = Creo que me gustaría vivir en el campo.\n",
            "They're too young to marry. = Ellos tienen demasiado chiquillos para casarse.\n",
            "Do you understand what I just said to you? = ¿Has entendido lo que acabo de decirte?\n",
            "The weather has gotten better recently. = El tiempo mejorá hace pasado.\n",
            "Tom has been arrested three times. = Tom han sido arrestado tres veces.\n",
            "Tom is a member of a secret society. = Tom es un miembro de una sociedad secreta.\n",
            "I'm the same age. = Soy tan misma años.\n",
            "Can you see Tom? = ¿Puedes ver a Tomás?\n",
            "I am going to bed. = Voy a acostar.\n",
            "Can I borrow a tape recorder? = ¿Puedo sacar una carta de rellenas?\n",
            "He was sent to an orphanage. = Él fue enviado a un orfanato.\n",
            "I don't even have your driver's license number. = Ni siquiera tengo tu número de línea.\n",
            "Tom doesn't know when Mary will leave Boston. = Tom no sabe cuándo Mary sale Mary Boston.\n",
            "The boy seemed to have been sick during the week. = El chico parecía haber estado enfermo durante la semana.\n",
            "How deep is Lake Biwa? = ¿Qué profundidad tiene el lago Biwa?\n",
            "They want to sell it. = Quieren vendrá.\n",
            "I'm not a bird. = Yo no soy un pájaro.\n",
            "She married him because she wanted to. = Se casó con él porque quería.\n",
            "He is a man of his word. = Se le da un hombre de palabra.\n",
            "You must be more polite. = Tiéntas que ser más respetuosas.\n",
            "He is getting old. = Siendo viejo.\n",
            "Tom has a secret admirer. = Tom tiene un admirer de secreto.\n",
            "Give me a break. = Dame un descanso.\n",
            "We have a lot of eggs in the fridge. = Tenemos muchos huevos en la nevera.\n",
            "He went home after dark. = Él se fue a casa después de que oscureciera la oscuridad.\n",
            "I'll call the police. = Llamaré a la policía.\n",
            "He told me his age. = Me dijo su edad.\n",
            "He's a good man. = Es un buen hombre.\n",
            "The children will be taking a walk when you arrive. = Las niñas se tomaron a dar un paseo cuando llegues.\n",
            "Please let me know if any change is needed. = Por favor, avísale si hay algún cambio.\n",
            "I think you ought to apologize to Tom. = Creo que deberías pedirle perdón a Tom.\n",
            "I saw him playing baseball. = Lo vi jugar al béisbol.\n",
            "You've been acting strange. = Habéis estado xicando.\n",
            "Tom has been trying to avoid Mary. = Tom ha estado intentando evitar a Mary.\n",
            "There are many islands in Greece. = Hay muchas islas en Grecia.\n",
            "You can't live without water. = No podés vivir sin agua.\n",
            "She was advised by him to spend more time reading. = Le aconsejaron\n",
            "\n",
            "[18800 | 2782.18] loss=0.88 avg=0.87\n",
            "[18801 | 2785.26] loss=0.71 avg=0.87\n",
            "[18802 | 2788.33] loss=0.94 avg=0.87\n",
            "[18803 | 2791.38] loss=0.79 avg=0.87\n",
            "[18804 | 2794.44] loss=0.74 avg=0.86\n",
            "[18805 | 2797.51] loss=0.88 avg=0.86\n",
            "[18806 | 2800.58] loss=0.92 avg=0.86\n",
            "[18807 | 2803.64] loss=0.80 avg=0.86\n",
            "[18808 | 2806.72] loss=0.81 avg=0.86\n",
            "[18809 | 2809.81] loss=0.87 avg=0.86\n",
            "[18810 | 2812.90] loss=0.79 avg=0.86\n",
            "[18811 | 2816.00] loss=0.92 avg=0.86\n",
            "[18812 | 2819.08] loss=0.82 avg=0.86\n",
            "[18813 | 2822.16] loss=0.68 avg=0.86\n",
            "[18814 | 2825.24] loss=0.75 avg=0.86\n",
            "[18815 | 2828.32] loss=0.93 avg=0.86\n",
            "[18816 | 2831.41] loss=0.69 avg=0.86\n",
            "[18817 | 2834.50] loss=0.79 avg=0.86\n",
            "[18818 | 2837.59] loss=0.76 avg=0.86\n",
            "[18819 | 2840.68] loss=0.95 avg=0.86\n",
            "[18820 | 2843.77] loss=0.74 avg=0.86\n",
            "[18821 | 2846.86] loss=0.74 avg=0.86\n",
            "[18822 | 2849.95] loss=0.80 avg=0.86\n",
            "[18823 | 2853.03] loss=0.68 avg=0.85\n",
            "[18824 | 2856.13] loss=0.87 avg=0.85\n",
            "[18825 | 2859.20] loss=0.85 avg=0.85\n",
            "[18826 | 2862.28] loss=1.03 avg=0.86\n",
            "[18827 | 2865.37] loss=0.87 avg=0.86\n",
            "[18828 | 2868.47] loss=0.85 avg=0.86\n",
            "[18829 | 2871.54] loss=0.93 avg=0.86\n",
            "[18830 | 2874.62] loss=0.90 avg=0.86\n",
            "[18831 | 2877.68] loss=1.09 avg=0.86\n",
            "[18832 | 2880.73] loss=0.92 avg=0.86\n",
            "[18833 | 2883.79] loss=0.73 avg=0.86\n",
            "[18834 | 2886.87] loss=0.86 avg=0.86\n",
            "[18835 | 2889.95] loss=0.90 avg=0.86\n",
            "[18836 | 2893.05] loss=0.85 avg=0.86\n",
            "[18837 | 2896.10] loss=0.95 avg=0.86\n",
            "[18838 | 2899.17] loss=0.90 avg=0.86\n",
            "[18839 | 2902.24] loss=0.89 avg=0.86\n",
            "[18840 | 2905.31] loss=0.73 avg=0.86\n",
            "[18841 | 2908.38] loss=0.89 avg=0.86\n",
            "[18842 | 2911.45] loss=0.93 avg=0.86\n",
            "[18843 | 2914.51] loss=0.84 avg=0.86\n",
            "[18844 | 2917.56] loss=0.91 avg=0.86\n",
            "[18845 | 2920.62] loss=0.92 avg=0.86\n",
            "[18846 | 2923.69] loss=0.94 avg=0.86\n",
            "[18847 | 2926.74] loss=0.72 avg=0.86\n",
            "[18848 | 2929.81] loss=0.94 avg=0.86\n",
            "[18849 | 2932.88] loss=0.69 avg=0.86\n",
            "[18850 | 2935.93] loss=0.77 avg=0.86\n",
            "[18851 | 2939.00] loss=0.95 avg=0.86\n",
            "[18852 | 2942.07] loss=0.88 avg=0.86\n",
            "[18853 | 2945.14] loss=0.93 avg=0.86\n",
            "[18854 | 2948.21] loss=0.91 avg=0.86\n",
            "[18855 | 2951.28] loss=0.77 avg=0.86\n",
            "[18856 | 2954.36] loss=0.82 avg=0.86\n",
            "[18857 | 2957.45] loss=0.94 avg=0.86\n",
            "[18858 | 2960.53] loss=0.88 avg=0.86\n",
            "[18859 | 2963.61] loss=0.90 avg=0.86\n",
            "[18860 | 2966.67] loss=0.83 avg=0.86\n",
            "[18861 | 2969.75] loss=0.91 avg=0.86\n",
            "[18862 | 2972.84] loss=0.83 avg=0.86\n",
            "[18863 | 2975.93] loss=0.73 avg=0.86\n",
            "[18864 | 2979.02] loss=0.78 avg=0.86\n",
            "[18865 | 2982.10] loss=0.89 avg=0.86\n",
            "[18866 | 2985.19] loss=0.95 avg=0.86\n",
            "[18867 | 2988.29] loss=0.78 avg=0.86\n",
            "[18868 | 2991.37] loss=0.88 avg=0.86\n",
            "[18869 | 2994.47] loss=0.76 avg=0.86\n",
            "[18870 | 2997.56] loss=0.75 avg=0.86\n",
            "[18871 | 3000.66] loss=0.93 avg=0.86\n",
            "[18872 | 3003.75] loss=0.76 avg=0.86\n",
            "[18873 | 3006.86] loss=0.77 avg=0.86\n",
            "[18874 | 3009.95] loss=0.95 avg=0.86\n",
            "[18875 | 3013.05] loss=0.98 avg=0.86\n",
            "[18876 | 3016.10] loss=0.94 avg=0.86\n",
            "[18877 | 3019.17] loss=0.79 avg=0.86\n",
            "[18878 | 3022.24] loss=0.92 avg=0.86\n",
            "[18879 | 3025.33] loss=0.92 avg=0.86\n",
            "[18880 | 3028.41] loss=0.83 avg=0.86\n",
            "[18881 | 3031.50] loss=0.96 avg=0.86\n",
            "[18882 | 3034.59] loss=0.81 avg=0.86\n",
            "[18883 | 3037.67] loss=0.85 avg=0.86\n",
            "[18884 | 3040.75] loss=0.82 avg=0.86\n",
            "[18885 | 3043.82] loss=0.88 avg=0.86\n",
            "[18886 | 3046.92] loss=0.96 avg=0.86\n",
            "[18887 | 3050.00] loss=0.95 avg=0.86\n",
            "[18888 | 3053.07] loss=0.68 avg=0.86\n",
            "[18889 | 3056.13] loss=0.89 avg=0.86\n",
            "[18890 | 3059.20] loss=0.79 avg=0.86\n",
            "[18891 | 3062.30] loss=0.85 avg=0.86\n",
            "[18892 | 3065.39] loss=0.94 avg=0.86\n",
            "[18893 | 3068.47] loss=0.95 avg=0.86\n",
            "[18894 | 3071.53] loss=0.73 avg=0.86\n",
            "[18895 | 3074.60] loss=0.94 avg=0.86\n",
            "[18896 | 3077.68] loss=0.91 avg=0.86\n",
            "[18897 | 3080.76] loss=0.89 avg=0.86\n",
            "[18898 | 3083.83] loss=0.86 avg=0.86\n",
            "[18899 | 3086.92] loss=0.98 avg=0.86\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "? = ¿Eres de ello?\n",
            "It would be good to take a taxi. = Sería bueno tomar un taxi.\n",
            "He is a man of culture. = Él es un hombre de la cultura.\n",
            "There's a possibility that the accident may have been prevented. = Está la posibilidad de que ese accidente se puede haber sido prevenido.\n",
            "Tom doesn't know what to do next. = Tom no sabe qué hacer después.\n",
            "I'm not going to let you do this. = No te voy a permitir que hagas esto.\n",
            "I don't have time for that now. = Ahora no tengo tiempo para esto.\n",
            "I need it right now. = Lo necesito ahora.\n",
            "I hope you have fun. = Ojalá te diviertas.\n",
            "Tom wanted to know what the problem was. = Tom quería saber cuál era el problema.\n",
            "I love the beach. = Me gusta el playa.\n",
            "Why do you want to hurt them? = ¿Por qué buscas los asustos?\n",
            "I have no choice. = No tengo otra opción.\n",
            "You guys have to do it. = Lo tienen que hacer lo que usted haya pedido.\n",
            "He was tired when he got home. = Él estaba cansado cuando llegó a casa.\n",
            "Did she come by train or by bus? = ¿Ella vino en tren o autobús?\n",
            "Tom is wearing a fake identity card. = Tom lleva puesto una nadie de falso falso.\n",
            "How much is this handkerchief? = ¿Cuánto cuesta este pañuelo?\n",
            "I know exactly what you did. = Sé exactamente lo que hiciste.\n",
            "Why don't you stay a bit longer? = ¿Por qué no te quedas un ratito más?\n",
            "I have to read these documents. = Tengo que leer estas documentas.\n",
            "Tom was surprised that Mary would want to go fishing. = Tom estaba sorprendidos de que Mary querría ir a pescar.\n",
            "I think Tom is lying. = Creo que Tom está mintiendo.\n",
            "He can't swim. = Él no nado.\n",
            "A girl appeared before him. = La chica apareció ante él.\n",
            "He's not busy now. = No está ocupado ahora.\n",
            "This room is very cold. = En este cuarto esta habitación es muy fría.\n",
            "I don't want to go to the movies tonight. = No quiero ir al cine esta noche.\n",
            "I'll call you at eight. = Te llamaré a las ocho a las ocho.\n",
            "Tom is in a bad mood this morning. = Esta mañana, Tom está de mal humor.\n",
            "Can I get you a coffee? = ¿Puedo traerte un café?\n",
            "Tom doesn't get along with his neighbors anymore. = Tom ya no se lleva a un lado de sus vecinos.\n",
            "Tom is not as old as you. = Tomás no es tan viejo como tú.\n",
            "The boy is always complaining. = El chico siempre se está quejando.\n",
            "Tom said that he wanted to be left alone. = Tom dijo que quería que lo dejaría solo.\n",
            "Please tell Tom I said hi. = Por favor, dile a Tom que yo dijera hola.\n",
            "Tom had to stay home because of the storm. = Tom tuvo que quedarse en casa debido al corto.\n",
            "Do you want a glass of wine? = ¿Quieres una copa de vino?\n",
            "Where can we park? = ¿Dónde podemos aparcar?\n",
            "You have to do it. = Tienes que hacerlo.\n",
            "Who are you? = ¿Quiénes eres?\n",
            "There were at least ten people in the room. = Había al menos diez personas en la habitación.\n",
            "He was given a gold watch. = Se le dieron un reloj de oro.\n",
            "I don't want to drink tea anymore. = Ya no te voy a tomar té.\n",
            "Tom didn't really like Mary much. = A Tom no le gustaba demasiado Mary.\n",
            "How old is your boyfriend? = ¿Cuántos años tiene tu novio?\n",
            "When can\n",
            "\n",
            "[18900 | 3126.97] loss=0.79 avg=0.86\n",
            "[18901 | 3130.06] loss=0.85 avg=0.86\n",
            "[18902 | 3133.14] loss=0.73 avg=0.86\n",
            "[18903 | 3136.19] loss=0.89 avg=0.86\n",
            "[18904 | 3139.25] loss=0.86 avg=0.86\n",
            "[18905 | 3142.32] loss=0.94 avg=0.86\n",
            "[18906 | 3145.39] loss=0.76 avg=0.86\n",
            "[18907 | 3148.45] loss=0.81 avg=0.86\n",
            "[18908 | 3151.51] loss=0.71 avg=0.86\n",
            "[18909 | 3154.58] loss=0.91 avg=0.86\n",
            "[18910 | 3157.64] loss=0.90 avg=0.86\n",
            "[18911 | 3160.71] loss=0.82 avg=0.86\n",
            "[18912 | 3163.79] loss=0.86 avg=0.86\n",
            "[18913 | 3166.85] loss=0.83 avg=0.86\n",
            "[18914 | 3169.92] loss=0.97 avg=0.86\n",
            "[18915 | 3172.99] loss=0.79 avg=0.86\n",
            "[18916 | 3176.04] loss=0.91 avg=0.86\n",
            "[18917 | 3179.11] loss=0.93 avg=0.86\n",
            "[18918 | 3182.18] loss=0.92 avg=0.86\n",
            "[18919 | 3185.25] loss=0.92 avg=0.86\n",
            "[18920 | 3188.30] loss=0.98 avg=0.86\n",
            "[18921 | 3191.37] loss=0.76 avg=0.86\n",
            "[18922 | 3194.44] loss=0.96 avg=0.86\n",
            "[18923 | 3197.52] loss=0.85 avg=0.86\n",
            "[18924 | 3200.59] loss=0.86 avg=0.86\n",
            "[18925 | 3203.66] loss=0.81 avg=0.86\n",
            "[18926 | 3206.75] loss=0.80 avg=0.86\n",
            "[18927 | 3209.82] loss=0.93 avg=0.86\n",
            "[18928 | 3212.89] loss=0.80 avg=0.86\n",
            "[18929 | 3215.96] loss=0.73 avg=0.86\n",
            "[18930 | 3219.05] loss=0.90 avg=0.86\n",
            "[18931 | 3222.12] loss=0.87 avg=0.86\n",
            "[18932 | 3225.18] loss=0.80 avg=0.86\n",
            "[18933 | 3228.26] loss=0.83 avg=0.86\n",
            "[18934 | 3231.32] loss=0.93 avg=0.86\n",
            "[18935 | 3234.39] loss=0.76 avg=0.86\n",
            "[18936 | 3237.47] loss=0.91 avg=0.86\n",
            "[18937 | 3240.54] loss=0.82 avg=0.86\n",
            "[18938 | 3243.60] loss=0.75 avg=0.86\n",
            "[18939 | 3246.67] loss=0.86 avg=0.86\n",
            "[18940 | 3249.75] loss=0.88 avg=0.86\n",
            "[18941 | 3252.84] loss=0.89 avg=0.86\n",
            "[18942 | 3255.90] loss=0.71 avg=0.86\n",
            "[18943 | 3258.96] loss=0.74 avg=0.86\n",
            "[18944 | 3262.02] loss=0.94 avg=0.86\n",
            "[18945 | 3265.09] loss=0.84 avg=0.86\n",
            "[18946 | 3268.12] loss=0.81 avg=0.86\n",
            "[18947 | 3271.17] loss=0.90 avg=0.86\n",
            "[18948 | 3274.23] loss=0.91 avg=0.86\n",
            "[18949 | 3277.28] loss=0.69 avg=0.86\n",
            "[18950 | 3280.32] loss=0.87 avg=0.86\n",
            "[18951 | 3283.39] loss=0.86 avg=0.86\n",
            "[18952 | 3286.46] loss=0.90 avg=0.86\n",
            "[18953 | 3289.52] loss=1.01 avg=0.86\n",
            "[18954 | 3292.57] loss=0.90 avg=0.86\n",
            "[18955 | 3295.64] loss=0.80 avg=0.86\n",
            "[18956 | 3298.71] loss=0.82 avg=0.86\n",
            "[18957 | 3301.77] loss=0.71 avg=0.86\n",
            "[18958 | 3304.83] loss=0.83 avg=0.86\n",
            "[18959 | 3307.91] loss=0.87 avg=0.86\n",
            "[18960 | 3310.98] loss=0.95 avg=0.86\n",
            "[18961 | 3314.06] loss=0.88 avg=0.86\n",
            "[18962 | 3317.12] loss=0.68 avg=0.86\n",
            "[18963 | 3320.19] loss=0.77 avg=0.85\n",
            "[18964 | 3323.27] loss=1.00 avg=0.86\n",
            "[18965 | 3326.33] loss=0.93 avg=0.86\n",
            "[18966 | 3329.39] loss=0.81 avg=0.86\n",
            "[18967 | 3332.45] loss=0.78 avg=0.86\n",
            "[18968 | 3335.51] loss=0.84 avg=0.86\n",
            "[18969 | 3338.57] loss=0.90 avg=0.86\n",
            "[18970 | 3341.63] loss=0.73 avg=0.85\n",
            "[18971 | 3344.70] loss=0.81 avg=0.85\n",
            "[18972 | 3347.77] loss=0.89 avg=0.85\n",
            "[18973 | 3350.84] loss=0.79 avg=0.85\n",
            "[18974 | 3353.90] loss=0.83 avg=0.85\n",
            "[18975 | 3356.97] loss=0.96 avg=0.85\n",
            "[18976 | 3360.04] loss=0.75 avg=0.85\n",
            "[18977 | 3363.12] loss=0.79 avg=0.85\n",
            "[18978 | 3366.19] loss=0.66 avg=0.85\n",
            "[18979 | 3369.26] loss=0.92 avg=0.85\n",
            "[18980 | 3372.33] loss=0.62 avg=0.85\n",
            "[18981 | 3375.39] loss=0.92 avg=0.85\n",
            "[18982 | 3378.44] loss=0.73 avg=0.85\n",
            "[18983 | 3381.52] loss=0.91 avg=0.85\n",
            "[18984 | 3384.59] loss=0.85 avg=0.85\n",
            "[18985 | 3387.68] loss=0.87 avg=0.85\n",
            "[18986 | 3390.75] loss=0.97 avg=0.85\n",
            "[18987 | 3393.83] loss=0.98 avg=0.85\n",
            "[18988 | 3396.90] loss=0.75 avg=0.85\n",
            "[18989 | 3399.96] loss=0.82 avg=0.85\n",
            "[18990 | 3403.03] loss=0.66 avg=0.85\n",
            "[18991 | 3406.09] loss=0.94 avg=0.85\n",
            "[18992 | 3409.16] loss=0.95 avg=0.85\n",
            "[18993 | 3412.25] loss=0.86 avg=0.85\n",
            "[18994 | 3415.32] loss=0.90 avg=0.85\n",
            "[18995 | 3418.39] loss=0.82 avg=0.85\n",
            "[18996 | 3421.46] loss=1.06 avg=0.85\n",
            "[18997 | 3424.52] loss=0.81 avg=0.85\n",
            "[18998 | 3427.60] loss=0.93 avg=0.85\n",
            "[18999 | 3430.68] loss=0.87 avg=0.85\n",
            "Saving /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-19000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "Just tell me the truth! = Solo diga la verdad.\n",
            "What's the biggest city in Europe? = ¿Cuál es la ciudad más grande de Europa?\n",
            "My cousin is one of the three girls that I take to the club every year. = Mi primo es una de las tres niñas que me llevan a la escuela cada año.\n",
            "I'm not a very good musician. = No soy un músico muy bueno.\n",
            "Don't go so fast, will you please stop? = No vaya tan gira, ¿quieres de marcho?\n",
            "The man who was shot was never to see another day in his life. = El hombre que fue disparado fue nunca verno nunca a ver a un día.\n",
            "It's your only chance. = Es tu única oportunidad.\n",
            "I love the smell of coffee. = Me gusta el olor de café.\n",
            "His plan sounds good to me. = Su plan me parece interesante.\n",
            "I'd rather go with Tom. = Preferiría ir con Tom.\n",
            "He's the one who feeds our dog. = Él es el que alimenta a nuestro perro.\n",
            "I'll give you some advice. = Te diré algunos consejos.\n",
            "The only reason Tom gave for not going was that he didn't have enough money. = Lo único razón por la que Tomás le di paisaje por no quedarse suficiente dinero.\n",
            "You don't need to hide your feelings. = No tienes que ocultar tus estribas.\n",
            "I need to do this right now. = Necesito hacer esto ahora mismo.\n",
            "What did Tom do then? = ¿Qué hizo Tom entonces?\n",
            "Tom knows that Mary is in love with John. = Tom sabe que Mary está enamorada de John.\n",
            "I don't give favors to people because I expect them to do my work for me. = No dices fuerzas a la gente porque esperaba que hagas mi trabajo meza a mí.\n",
            "Tom wants to know what you did. = Tom quiere saber qué hiciste.\n",
            "I'd try to understand. = Yo trataría a entender.\n",
            "Tom isn't coming back. = Tom no va a volver.\n",
            "Tom's car got caught in a mudslide and he lost control. = El carro de Tom se ha capturado en una escalola y se perdió con el control.\n",
            "This is all Tom's fault. = Este es culpa de Tom.\n",
            "He was not there. = No estaba allí.\n",
            "I am studying law at the moment. = Yo estoy estudiando la ley en este momento.\n",
            "Are you seriously thinking about selling this on eBay? = ¿Estás pensando seriamente en vender esto en eBay?\n",
            "That's all. = Es todo.\n",
            "It makes little difference. = No tiene ninguna dificultad.\n",
            "I'm looking for my car keys. = Busco las llaves de mi coche.\n",
            "He did the work by himself. = Él hizo por sí solo.\n",
            "I think she's a famous actress. = Creo que ella es una actriz famosa.\n",
            "You won't get there on time, and I won't either. = No llegarás ahí a tiempo, y yo tampoco.\n",
            "I don't want any chocolate ice cream. = No quiero helado de chocolate.\n",
            "She had a strange dream last night. = Ella tuvo un sueño extraño anoche.\n",
            "It's too short. = Es demasiado corto.\n",
            "I don't think it's something that would happen in this world. = No pienso que es algo que suceda en este mundo.\n",
            "All the players bowed in respect. = Todos los jugadores inclinaron bien.\n",
            "I want to go with Tom. = Quiero ir con Tom.\n",
            "Tom told Mary that he would explain the matter to her father. = Tom le dijo a Mary que les explicaría el asunto a su padre.\n",
            "It's difficult to be a native speaker in that town. = Es difícil ser hablar un hablante nativo en esa ciudad.\n",
            "We will never understand. = No lo lamentamos nunca.\n",
            "I'll wait until 2:30. = Esperaré hasta las dos y\n",
            "\n",
            "[19000 | 3487.97] loss=0.92 avg=0.85\n",
            "[19001 | 3491.04] loss=0.99 avg=0.86\n",
            "[19002 | 3494.09] loss=0.82 avg=0.85\n",
            "[19003 | 3497.15] loss=0.86 avg=0.85\n",
            "[19004 | 3500.21] loss=0.94 avg=0.86\n",
            "[19005 | 3503.27] loss=0.96 avg=0.86\n",
            "[19006 | 3506.34] loss=0.93 avg=0.86\n",
            "[19007 | 3509.40] loss=0.85 avg=0.86\n",
            "[19008 | 3512.47] loss=0.85 avg=0.86\n",
            "[19009 | 3515.53] loss=0.82 avg=0.86\n",
            "[19010 | 3518.59] loss=0.87 avg=0.86\n",
            "[19011 | 3521.66] loss=0.86 avg=0.86\n",
            "[19012 | 3524.74] loss=0.85 avg=0.86\n",
            "[19013 | 3527.81] loss=0.89 avg=0.86\n",
            "[19014 | 3530.88] loss=0.84 avg=0.86\n",
            "[19015 | 3533.95] loss=0.93 avg=0.86\n",
            "[19016 | 3537.03] loss=0.83 avg=0.86\n",
            "[19017 | 3540.08] loss=0.96 avg=0.86\n",
            "[19018 | 3543.16] loss=0.75 avg=0.86\n",
            "[19019 | 3546.22] loss=0.87 avg=0.86\n",
            "[19020 | 3549.30] loss=0.86 avg=0.86\n",
            "[19021 | 3552.35] loss=0.69 avg=0.86\n",
            "[19022 | 3555.43] loss=0.90 avg=0.86\n",
            "[19023 | 3558.52] loss=0.83 avg=0.86\n",
            "[19024 | 3561.59] loss=0.88 avg=0.86\n",
            "[19025 | 3564.66] loss=0.72 avg=0.86\n",
            "[19026 | 3567.72] loss=0.81 avg=0.85\n",
            "[19027 | 3570.80] loss=0.94 avg=0.86\n",
            "[19028 | 3573.84] loss=0.88 avg=0.86\n",
            "[19029 | 3576.90] loss=0.89 avg=0.86\n",
            "[19030 | 3579.97] loss=0.86 avg=0.86\n",
            "[19031 | 3583.02] loss=0.93 avg=0.86\n",
            "[19032 | 3586.09] loss=0.80 avg=0.86\n",
            "[19033 | 3589.13] loss=0.89 avg=0.86\n",
            "[19034 | 3592.19] loss=0.82 avg=0.86\n",
            "[19035 | 3595.26] loss=0.83 avg=0.86\n",
            "[19036 | 3598.33] loss=0.92 avg=0.86\n",
            "[19037 | 3601.40] loss=0.97 avg=0.86\n",
            "[19038 | 3604.48] loss=0.79 avg=0.86\n",
            "[19039 | 3607.55] loss=0.73 avg=0.86\n",
            "[19040 | 3610.62] loss=0.95 avg=0.86\n",
            "[19041 | 3613.69] loss=0.88 avg=0.86\n",
            "[19042 | 3616.73] loss=0.80 avg=0.86\n",
            "[19043 | 3619.78] loss=0.82 avg=0.86\n",
            "[19044 | 3622.86] loss=0.92 avg=0.86\n",
            "[19045 | 3625.92] loss=0.89 avg=0.86\n",
            "[19046 | 3628.98] loss=0.91 avg=0.86\n",
            "[19047 | 3632.04] loss=0.96 avg=0.86\n",
            "[19048 | 3635.10] loss=0.88 avg=0.86\n",
            "[19049 | 3638.15] loss=1.01 avg=0.86\n",
            "[19050 | 3641.20] loss=0.88 avg=0.86\n",
            "[19051 | 3644.26] loss=0.91 avg=0.86\n",
            "[19052 | 3647.31] loss=0.85 avg=0.86\n",
            "[19053 | 3650.37] loss=0.84 avg=0.86\n",
            "[19054 | 3653.42] loss=0.84 avg=0.86\n",
            "[19055 | 3656.50] loss=0.73 avg=0.86\n",
            "[19056 | 3659.55] loss=0.86 avg=0.86\n",
            "[19057 | 3662.62] loss=0.88 avg=0.86\n",
            "[19058 | 3665.68] loss=0.76 avg=0.86\n",
            "[19059 | 3668.76] loss=0.88 avg=0.86\n",
            "[19060 | 3671.83] loss=0.89 avg=0.86\n",
            "[19061 | 3674.90] loss=0.91 avg=0.86\n",
            "[19062 | 3677.98] loss=0.93 avg=0.86\n",
            "[19063 | 3681.05] loss=0.90 avg=0.86\n",
            "[19064 | 3684.13] loss=0.99 avg=0.86\n",
            "[19065 | 3687.20] loss=0.94 avg=0.86\n",
            "[19066 | 3690.26] loss=0.86 avg=0.86\n",
            "[19067 | 3693.34] loss=0.90 avg=0.86\n",
            "[19068 | 3696.42] loss=0.94 avg=0.86\n",
            "[19069 | 3699.49] loss=0.70 avg=0.86\n",
            "[19070 | 3702.57] loss=0.94 avg=0.86\n",
            "[19071 | 3705.63] loss=0.87 avg=0.86\n",
            "[19072 | 3708.71] loss=0.80 avg=0.86\n",
            "[19073 | 3711.79] loss=0.99 avg=0.86\n",
            "[19074 | 3714.86] loss=0.81 avg=0.86\n",
            "[19075 | 3717.92] loss=0.85 avg=0.86\n",
            "[19076 | 3721.00] loss=0.77 avg=0.86\n",
            "[19077 | 3724.06] loss=0.91 avg=0.86\n",
            "[19078 | 3727.13] loss=0.89 avg=0.86\n",
            "[19079 | 3730.21] loss=0.84 avg=0.86\n",
            "[19080 | 3733.28] loss=0.81 avg=0.86\n",
            "[19081 | 3736.34] loss=0.80 avg=0.86\n",
            "[19082 | 3739.40] loss=0.90 avg=0.86\n",
            "[19083 | 3742.46] loss=0.67 avg=0.86\n",
            "[19084 | 3745.53] loss=0.80 avg=0.86\n",
            "[19085 | 3748.59] loss=0.76 avg=0.86\n",
            "[19086 | 3751.66] loss=0.90 avg=0.86\n",
            "[19087 | 3754.73] loss=0.91 avg=0.86\n",
            "[19088 | 3757.79] loss=0.81 avg=0.86\n",
            "[19089 | 3760.86] loss=0.90 avg=0.86\n",
            "[19090 | 3763.94] loss=0.90 avg=0.86\n",
            "[19091 | 3767.01] loss=1.00 avg=0.86\n",
            "[19092 | 3770.08] loss=0.84 avg=0.86\n",
            "[19093 | 3773.14] loss=0.80 avg=0.86\n",
            "[19094 | 3776.21] loss=1.05 avg=0.86\n",
            "[19095 | 3779.28] loss=0.87 avg=0.86\n",
            "[19096 | 3782.37] loss=0.84 avg=0.86\n",
            "[19097 | 3785.44] loss=0.95 avg=0.86\n",
            "[19098 | 3788.52] loss=0.92 avg=0.86\n",
            "[19099 | 3791.59] loss=0.90 avg=0.86\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " obja.\n",
            "This is too much. = Esto es demasiado.\n",
            "Tom hasn't paid any attention. = Tom no ha pagado ninguna atención.\n",
            "Tom wants to visit England. = Tom quiere visitar Inglaterra.\n",
            "Tom is a talented musician. = Tom es un músico talentoso.\n",
            "He did it to spite me. = Lo hizo para molestarme.\n",
            "What time does the train for New York depart? = ¿A qué hora parte el tren a Nueva York?\n",
            "Tom and Mary are very happy together. = Tom y Mary están muy felices juntos.\n",
            "Tom should've accepted the offer. = Tom debería haber habido la oferta.\n",
            "You have only to call them. = Solo tienes que llamarla.\n",
            "What time do you go to bed? = ¿A qué hora os van a dormir?\n",
            "She loves him. = Ella lo ama.\n",
            "They are going to Paris next Sunday. = Ellos van a París el domingo que viene.\n",
            "This is not a joke. = Esto no es broma.\n",
            "I have to take the entrance examination today. = Debo tomar el examen de ingreso hoy.\n",
            "We didn't know where to go. = No sabíamos adentro.\n",
            "The situation escalated quickly. = La situación acusa veloz.\n",
            "I was very scared. = Estaba muy asustada.\n",
            "The dog was run over by a truck. = Un carro atropelló al perro.\n",
            "Why are you so tired today? = ¿Por qué estás tan cansada hoy?\n",
            "I know you're upset. = Sé que está molesto.\n",
            "Do you want to eat? = ¿Quieres comer?\n",
            "Tom put the package on the table. = Tom puso el paquete en la mesa.\n",
            "I had to get a band-aid on one of the cuts. = Tuve que traer una bala en uno de las círculas.\n",
            "This medicine has cured three times of rheumatism. = Este medicamento cura tres veces de agenería.\n",
            "Somebody's in the bathroom. = Sabía que alguien está en el baño.\n",
            "You're smarter than that. = Eres más listo que eso.\n",
            "I'm tired of all this noise. = Estoy cansando de todo este ruido.\n",
            "It will rain soon. = Pronto lloverá.\n",
            "That's the problem. = Eso es el problema.\n",
            "It's a real possibility. = Es un auténtico real.\n",
            "She used to work as a maid at a castle. = Ella solía trabajar como asesina en un castle.\n",
            "Your watch is less than one meter. = Tu reloj de poco es menos de una mesa.\n",
            "I was just here last Tuesday. = Estuve aquí apenado el martes pasado.\n",
            "Tom's mother is a very good cook. = La madre de Tom es una muy buena cocinera.\n",
            "Do not go into the jungle alone. = No vayas a la jungla solos.\n",
            "I can't believe Tom is still in love with Mary. = No me puedo creer que Tom siga amar a Mary.\n",
            "Can you speak a little louder please? = Hable ya un poco más alto, por favor.\n",
            "My name's Tom. = Soy Tom.\n",
            "I thought the old man told me when he was going to die. = Pensé que el anciano me contó cuando iba a morir.\n",
            "You need more practice. = Tienes que más práctica.\n",
            "They have nothing against terrorism. = No tienen nada en contra de el terrorisma.\n",
            "I'm not a Canadian. = No soy canadiense.\n",
            "Where's the best place to eat Chinese food? = ¿Dónde está el mejor lugar para comer comida china?\n",
            "I wish you a pleasant journey. = Te deseo un bien viaje.\n",
            "You don't seem particularly pleased. = Tú no pareces especialmente complacido.\n",
            "We were disappointed in that. = Nos decepcionamos en eso.\n",
            "I can't say anything about that. = No puedo decir nada acerca de eso.\n",
            "I'm not in a hurry. = No tengo prisa.\n",
            "I am ashamed of you. = Me\n",
            "\n",
            "[19100 | 3831.66] loss=0.98 avg=0.87\n",
            "[19101 | 3834.73] loss=0.91 avg=0.87\n",
            "[19102 | 3837.78] loss=0.82 avg=0.87\n",
            "[19103 | 3840.84] loss=0.86 avg=0.87\n",
            "[19104 | 3843.90] loss=0.92 avg=0.87\n",
            "[19105 | 3846.95] loss=0.70 avg=0.86\n",
            "[19106 | 3850.01] loss=0.91 avg=0.86\n",
            "[19107 | 3853.06] loss=0.88 avg=0.86\n",
            "[19108 | 3856.11] loss=0.81 avg=0.86\n",
            "[19109 | 3859.15] loss=0.78 avg=0.86\n",
            "[19110 | 3862.21] loss=0.97 avg=0.86\n",
            "[19111 | 3865.26] loss=0.89 avg=0.86\n",
            "[19112 | 3868.32] loss=0.91 avg=0.87\n",
            "[19113 | 3871.38] loss=0.65 avg=0.86\n",
            "[19114 | 3874.44] loss=0.82 avg=0.86\n",
            "[19115 | 3877.49] loss=0.72 avg=0.86\n",
            "[19116 | 3880.54] loss=0.93 avg=0.86\n",
            "[19117 | 3883.61] loss=0.82 avg=0.86\n",
            "[19118 | 3886.67] loss=0.75 avg=0.86\n",
            "[19119 | 3889.74] loss=1.01 avg=0.86\n",
            "[19120 | 3892.80] loss=0.74 avg=0.86\n",
            "[19121 | 3895.86] loss=0.77 avg=0.86\n",
            "[19122 | 3898.90] loss=0.88 avg=0.86\n",
            "[19123 | 3901.95] loss=0.97 avg=0.86\n",
            "[19124 | 3905.00] loss=0.80 avg=0.86\n",
            "[19125 | 3908.07] loss=0.85 avg=0.86\n",
            "[19126 | 3911.14] loss=0.80 avg=0.86\n",
            "[19127 | 3914.21] loss=0.91 avg=0.86\n",
            "[19128 | 3917.28] loss=0.83 avg=0.86\n",
            "[19129 | 3920.33] loss=0.71 avg=0.86\n",
            "[19130 | 3923.40] loss=0.84 avg=0.86\n",
            "[19131 | 3926.47] loss=0.73 avg=0.86\n",
            "[19132 | 3929.52] loss=0.76 avg=0.86\n",
            "[19133 | 3932.58] loss=0.94 avg=0.86\n",
            "[19134 | 3935.64] loss=0.79 avg=0.86\n",
            "[19135 | 3938.70] loss=0.85 avg=0.86\n",
            "[19136 | 3941.77] loss=0.81 avg=0.86\n",
            "[19137 | 3944.83] loss=0.93 avg=0.86\n",
            "[19138 | 3947.90] loss=0.80 avg=0.86\n",
            "[19139 | 3950.96] loss=0.89 avg=0.86\n",
            "[19140 | 3954.02] loss=0.92 avg=0.86\n",
            "[19141 | 3957.08] loss=0.89 avg=0.86\n",
            "[19142 | 3960.14] loss=0.87 avg=0.86\n",
            "[19143 | 3963.21] loss=0.90 avg=0.86\n",
            "[19144 | 3966.28] loss=0.88 avg=0.86\n",
            "[19145 | 3969.37] loss=0.77 avg=0.86\n",
            "[19146 | 3972.47] loss=0.91 avg=0.86\n",
            "[19147 | 3975.56] loss=0.90 avg=0.86\n",
            "[19148 | 3978.64] loss=0.94 avg=0.86\n",
            "[19149 | 3981.73] loss=0.79 avg=0.86\n",
            "[19150 | 3984.84] loss=0.90 avg=0.86\n",
            "[19151 | 3987.93] loss=0.85 avg=0.86\n",
            "[19152 | 3991.03] loss=0.86 avg=0.86\n",
            "[19153 | 3994.13] loss=0.81 avg=0.86\n",
            "[19154 | 3997.22] loss=0.85 avg=0.86\n",
            "[19155 | 4000.30] loss=0.72 avg=0.86\n",
            "[19156 | 4003.39] loss=1.05 avg=0.86\n",
            "[19157 | 4006.48] loss=0.85 avg=0.86\n",
            "[19158 | 4009.59] loss=0.84 avg=0.86\n",
            "[19159 | 4012.68] loss=0.89 avg=0.86\n",
            "[19160 | 4015.76] loss=0.79 avg=0.86\n",
            "[19161 | 4018.85] loss=0.89 avg=0.86\n",
            "[19162 | 4021.94] loss=0.90 avg=0.86\n",
            "[19163 | 4024.99] loss=0.68 avg=0.86\n",
            "[19164 | 4028.08] loss=0.85 avg=0.86\n",
            "[19165 | 4031.17] loss=0.90 avg=0.86\n",
            "[19166 | 4034.27] loss=0.85 avg=0.86\n",
            "[19167 | 4037.37] loss=0.73 avg=0.86\n",
            "[19168 | 4040.46] loss=0.90 avg=0.86\n",
            "[19169 | 4043.57] loss=0.74 avg=0.86\n",
            "[19170 | 4046.68] loss=0.89 avg=0.86\n",
            "[19171 | 4049.77] loss=0.87 avg=0.86\n",
            "[19172 | 4052.86] loss=0.82 avg=0.86\n",
            "[19173 | 4055.96] loss=0.94 avg=0.86\n",
            "[19174 | 4059.06] loss=0.84 avg=0.86\n",
            "[19175 | 4062.16] loss=0.93 avg=0.86\n",
            "[19176 | 4065.26] loss=0.90 avg=0.86\n",
            "[19177 | 4068.34] loss=0.64 avg=0.85\n",
            "[19178 | 4071.42] loss=0.81 avg=0.85\n",
            "[19179 | 4074.52] loss=0.75 avg=0.85\n",
            "[19180 | 4077.63] loss=0.75 avg=0.85\n",
            "[19181 | 4080.73] loss=0.88 avg=0.85\n",
            "[19182 | 4083.83] loss=0.76 avg=0.85\n",
            "[19183 | 4086.93] loss=0.91 avg=0.85\n",
            "[19184 | 4090.02] loss=0.81 avg=0.85\n",
            "[19185 | 4093.10] loss=0.82 avg=0.85\n",
            "[19186 | 4096.18] loss=0.83 avg=0.85\n",
            "[19187 | 4099.27] loss=0.74 avg=0.85\n",
            "[19188 | 4102.36] loss=0.75 avg=0.85\n",
            "[19189 | 4105.45] loss=0.76 avg=0.85\n",
            "[19190 | 4108.55] loss=0.73 avg=0.85\n",
            "[19191 | 4111.64] loss=0.76 avg=0.85\n",
            "[19192 | 4114.73] loss=0.89 avg=0.85\n",
            "[19193 | 4117.83] loss=0.68 avg=0.85\n",
            "[19194 | 4120.92] loss=0.94 avg=0.85\n",
            "[19195 | 4124.02] loss=0.89 avg=0.85\n",
            "[19196 | 4127.11] loss=0.82 avg=0.85\n",
            "[19197 | 4130.20] loss=0.80 avg=0.85\n",
            "[19198 | 4133.29] loss=0.89 avg=0.85\n",
            "[19199 | 4136.38] loss=0.93 avg=0.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " de año.\n",
            "The weatherman was silent during the bad weather. = El hombre hizo callado durante el mal clima.\n",
            "I've finished reading the book. = Terminé de leer el libro.\n",
            "The plane made an emergency landing. = El avión hizo un aterrizaje a emergencias.\n",
            "Do you think he likes being rich? = ¿Usted cree que le gusta ser rica?\n",
            "He's taller than me. = Él es más alto que yo.\n",
            "You look beautiful tonight. = Esta noche te ves bien.\n",
            "Tom wants to go with you. = Tom quiere ir contigo.\n",
            "Tom and Mary were talking to each other when the phone rang. = Tom y Mary estaban conversando al conferencia cuando sonó el teléfono.\n",
            "Tom told Mary what John did. = Tom le dijo a Mary lo que John hizo John.\n",
            "We need to take care of ourselves. = Tenemos que cuidarnos.\n",
            "Let's have another beer. = Tomemos otra cerveza.\n",
            "I thought we knew each other. = Pensaba que nos conocíamos.\n",
            "The sky was as black as pitch. = El cielo estaba tan blanco como el pulpo.\n",
            "I will get Tom to listen to me. = Me voy a hacer que Tom me escuche.\n",
            "I got up a few minutes ago. = Me levanté hace unos minutos.\n",
            "Tom and Mary have different philosophies of life. = Tom y Mary tienen diferentes filosofías de vida.\n",
            "They're my friends. = Es mis amigos.\n",
            "I didn't have many friends. = No tenía muchos amigos.\n",
            "It will not be long before we can go on a vacation in the United States. = No tardará mucho para que podemos ir de vacaciones de Estados Unidos.\n",
            "What happened? = ¿Qué ha pasado?\n",
            "I'm a little crazy. = Estoy un poco loca.\n",
            "Do you want to see me again? = ¿Quieres verme de nuevo?\n",
            "I'm happy to have this homework. = Estoy contento de tener este tarea.\n",
            "This is the reason he came here. = Esa es la razón por la que vine aquí.\n",
            "He was not here when you called. = Él no estaba aquí cuando llamaste.\n",
            "He was too shy to talk to girls. = Él era demasiado tímido para hablarle a las niñas.\n",
            "Tom should've called. = Tom debería haber llamado.\n",
            "I know you. = Estos yo.\n",
            "You're the person I've been looking for. = Sos la persona que estaba buscando.\n",
            "The dog is always barking. = El perro siempre ladra.\n",
            "He had a terrible hangover. = Tuvo una resaca terrible.\n",
            "I bought this hat yesterday. = Compré esta coroba ayer.\n",
            "Let's put these flowers on the desk. = Paras necesitándoles en el escritorio.\n",
            "I think I've lost them. = Creo que me los dejé.\n",
            "This is how I wash dishes. = Así es como me lavo la cámara.\n",
            "I wish you wouldn't speak ill of others. = Desearía que no hables de otros.\n",
            "My husband is in his forties. = Mi marido tiene tres deits.\n",
            "A thousand yen will cover the cost of this trip. = Mil yenes socués a través de esta viaje.\n",
            "It's almost three. = Son casi tres.\n",
            "She's good at gardening. = Ella es buena agricole.\n",
            "I can't find him anywhere. = No puedo encontrarlos en ningún sitio.\n",
            "We need your key. = Necesitamos su tarjeta.\n",
            "We had a short holiday. = Pasamos unos cortos cortos.\n",
            "My father is a man of few words. = Mi padre es un hombre de pocas palabras.\n",
            "He had to pay for the books. = Él tuvo que pagar los libros.\n",
            "We have no reason for his arrest. = No hay motivo para su arresto.\n",
            "The teacher told the boy that he should not open the book. = El profesor le dijo a al niño que no debería abrir el libro.\n",
            "\n",
            "\n",
            "[19200 | 4177.00] loss=0.85 avg=0.85\n",
            "[19201 | 4180.10] loss=0.86 avg=0.85\n",
            "[19202 | 4183.20] loss=0.74 avg=0.85\n",
            "[19203 | 4186.29] loss=0.91 avg=0.85\n",
            "[19204 | 4189.39] loss=0.89 avg=0.85\n",
            "[19205 | 4192.49] loss=0.69 avg=0.85\n",
            "[19206 | 4195.57] loss=0.87 avg=0.85\n",
            "[19207 | 4198.67] loss=0.77 avg=0.84\n",
            "[19208 | 4201.78] loss=0.86 avg=0.85\n",
            "[19209 | 4204.84] loss=1.02 avg=0.85\n",
            "[19210 | 4207.91] loss=0.87 avg=0.85\n",
            "[19211 | 4210.98] loss=0.90 avg=0.85\n",
            "[19212 | 4214.05] loss=0.80 avg=0.85\n",
            "[19213 | 4217.09] loss=0.88 avg=0.85\n",
            "[19214 | 4220.18] loss=0.72 avg=0.85\n",
            "[19215 | 4223.28] loss=0.93 avg=0.85\n",
            "[19216 | 4226.38] loss=1.06 avg=0.85\n",
            "[19217 | 4229.48] loss=0.97 avg=0.85\n",
            "[19218 | 4232.58] loss=0.91 avg=0.85\n",
            "[19219 | 4235.67] loss=0.97 avg=0.85\n",
            "[19220 | 4238.74] loss=0.82 avg=0.85\n",
            "[19221 | 4241.84] loss=0.88 avg=0.85\n",
            "[19222 | 4244.93] loss=0.86 avg=0.85\n",
            "[19223 | 4248.03] loss=0.87 avg=0.85\n",
            "[19224 | 4251.11] loss=0.72 avg=0.85\n",
            "[19225 | 4254.21] loss=0.74 avg=0.85\n",
            "[19226 | 4257.29] loss=0.95 avg=0.85\n",
            "[19227 | 4260.39] loss=0.78 avg=0.85\n",
            "[19228 | 4263.47] loss=0.92 avg=0.85\n",
            "[19229 | 4266.57] loss=0.80 avg=0.85\n",
            "[19230 | 4269.66] loss=0.88 avg=0.85\n",
            "[19231 | 4272.76] loss=0.84 avg=0.85\n",
            "[19232 | 4275.84] loss=0.85 avg=0.85\n",
            "[19233 | 4278.93] loss=0.93 avg=0.85\n",
            "[19234 | 4282.02] loss=0.86 avg=0.85\n",
            "[19235 | 4285.12] loss=0.82 avg=0.85\n",
            "[19236 | 4288.21] loss=0.72 avg=0.85\n",
            "[19237 | 4291.30] loss=0.86 avg=0.85\n",
            "[19238 | 4294.40] loss=0.90 avg=0.85\n",
            "[19239 | 4297.48] loss=0.92 avg=0.85\n",
            "[19240 | 4300.58] loss=0.71 avg=0.85\n",
            "[19241 | 4303.69] loss=0.81 avg=0.85\n",
            "[19242 | 4306.79] loss=0.61 avg=0.85\n",
            "[19243 | 4309.88] loss=0.71 avg=0.85\n",
            "[19244 | 4312.98] loss=0.80 avg=0.84\n",
            "[19245 | 4316.07] loss=1.07 avg=0.85\n",
            "[19246 | 4319.15] loss=0.86 avg=0.85\n",
            "[19247 | 4322.25] loss=0.91 avg=0.85\n",
            "[19248 | 4325.34] loss=1.06 avg=0.85\n",
            "[19249 | 4328.44] loss=0.97 avg=0.85\n",
            "[19250 | 4331.54] loss=0.77 avg=0.85\n",
            "[19251 | 4334.62] loss=0.86 avg=0.85\n",
            "[19252 | 4337.68] loss=0.83 avg=0.85\n",
            "[19253 | 4340.75] loss=0.93 avg=0.85\n",
            "[19254 | 4343.82] loss=0.78 avg=0.85\n",
            "[19255 | 4346.89] loss=0.78 avg=0.85\n",
            "[19256 | 4349.95] loss=0.71 avg=0.85\n",
            "[19257 | 4353.03] loss=0.92 avg=0.85\n",
            "[19258 | 4356.11] loss=0.87 avg=0.85\n",
            "[19259 | 4359.16] loss=0.83 avg=0.85\n",
            "[19260 | 4362.22] loss=0.81 avg=0.85\n",
            "[19261 | 4365.28] loss=0.71 avg=0.85\n",
            "[19262 | 4368.35] loss=0.74 avg=0.85\n",
            "[19263 | 4371.42] loss=0.83 avg=0.85\n",
            "[19264 | 4374.50] loss=0.85 avg=0.85\n",
            "[19265 | 4377.56] loss=0.87 avg=0.85\n",
            "[19266 | 4380.63] loss=0.85 avg=0.85\n",
            "[19267 | 4383.69] loss=0.88 avg=0.85\n",
            "[19268 | 4386.77] loss=0.92 avg=0.85\n",
            "[19269 | 4389.86] loss=0.97 avg=0.85\n",
            "[19270 | 4392.93] loss=0.83 avg=0.85\n",
            "[19271 | 4395.99] loss=0.93 avg=0.85\n",
            "[19272 | 4399.07] loss=0.81 avg=0.85\n",
            "[19273 | 4402.12] loss=0.82 avg=0.85\n",
            "[19274 | 4405.19] loss=0.87 avg=0.85\n",
            "[19275 | 4408.27] loss=0.74 avg=0.85\n",
            "[19276 | 4411.36] loss=0.81 avg=0.85\n",
            "[19277 | 4414.43] loss=0.67 avg=0.85\n",
            "[19278 | 4417.50] loss=0.90 avg=0.85\n",
            "[19279 | 4420.58] loss=0.89 avg=0.85\n",
            "[19280 | 4423.65] loss=0.89 avg=0.85\n",
            "[19281 | 4426.73] loss=0.91 avg=0.85\n",
            "[19282 | 4429.80] loss=0.86 avg=0.85\n",
            "[19283 | 4432.87] loss=0.89 avg=0.85\n",
            "[19284 | 4435.93] loss=0.87 avg=0.85\n",
            "[19285 | 4439.00] loss=0.88 avg=0.85\n",
            "[19286 | 4442.07] loss=0.94 avg=0.85\n",
            "[19287 | 4445.14] loss=0.85 avg=0.85\n",
            "[19288 | 4448.21] loss=0.95 avg=0.85\n",
            "[19289 | 4451.28] loss=0.89 avg=0.85\n",
            "[19290 | 4454.35] loss=0.86 avg=0.85\n",
            "[19291 | 4457.42] loss=0.75 avg=0.85\n",
            "[19292 | 4460.48] loss=0.94 avg=0.85\n",
            "[19293 | 4463.55] loss=0.98 avg=0.85\n",
            "[19294 | 4466.61] loss=0.87 avg=0.85\n",
            "[19295 | 4469.67] loss=0.85 avg=0.85\n",
            "[19296 | 4472.74] loss=0.75 avg=0.85\n",
            "[19297 | 4475.82] loss=0.90 avg=0.85\n",
            "[19298 | 4478.90] loss=0.78 avg=0.85\n",
            "[19299 | 4481.95] loss=0.90 avg=0.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "I saw him yesterday for the very first time. = Lo vi ayer por una vez por veces.\n",
            "What are these for? = ¿Para qué son éstos?\n",
            "She turned off the television. = Ella apagó la televisión.\n",
            "It's on the table. = Está en la mesa.\n",
            "Tom was the best player on the team. = Tom era el mejor playero del equipo.\n",
            "You'll understand soon. = Entenderás pronto.\n",
            "I'm happy you could come. = Estoy contento de que entres.\n",
            "Why are you so worried? = ¿Por qué estás tan preocupada?\n",
            "You'd better go. = Mejor te vayas.\n",
            "I'm not in a rush. = No estoy de prisa.\n",
            "She is as big as I am. = Ella es tan hubo como yo.\n",
            "He is not that kind of guy. = Él no es esa clase de tipo.\n",
            "A stranger approached them. = Les se acercó un extraño.\n",
            "You're not the only one who's worried. = No sos el único que está preocupado.\n",
            "I really wish I could figure out how to get more people to visit my website. = Realmente desearía saber adelante que pueda averiguar cómo hacer más personas visitar mi sitio web.\n",
            "Who killed Tom? = ¿Quién mató a Tom?\n",
            "Don't go in that building. There's a fire. = No entres en ese edificio, hay un incendio.\n",
            "I don't have anything to give Tom. = No tengo nada que darle a Tom.\n",
            "I don't know your real purpose in life. = No sé su verdadero propósito en la vida.\n",
            "When he heard the whistle, he knew instantly that somebody had been shot. = Cuando oyó el guto, él sabió de un instante que alguien hubiera sufrido.\n",
            "Tom can sing like a baritone. = Tom puede cantar como un barco.\n",
            "I need a new one. = Necesito una nueva.\n",
            "He is the last man I expected to meet. = Es la última persona que esperaba encontrártelo.\n",
            "Do that while wearing your socks. = Haz eso mientras llevas los calcetines.\n",
            "Tom doesn't want anyone to know he was here. = Tom no quiere que nadie supiera que yo estaba aquí.\n",
            "Please take care of this dog. = Por favor, cuida de este perro.\n",
            "Tom isn't much older than Mary, but they're both really shy. = Tom no es mucho mayor que Mary, pero son dos muy tímidos.\n",
            "I have to find out where Tom lives. = Tengo que averiguar dónde vive Tom.\n",
            "I'll meet you upstairs. = Te veré arriba.\n",
            "I've been to Boston for four days. = Nehe como a Boston por cuatro días.\n",
            "The doctor told Tom that he had a cold. = El médico le dijo a Tom que tenía un resfriado.\n",
            "I really like Italian food. = Me gusta mucho la comida italiana.\n",
            "You're not allowed to use mine. = Usted no consume el cabo.\n",
            "I like eating Chinese food. = Me gusta comer comida china.\n",
            "Don't you like him? = ¿No te gusta?\n",
            "Can we talk? = ¿Podemos hablar?\n",
            "You can't put a price on that. = No le puedes poner un precio a eso.\n",
            "You were stupid. = Eras estúpido.\n",
            "I'm sure she'll succeed. = Estoy seguro de que ella triunfará.\n",
            "He looked his age. = Él menó de su edad.\n",
            "Where did you buy that dress? = ¿Dónde compraste ese vestido?\n",
            "Tom went to see Mary. = Tom fue a ver a Mary.\n",
            "We are going back home. = Vamos a volver a casa.\n",
            "Don't trust him. = No se fío en él.\n",
            "Tom didn't feel like telling Mary the bad news. = Tom no tenía ganas de denunciarle la mala noticia.\n",
            "My grandmother always sings in the car. = Mi abuela siempre canta\n",
            "\n",
            "[19300 | 4522.73] loss=0.79 avg=0.85\n",
            "[19301 | 4525.79] loss=0.79 avg=0.85\n",
            "[19302 | 4528.85] loss=0.95 avg=0.85\n",
            "[19303 | 4531.90] loss=0.94 avg=0.85\n",
            "[19304 | 4534.95] loss=0.81 avg=0.85\n",
            "[19305 | 4537.99] loss=0.91 avg=0.85\n",
            "[19306 | 4541.05] loss=0.98 avg=0.85\n",
            "[19307 | 4544.11] loss=0.72 avg=0.85\n",
            "[19308 | 4547.18] loss=0.87 avg=0.85\n",
            "[19309 | 4550.24] loss=0.81 avg=0.85\n",
            "[19310 | 4553.29] loss=0.77 avg=0.85\n",
            "[19311 | 4556.36] loss=0.76 avg=0.85\n",
            "[19312 | 4559.43] loss=0.90 avg=0.85\n",
            "[19313 | 4562.50] loss=0.72 avg=0.85\n",
            "[19314 | 4565.57] loss=0.85 avg=0.85\n",
            "[19315 | 4568.65] loss=0.88 avg=0.85\n",
            "[19316 | 4571.72] loss=0.97 avg=0.85\n",
            "[19317 | 4574.79] loss=0.94 avg=0.85\n",
            "[19318 | 4577.85] loss=0.79 avg=0.85\n",
            "[19319 | 4580.92] loss=0.89 avg=0.85\n",
            "[19320 | 4583.98] loss=0.62 avg=0.85\n",
            "[19321 | 4587.04] loss=0.95 avg=0.85\n",
            "[19322 | 4590.11] loss=0.92 avg=0.85\n",
            "[19323 | 4593.17] loss=0.69 avg=0.85\n",
            "[19324 | 4596.25] loss=1.00 avg=0.85\n",
            "[19325 | 4599.33] loss=0.94 avg=0.85\n",
            "[19326 | 4602.39] loss=0.91 avg=0.85\n",
            "[19327 | 4605.46] loss=0.79 avg=0.85\n",
            "[19328 | 4608.54] loss=0.77 avg=0.85\n",
            "[19329 | 4611.60] loss=0.73 avg=0.85\n",
            "[19330 | 4614.67] loss=0.69 avg=0.85\n",
            "[19331 | 4617.75] loss=0.82 avg=0.85\n",
            "[19332 | 4620.81] loss=0.90 avg=0.85\n",
            "[19333 | 4623.89] loss=0.77 avg=0.85\n",
            "[19334 | 4626.96] loss=0.76 avg=0.85\n",
            "[19335 | 4630.03] loss=0.88 avg=0.85\n",
            "[19336 | 4633.11] loss=0.88 avg=0.85\n",
            "[19337 | 4636.18] loss=0.80 avg=0.85\n",
            "[19338 | 4639.28] loss=0.77 avg=0.85\n",
            "[19339 | 4642.37] loss=0.75 avg=0.85\n",
            "[19340 | 4645.46] loss=0.82 avg=0.85\n",
            "[19341 | 4648.54] loss=0.67 avg=0.84\n",
            "[19342 | 4651.63] loss=0.83 avg=0.84\n",
            "[19343 | 4654.74] loss=0.91 avg=0.84\n",
            "[19344 | 4657.83] loss=0.85 avg=0.84\n",
            "[19345 | 4660.91] loss=0.80 avg=0.84\n",
            "[19346 | 4664.00] loss=0.91 avg=0.84\n",
            "[19347 | 4667.09] loss=0.89 avg=0.84\n",
            "[19348 | 4670.18] loss=0.86 avg=0.84\n",
            "[19349 | 4673.27] loss=0.79 avg=0.84\n",
            "[19350 | 4676.34] loss=0.87 avg=0.84\n",
            "[19351 | 4679.43] loss=0.94 avg=0.85\n",
            "[19352 | 4682.51] loss=0.80 avg=0.85\n",
            "[19353 | 4685.60] loss=0.86 avg=0.85\n",
            "[19354 | 4688.69] loss=0.74 avg=0.84\n",
            "[19355 | 4691.78] loss=0.89 avg=0.84\n",
            "[19356 | 4694.88] loss=0.81 avg=0.84\n",
            "[19357 | 4697.95] loss=0.97 avg=0.85\n",
            "[19358 | 4701.04] loss=0.91 avg=0.85\n",
            "[19359 | 4704.13] loss=0.84 avg=0.85\n",
            "[19360 | 4707.20] loss=1.06 avg=0.85\n",
            "[19361 | 4710.27] loss=1.07 avg=0.85\n",
            "[19362 | 4713.34] loss=0.76 avg=0.85\n",
            "[19363 | 4716.43] loss=0.97 avg=0.85\n",
            "[19364 | 4719.52] loss=0.77 avg=0.85\n",
            "[19365 | 4722.60] loss=0.92 avg=0.85\n",
            "[19366 | 4725.68] loss=0.89 avg=0.85\n",
            "[19367 | 4728.75] loss=0.74 avg=0.85\n",
            "[19368 | 4731.82] loss=0.86 avg=0.85\n",
            "[19369 | 4734.91] loss=0.97 avg=0.85\n",
            "[19370 | 4737.99] loss=0.93 avg=0.85\n",
            "[19371 | 4741.07] loss=0.90 avg=0.85\n",
            "[19372 | 4744.15] loss=1.05 avg=0.85\n",
            "[19373 | 4747.23] loss=0.86 avg=0.85\n",
            "[19374 | 4750.32] loss=0.94 avg=0.86\n",
            "[19375 | 4753.40] loss=0.78 avg=0.85\n",
            "[19376 | 4756.49] loss=0.83 avg=0.85\n",
            "[19377 | 4759.57] loss=0.71 avg=0.85\n",
            "[19378 | 4762.66] loss=0.97 avg=0.85\n",
            "[19379 | 4765.74] loss=0.88 avg=0.85\n",
            "[19380 | 4768.81] loss=0.71 avg=0.85\n",
            "[19381 | 4771.90] loss=0.76 avg=0.85\n",
            "[19382 | 4774.99] loss=0.92 avg=0.85\n",
            "[19383 | 4778.08] loss=0.85 avg=0.85\n",
            "[19384 | 4781.17] loss=0.92 avg=0.85\n",
            "[19385 | 4784.24] loss=0.90 avg=0.85\n",
            "[19386 | 4787.32] loss=0.77 avg=0.85\n",
            "[19387 | 4790.41] loss=0.75 avg=0.85\n",
            "[19388 | 4793.48] loss=0.92 avg=0.85\n",
            "[19389 | 4796.55] loss=0.93 avg=0.85\n",
            "[19390 | 4799.64] loss=0.82 avg=0.85\n",
            "[19391 | 4802.72] loss=0.86 avg=0.85\n",
            "[19392 | 4805.80] loss=0.83 avg=0.85\n",
            "[19393 | 4808.88] loss=0.91 avg=0.85\n",
            "[19394 | 4811.97] loss=0.93 avg=0.85\n",
            "[19395 | 4815.04] loss=0.82 avg=0.85\n",
            "[19396 | 4818.08] loss=0.86 avg=0.85\n",
            "[19397 | 4821.16] loss=0.83 avg=0.85\n",
            "[19398 | 4824.24] loss=0.69 avg=0.85\n",
            "[19399 | 4827.29] loss=0.61 avg=0.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "es.\n",
            "My name is Tom. = Mi nombre es Tom.\n",
            "You have no friends to talk with. = No tienes amigos con los que charlar.\n",
            "You should see this film if you get the opportunity. = Deberías ver una película si tuviese la ocasión.\n",
            "If I had known her history would have changed. = Si hubiera sabido sabía la historia.\n",
            "It's raining again. = Vuelve llover.\n",
            "I do like it. = Me gusta.\n",
            "I am now as tall as she is. = Ahora soy tan alto como ella.\n",
            "Do you think Tom was in the meeting? = ¿Pensás que Tom estaba en la reunión?\n",
            "Tom never gave up. = Tom nunca se rindió.\n",
            "I feel fine. = Yo me siento bien.\n",
            "The door won't budge. = La puerta no se moverá.\n",
            "I'll take care of your dog while you're away. = Yo cuidaré de tu perro cuando estés fuera.\n",
            "Tom had nothing to eat. = Tom no tenía nada de comer.\n",
            "Tom doesn't have the energy to keep trying. = Tom no tiene energy para seguir intentando.\n",
            "Tom and Mary met on their school days. = Tom y Mary se conocieron en sus días de escuela.\n",
            "Come right in. = Entre.\n",
            "It doesn't really matter, does it? = En realidad no le importa, ¿o vo?\n",
            "I'll make you happy. = Te haré feliz.\n",
            "I need to know something. = Necesito saber algo.\n",
            "Do you want to talk about it? = ¿Querés hablar al respecto?\n",
            "I want you to help us. = Quiero que lo ayucan.\n",
            "I've been waiting a long time for this. = Estaba esperando mucho tiempo por esto.\n",
            "When it rained, my grandfather used to bathe in the bathroom. = Cuando duó la niebla, mi abuelo solía bañar en el baño.\n",
            "Tom should start right away. = Tom debería empezar inmediatamente.\n",
            "I'm not good at math. = No soy buena en matemáticas.\n",
            "It doesn't surprise me. = Yo no me sorprende.\n",
            "He's a man you can rely on. = Él es un hombre en el que puedes confiar.\n",
            "It's already Friday. = Ya es viernes.\n",
            "When we came across his house, we shook hands with him. = Cuando se nos conocía su casa échamos la mano, dieron la mano con él.\n",
            "All things taken into consideration, my father is the best piano teacher I've ever had. = Yo todos lo toques invitaron a considerar a mi padre, a su madre es la mejor mojado que he había tenido nunca.\n",
            "Tom has a beautiful wife. = Tom tiene una hermosa cheza.\n",
            "Tom could tell Mary was nervous because she could hear birds flying in the distance. = Tom podría decir que María estaba nerviosa porque podía oír a los pájaros cerca del lejan.\n",
            "If I'd had time to do that, I would've. = Si hubiera teniado tiempo para hacerlo, lo habría hecho.\n",
            "I'll give it to you. = Os lo daré.\n",
            "She didn't go to the office. = Ella no fue a la oficina.\n",
            "Can we go? = ¿Podemos marcharnos?\n",
            "Is this a hoax? = ¿Esto es un engaño?\n",
            "I am tired from working too much. = Estoy cansado por trabajar demasiado.\n",
            "I didn't expect it to be so easy to do. = No esperaba que fuera tan fácil de hacer.\n",
            "I'll start taking notes in August. = Regresaré cuando tomarme aprenderme por verano.\n",
            "The cat frightened us badly. = El gato nos asustó.\n",
            "I thought this job was easy, but it was actually quite the work. = Pensé que ese trabajo era fácil, pero era fue un trabajo muy fácil.\n",
            "I've never met her. = Nunca la he conocido a ella.\n",
            "He is in a\n",
            "\n",
            "[19400 | 4868.38] loss=0.98 avg=0.85\n",
            "[19401 | 4871.49] loss=0.86 avg=0.85\n",
            "[19402 | 4874.58] loss=0.89 avg=0.85\n",
            "[19403 | 4877.66] loss=0.74 avg=0.85\n",
            "[19404 | 4880.73] loss=0.73 avg=0.85\n",
            "[19405 | 4883.80] loss=0.83 avg=0.85\n",
            "[19406 | 4886.86] loss=0.79 avg=0.85\n",
            "[19407 | 4889.94] loss=0.82 avg=0.85\n",
            "[19408 | 4893.02] loss=0.92 avg=0.85\n",
            "[19409 | 4896.09] loss=0.86 avg=0.85\n",
            "[19410 | 4899.17] loss=0.95 avg=0.85\n",
            "[19411 | 4902.23] loss=0.86 avg=0.85\n",
            "[19412 | 4905.31] loss=0.54 avg=0.85\n",
            "[19413 | 4908.39] loss=0.74 avg=0.85\n",
            "[19414 | 4911.46] loss=0.79 avg=0.84\n",
            "[19415 | 4914.52] loss=0.70 avg=0.84\n",
            "[19416 | 4917.58] loss=0.99 avg=0.84\n",
            "[19417 | 4920.64] loss=0.87 avg=0.85\n",
            "[19418 | 4923.70] loss=0.77 avg=0.84\n",
            "[19419 | 4926.77] loss=0.91 avg=0.85\n",
            "[19420 | 4929.87] loss=0.90 avg=0.85\n",
            "[19421 | 4932.98] loss=0.82 avg=0.85\n",
            "[19422 | 4936.05] loss=0.92 avg=0.85\n",
            "[19423 | 4939.14] loss=0.85 avg=0.85\n",
            "[19424 | 4942.23] loss=0.84 avg=0.85\n",
            "[19425 | 4945.31] loss=0.72 avg=0.84\n",
            "[19426 | 4948.39] loss=0.78 avg=0.84\n",
            "[19427 | 4951.46] loss=0.94 avg=0.84\n",
            "[19428 | 4954.56] loss=0.87 avg=0.85\n",
            "[19429 | 4957.65] loss=0.84 avg=0.85\n",
            "[19430 | 4960.74] loss=0.83 avg=0.84\n",
            "[19431 | 4963.84] loss=0.82 avg=0.84\n",
            "[19432 | 4966.93] loss=0.79 avg=0.84\n",
            "[19433 | 4970.01] loss=0.85 avg=0.84\n",
            "[19434 | 4973.08] loss=0.99 avg=0.85\n",
            "[19435 | 4976.15] loss=0.88 avg=0.85\n",
            "[19436 | 4979.22] loss=0.84 avg=0.85\n",
            "[19437 | 4982.30] loss=0.84 avg=0.85\n",
            "[19438 | 4985.38] loss=0.94 avg=0.85\n",
            "[19439 | 4988.45] loss=0.74 avg=0.85\n",
            "[19440 | 4991.52] loss=0.92 avg=0.85\n",
            "[19441 | 4994.60] loss=0.89 avg=0.85\n",
            "[19442 | 4997.68] loss=0.82 avg=0.85\n",
            "[19443 | 5000.77] loss=0.80 avg=0.85\n",
            "[19444 | 5003.87] loss=0.76 avg=0.85\n",
            "[19445 | 5006.95] loss=0.92 avg=0.85\n",
            "[19446 | 5010.03] loss=0.86 avg=0.85\n",
            "[19447 | 5013.12] loss=0.93 avg=0.85\n",
            "[19448 | 5016.20] loss=0.88 avg=0.85\n",
            "[19449 | 5019.29] loss=0.88 avg=0.85\n",
            "[19450 | 5022.36] loss=0.85 avg=0.85\n",
            "[19451 | 5025.43] loss=0.90 avg=0.85\n",
            "[19452 | 5028.50] loss=0.75 avg=0.85\n",
            "[19453 | 5031.59] loss=0.80 avg=0.85\n",
            "[19454 | 5034.66] loss=0.88 avg=0.85\n",
            "[19455 | 5037.72] loss=0.80 avg=0.85\n",
            "[19456 | 5040.80] loss=0.94 avg=0.85\n",
            "[19457 | 5043.90] loss=0.76 avg=0.85\n",
            "[19458 | 5046.98] loss=0.95 avg=0.85\n",
            "[19459 | 5050.06] loss=0.74 avg=0.85\n",
            "[19460 | 5053.16] loss=0.78 avg=0.85\n",
            "[19461 | 5056.24] loss=0.97 avg=0.85\n",
            "[19462 | 5059.33] loss=0.86 avg=0.85\n",
            "[19463 | 5062.42] loss=0.75 avg=0.85\n",
            "[19464 | 5065.51] loss=0.67 avg=0.84\n",
            "[19465 | 5068.58] loss=0.84 avg=0.84\n",
            "[19466 | 5071.65] loss=0.67 avg=0.84\n",
            "[19467 | 5074.73] loss=0.88 avg=0.84\n",
            "[19468 | 5077.82] loss=0.77 avg=0.84\n",
            "[19469 | 5080.90] loss=0.68 avg=0.84\n",
            "[19470 | 5083.99] loss=0.75 avg=0.84\n",
            "[19471 | 5087.09] loss=0.87 avg=0.84\n",
            "[19472 | 5090.16] loss=0.78 avg=0.84\n",
            "[19473 | 5093.25] loss=0.62 avg=0.84\n",
            "[19474 | 5096.35] loss=0.81 avg=0.84\n",
            "[19475 | 5099.44] loss=0.73 avg=0.84\n",
            "[19476 | 5102.51] loss=0.74 avg=0.84\n",
            "[19477 | 5105.59] loss=0.85 avg=0.84\n",
            "[19478 | 5108.67] loss=0.90 avg=0.84\n",
            "[19479 | 5111.73] loss=0.89 avg=0.84\n",
            "[19480 | 5114.81] loss=1.19 avg=0.84\n",
            "[19481 | 5117.89] loss=0.91 avg=0.84\n",
            "[19482 | 5120.96] loss=1.03 avg=0.84\n",
            "[19483 | 5124.04] loss=0.84 avg=0.84\n",
            "[19484 | 5127.11] loss=0.73 avg=0.84\n",
            "[19485 | 5130.20] loss=0.79 avg=0.84\n",
            "[19486 | 5133.29] loss=0.92 avg=0.84\n",
            "[19487 | 5136.37] loss=0.79 avg=0.84\n",
            "[19488 | 5139.45] loss=0.73 avg=0.84\n",
            "[19489 | 5142.51] loss=0.68 avg=0.84\n",
            "[19490 | 5145.59] loss=0.90 avg=0.84\n",
            "[19491 | 5148.68] loss=0.74 avg=0.84\n",
            "[19492 | 5151.76] loss=0.78 avg=0.84\n",
            "[19493 | 5154.85] loss=0.90 avg=0.84\n",
            "[19494 | 5157.93] loss=0.95 avg=0.84\n",
            "[19495 | 5160.99] loss=0.78 avg=0.84\n",
            "[19496 | 5164.04] loss=0.93 avg=0.84\n",
            "[19497 | 5167.11] loss=0.78 avg=0.84\n",
            "[19498 | 5170.18] loss=0.73 avg=0.84\n",
            "[19499 | 5173.23] loss=0.93 avg=0.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " pisado?\n",
            "Can you remember anything? = ¿Puedes recordar algo?\n",
            "Tom told his children they didn't need to call their father. = Tom les dijo a sus hijos que no necesitaban llamar a su padre.\n",
            "It's not my fault. = No es culpa mía.\n",
            "Tom and Mary looked at each other, puzzled, then at each other. = Tom y María se miraron el uno al otro, desorientados, entre el otro o por el otro.\n",
            "What happened to my book? = ¿Qué le pasó a mi libro?\n",
            "I'll tell you what I did today. = Le diré lo que hice hoy.\n",
            "There is a long line at every cash register. = Hay una larga cola en cada una de las cuenta registradas.\n",
            "Tom didn't want to read this book. = Tom no quería leer este libro.\n",
            "Tom got back into Mary's truck. = Tom regresó en el camión de Mary.\n",
            "Tom wants it back. = Tom lo quiere de vuelta.\n",
            "Mary is very pretty, but she's also very cruel. = Mary es muy guapa, pero también es muy cruel.\n",
            "My grandmother lived to be ninety-five years old. = Mi abuela vivió hasta los noventa y cinco.\n",
            "I don't care. = No importa.\n",
            "Don't trust Tom. = No confíes en Tom.\n",
            "I'd like to be a reporter. = Quisiera ser un periodista.\n",
            "Mary looked up at the sky. = Mary echó la cabeza al cielo.\n",
            "Do you want me to be cruel or to be cruel? = ¿Quieres que sea cruel o está cruel?\n",
            "There are some apples in the basket. = Hay algunas manzanas en la canasta.\n",
            "I went to school with an \"A\". = Fui al mismo \"A\".\n",
            "Tom and Mary are sitting on the couch. = Tom y Mary están sentados en el sofá.\n",
            "It is very hot in the summer in Japan. = Hace mucho calor en el verano en Japón.\n",
            "Is English spoken in Australia? = ¿Es acento inglés en Australia?\n",
            "My son asked me how Tom did it. = Mi hijo me preguntó está cómo lo hizo Tom.\n",
            "He's my stepfather. = Él es mi padrastro.\n",
            "You shouldn't have made me wait that long. = No deberías haberme hincé por eso.\n",
            "She wants to go. = Ella quiere ir.\n",
            "She always reads without studying. = Ella empeora sin estudiar.\n",
            "The boy caught a cricket. = El chico cogió un alballo.\n",
            "Tom didn't expect to be greeted that way. = Tom no esperaba que se llamaran de esa manera.\n",
            "They are always complaining. = Siempre se están quejando.\n",
            "I'm coming for you. = Vengo por tú.\n",
            "No matter how old he is, he never falls in love with a girl over thirty. = No importa cuántos años vaya a amar a una niña más religión, él nunca cayó enamorado de una niña más treinta.\n",
            "The police are interrogating him. = La policía lo está interrogando.\n",
            "The sky will soon clear up. = El cielo se aclarará pronto.\n",
            "It looked like the dog had been bitten by a bee. = Parecía que el perro había recibido una abeja.\n",
            "I have been in Boston for more than six months. = Yo he estado en Boston por más seis meses.\n",
            "This was Tom's idea. = Esto fue idea de Tom.\n",
            "Let's move on. = Espera adelante.\n",
            "It's possible that Tom saw what you did. = Puede que Tom ha visto lo que tú has visto.\n",
            "We had a great time. = Lo pasamos bien.\n",
            "Don't eat too much cake. = No coma tanto torta.\n",
            "She went without shoes. = Ella se fue sin zapatos.\n",
            "I didn't want to do that. = No quería hacerlo.\n",
            "He has two cars. = Tiene dos coches.\n",
            "It's a beautiful day, isn't it? = Ha pasado un hermoso día\n",
            "\n",
            "[19500 | 5213.26] loss=0.83 avg=0.84\n",
            "[19501 | 5216.35] loss=0.87 avg=0.84\n",
            "[19502 | 5219.42] loss=0.72 avg=0.84\n",
            "[19503 | 5222.48] loss=0.89 avg=0.84\n",
            "[19504 | 5225.54] loss=0.77 avg=0.84\n",
            "[19505 | 5228.62] loss=0.97 avg=0.84\n",
            "[19506 | 5231.68] loss=0.89 avg=0.84\n",
            "[19507 | 5234.74] loss=0.96 avg=0.84\n",
            "[19508 | 5237.80] loss=0.91 avg=0.84\n",
            "[19509 | 5240.86] loss=0.83 avg=0.84\n",
            "[19510 | 5243.94] loss=0.83 avg=0.84\n",
            "[19511 | 5247.01] loss=0.95 avg=0.84\n",
            "[19512 | 5250.09] loss=0.82 avg=0.84\n",
            "[19513 | 5253.15] loss=0.82 avg=0.84\n",
            "[19514 | 5256.24] loss=0.81 avg=0.84\n",
            "[19515 | 5259.32] loss=0.66 avg=0.84\n",
            "[19516 | 5262.40] loss=0.93 avg=0.84\n",
            "[19517 | 5265.49] loss=0.86 avg=0.84\n",
            "[19518 | 5268.57] loss=0.88 avg=0.84\n",
            "[19519 | 5271.67] loss=0.77 avg=0.84\n",
            "[19520 | 5274.77] loss=0.87 avg=0.84\n",
            "[19521 | 5277.85] loss=1.02 avg=0.84\n",
            "[19522 | 5280.94] loss=0.83 avg=0.84\n",
            "[19523 | 5284.04] loss=0.76 avg=0.84\n",
            "[19524 | 5287.12] loss=0.92 avg=0.84\n",
            "[19525 | 5290.21] loss=0.77 avg=0.84\n",
            "[19526 | 5293.28] loss=0.93 avg=0.84\n",
            "[19527 | 5296.36] loss=0.63 avg=0.84\n",
            "[19528 | 5299.45] loss=0.78 avg=0.84\n",
            "[19529 | 5302.55] loss=0.84 avg=0.84\n",
            "[19530 | 5305.63] loss=0.91 avg=0.84\n",
            "[19531 | 5308.74] loss=0.74 avg=0.84\n",
            "[19532 | 5311.82] loss=0.85 avg=0.84\n",
            "[19533 | 5314.91] loss=0.80 avg=0.84\n",
            "[19534 | 5318.00] loss=0.72 avg=0.84\n",
            "[19535 | 5321.09] loss=0.85 avg=0.84\n",
            "[19536 | 5324.16] loss=0.73 avg=0.84\n",
            "[19537 | 5327.23] loss=1.04 avg=0.84\n",
            "[19538 | 5330.30] loss=0.92 avg=0.84\n",
            "[19539 | 5333.37] loss=0.74 avg=0.84\n",
            "[19540 | 5336.42] loss=0.91 avg=0.84\n",
            "[19541 | 5339.51] loss=0.87 avg=0.84\n",
            "[19542 | 5342.58] loss=0.86 avg=0.84\n",
            "[19543 | 5345.65] loss=0.78 avg=0.84\n",
            "[19544 | 5348.73] loss=0.76 avg=0.84\n",
            "[19545 | 5351.80] loss=0.63 avg=0.84\n",
            "[19546 | 5354.88] loss=0.95 avg=0.84\n",
            "[19547 | 5357.94] loss=0.89 avg=0.84\n",
            "[19548 | 5361.02] loss=0.81 avg=0.84\n",
            "[19549 | 5364.09] loss=0.73 avg=0.84\n",
            "[19550 | 5367.17] loss=0.87 avg=0.84\n",
            "[19551 | 5370.23] loss=0.76 avg=0.84\n",
            "[19552 | 5373.32] loss=0.66 avg=0.83\n",
            "[19553 | 5376.42] loss=0.85 avg=0.84\n",
            "[19554 | 5379.50] loss=0.85 avg=0.84\n",
            "[19555 | 5382.58] loss=0.86 avg=0.84\n",
            "[19556 | 5385.65] loss=0.82 avg=0.84\n",
            "[19557 | 5388.73] loss=0.95 avg=0.84\n",
            "[19558 | 5391.81] loss=0.84 avg=0.84\n",
            "[19559 | 5394.89] loss=0.85 avg=0.84\n",
            "[19560 | 5397.98] loss=0.78 avg=0.84\n",
            "[19561 | 5401.06] loss=0.85 avg=0.84\n",
            "[19562 | 5404.15] loss=0.90 avg=0.84\n",
            "[19563 | 5407.22] loss=0.69 avg=0.84\n",
            "[19564 | 5410.32] loss=0.85 avg=0.84\n",
            "[19565 | 5413.41] loss=0.90 avg=0.84\n",
            "[19566 | 5416.46] loss=0.85 avg=0.84\n",
            "[19567 | 5419.53] loss=1.00 avg=0.84\n",
            "[19568 | 5422.61] loss=0.83 avg=0.84\n",
            "[19569 | 5425.71] loss=0.67 avg=0.84\n",
            "[19570 | 5428.81] loss=0.82 avg=0.84\n",
            "[19571 | 5431.91] loss=0.84 avg=0.84\n",
            "[19572 | 5435.01] loss=0.90 avg=0.84\n",
            "[19573 | 5438.12] loss=0.62 avg=0.83\n",
            "[19574 | 5441.21] loss=0.76 avg=0.83\n",
            "[19575 | 5444.30] loss=0.93 avg=0.83\n",
            "[19576 | 5447.41] loss=0.88 avg=0.84\n",
            "[19577 | 5450.50] loss=0.83 avg=0.84\n",
            "[19578 | 5453.58] loss=0.75 avg=0.83\n",
            "[19579 | 5456.67] loss=0.73 avg=0.83\n",
            "[19580 | 5459.78] loss=0.86 avg=0.83\n",
            "[19581 | 5462.89] loss=0.76 avg=0.83\n",
            "[19582 | 5466.00] loss=0.73 avg=0.83\n",
            "[19583 | 5469.10] loss=0.85 avg=0.83\n",
            "[19584 | 5472.21] loss=0.70 avg=0.83\n",
            "[19585 | 5475.31] loss=0.97 avg=0.83\n",
            "[19586 | 5478.42] loss=0.79 avg=0.83\n",
            "[19587 | 5481.50] loss=0.87 avg=0.83\n",
            "[19588 | 5484.59] loss=0.66 avg=0.83\n",
            "[19589 | 5487.67] loss=0.91 avg=0.83\n",
            "[19590 | 5490.76] loss=0.73 avg=0.83\n",
            "[19591 | 5493.84] loss=0.74 avg=0.83\n",
            "[19592 | 5496.92] loss=0.98 avg=0.83\n",
            "[19593 | 5500.00] loss=0.83 avg=0.83\n",
            "[19594 | 5503.08] loss=0.74 avg=0.83\n",
            "[19595 | 5506.17] loss=0.81 avg=0.83\n",
            "[19596 | 5509.24] loss=0.96 avg=0.83\n",
            "[19597 | 5512.33] loss=0.77 avg=0.83\n",
            "[19598 | 5515.41] loss=0.90 avg=0.83\n",
            "[19599 | 5518.47] loss=0.86 avg=0.83\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " you have a problem with. = Tengo problemas con un problema.\n",
            "I am trying to find out a secret to surviving as a horse rider. = Estoy tratando de encontrar un secreto para sobrevivir como saddleo de caballo.\n",
            "Why would Tom want to help Mary? = ¿Por qué Tom querría ayudar a María?\n",
            "I'm afraid you have to go tomorrow. = Me preocupas que tienes que marchir mañana.\n",
            "How many times a month do you write your diary? = ¿Cuántas veces al mes escribes tu diario?\n",
            "The two of them got into a fight. = Los dos se pusieron una pelea.\n",
            "The fire burned for three hours. = El fuego cerró durante tres horas.\n",
            "My room has only one window. = Mi cuarto tiene una sola ventana.\n",
            "The rain ended. = La lluvia se acabó.\n",
            "There is a little girl playing on the street. = Hay una niña juega en la calle.\n",
            "I can't stand noisy children. = No soporto los chicos son unsos.\n",
            "We did it. = Lo hicimos.\n",
            "I know your parents very well. = Conozco muy bien a tus padres.\n",
            "Tom is the man you should really date. = Tom es el hombre conocemos muy interesante.\n",
            "I wanted to speak with you about this. = Quería hablar contigo sobre esto.\n",
            "I'm not interested in her advice. = No estoy interesada en su consejo.\n",
            "Don't talk so loud. = No hables tan fuerte.\n",
            "We must take care of the dog once and for all. = Tenemos que cuidar de pero de nosotros una vidas.\n",
            "The car he's driving is not his. = El auto que él conduce no es suyo.\n",
            "It seemed that I couldn't play the piano because I'm not a singer. = Parecía que no piscía tocar el piano porque no soy cantante.\n",
            "Tom could've been murdered if he knew the truth. = Tom podría haber sido asesinado si sabía la verdad.\n",
            "Why is it important? = ¿Por qué es importante?\n",
            "Some of these are mine. = Algunas de éstas son mías.\n",
            "My brother took my camera. = Mi hermano cogió mi cámara.\n",
            "That's a lot of information. = Eso es mucho información.\n",
            "He wants to work. = Quiere trabajar.\n",
            "Tom did it to impress Mary. = Tom lo hizo para impresionar a Mary.\n",
            "I don't mind the heat. = No me molesta el calor.\n",
            "Can you make the cuter ones with less red? = ¿Puedes hacer los más chasquillas con menos rojo?\n",
            "There must have been a third party involved in this terrible crime. = Tiene que haber fuegiado una tercera crimen espantoso.\n",
            "Tom has told me plenty. = Tom ha dicho mucho.\n",
            "She is not his age. = Ella no es su edad.\n",
            "We're here to serve you. = Estamos aquí para servirles.\n",
            "She was wearing a hat. = Ella llevaba un sombrero.\n",
            "The girl left the classroom in order to smoke a cigarette. = La chica dejó salir de clases para fumar un cigarrillo.\n",
            "How long are you going? = ¿Cuánto tiempo vas a gustar?\n",
            "The sun shines in this picture. = El sol brilla en esta fotografía.\n",
            "I have a stomachache. = Me duele la barriga.\n",
            "You don't know what it's like to lose your entire family in a war. = No sabís lo que es perder a la familia entera en una guerra.\n",
            "Is that the reason you're not here today? = ¿Es ésa la razón por esa lugar no esta hoy?\n",
            "The doctor suggested that she return to her country. = El médico sugirió que regresara a su país.\n",
            "Tom knew this was going to happen. = Tom sabía que iba a pasar esto.\n",
            "What time do you get up every day? = ¿A qué hora te levantas\n",
            "\n",
            "[19600 | 5558.90] loss=0.92 avg=0.83\n",
            "[19601 | 5561.98] loss=0.86 avg=0.83\n",
            "[19602 | 5565.07] loss=0.83 avg=0.83\n",
            "[19603 | 5568.14] loss=0.81 avg=0.83\n",
            "[19604 | 5571.22] loss=0.84 avg=0.83\n",
            "[19605 | 5574.32] loss=0.87 avg=0.83\n",
            "[19606 | 5577.40] loss=0.73 avg=0.83\n",
            "[19607 | 5580.48] loss=0.85 avg=0.83\n",
            "[19608 | 5583.57] loss=0.91 avg=0.83\n",
            "[19609 | 5586.66] loss=0.78 avg=0.83\n",
            "[19610 | 5589.75] loss=0.86 avg=0.83\n",
            "[19611 | 5592.86] loss=0.88 avg=0.83\n",
            "[19612 | 5595.95] loss=0.77 avg=0.83\n",
            "[19613 | 5599.04] loss=0.95 avg=0.83\n",
            "[19614 | 5602.12] loss=0.80 avg=0.83\n",
            "[19615 | 5605.20] loss=0.82 avg=0.83\n",
            "[19616 | 5608.30] loss=0.86 avg=0.83\n",
            "[19617 | 5611.37] loss=0.82 avg=0.83\n",
            "[19618 | 5614.46] loss=0.83 avg=0.83\n",
            "[19619 | 5617.54] loss=0.71 avg=0.83\n",
            "[19620 | 5620.64] loss=0.75 avg=0.83\n",
            "[19621 | 5623.71] loss=0.76 avg=0.83\n",
            "[19622 | 5626.82] loss=0.88 avg=0.83\n",
            "[19623 | 5629.90] loss=0.90 avg=0.83\n",
            "[19624 | 5632.99] loss=0.94 avg=0.83\n",
            "[19625 | 5636.07] loss=0.64 avg=0.83\n",
            "[19626 | 5639.16] loss=0.97 avg=0.83\n",
            "[19627 | 5642.25] loss=0.75 avg=0.83\n",
            "[19628 | 5645.34] loss=0.81 avg=0.83\n",
            "[19629 | 5648.42] loss=0.88 avg=0.83\n",
            "[19630 | 5651.51] loss=0.89 avg=0.83\n",
            "[19631 | 5654.59] loss=0.77 avg=0.83\n",
            "[19632 | 5657.63] loss=0.95 avg=0.83\n",
            "[19633 | 5660.70] loss=0.92 avg=0.83\n",
            "[19634 | 5663.77] loss=0.85 avg=0.83\n",
            "[19635 | 5666.82] loss=0.94 avg=0.83\n",
            "[19636 | 5669.88] loss=0.72 avg=0.83\n",
            "[19637 | 5672.94] loss=0.91 avg=0.83\n",
            "[19638 | 5676.00] loss=0.84 avg=0.83\n",
            "[19639 | 5679.06] loss=0.98 avg=0.84\n",
            "[19640 | 5682.12] loss=0.76 avg=0.83\n",
            "[19641 | 5685.19] loss=0.95 avg=0.84\n",
            "[19642 | 5688.25] loss=0.91 avg=0.84\n",
            "[19643 | 5691.32] loss=0.82 avg=0.84\n",
            "[19644 | 5694.40] loss=0.76 avg=0.84\n",
            "[19645 | 5697.48] loss=0.75 avg=0.83\n",
            "[19646 | 5700.55] loss=0.98 avg=0.84\n",
            "[19647 | 5703.62] loss=0.78 avg=0.84\n",
            "[19648 | 5706.70] loss=0.83 avg=0.84\n",
            "[19649 | 5709.77] loss=0.87 avg=0.84\n",
            "[19650 | 5712.83] loss=0.76 avg=0.84\n",
            "[19651 | 5715.91] loss=0.79 avg=0.83\n",
            "[19652 | 5718.98] loss=0.83 avg=0.83\n",
            "[19653 | 5722.06] loss=0.77 avg=0.83\n",
            "[19654 | 5725.14] loss=0.89 avg=0.83\n",
            "[19655 | 5728.20] loss=0.84 avg=0.83\n",
            "[19656 | 5731.26] loss=1.02 avg=0.84\n",
            "[19657 | 5734.34] loss=0.91 avg=0.84\n",
            "[19658 | 5737.42] loss=0.79 avg=0.84\n",
            "[19659 | 5740.52] loss=0.75 avg=0.84\n",
            "[19660 | 5743.62] loss=0.98 avg=0.84\n",
            "[19661 | 5746.70] loss=0.84 avg=0.84\n",
            "[19662 | 5749.77] loss=0.76 avg=0.84\n",
            "[19663 | 5752.84] loss=0.89 avg=0.84\n",
            "[19664 | 5755.90] loss=0.67 avg=0.84\n",
            "[19665 | 5758.97] loss=0.86 avg=0.84\n",
            "[19666 | 5762.03] loss=0.95 avg=0.84\n",
            "[19667 | 5765.08] loss=0.86 avg=0.84\n",
            "[19668 | 5768.13] loss=0.73 avg=0.84\n",
            "[19669 | 5771.20] loss=0.94 avg=0.84\n",
            "[19670 | 5774.30] loss=0.73 avg=0.84\n",
            "[19671 | 5777.37] loss=0.85 avg=0.84\n",
            "[19672 | 5780.46] loss=0.91 avg=0.84\n",
            "[19673 | 5783.57] loss=0.92 avg=0.84\n",
            "[19674 | 5786.66] loss=0.88 avg=0.84\n",
            "[19675 | 5789.74] loss=0.90 avg=0.84\n",
            "[19676 | 5792.81] loss=0.76 avg=0.84\n",
            "[19677 | 5795.88] loss=0.85 avg=0.84\n",
            "[19678 | 5798.95] loss=0.96 avg=0.84\n",
            "[19679 | 5802.03] loss=0.70 avg=0.84\n",
            "[19680 | 5805.11] loss=0.92 avg=0.84\n",
            "[19681 | 5808.17] loss=0.73 avg=0.84\n",
            "[19682 | 5811.24] loss=0.80 avg=0.84\n",
            "[19683 | 5814.31] loss=0.65 avg=0.84\n",
            "[19684 | 5817.38] loss=0.83 avg=0.84\n",
            "[19685 | 5820.47] loss=0.76 avg=0.84\n",
            "[19686 | 5823.53] loss=0.92 avg=0.84\n",
            "[19687 | 5826.60] loss=0.85 avg=0.84\n",
            "[19688 | 5829.67] loss=0.89 avg=0.84\n",
            "[19689 | 5832.74] loss=0.88 avg=0.84\n",
            "[19690 | 5835.84] loss=0.92 avg=0.84\n",
            "[19691 | 5838.92] loss=0.80 avg=0.84\n",
            "[19692 | 5841.99] loss=0.75 avg=0.84\n",
            "[19693 | 5845.07] loss=0.85 avg=0.84\n",
            "[19694 | 5848.14] loss=0.98 avg=0.84\n",
            "[19695 | 5851.20] loss=0.88 avg=0.84\n",
            "[19696 | 5854.29] loss=0.75 avg=0.84\n",
            "[19697 | 5857.37] loss=0.86 avg=0.84\n",
            "[19698 | 5860.46] loss=0.88 avg=0.84\n",
            "[19699 | 5863.53] loss=0.78 avg=0.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". = Lo trancado muy bien que te parece?\n",
            "This is the best soup in the world. = Esta es la mejor sopa del mundo.\n",
            "Do you want this shirt? = ¿Querés esta camisa?\n",
            "Let's get out of here. = Vayámonos de aquí.\n",
            "I'm tired of living this kind of life. = Estoy cansado de vivir esta vida.\n",
            "I don't get it either. = Ni a cualquiera.\n",
            "She turned him down. = Ella lo rechazó.\n",
            "You should not have done it without my permission. = No deberías haberlo hecho sin márquez.\n",
            "You don't have to talk so much. = No tenés que hablar tanto.\n",
            "You don't need to come so early. = No necesitas venir tan temprano.\n",
            "I don't see any stars. = No veo nada estrellas.\n",
            "He's very interested in the weather. = Le interesa el tiempo.\n",
            "What's in this box? = ¿Qué hay en esta caja?\n",
            "If I had more time, I would learn the answer. = Si tuviera más tiempo, aprendería la respuesta.\n",
            "Do you know how to tell brothers apart? = ¿Acaso sabe decir dichos en ellos?\n",
            "I don't like you. = No me gustas.\n",
            "My mom bought me this book at a 20 percent discount. = Mi mamá me compró este libro a un 20% descuento.\n",
            "Tom has trouble concentrating. = Tom tiene problemas para concentrarse.\n",
            "I thought that we had a deal. = Creía que teníamos un trato.\n",
            "Are you sure you don't want to eat some pizza? = ¿Estás seguro de que no quieres comer un pizza?\n",
            "You're the only one I know has money. = Eres la única persona que conozco con el dinero.\n",
            "Tom had a stroke. = Tom tuvo un golpe.\n",
            "The bus was made out of wood. = En ese bicicleta el bus estaba hecha de madera.\n",
            "We only want to help you. = Solo queremos ayudarte.\n",
            "I saw my father on the stairs. = Yo vi a mi padre en las escaleras.\n",
            "Can I leave a message? = ¿Puedo dejar un mensaje?\n",
            "I don't mind doing whatever you tell me to do. = No me importa hacer lo que me digas que haga.\n",
            "I do not understand their plan very well. = No comprendo bien su planeo.\n",
            "He had an accident on his way to school. = Tuvo un accidente de camino al colegio.\n",
            "Tom won't have to do that. = Tom no tendrá que hacer eso.\n",
            "We have a cat and two dogs. = Tenemos un gato y dos perros.\n",
            "He had his collar around his neck. = Él llevaba su cuello alrededor de su cuello.\n",
            "That's a great theory. = Esa es una gran teoría.\n",
            "We should be free to choose whenever we like. = Deberíamos ser libres de elegir cualquier cosa!\n",
            "The old house is being torn down. = Los viejos casas están abajo.\n",
            "Your name sounds familiar to me. = Tu nombre me resulta prefreso.\n",
            "This is the girl my mother gave me for my wedding. = Esta es la chica que mi madre me regaló para mi boda.\n",
            "Would you like to drink some tea or something? = ¿Quieren tomar algo de té arroz o corazonable?\n",
            "I do not know whether I shall learn French next year. = No sé si voy a aprender francés el año que viene.\n",
            "They're always complaining. = Ellos siempre se están quejando.\n",
            "Tom is still young. = Tomás todavía es joven.\n",
            "The doctor says to be careful. = El médico dice ser cuidadoso.\n",
            "Tom has no idea what Mary is planning. = Tom no tiene idea de lo que Mary está planeando.\n",
            "I don't want this house to fall into disrepair. = No quiero que este casa se caerse en disfrutación.\n",
            "Do you have any tickets left? = ¿Le quedáis\n",
            "\n",
            "[19700 | 5903.92] loss=0.81 avg=0.84\n",
            "[19701 | 5906.99] loss=0.68 avg=0.84\n",
            "[19702 | 5910.08] loss=0.89 avg=0.84\n",
            "[19703 | 5913.16] loss=0.67 avg=0.83\n",
            "[19704 | 5916.23] loss=0.87 avg=0.84\n",
            "[19705 | 5919.32] loss=0.96 avg=0.84\n",
            "[19706 | 5922.41] loss=0.73 avg=0.84\n",
            "[19707 | 5925.51] loss=0.87 avg=0.84\n",
            "[19708 | 5928.58] loss=0.78 avg=0.84\n",
            "[19709 | 5931.67] loss=0.99 avg=0.84\n",
            "[19710 | 5934.78] loss=0.99 avg=0.84\n",
            "[19711 | 5937.86] loss=0.81 avg=0.84\n",
            "[19712 | 5940.98] loss=0.89 avg=0.84\n",
            "[19713 | 5944.07] loss=0.94 avg=0.84\n",
            "[19714 | 5947.17] loss=0.67 avg=0.84\n",
            "[19715 | 5950.26] loss=0.81 avg=0.84\n",
            "[19716 | 5953.35] loss=0.79 avg=0.84\n",
            "[19717 | 5956.44] loss=0.95 avg=0.84\n",
            "[19718 | 5959.53] loss=0.83 avg=0.84\n",
            "[19719 | 5962.63] loss=0.80 avg=0.84\n",
            "[19720 | 5965.72] loss=0.77 avg=0.84\n",
            "[19721 | 5968.81] loss=0.73 avg=0.84\n",
            "[19722 | 5971.91] loss=0.67 avg=0.83\n",
            "[19723 | 5974.99] loss=0.81 avg=0.83\n",
            "[19724 | 5978.08] loss=0.79 avg=0.83\n",
            "[19725 | 5981.18] loss=0.86 avg=0.83\n",
            "[19726 | 5984.27] loss=0.79 avg=0.83\n",
            "[19727 | 5987.37] loss=0.84 avg=0.83\n",
            "[19728 | 5990.46] loss=0.96 avg=0.83\n",
            "[19729 | 5993.55] loss=1.02 avg=0.84\n",
            "[19730 | 5996.62] loss=0.78 avg=0.84\n",
            "[19731 | 5999.68] loss=0.92 avg=0.84\n",
            "[19732 | 6002.73] loss=0.78 avg=0.84\n",
            "[19733 | 6005.79] loss=0.76 avg=0.84\n",
            "[19734 | 6008.86] loss=0.95 avg=0.84\n",
            "[19735 | 6011.94] loss=0.93 avg=0.84\n",
            "[19736 | 6014.99] loss=0.85 avg=0.84\n",
            "[19737 | 6018.04] loss=0.88 avg=0.84\n",
            "[19738 | 6021.10] loss=0.71 avg=0.84\n",
            "[19739 | 6024.16] loss=0.79 avg=0.84\n",
            "[19740 | 6027.22] loss=0.77 avg=0.84\n",
            "[19741 | 6030.28] loss=0.79 avg=0.84\n",
            "[19742 | 6033.34] loss=0.72 avg=0.83\n",
            "[19743 | 6036.39] loss=0.80 avg=0.83\n",
            "[19744 | 6039.47] loss=0.90 avg=0.83\n",
            "[19745 | 6042.53] loss=0.89 avg=0.83\n",
            "[19746 | 6045.60] loss=0.85 avg=0.83\n",
            "[19747 | 6048.66] loss=0.85 avg=0.84\n",
            "[19748 | 6051.73] loss=0.82 avg=0.83\n",
            "[19749 | 6054.81] loss=0.74 avg=0.83\n",
            "[19750 | 6057.87] loss=0.93 avg=0.83\n",
            "[19751 | 6060.94] loss=0.94 avg=0.84\n",
            "[19752 | 6064.01] loss=0.83 avg=0.84\n",
            "[19753 | 6067.07] loss=0.85 avg=0.84\n",
            "[19754 | 6070.13] loss=0.80 avg=0.84\n",
            "[19755 | 6073.20] loss=0.83 avg=0.84\n",
            "[19756 | 6076.26] loss=0.84 avg=0.84\n",
            "[19757 | 6079.35] loss=0.92 avg=0.84\n",
            "[19758 | 6082.44] loss=0.87 avg=0.84\n",
            "[19759 | 6085.53] loss=0.88 avg=0.84\n",
            "[19760 | 6088.60] loss=0.82 avg=0.84\n",
            "[19761 | 6091.68] loss=0.79 avg=0.84\n",
            "[19762 | 6094.77] loss=0.93 avg=0.84\n",
            "[19763 | 6097.85] loss=0.88 avg=0.84\n",
            "[19764 | 6100.93] loss=0.80 avg=0.84\n",
            "[19765 | 6104.01] loss=0.77 avg=0.84\n",
            "[19766 | 6107.08] loss=0.75 avg=0.84\n",
            "[19767 | 6110.15] loss=0.81 avg=0.84\n",
            "[19768 | 6113.25] loss=0.73 avg=0.83\n",
            "[19769 | 6116.33] loss=0.93 avg=0.84\n",
            "[19770 | 6119.40] loss=0.78 avg=0.84\n",
            "[19771 | 6122.49] loss=0.82 avg=0.83\n",
            "[19772 | 6125.56] loss=0.73 avg=0.83\n",
            "[19773 | 6128.63] loss=0.89 avg=0.83\n",
            "[19774 | 6131.71] loss=0.75 avg=0.83\n",
            "[19775 | 6134.77] loss=0.84 avg=0.83\n",
            "[19776 | 6137.81] loss=0.84 avg=0.83\n",
            "[19777 | 6140.88] loss=0.83 avg=0.83\n",
            "[19778 | 6143.95] loss=0.86 avg=0.83\n",
            "[19779 | 6147.03] loss=0.84 avg=0.83\n",
            "[19780 | 6150.12] loss=0.94 avg=0.84\n",
            "[19781 | 6153.21] loss=0.98 avg=0.84\n",
            "[19782 | 6156.28] loss=0.93 avg=0.84\n",
            "[19783 | 6159.36] loss=0.92 avg=0.84\n",
            "[19784 | 6162.43] loss=0.83 avg=0.84\n",
            "[19785 | 6165.51] loss=0.74 avg=0.84\n",
            "[19786 | 6168.59] loss=0.76 avg=0.84\n",
            "[19787 | 6171.68] loss=0.88 avg=0.84\n",
            "[19788 | 6174.78] loss=0.83 avg=0.84\n",
            "[19789 | 6177.86] loss=0.76 avg=0.84\n",
            "[19790 | 6180.96] loss=0.93 avg=0.84\n",
            "[19791 | 6184.04] loss=0.82 avg=0.84\n",
            "[19792 | 6187.13] loss=0.83 avg=0.84\n",
            "[19793 | 6190.22] loss=0.88 avg=0.84\n",
            "[19794 | 6193.31] loss=0.92 avg=0.84\n",
            "[19795 | 6196.40] loss=0.86 avg=0.84\n",
            "[19796 | 6199.49] loss=0.70 avg=0.84\n",
            "[19797 | 6202.59] loss=0.75 avg=0.84\n",
            "[19798 | 6205.69] loss=0.91 avg=0.84\n",
            "[19799 | 6208.77] loss=0.87 avg=0.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "But it won't benefit you. = No te para que te beneficiará.\n",
            "Tom thought his cat was still asleep. = Tom pensó que hoy cometía su gato.\n",
            "Your shirt isn't clean. = Tu camisa está limpia.\n",
            "The first step is realizing that someone else has already done something. = El primer paso está en darse cuenta de que alguien ya había already hizo la ley.\n",
            "I'll get you to the airport as soon as I can. = Te consigüé hasta el aeropuerto tan pronto como pueda.\n",
            "I'm not the one who needs to be reminded. = No soy yo el que necesita recordarle.\n",
            "He'll be angry at me, too. = Él también se enfadará conmigo.\n",
            "Tom asked for a receipt, but he wasn't able to get one. = Todo Tom pidió recibo, pero no podía conseguirse suerte.\n",
            "Tom knows nothing about her. = Tom no sabe nada acerca de ella.\n",
            "Tom will be back soon. = Tom volverá en seguida.\n",
            "She used to study English when she was in high school. = Solía estudiando inglés cuando estaba en la prepa.\n",
            "We know that Tom knows. = Sabemos que Tom sabe.\n",
            "The police will catch you sooner or later. = La policía te captura tarde o temprano.\n",
            "I need a couple more people. = Necesito un par de personas más.\n",
            "You can't buy a ticket online. = No puedes comprar una entrada en e-bay.\n",
            "I have no money in my pocket. = No tengo dinero en el bolsillo.\n",
            "I think you'll want to talk to the doctor. = Creo que quieras hablar con el doctor.\n",
            "I want to make sure that you are who you say you are. = Quiero asegurarme de que eres clase.\n",
            "He's in a good mood today. = Hoy hace buen humor.\n",
            "Did you really just call up an hour ago and ask if we could have a party? = ¿Acabas de llamar tiempo y preguntaste si podíamos hacer una fiesta?\n",
            "I'm going to keep this one. = Me quedo este.\n",
            "Do I need to buy a new dress? = ¿Tengo que comprar un nuevo vestido?\n",
            "I am very grateful for your cooperation. = Estoy muy agradecido por tu cooperación.\n",
            "I don't know what to suggest. = No sé que sugerir.\n",
            "Have a croissant. = Tome un cruasán.\n",
            "Tom can't understand why Mary got so angry. = Tom no puede entender por qué se puso tan furiosa.\n",
            "You needn't have taken a taxi. = No necesitaban haber tomado un taxi.\n",
            "I was not at home. = Yo no estaba en casa.\n",
            "What's the meaning of this? = ¿Qué significa esto?\n",
            "His advice would be very useful to you. = Su consejo les resultaría muy útil a ustedes.\n",
            "This has to be done tomorrow. = Esto tiene que estar hacer mañana.\n",
            "Can he play the trumpet? = ¿Él sabe tocar la trompeta?\n",
            "Please be reasonable. = Por favor sé razonable.\n",
            "Please listen to me. = Por favor, escuche.\n",
            "I just wanted to give you a couple of days. = Solo quería darte un par de días.\n",
            "I'm in the truck. = Estoy en el camión.\n",
            "No one believes Tom anymore. = Ya a nadie le creerá de Tom.\n",
            "The police have caught him. = Ha llevado la policía.\n",
            "These plants can be eaten raw. = Estas plantas pueden comer crudas.\n",
            "I hope we can start taking better care of ourselves. = Espero que podamos empezar a cuidar usuario.\n",
            "He is an energetic person and really excels at sports. = Él es una persona energista y verdaderamente el acostumbrado.\n",
            "Please wait here. = Sujeta aquí, por favor.\n",
            "You need to do more than that. = Tienes que hacer más que eso.\n",
            "I wanted to ask you something. = Quería preg\n",
            "\n",
            "[19800 | 6249.39] loss=0.55 avg=0.83\n",
            "[19801 | 6252.49] loss=0.92 avg=0.84\n",
            "[19802 | 6255.56] loss=0.93 avg=0.84\n",
            "[19803 | 6258.63] loss=0.86 avg=0.84\n",
            "[19804 | 6261.71] loss=0.92 avg=0.84\n",
            "[19805 | 6264.79] loss=0.92 avg=0.84\n",
            "[19806 | 6267.89] loss=0.85 avg=0.84\n",
            "[19807 | 6270.98] loss=0.72 avg=0.84\n",
            "[19808 | 6274.07] loss=0.85 avg=0.84\n",
            "[19809 | 6277.16] loss=0.82 avg=0.84\n",
            "[19810 | 6280.24] loss=0.96 avg=0.84\n",
            "[19811 | 6283.34] loss=0.86 avg=0.84\n",
            "[19812 | 6286.42] loss=0.83 avg=0.84\n",
            "[19813 | 6289.51] loss=0.81 avg=0.84\n",
            "[19814 | 6292.59] loss=0.77 avg=0.84\n",
            "[19815 | 6295.67] loss=0.90 avg=0.84\n",
            "[19816 | 6298.75] loss=0.74 avg=0.84\n",
            "[19817 | 6301.83] loss=0.78 avg=0.84\n",
            "[19818 | 6304.93] loss=0.97 avg=0.84\n",
            "[19819 | 6308.03] loss=0.88 avg=0.84\n",
            "[19820 | 6311.12] loss=0.86 avg=0.84\n",
            "[19821 | 6314.20] loss=0.90 avg=0.84\n",
            "[19822 | 6317.29] loss=0.86 avg=0.84\n",
            "[19823 | 6320.39] loss=0.73 avg=0.84\n",
            "[19824 | 6323.45] loss=0.91 avg=0.84\n",
            "[19825 | 6326.53] loss=0.78 avg=0.84\n",
            "[19826 | 6329.61] loss=0.91 avg=0.84\n",
            "[19827 | 6332.69] loss=0.72 avg=0.84\n",
            "[19828 | 6335.79] loss=0.93 avg=0.84\n",
            "[19829 | 6338.88] loss=0.82 avg=0.84\n",
            "[19830 | 6341.97] loss=0.75 avg=0.84\n",
            "[19831 | 6345.03] loss=0.82 avg=0.84\n",
            "[19832 | 6348.12] loss=0.71 avg=0.84\n",
            "[19833 | 6351.20] loss=0.89 avg=0.84\n",
            "[19834 | 6354.28] loss=0.68 avg=0.83\n",
            "[19835 | 6357.37] loss=0.83 avg=0.83\n",
            "[19836 | 6360.46] loss=0.96 avg=0.84\n",
            "[19837 | 6363.54] loss=0.75 avg=0.84\n",
            "[19838 | 6366.61] loss=0.96 avg=0.84\n",
            "[19839 | 6369.67] loss=0.86 avg=0.84\n",
            "[19840 | 6372.75] loss=0.92 avg=0.84\n",
            "[19841 | 6375.83] loss=0.92 avg=0.84\n",
            "[19842 | 6378.92] loss=0.87 avg=0.84\n",
            "[19843 | 6382.01] loss=0.66 avg=0.84\n",
            "[19844 | 6385.08] loss=1.04 avg=0.84\n",
            "[19845 | 6388.15] loss=1.03 avg=0.84\n",
            "[19846 | 6391.23] loss=0.83 avg=0.84\n",
            "[19847 | 6394.32] loss=0.71 avg=0.84\n",
            "[19848 | 6397.38] loss=0.79 avg=0.84\n",
            "[19849 | 6400.48] loss=0.70 avg=0.84\n",
            "[19850 | 6403.57] loss=0.91 avg=0.84\n",
            "[19851 | 6406.65] loss=0.74 avg=0.84\n",
            "[19852 | 6409.73] loss=0.85 avg=0.84\n",
            "[19853 | 6412.81] loss=0.78 avg=0.84\n",
            "[19854 | 6415.90] loss=0.67 avg=0.83\n",
            "[19855 | 6418.99] loss=0.94 avg=0.84\n",
            "[19856 | 6422.08] loss=0.90 avg=0.84\n",
            "[19857 | 6425.17] loss=0.87 avg=0.84\n",
            "[19858 | 6428.27] loss=1.06 avg=0.84\n",
            "[19859 | 6431.37] loss=1.00 avg=0.84\n",
            "[19860 | 6434.46] loss=0.80 avg=0.84\n",
            "[19861 | 6437.56] loss=0.71 avg=0.84\n",
            "[19862 | 6440.65] loss=0.87 avg=0.84\n",
            "[19863 | 6443.75] loss=0.67 avg=0.84\n",
            "[19864 | 6446.84] loss=1.01 avg=0.84\n",
            "[19865 | 6449.94] loss=0.84 avg=0.84\n",
            "[19866 | 6453.02] loss=0.90 avg=0.84\n",
            "[19867 | 6456.10] loss=0.95 avg=0.84\n",
            "[19868 | 6459.20] loss=0.96 avg=0.84\n",
            "[19869 | 6462.30] loss=0.96 avg=0.84\n",
            "[19870 | 6465.38] loss=0.72 avg=0.84\n",
            "[19871 | 6468.47] loss=0.88 avg=0.84\n",
            "[19872 | 6471.56] loss=0.94 avg=0.84\n",
            "[19873 | 6474.66] loss=0.84 avg=0.84\n",
            "[19874 | 6477.76] loss=0.86 avg=0.84\n",
            "[19875 | 6480.85] loss=0.91 avg=0.84\n",
            "[19876 | 6483.95] loss=1.01 avg=0.85\n",
            "[19877 | 6487.05] loss=0.68 avg=0.84\n",
            "[19878 | 6490.14] loss=0.81 avg=0.84\n",
            "[19879 | 6493.23] loss=0.87 avg=0.84\n",
            "[19880 | 6496.31] loss=0.93 avg=0.85\n",
            "[19881 | 6499.39] loss=0.87 avg=0.85\n",
            "[19882 | 6502.48] loss=0.70 avg=0.84\n",
            "[19883 | 6505.57] loss=0.93 avg=0.84\n",
            "[19884 | 6508.66] loss=0.92 avg=0.85\n",
            "[19885 | 6511.74] loss=0.90 avg=0.85\n",
            "[19886 | 6514.84] loss=0.72 avg=0.84\n",
            "[19887 | 6517.95] loss=0.85 avg=0.84\n",
            "[19888 | 6521.05] loss=0.70 avg=0.84\n",
            "[19889 | 6524.14] loss=0.77 avg=0.84\n",
            "[19890 | 6527.23] loss=0.83 avg=0.84\n",
            "[19891 | 6530.31] loss=0.84 avg=0.84\n",
            "[19892 | 6533.40] loss=0.83 avg=0.84\n",
            "[19893 | 6536.49] loss=0.87 avg=0.84\n",
            "[19894 | 6539.59] loss=0.84 avg=0.84\n",
            "[19895 | 6542.68] loss=0.80 avg=0.84\n",
            "[19896 | 6545.77] loss=0.80 avg=0.84\n",
            "[19897 | 6548.85] loss=0.70 avg=0.84\n",
            "[19898 | 6551.94] loss=0.73 avg=0.84\n",
            "[19899 | 6555.05] loss=0.76 avg=0.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". ¿Está bienvenido de los gatos?\n",
            "It's been a long time since I've seen you smile. = Ha pasado mucho tiempo desde que te he visto sonreír.\n",
            "Tom and his father stayed in touch by email. = Tom y su padre se permanecieron en contacto por correo electrónico.\n",
            "Do you want to eat something? = ¿Querés comer algo?\n",
            "How do you explain that? = ¿Cómo explica eso?\n",
            "He was about to call off the marriage when the phone rang. = Estaba a punto de suspendir el matrimonio cuando sonó el teléfono.\n",
            "I don't know who I am. = No sé quién soy.\n",
            "A lot of fish died. = Murieron muchos peces.\n",
            "I wonder what I should do. = Me pregunto quién debería hacer.\n",
            "He is still young. = Él es todavía joven.\n",
            "I just want to be happy. = Solo quiero ser feliz.\n",
            "Tom wanted to speak to Mary. = Tom quería hablar con María.\n",
            "Don't cry. = ¡No tuns!\n",
            "That was a tough choice. = No fue una elección difícil.\n",
            "She didn't bring us gifts. = Ella no nos trajo regalos.\n",
            "This was her first love. = Esta fue su primer amor.\n",
            "I can't help laughing at the girl. = No puedo dejar de reírme de la chica.\n",
            "They're going. = Ellas están viniendo.\n",
            "How many years has she been working here? = ¿Cuántos años lleva trabajándose aquí?\n",
            "Give me that. = Deme eso.\n",
            "He looks just like my sister. = Él se parece a mi hermana.\n",
            "I bought Tom some groceries. = Le compré Tom algunas comestibles.\n",
            "I'd like to speak to one of your representatives in Washington, D.C. = Quisiera hablar con uno de vuestros representados en el Pérdo, D.C.\n",
            "I like watching planes take off. = Me gusta ver las aviones despegando.\n",
            "Tom doesn't have anything to worry about anymore. = Tom ya no tiene nada de qué preocuparnos.\n",
            "Tom will eventually need a job. = A la larga, Tom deben necesitar un trabajo.\n",
            "I have to make a few calls. = Yo debo hacer unas cuantas llamadas.\n",
            "Tom has an ear for music. = Tom tiene hecho para la música.\n",
            "How many people are in this room? = ¿Cuántas personas hay en esta habitación?\n",
            "She has a bicycle. = Ella tiene una bicicleta.\n",
            "My father is a very patient man. = Mi padre es un hombre muy paciente.\n",
            "He must come out of his room. = Él debió ir a su habitación.\n",
            "He's not a fool. = No es un tonto.\n",
            "I have only one cat. = Yo tengo un círculo.\n",
            "Tom was sitting in his car outside the window. = Tom estaba sentado en su auto afuera de la ventana.\n",
            "He was a bad student. = Era un mal estudiante.\n",
            "Tom will be there tomorrow. = Tom estará allá mañana.\n",
            "Would you please explain to me where the market is? = ¿Podrías por favor dado darme la mercancija, ¿por qué es?\n",
            "I have no intention of stopping doing that. = No tengo intención de dejar de hacerlo.\n",
            "Tom should be here within fifteen minutes. = Tom debería estar aquí dentro de quince minutos.\n",
            "Tom was the victim of an attempted crime. = Tom fue víctima de un crimen intentado.\n",
            "Tom is going to teach me his new course. = Tom va a enseñarme su nueva curso.\n",
            "How much is this handkerchief? = ¿Cuánto cuesta este pañuelo?\n",
            "The old man caught the child by the hand. = El anciano capturó al niño por la mano.\n",
            "Tom likes me. = A Tom le gusto.\n",
            "This is my sister's book. = Este es el\n",
            "\n",
            "[19900 | 6595.68] loss=0.91 avg=0.84\n",
            "[19901 | 6598.78] loss=0.77 avg=0.84\n",
            "[19902 | 6601.86] loss=0.76 avg=0.84\n",
            "[19903 | 6604.94] loss=0.88 avg=0.84\n",
            "[19904 | 6608.04] loss=0.75 avg=0.84\n",
            "[19905 | 6611.14] loss=0.84 avg=0.84\n",
            "[19906 | 6614.21] loss=0.95 avg=0.84\n",
            "[19907 | 6617.27] loss=0.75 avg=0.84\n",
            "[19908 | 6620.33] loss=0.66 avg=0.84\n",
            "[19909 | 6623.42] loss=0.76 avg=0.83\n",
            "[19910 | 6626.51] loss=0.89 avg=0.84\n",
            "[19911 | 6629.62] loss=0.87 avg=0.84\n",
            "[19912 | 6632.71] loss=0.81 avg=0.84\n",
            "[19913 | 6635.78] loss=0.67 avg=0.83\n",
            "[19914 | 6638.88] loss=0.65 avg=0.83\n",
            "[19915 | 6641.97] loss=1.02 avg=0.83\n",
            "[19916 | 6645.06] loss=0.91 avg=0.83\n",
            "[19917 | 6648.14] loss=0.80 avg=0.83\n",
            "[19918 | 6651.24] loss=0.85 avg=0.83\n",
            "[19919 | 6654.34] loss=0.90 avg=0.84\n",
            "[19920 | 6657.42] loss=0.86 avg=0.84\n",
            "[19921 | 6660.53] loss=0.99 avg=0.84\n",
            "[19922 | 6663.60] loss=0.84 avg=0.84\n",
            "[19923 | 6666.70] loss=0.84 avg=0.84\n",
            "[19924 | 6669.78] loss=0.95 avg=0.84\n",
            "[19925 | 6672.88] loss=0.87 avg=0.84\n",
            "[19926 | 6675.97] loss=0.90 avg=0.84\n",
            "[19927 | 6679.05] loss=0.93 avg=0.84\n",
            "[19928 | 6682.15] loss=0.76 avg=0.84\n",
            "[19929 | 6685.23] loss=0.81 avg=0.84\n",
            "[19930 | 6688.34] loss=0.79 avg=0.84\n",
            "[19931 | 6691.39] loss=0.85 avg=0.84\n",
            "[19932 | 6694.47] loss=0.90 avg=0.84\n",
            "[19933 | 6697.55] loss=0.83 avg=0.84\n",
            "[19934 | 6700.64] loss=0.79 avg=0.84\n",
            "[19935 | 6703.73] loss=0.73 avg=0.84\n",
            "[19936 | 6706.82] loss=0.78 avg=0.84\n",
            "[19937 | 6709.91] loss=0.91 avg=0.84\n",
            "[19938 | 6712.98] loss=0.70 avg=0.84\n",
            "[19939 | 6716.05] loss=0.90 avg=0.84\n",
            "[19940 | 6719.16] loss=0.78 avg=0.84\n",
            "[19941 | 6722.26] loss=0.66 avg=0.83\n",
            "[19942 | 6725.36] loss=1.07 avg=0.84\n",
            "[19943 | 6728.45] loss=0.96 avg=0.84\n",
            "[19944 | 6731.54] loss=0.70 avg=0.84\n",
            "[19945 | 6734.62] loss=0.78 avg=0.84\n",
            "[19946 | 6737.70] loss=0.74 avg=0.84\n",
            "[19947 | 6740.76] loss=0.88 avg=0.84\n",
            "[19948 | 6743.84] loss=0.64 avg=0.83\n",
            "[19949 | 6746.93] loss=0.85 avg=0.83\n",
            "[19950 | 6750.03] loss=0.87 avg=0.83\n",
            "[19951 | 6753.12] loss=0.72 avg=0.83\n",
            "[19952 | 6756.19] loss=0.77 avg=0.83\n",
            "[19953 | 6759.28] loss=0.76 avg=0.83\n",
            "[19954 | 6762.37] loss=0.70 avg=0.83\n",
            "[19955 | 6765.46] loss=0.84 avg=0.83\n",
            "[19956 | 6768.54] loss=0.89 avg=0.83\n",
            "[19957 | 6771.64] loss=0.98 avg=0.83\n",
            "[19958 | 6774.72] loss=0.74 avg=0.83\n",
            "[19959 | 6777.81] loss=0.83 avg=0.83\n",
            "[19960 | 6780.91] loss=0.85 avg=0.83\n",
            "[19961 | 6784.01] loss=0.69 avg=0.83\n",
            "[19962 | 6787.11] loss=0.91 avg=0.83\n",
            "[19963 | 6790.20] loss=0.72 avg=0.83\n",
            "[19964 | 6793.29] loss=0.89 avg=0.83\n",
            "[19965 | 6796.38] loss=0.94 avg=0.83\n",
            "[19966 | 6799.45] loss=0.82 avg=0.83\n",
            "[19967 | 6802.52] loss=0.90 avg=0.83\n",
            "[19968 | 6805.61] loss=0.88 avg=0.83\n",
            "[19969 | 6808.67] loss=0.75 avg=0.83\n",
            "[19970 | 6811.74] loss=0.87 avg=0.83\n",
            "[19971 | 6814.81] loss=0.90 avg=0.83\n",
            "[19972 | 6817.90] loss=0.93 avg=0.83\n",
            "[19973 | 6820.99] loss=0.89 avg=0.83\n",
            "[19974 | 6824.08] loss=0.84 avg=0.83\n",
            "[19975 | 6827.17] loss=0.85 avg=0.83\n",
            "[19976 | 6830.27] loss=0.80 avg=0.83\n",
            "[19977 | 6833.36] loss=0.91 avg=0.84\n",
            "[19978 | 6836.44] loss=0.96 avg=0.84\n",
            "[19979 | 6839.53] loss=0.96 avg=0.84\n",
            "[19980 | 6842.61] loss=0.89 avg=0.84\n",
            "[19981 | 6845.69] loss=0.98 avg=0.84\n",
            "[19982 | 6848.77] loss=0.80 avg=0.84\n",
            "[19983 | 6851.86] loss=0.88 avg=0.84\n",
            "[19984 | 6854.95] loss=0.84 avg=0.84\n",
            "[19985 | 6858.02] loss=0.82 avg=0.84\n",
            "[19986 | 6861.11] loss=0.99 avg=0.84\n",
            "[19987 | 6864.20] loss=0.91 avg=0.84\n",
            "[19988 | 6867.30] loss=0.75 avg=0.84\n",
            "[19989 | 6870.39] loss=0.84 avg=0.84\n",
            "[19990 | 6873.47] loss=0.83 avg=0.84\n",
            "[19991 | 6876.56] loss=0.90 avg=0.84\n",
            "[19992 | 6879.65] loss=0.79 avg=0.84\n",
            "[19993 | 6882.75] loss=0.76 avg=0.84\n",
            "[19994 | 6885.85] loss=0.90 avg=0.84\n",
            "[19995 | 6888.93] loss=0.72 avg=0.84\n",
            "[19996 | 6892.00] loss=0.79 avg=0.84\n",
            "[19997 | 6895.10] loss=0.94 avg=0.84\n",
            "[19998 | 6898.16] loss=0.96 avg=0.84\n",
            "[19999 | 6901.24] loss=0.84 avg=0.84\n",
            "Saving /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-20000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " la suerte de Tom.\n",
            "It's raining now, isn't it? = Está lloviendo ahora, ¿no?\n",
            "Tom is ready to fight. = Tom está listo para luchar.\n",
            "Your house is ten minutes' walk from the station. = Tu casa queda a diez minutos de pie de la estación.\n",
            "Tom died on November 8, 1829. = Tom murió el 8 de octubre, 1829.\n",
            "She has two daughters. = Ella tiene dos hijas.\n",
            "Tom wants to live in France. = Tom quiere vivir en Francia.\n",
            "Do you know why? = ¿Sabes por qué?\n",
            "I know the man standing behind the curtain. = Conozco al hombre que está parado detrás de la cortina.\n",
            "Tom is the one who pays the rent. = Tom es el que paga el renta.\n",
            "I don't want to end up in court. = No quiero terminar en un juicio.\n",
            "Tom has never met Mary. = Tom no ha conocido nunca a Mary.\n",
            "I don't belong to the club. = Yo no pertenezco al club.\n",
            "We were only joking. = Solo estaban bromeando.\n",
            "I don't want to do it that way. = No quiero hacerlo de esa manera.\n",
            "Tom was at school. = Tom estaba en el colegio.\n",
            "I'm at my parents' place. = Soy donde mis padres.\n",
            "Tell him to call me this afternoon. = Dile que me llame esta tarde.\n",
            "The cat will have to be killed. = El gato tendrá que ser matado.\n",
            "I don't remember you at all. = No te recuerdo para nada.\n",
            "Tom didn't answer. = Tomás no respondió.\n",
            "This isn't fair. = Esto no es justo.\n",
            "I can't tell you what Tom said. = No puedo contarte lo que dijo Tom.\n",
            "Please take a rest. = Por favor descansa un descanso.\n",
            "Who is that man? = ¿Quién es ese hombre?\n",
            "I'll tell Tom you said that. = Le diré a Tom que dijese eso.\n",
            "What does Tom think of Mary? = ¿Qué tal en Tom de Mary?\n",
            "He got hurt when he tripped over the egg. = Tenía años de que resbaló al golpe.\n",
            "It would not have been necessary for me to go there. = No habría sido necesario que yo vaya allí.\n",
            "I had no money left. = No I tenido nada de dinero.\n",
            "I want to play a game. = Quiero jugar un macho.\n",
            "I like my coffee strong. = Me gusta el café fuerte.\n",
            "Tom is not a typical businessman like his father. = Tom no es un típico como un transactivo.\n",
            "All of us look alike. = ¡A todos nosotros son iglesias!\n",
            "Let's hope Tom doesn't walk in. = Esperemos que Tom no entrá.\n",
            "She made him go there himself. = Ella le hizo ir allí en su su y cuando él se fue allá.\n",
            "The boy is very interested in climbing mountains. = El niño se interesan en montaña arrearse.\n",
            "Are they Japanese? = ¿Son japoneses?\n",
            "They can't understand you. = No pueden entenderlo.\n",
            "I love him all of the time. = Lo amo todo el tiempo.\n",
            "He is a good student. = Es un buen estudiante.\n",
            "I think I'll start doing that today. = Me parece a que empezará hoy.\n",
            "You were right, Tom. = Tenías razón, Tom.\n",
            "He is just like you. = Es ni siquiera como tú.\n",
            "My father gets up early in the morning. = Mi padre se levanta temprano por la mañana.\n",
            "Can you solve this problem? = ¿Puede resolver este problema?\n",
            "The old man lives alone. = El hombre vieja vive solo.\n",
            "What exactly did you do there? = ¿Qué hiciste eso exactamente qué hiciste allí?\n",
            "I would much rather be a bird than a fish. = Preferiría ser un pájaro a ser un pez.\n",
            "We are having tea now. = El té est\n",
            "\n",
            "[20000 | 6956.86] loss=0.89 avg=0.84\n",
            "[20001 | 6959.95] loss=0.89 avg=0.84\n",
            "[20002 | 6963.04] loss=0.80 avg=0.84\n",
            "[20003 | 6966.14] loss=0.82 avg=0.84\n",
            "[20004 | 6969.23] loss=0.73 avg=0.84\n",
            "[20005 | 6972.32] loss=0.93 avg=0.84\n",
            "[20006 | 6975.40] loss=0.79 avg=0.84\n",
            "[20007 | 6978.48] loss=0.69 avg=0.84\n",
            "[20008 | 6981.58] loss=0.93 avg=0.84\n",
            "[20009 | 6984.68] loss=0.87 avg=0.84\n",
            "[20010 | 6987.77] loss=0.89 avg=0.84\n",
            "[20011 | 6990.85] loss=0.69 avg=0.84\n",
            "[20012 | 6993.95] loss=0.94 avg=0.84\n",
            "[20013 | 6997.04] loss=0.93 avg=0.84\n",
            "[20014 | 7000.13] loss=0.96 avg=0.84\n",
            "[20015 | 7003.21] loss=0.72 avg=0.84\n",
            "[20016 | 7006.30] loss=0.89 avg=0.84\n",
            "[20017 | 7009.39] loss=0.74 avg=0.84\n",
            "[20018 | 7012.50] loss=0.95 avg=0.84\n",
            "[20019 | 7015.61] loss=0.81 avg=0.84\n",
            "[20020 | 7018.69] loss=0.77 avg=0.84\n",
            "[20021 | 7021.79] loss=0.77 avg=0.84\n",
            "[20022 | 7024.88] loss=0.95 avg=0.84\n",
            "[20023 | 7027.99] loss=0.78 avg=0.84\n",
            "[20024 | 7031.08] loss=0.74 avg=0.84\n",
            "[20025 | 7034.19] loss=0.84 avg=0.84\n",
            "[20026 | 7037.31] loss=0.86 avg=0.84\n",
            "[20027 | 7040.41] loss=0.73 avg=0.84\n",
            "[20028 | 7043.50] loss=0.87 avg=0.84\n",
            "[20029 | 7046.59] loss=0.90 avg=0.84\n",
            "[20030 | 7049.67] loss=0.73 avg=0.84\n",
            "[20031 | 7052.77] loss=0.86 avg=0.84\n",
            "[20032 | 7055.87] loss=0.89 avg=0.84\n",
            "[20033 | 7058.98] loss=0.81 avg=0.84\n",
            "[20034 | 7062.07] loss=0.86 avg=0.84\n",
            "[20035 | 7065.17] loss=0.75 avg=0.84\n",
            "[20036 | 7068.27] loss=0.75 avg=0.84\n",
            "[20037 | 7071.36] loss=0.98 avg=0.84\n",
            "[20038 | 7074.46] loss=0.89 avg=0.84\n",
            "[20039 | 7077.55] loss=0.83 avg=0.84\n",
            "[20040 | 7080.65] loss=0.90 avg=0.84\n",
            "[20041 | 7083.75] loss=0.78 avg=0.84\n",
            "[20042 | 7086.84] loss=0.67 avg=0.84\n",
            "[20043 | 7089.94] loss=0.77 avg=0.84\n",
            "[20044 | 7093.02] loss=0.73 avg=0.84\n",
            "[20045 | 7096.11] loss=0.77 avg=0.83\n",
            "[20046 | 7099.19] loss=0.96 avg=0.84\n",
            "[20047 | 7102.29] loss=0.92 avg=0.84\n",
            "[20048 | 7105.37] loss=0.73 avg=0.84\n",
            "[20049 | 7108.46] loss=0.92 avg=0.84\n",
            "[20050 | 7111.54] loss=0.71 avg=0.84\n",
            "[20051 | 7114.62] loss=0.83 avg=0.84\n",
            "[20052 | 7117.72] loss=0.86 avg=0.84\n",
            "[20053 | 7120.81] loss=0.70 avg=0.83\n",
            "[20054 | 7123.90] loss=0.92 avg=0.83\n",
            "[20055 | 7126.98] loss=0.85 avg=0.84\n",
            "[20056 | 7130.09] loss=0.90 avg=0.84\n",
            "[20057 | 7133.18] loss=0.97 avg=0.84\n",
            "[20058 | 7136.28] loss=0.87 avg=0.84\n",
            "[20059 | 7139.38] loss=0.92 avg=0.84\n",
            "[20060 | 7142.47] loss=0.86 avg=0.84\n",
            "[20061 | 7145.55] loss=0.85 avg=0.84\n",
            "[20062 | 7148.64] loss=1.00 avg=0.84\n",
            "[20063 | 7151.73] loss=0.70 avg=0.84\n",
            "[20064 | 7154.84] loss=0.86 avg=0.84\n",
            "[20065 | 7157.95] loss=0.93 avg=0.84\n",
            "[20066 | 7161.04] loss=0.87 avg=0.84\n",
            "[20067 | 7164.14] loss=0.71 avg=0.84\n",
            "[20068 | 7167.24] loss=0.83 avg=0.84\n",
            "[20069 | 7170.34] loss=0.77 avg=0.84\n",
            "[20070 | 7173.43] loss=0.83 avg=0.84\n",
            "[20071 | 7176.53] loss=0.81 avg=0.84\n",
            "[20072 | 7179.62] loss=0.77 avg=0.84\n",
            "[20073 | 7182.73] loss=0.62 avg=0.83\n",
            "[20074 | 7185.81] loss=0.71 avg=0.83\n",
            "[20075 | 7188.91] loss=0.84 avg=0.83\n",
            "[20076 | 7192.00] loss=0.94 avg=0.83\n",
            "[20077 | 7195.10] loss=0.95 avg=0.84\n",
            "[20078 | 7198.18] loss=0.96 avg=0.84\n",
            "[20079 | 7201.27] loss=0.80 avg=0.84\n",
            "[20080 | 7204.36] loss=0.98 avg=0.84\n",
            "[20081 | 7207.46] loss=0.82 avg=0.84\n",
            "[20082 | 7210.55] loss=0.77 avg=0.84\n",
            "[20083 | 7213.65] loss=0.79 avg=0.84\n",
            "[20084 | 7216.70] loss=1.04 avg=0.84\n",
            "[20085 | 7219.76] loss=0.79 avg=0.84\n",
            "[20086 | 7222.85] loss=0.84 avg=0.84\n",
            "[20087 | 7225.94] loss=0.72 avg=0.84\n",
            "[20088 | 7229.02] loss=0.82 avg=0.84\n",
            "[20089 | 7232.11] loss=0.85 avg=0.84\n",
            "[20090 | 7235.19] loss=0.97 avg=0.84\n",
            "[20091 | 7238.28] loss=0.82 avg=0.84\n",
            "[20092 | 7241.36] loss=0.71 avg=0.84\n",
            "[20093 | 7244.42] loss=0.76 avg=0.84\n",
            "[20094 | 7247.51] loss=0.96 avg=0.84\n",
            "[20095 | 7250.61] loss=0.85 avg=0.84\n",
            "[20096 | 7253.70] loss=0.88 avg=0.84\n",
            "[20097 | 7256.77] loss=0.83 avg=0.84\n",
            "[20098 | 7259.86] loss=0.94 avg=0.84\n",
            "[20099 | 7262.94] loss=0.92 avg=0.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " the same size as mine. = La lengua de Tom es lo mismo de la mía.\n",
            "We've been talking about this topic all day. = Hemos estado hablando de este texto todo el día.\n",
            "You want me to help you, don't you? = Quieres que te ayude, ¿verdad?\n",
            "Tom isn't as talkative as you. = Tom no es tan parlanchín como tú.\n",
            "The house was not destroyed by a flood. = La casa no fue destruida por un gullión.\n",
            "You're the only person I know in Boston that enjoys eating out every night. = Eres la única persona de Boston que conozco a comer todo por esta noche.\n",
            "Is today Friday? = ¿Es viernes hoy?\n",
            "It's possible that he will return to his seat. = Es posible que regrese a su asiento.\n",
            "We went in the dark. = Fuimos en la oscuridad.\n",
            "Tom wasn't very interested in the new movie we showed him. = Tom no estaba muy interesado en la nueva película que le mostele.\n",
            "Can you give me an estimate of how much this costs? = ¿Me puede dar un doble del anteación de comprar esto?\n",
            "We need more space. = Necesitamos más espacio.\n",
            "I told Tom it was my mistake. = Le dije a Tom que fue mi error.\n",
            "I think this could be dangerous. = Creo que podría ser peligroso.\n",
            "I don't know why they do it. = No sé por qué les hacen.\n",
            "I've never seen a rainbow. = Nunca he visto un arco iris.\n",
            "I'm not interested in Italian food. = No estoy interesada en la comida italiana.\n",
            "Please take this medicine twice a day. = Por favor 2 coron este medicamento dos veces al día.\n",
            "I'll be watching you just like you are. = Te estaré observando igual de ti mismo.\n",
            "The weatherman says that it will rain between eight and nine. = El hombre del tiempo dice que tendremos lluvia entre a las ocho y no.\n",
            "It's the least we can do for now. = Es por lo menos que podemos hacer por ahora.\n",
            "He often comes late, often staying a few minutes late. = A menudo llega tarde, por casualidad se quede que se queda un par de minutos tarde.\n",
            "Let's take a taxi. = Tomemos un taxi.\n",
            "Do you really want to wait like this? = ¿De verdad quieres esperar así?\n",
            "My family is not the type to take time off. = Mi familia no es el tipo de tomar tiempo.\n",
            "He's a good artist. = Él es un buen artista.\n",
            "I was just about to go out when it began raining. = Justo estaba a punto de salir cuando empezó a llover.\n",
            "I like watching movies. = Me gusta ver películas.\n",
            "The king and the queen visited the students. = El rey y la reina visitaron a los estudiantes.\n",
            "I'm going out for a walk. = Voy a salir a dar un paseo.\n",
            "I have one question for you. = Tengo una pregunta para vosotros.\n",
            "I didn't think it was that bad. = No pensaba que no era tan malo.\n",
            "You have to be more patient. = Tenéis que ser más pacientes.\n",
            "Tom was looking for the key. = Tom buscaba la llave.\n",
            "I will teach you how to play tennis. = Te enseñaré cómo tocar al tenis.\n",
            "You haven't said a word. = No has dicho ni una palabra.\n",
            "Have you tried the cake we had last night? = ¿Has intentado el pastel que nos había pasado anoche?\n",
            "What are you drinking? = ¿Qué estás tomando?\n",
            "Did you catch anything? = ¿Le cavaron algo?\n",
            "Tom's still sick. = Tom still está enfermo.\n",
            "He likes to learn new things. = A él le gusta aprender cosas nuevas.\n",
            "I don't think he's capable of doing it. = No pienso que él sea capaz de hacerlo.\n",
            "\n",
            "[20100 | 7303.57] loss=0.80 avg=0.84\n",
            "[20101 | 7306.67] loss=0.86 avg=0.84\n",
            "[20102 | 7309.76] loss=0.68 avg=0.84\n",
            "[20103 | 7312.84] loss=0.79 avg=0.84\n",
            "[20104 | 7315.92] loss=0.88 avg=0.84\n",
            "[20105 | 7319.00] loss=0.84 avg=0.84\n",
            "[20106 | 7322.07] loss=0.89 avg=0.84\n",
            "[20107 | 7325.16] loss=0.90 avg=0.84\n",
            "[20108 | 7328.25] loss=0.87 avg=0.84\n",
            "[20109 | 7331.32] loss=0.91 avg=0.84\n",
            "[20110 | 7334.41] loss=0.75 avg=0.84\n",
            "[20111 | 7337.48] loss=0.78 avg=0.84\n",
            "[20112 | 7340.58] loss=0.76 avg=0.84\n",
            "[20113 | 7343.68] loss=0.86 avg=0.84\n",
            "[20114 | 7346.76] loss=0.86 avg=0.84\n",
            "[20115 | 7349.85] loss=0.75 avg=0.84\n",
            "[20116 | 7352.95] loss=0.70 avg=0.84\n",
            "[20117 | 7356.04] loss=0.81 avg=0.84\n",
            "[20118 | 7359.13] loss=0.70 avg=0.83\n",
            "[20119 | 7362.21] loss=0.84 avg=0.83\n",
            "[20120 | 7365.31] loss=0.77 avg=0.83\n",
            "[20121 | 7368.40] loss=0.87 avg=0.83\n",
            "[20122 | 7371.49] loss=0.79 avg=0.83\n",
            "[20123 | 7374.57] loss=0.82 avg=0.83\n",
            "[20124 | 7377.66] loss=0.67 avg=0.83\n",
            "[20125 | 7380.75] loss=0.81 avg=0.83\n",
            "[20126 | 7383.85] loss=0.93 avg=0.83\n",
            "[20127 | 7386.95] loss=0.89 avg=0.83\n",
            "[20128 | 7390.06] loss=0.77 avg=0.83\n",
            "[20129 | 7393.17] loss=0.93 avg=0.83\n",
            "[20130 | 7396.26] loss=0.84 avg=0.83\n",
            "[20131 | 7399.33] loss=0.75 avg=0.83\n",
            "[20132 | 7402.42] loss=0.82 avg=0.83\n",
            "[20133 | 7405.50] loss=0.78 avg=0.83\n",
            "[20134 | 7408.59] loss=0.89 avg=0.83\n",
            "[20135 | 7411.66] loss=0.97 avg=0.83\n",
            "[20136 | 7414.72] loss=0.72 avg=0.83\n",
            "[20137 | 7417.78] loss=0.77 avg=0.83\n",
            "[20138 | 7420.85] loss=0.92 avg=0.83\n",
            "[20139 | 7423.91] loss=0.88 avg=0.83\n",
            "[20140 | 7426.99] loss=0.76 avg=0.83\n",
            "[20141 | 7430.07] loss=1.03 avg=0.83\n",
            "[20142 | 7433.14] loss=0.90 avg=0.84\n",
            "[20143 | 7436.21] loss=0.88 avg=0.84\n",
            "[20144 | 7439.30] loss=0.82 avg=0.84\n",
            "[20145 | 7442.38] loss=0.96 avg=0.84\n",
            "[20146 | 7445.49] loss=0.89 avg=0.84\n",
            "[20147 | 7448.59] loss=0.83 avg=0.84\n",
            "[20148 | 7451.68] loss=0.68 avg=0.84\n",
            "[20149 | 7454.75] loss=0.93 avg=0.84\n",
            "[20150 | 7457.85] loss=0.73 avg=0.84\n",
            "[20151 | 7460.95] loss=0.75 avg=0.83\n",
            "[20152 | 7464.05] loss=0.92 avg=0.84\n",
            "[20153 | 7467.14] loss=0.76 avg=0.83\n",
            "[20154 | 7470.23] loss=0.89 avg=0.84\n",
            "[20155 | 7473.32] loss=0.72 avg=0.83\n",
            "[20156 | 7476.41] loss=0.80 avg=0.83\n",
            "[20157 | 7479.51] loss=0.90 avg=0.83\n",
            "[20158 | 7482.60] loss=0.89 avg=0.84\n",
            "[20159 | 7485.66] loss=0.96 avg=0.84\n",
            "[20160 | 7488.76] loss=0.95 avg=0.84\n",
            "[20161 | 7491.84] loss=0.92 avg=0.84\n",
            "[20162 | 7494.92] loss=0.99 avg=0.84\n",
            "[20163 | 7497.99] loss=1.00 avg=0.84\n",
            "[20164 | 7501.07] loss=0.69 avg=0.84\n",
            "[20165 | 7504.14] loss=0.82 avg=0.84\n",
            "[20166 | 7507.22] loss=0.78 avg=0.84\n",
            "[20167 | 7510.29] loss=0.72 avg=0.84\n",
            "[20168 | 7513.38] loss=0.84 avg=0.84\n",
            "[20169 | 7516.46] loss=0.68 avg=0.84\n",
            "[20170 | 7519.55] loss=0.75 avg=0.84\n",
            "[20171 | 7522.63] loss=0.77 avg=0.83\n",
            "[20172 | 7525.73] loss=0.83 avg=0.83\n",
            "[20173 | 7528.81] loss=0.81 avg=0.83\n",
            "[20174 | 7531.88] loss=0.91 avg=0.84\n",
            "[20175 | 7534.95] loss=0.80 avg=0.83\n",
            "[20176 | 7538.02] loss=0.63 avg=0.83\n",
            "[20177 | 7541.09] loss=0.85 avg=0.83\n",
            "[20178 | 7544.15] loss=0.87 avg=0.83\n",
            "[20179 | 7547.21] loss=0.87 avg=0.83\n",
            "[20180 | 7550.28] loss=0.77 avg=0.83\n",
            "[20181 | 7553.36] loss=0.92 avg=0.83\n",
            "[20182 | 7556.46] loss=0.82 avg=0.83\n",
            "[20183 | 7559.54] loss=0.82 avg=0.83\n",
            "[20184 | 7562.62] loss=0.71 avg=0.83\n",
            "[20185 | 7565.70] loss=0.82 avg=0.83\n",
            "[20186 | 7568.78] loss=0.90 avg=0.83\n",
            "[20187 | 7571.87] loss=0.81 avg=0.83\n",
            "[20188 | 7574.95] loss=0.84 avg=0.83\n",
            "[20189 | 7578.02] loss=0.71 avg=0.83\n",
            "[20190 | 7581.11] loss=0.73 avg=0.83\n",
            "[20191 | 7584.21] loss=0.74 avg=0.83\n",
            "[20192 | 7587.30] loss=0.72 avg=0.83\n",
            "[20193 | 7590.39] loss=0.85 avg=0.83\n",
            "[20194 | 7593.47] loss=0.82 avg=0.83\n",
            "[20195 | 7596.54] loss=0.89 avg=0.83\n",
            "[20196 | 7599.60] loss=0.78 avg=0.83\n",
            "[20197 | 7602.69] loss=0.87 avg=0.83\n",
            "[20198 | 7605.79] loss=0.78 avg=0.83\n",
            "[20199 | 7608.90] loss=0.71 avg=0.83\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " cámara a Tom?\n",
            "Somebody wants to talk to you. = Alguien quiere hablar con usted.\n",
            "He went to see her. = Él fue a verla.\n",
            "There were some people in the hall who did not want to give us a concert. = Había algunas personas en la sala que no nos quería dar una concierto.\n",
            "Tom didn't want to argue with Mary. = Tom no quiso discutir con Mary.\n",
            "Tom had no chance to talk to Mary. = Tom no tuvo oportunidad de hablar con Mary.\n",
            "If I had eaten breakfast this morning, I would not be hungry now. = Si yo hubiera desayunado esta mañana ahora no tendría hambre.\n",
            "Her face is red. = Su cara es rojo.\n",
            "You can't imagine what we went through, yet, still smiling. = No te puedes imaginar lo que hicimos, pero tardas, still sonriendo.\n",
            "I don't know where to open the box. = No sé dónde abre la caja.\n",
            "What did you go to church for? = ¿Para qué fuiste aledor del gitarra?\n",
            "We're going in that direction. = Nos vamos a ir en esa dirección.\n",
            "We are glad you will be coming. = Nos alegramos que venga ustedes.\n",
            "It was necessary to escape from the difficulties. = Había que escapar del desacuerdo.\n",
            "The old man has lived abroad. = El anciano ha vivido en el extranjero.\n",
            "I am sure of getting what I want. = Estoy seguro de recobrarno lo que yo quiero.\n",
            "We'll need a little more time. = Necesitaremos un poco más de tiempo.\n",
            "The old man is sick. = El anciano está enfermo.\n",
            "You are an idiot. = Sos un idiota.\n",
            "This is the first time I've sung in public. = Esta es la primera vez que he cantado en público.\n",
            "My house is somewhere around here. = Mi casa esta alrededor de aquí.\n",
            "I can't stop laughing. = No puedo dejar de reír.\n",
            "I am very tired of it. = Estoy muy cansado de ello.\n",
            "I think I would've liked Tom better if he had come earlier. = Creo que me habría apreciado Tom más si había venido antes.\n",
            "I will keep that knowledge to myself. = Guardaré esa conocimiento para mí mismo.\n",
            "Don't let go of the rope, or you'll catch something sharp. = No sueltes la cuerda, o te te vas a molestar algo acertada.\n",
            "He is about to leave. = Él está a punto de irse.\n",
            "The children were busy. = Los niños estaban ocupados.\n",
            "No other city in Japan is as large as Tokyo. = No hay otra ciudad de Japón más grande como Tōkyoku.\n",
            "Tom gave me a book. = Tom me dio un libro.\n",
            "\"May I have one of these?\" \"You don't have to buy it.\" = \"¿Puede darme uno de éstos?\" \"Tú no tienes que compreslo.\"\n",
            "Don't be afraid of him if you don't have anything to hide from me. = No le tengas miedo si no tengas nada para esconderme.\n",
            "I saw Tom kissing Mary. = Vi a Tom besar a Mary.\n",
            "This story is very hard to understand. = Esta historia es muy difícil de entender.\n",
            "My father often reads magazines on the subway. = Mi padre suele leer revistas por el metro.\n",
            "Tom bought Mary a bottle of vodka. = Tom le compró una botella de fermenta a Mary.\n",
            "Tom is a professional baseball player. = Tomás es un testigo de béisbol profesional.\n",
            "I haven't forgotten you. = No te he olvidado.\n",
            "They don't know how to write their names. = No saben cómo escribir sus nombres.\n",
            "There's a big gap in the floor. = Hay un gran puente en la pieza.\n",
            "I'll have to help Tom. = Yo tendré que ayudar a Tom.\n",
            "A new car will\n",
            "\n",
            "[20200 | 7649.97] loss=0.86 avg=0.83\n",
            "[20201 | 7653.04] loss=0.88 avg=0.83\n",
            "[20202 | 7656.11] loss=0.74 avg=0.83\n",
            "[20203 | 7659.16] loss=0.83 avg=0.83\n",
            "[20204 | 7662.25] loss=0.89 avg=0.83\n",
            "[20205 | 7665.34] loss=0.89 avg=0.83\n",
            "[20206 | 7668.43] loss=0.78 avg=0.83\n",
            "[20207 | 7671.52] loss=0.92 avg=0.83\n",
            "[20208 | 7674.59] loss=0.78 avg=0.83\n",
            "[20209 | 7677.65] loss=0.74 avg=0.83\n",
            "[20210 | 7680.75] loss=0.75 avg=0.83\n",
            "[20211 | 7683.85] loss=0.96 avg=0.83\n",
            "[20212 | 7686.96] loss=0.71 avg=0.83\n",
            "[20213 | 7690.02] loss=0.70 avg=0.83\n",
            "[20214 | 7693.09] loss=0.84 avg=0.83\n",
            "[20215 | 7696.15] loss=0.98 avg=0.83\n",
            "[20216 | 7699.21] loss=0.79 avg=0.83\n",
            "[20217 | 7702.29] loss=0.81 avg=0.83\n",
            "[20218 | 7705.35] loss=0.82 avg=0.83\n",
            "[20219 | 7708.43] loss=0.87 avg=0.83\n",
            "[20220 | 7711.51] loss=0.80 avg=0.83\n",
            "[20221 | 7714.60] loss=0.65 avg=0.83\n",
            "[20222 | 7717.67] loss=0.75 avg=0.83\n",
            "[20223 | 7720.73] loss=0.94 avg=0.83\n",
            "[20224 | 7723.80] loss=0.79 avg=0.83\n",
            "[20225 | 7726.87] loss=0.78 avg=0.83\n",
            "[20226 | 7729.95] loss=0.86 avg=0.83\n",
            "[20227 | 7733.05] loss=0.92 avg=0.83\n",
            "[20228 | 7736.13] loss=0.90 avg=0.83\n",
            "[20229 | 7739.21] loss=0.67 avg=0.83\n",
            "[20230 | 7742.28] loss=0.71 avg=0.82\n",
            "[20231 | 7745.37] loss=1.00 avg=0.83\n",
            "[20232 | 7748.44] loss=0.88 avg=0.83\n",
            "[20233 | 7751.50] loss=0.84 avg=0.83\n",
            "[20234 | 7754.60] loss=0.95 avg=0.83\n",
            "[20235 | 7757.68] loss=0.88 avg=0.83\n",
            "[20236 | 7760.78] loss=0.97 avg=0.83\n",
            "[20237 | 7763.88] loss=0.69 avg=0.83\n",
            "[20238 | 7766.97] loss=0.99 avg=0.83\n",
            "[20239 | 7770.05] loss=0.89 avg=0.83\n",
            "[20240 | 7773.13] loss=0.97 avg=0.83\n",
            "[20241 | 7776.21] loss=0.87 avg=0.83\n",
            "[20242 | 7779.30] loss=0.75 avg=0.83\n",
            "[20243 | 7782.38] loss=0.74 avg=0.83\n",
            "[20244 | 7785.46] loss=0.70 avg=0.83\n",
            "[20245 | 7788.54] loss=0.75 avg=0.83\n",
            "[20246 | 7791.62] loss=0.89 avg=0.83\n",
            "[20247 | 7794.70] loss=0.81 avg=0.83\n",
            "[20248 | 7797.81] loss=0.77 avg=0.83\n",
            "[20249 | 7800.92] loss=0.77 avg=0.83\n",
            "[20250 | 7804.01] loss=0.84 avg=0.83\n",
            "[20251 | 7807.09] loss=0.87 avg=0.83\n",
            "[20252 | 7810.17] loss=0.69 avg=0.83\n",
            "[20253 | 7813.26] loss=0.81 avg=0.83\n",
            "[20254 | 7816.31] loss=0.92 avg=0.83\n",
            "[20255 | 7819.39] loss=0.89 avg=0.83\n",
            "[20256 | 7822.47] loss=0.82 avg=0.83\n",
            "[20257 | 7825.57] loss=0.97 avg=0.83\n",
            "[20258 | 7828.68] loss=0.82 avg=0.83\n",
            "[20259 | 7831.80] loss=0.87 avg=0.83\n",
            "[20260 | 7834.94] loss=0.87 avg=0.83\n",
            "[20261 | 7838.05] loss=0.81 avg=0.83\n",
            "[20262 | 7841.16] loss=0.89 avg=0.83\n",
            "[20263 | 7844.26] loss=0.70 avg=0.83\n",
            "[20264 | 7847.34] loss=0.77 avg=0.83\n",
            "[20265 | 7850.40] loss=0.80 avg=0.83\n",
            "[20266 | 7853.46] loss=0.86 avg=0.83\n",
            "[20267 | 7856.54] loss=0.98 avg=0.83\n",
            "[20268 | 7859.61] loss=0.84 avg=0.83\n",
            "[20269 | 7862.69] loss=0.73 avg=0.83\n",
            "[20270 | 7865.77] loss=0.82 avg=0.83\n",
            "[20271 | 7868.84] loss=0.78 avg=0.83\n",
            "[20272 | 7871.90] loss=0.88 avg=0.83\n",
            "[20273 | 7874.94] loss=0.71 avg=0.83\n",
            "[20274 | 7878.01] loss=0.90 avg=0.83\n",
            "[20275 | 7881.10] loss=0.79 avg=0.83\n",
            "[20276 | 7884.17] loss=0.78 avg=0.83\n",
            "[20277 | 7887.25] loss=0.79 avg=0.83\n",
            "[20278 | 7890.32] loss=0.78 avg=0.83\n",
            "[20279 | 7893.40] loss=0.87 avg=0.83\n",
            "[20280 | 7896.48] loss=0.96 avg=0.83\n",
            "[20281 | 7899.53] loss=1.02 avg=0.83\n",
            "[20282 | 7902.61] loss=0.74 avg=0.83\n",
            "[20283 | 7905.69] loss=0.98 avg=0.83\n",
            "[20284 | 7908.77] loss=0.96 avg=0.83\n",
            "[20285 | 7911.84] loss=0.84 avg=0.83\n",
            "[20286 | 7914.90] loss=0.73 avg=0.83\n",
            "[20287 | 7917.96] loss=0.77 avg=0.83\n",
            "[20288 | 7921.03] loss=0.93 avg=0.83\n",
            "[20289 | 7924.10] loss=0.82 avg=0.83\n",
            "[20290 | 7927.19] loss=0.80 avg=0.83\n",
            "[20291 | 7930.24] loss=0.87 avg=0.83\n",
            "[20292 | 7933.33] loss=0.77 avg=0.83\n",
            "[20293 | 7936.40] loss=0.95 avg=0.83\n",
            "[20294 | 7939.48] loss=0.91 avg=0.83\n",
            "[20295 | 7942.54] loss=0.97 avg=0.84\n",
            "[20296 | 7945.63] loss=0.89 avg=0.84\n",
            "[20297 | 7948.73] loss=0.87 avg=0.84\n",
            "[20298 | 7951.83] loss=0.72 avg=0.83\n",
            "[20299 | 7954.92] loss=0.96 avg=0.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ie.\n",
            "The problem is that money can't buy happiness. = El problema es que el dinero no puede comprar la felicidad.\n",
            "I don't need to do that. = No necesito hacer eso.\n",
            "This book belonged to you. = Este libro le pertenecía.\n",
            "Tom is a musician. = Tom es un músico.\n",
            "Don't let anyone else. = No dejes que alguien más.\n",
            "The police have already surrounded the building. = La Policía ya se llevaron al edificio.\n",
            "This is not a good time for us to be together. = No es un buen momento para estar juntos.\n",
            "He didn't give in to the temptation. = Él no cedió a la tentación.\n",
            "It was not easy to persuade him. = No fue fácil convencerle.\n",
            "I just thought Tom was British. = Simplemente creí que Tom era uno británico.\n",
            "This is my car. Where can I park it? = Este es mi coche. ¿Cuándo puedo aparcar encima?\n",
            "I am at home, and I can't wait till tomorrow. = Estoy en casa y no puedo esperar hasta mañana.\n",
            "Tom asked Mary to sign a new lease of life. = Tom le pidió a Mary que firmara un nuevo leído de vida.\n",
            "There's no point in waiting for Tom. = Esperar hacer eso de nuevo, pero es un vistazo para Tom.\n",
            "Tom wants to see Mary face to face. = Tom quiere ver a Mary cara a cara.\n",
            "I want to give you something as good a present as you could ask for. = Quiero darte algo tan bueno como podrías pedirlo.\n",
            "Tom told us not to be afraid. = Tomás nos dijo que no teníamos miedo.\n",
            "I heard the story from him. = Escuché ese conte de él.\n",
            "His plan will be implemented soon. = Su plan se celebrará pronto.\n",
            "Give me that. = Dame eso.\n",
            "Tom is out of breath. = Tom se está sin aliento.\n",
            "I'm still looking for a job. = Aún estoy buscando trabajo.\n",
            "The door was locked from the outside. = La puerta debió de llave a la interior.\n",
            "Tom is a liar. = Tom es un mentiroso.\n",
            "Tom is not as stupid as he looks. = Tom no es tan estúpido como parece.\n",
            "We're not here to have a good time. = No estamos aquí para alandlos.\n",
            "It's so hard to tell which bank he's referring to. = Es tan difícil distinguir más de qué banco él está referiendo.\n",
            "He never talks about it. = Nunca habla al respecto.\n",
            "I don't like the sun. = No me gusta el sol.\n",
            "It seems that the plane is delayed. = Parece que el avión está retrasado.\n",
            "This is an order. = Esto es una orden.\n",
            "She asked him to leave the baby in the cradle. = Le pidió que dejara la bebé en el cinturón.\n",
            "If he calls again, I don't know what to say. = Si llama otra vez, no sé qué decir.\n",
            "I don't feel like meeting her. = No tengo ganas de reunirme con ella.\n",
            "Tom has a beautiful voice. = Tom tiene una hermosa voz.\n",
            "There are some problems we have to solve. = Hay algunos problemas que tenemos que resolver.\n",
            "I am going to play baseball in the afternoon. = Voy a jugar al béisbol por la tarde.\n",
            "Tom asked Mary how many people had come to see him. = Tom preguntó a Mary cuántas personas habían venido a verle.\n",
            "This problem is too difficult. = Este problema es demasiado difícil.\n",
            "I can't make out what he says. = No puedo entender lo que él dice.\n",
            "There is no excuse for his delay. = Su retraso no admite lugar a torturas.\n",
            "The teacher told the boy not to cross the road. = El maestro denunció al niño que no cruce la calle.\n",
            "Don't get cocky. =\n",
            "\n",
            "[20300 | 7995.64] loss=0.91 avg=0.84\n",
            "[20301 | 7998.73] loss=0.89 avg=0.84\n",
            "[20302 | 8001.80] loss=0.80 avg=0.84\n",
            "[20303 | 8004.88] loss=0.88 avg=0.84\n",
            "[20304 | 8007.98] loss=0.87 avg=0.84\n",
            "[20305 | 8011.07] loss=0.76 avg=0.84\n",
            "[20306 | 8014.15] loss=0.77 avg=0.84\n",
            "[20307 | 8017.24] loss=0.95 avg=0.84\n",
            "[20308 | 8020.31] loss=0.76 avg=0.84\n",
            "[20309 | 8023.38] loss=0.91 avg=0.84\n",
            "[20310 | 8026.47] loss=0.87 avg=0.84\n",
            "[20311 | 8029.55] loss=0.66 avg=0.84\n",
            "[20312 | 8032.63] loss=0.90 avg=0.84\n",
            "[20313 | 8035.71] loss=0.88 avg=0.84\n",
            "[20314 | 8038.81] loss=0.66 avg=0.84\n",
            "[20315 | 8041.91] loss=0.82 avg=0.84\n",
            "[20316 | 8045.01] loss=0.75 avg=0.83\n",
            "[20317 | 8048.12] loss=0.69 avg=0.83\n",
            "[20318 | 8051.22] loss=0.67 avg=0.83\n",
            "[20319 | 8054.31] loss=0.94 avg=0.83\n",
            "[20320 | 8057.40] loss=0.83 avg=0.83\n",
            "[20321 | 8060.49] loss=0.94 avg=0.83\n",
            "[20322 | 8063.59] loss=0.87 avg=0.83\n",
            "[20323 | 8066.69] loss=0.78 avg=0.83\n",
            "[20324 | 8069.78] loss=0.97 avg=0.83\n",
            "[20325 | 8072.89] loss=0.79 avg=0.83\n",
            "[20326 | 8075.98] loss=0.92 avg=0.84\n",
            "[20327 | 8079.08] loss=0.83 avg=0.84\n",
            "[20328 | 8082.17] loss=0.81 avg=0.83\n",
            "[20329 | 8085.27] loss=0.65 avg=0.83\n",
            "[20330 | 8088.38] loss=0.82 avg=0.83\n",
            "[20331 | 8091.47] loss=0.75 avg=0.83\n",
            "[20332 | 8094.57] loss=0.81 avg=0.83\n",
            "[20333 | 8097.66] loss=0.91 avg=0.83\n",
            "[20334 | 8100.75] loss=0.79 avg=0.83\n",
            "[20335 | 8103.85] loss=0.70 avg=0.83\n",
            "[20336 | 8106.95] loss=0.66 avg=0.83\n",
            "[20337 | 8110.05] loss=0.75 avg=0.83\n",
            "[20338 | 8113.15] loss=0.80 avg=0.83\n",
            "[20339 | 8116.25] loss=0.94 avg=0.83\n",
            "[20340 | 8119.33] loss=0.71 avg=0.83\n",
            "[20341 | 8122.43] loss=0.77 avg=0.83\n",
            "[20342 | 8125.52] loss=0.77 avg=0.83\n",
            "[20343 | 8128.61] loss=0.82 avg=0.83\n",
            "[20344 | 8131.70] loss=0.87 avg=0.83\n",
            "[20345 | 8134.81] loss=0.85 avg=0.83\n",
            "[20346 | 8137.90] loss=0.74 avg=0.83\n",
            "[20347 | 8140.99] loss=0.83 avg=0.83\n",
            "[20348 | 8144.08] loss=0.94 avg=0.83\n",
            "[20349 | 8147.17] loss=0.68 avg=0.83\n",
            "[20350 | 8150.28] loss=0.89 avg=0.83\n",
            "[20351 | 8153.37] loss=0.86 avg=0.83\n",
            "[20352 | 8156.45] loss=0.69 avg=0.83\n",
            "[20353 | 8159.53] loss=0.81 avg=0.83\n",
            "[20354 | 8162.61] loss=1.02 avg=0.83\n",
            "[20355 | 8165.71] loss=0.84 avg=0.83\n",
            "[20356 | 8168.81] loss=0.96 avg=0.83\n",
            "[20357 | 8171.91] loss=0.88 avg=0.83\n",
            "[20358 | 8174.99] loss=0.88 avg=0.83\n",
            "[20359 | 8178.07] loss=0.70 avg=0.83\n",
            "[20360 | 8181.16] loss=0.85 avg=0.83\n",
            "[20361 | 8184.23] loss=0.91 avg=0.83\n",
            "[20362 | 8187.30] loss=0.99 avg=0.83\n",
            "[20363 | 8190.38] loss=0.76 avg=0.83\n",
            "[20364 | 8193.45] loss=0.79 avg=0.83\n",
            "[20365 | 8196.55] loss=0.85 avg=0.83\n",
            "[20366 | 8199.63] loss=0.80 avg=0.83\n",
            "[20367 | 8202.72] loss=0.91 avg=0.83\n",
            "[20368 | 8205.79] loss=0.82 avg=0.83\n",
            "[20369 | 8208.83] loss=0.83 avg=0.83\n",
            "[20370 | 8211.92] loss=0.91 avg=0.83\n",
            "[20371 | 8215.00] loss=0.89 avg=0.83\n",
            "[20372 | 8218.10] loss=0.83 avg=0.83\n",
            "[20373 | 8221.19] loss=0.80 avg=0.83\n",
            "[20374 | 8224.27] loss=0.86 avg=0.83\n",
            "[20375 | 8227.34] loss=0.79 avg=0.83\n",
            "[20376 | 8230.42] loss=0.70 avg=0.83\n",
            "[20377 | 8233.48] loss=0.78 avg=0.83\n",
            "[20378 | 8236.57] loss=0.68 avg=0.83\n",
            "[20379 | 8239.63] loss=0.65 avg=0.83\n",
            "[20380 | 8242.69] loss=0.71 avg=0.83\n",
            "[20381 | 8245.76] loss=0.84 avg=0.83\n",
            "[20382 | 8248.82] loss=0.89 avg=0.83\n",
            "[20383 | 8251.87] loss=0.80 avg=0.83\n",
            "[20384 | 8254.95] loss=0.71 avg=0.83\n",
            "[20385 | 8258.04] loss=0.68 avg=0.82\n",
            "[20386 | 8261.13] loss=0.64 avg=0.82\n",
            "[20387 | 8264.21] loss=0.82 avg=0.82\n",
            "[20388 | 8267.30] loss=0.68 avg=0.82\n",
            "[20389 | 8270.39] loss=0.77 avg=0.82\n",
            "[20390 | 8273.49] loss=0.88 avg=0.82\n",
            "[20391 | 8276.58] loss=0.82 avg=0.82\n",
            "[20392 | 8279.67] loss=0.94 avg=0.82\n",
            "[20393 | 8282.77] loss=0.89 avg=0.82\n",
            "[20394 | 8285.86] loss=0.79 avg=0.82\n",
            "[20395 | 8288.95] loss=0.80 avg=0.82\n",
            "[20396 | 8292.03] loss=0.66 avg=0.82\n",
            "[20397 | 8295.12] loss=0.88 avg=0.82\n",
            "[20398 | 8298.20] loss=1.00 avg=0.82\n",
            "[20399 | 8301.30] loss=0.89 avg=0.82\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "a?\n",
            "How many languages do you speak? = ¿Cuántos idiomas hablas tienes?\n",
            "He is kind to me. = Él es amable conmigo.\n",
            "Tom could hear what was being said but couldn't do anything to stop it. = Tom puede oír lo que estaba diciendo pero no pude hacer nada para parar.\n",
            "Tom wasn't able to get in touch with Mary. = Tom no pudo contactarme con Mary.\n",
            "His explanation was too abstract and too strange. = Su explicación era demasiado abstracto y demasiado especial.\n",
            "You're just a kid. = Eres solo un niño.\n",
            "I feel so lost. = Me siento tan perdido.\n",
            "The children are playing in the sandbox. = Los niños están jugando en el arenero.\n",
            "You should know it. = Debes saberlo.\n",
            "It's all yours. = Todo es tuyo.\n",
            "The bus will arrive soon. = El autobús llegará pronto.\n",
            "I'm ready. = Yo estoy listo.\n",
            "I wanted so badly you'd remember. = Quería mucho tratarme en caso de mí.\n",
            "How many books can I borrow? = ¿Cuántos libros puedo tomar?\n",
            "I'm getting sick and tired of your complaining. = Me estoy harto de tus mal tareas.\n",
            "I was just trying to help out. = Solo trato de ayudar.\n",
            "We're going to start on Monday. = Nos vamos a empezar el lunes.\n",
            "Tom should've married Mary. = Tom debió haberse casado Mary.\n",
            "He is kind. = Él es gentil.\n",
            "He is very mean to me. = Él es muy gacho conmigo.\n",
            "He is a kind boy. = Él es un chico gentil.\n",
            "He is a doctor and so am I. = Él es médico, y yo tampoco.\n",
            "Tom isn't good at telling jokes. = A Tom se le da bien les coge chistes.\n",
            "She was advised by him on how to stay healthy. = Él le aconsejó sobre cómo mantenerse sana.\n",
            "Mary is Tom's grandmother. = Mary es la abuela de Tom.\n",
            "I think we should do something funny to commemorate the occasion. = Creo que deberíamos hacer algo divertido para celebrar el evento.\n",
            "I need to know what to say. = Necesito saber qué decir.\n",
            "Tell me which of the two options you prefer. = Decime cuál de los dos alternativas prefieren.\n",
            "Tom wants to know what time Mary gets home. = Tom quiere saber a qué hora Maria sale María.\n",
            "Where can I change the port in my cruise? = ¿En dónde puedo cambiar la vehícula del crucero?\n",
            "I didn't think about it that much. = No pensé tanto en ello.\n",
            "Are you certain that you don't know what you want? = ¿Estás seguro de que sí no sabes qué quieres?\n",
            "When do you eat? = ¿Cuándo comes?\n",
            "I'm ready to go to jail. = Estoy listo para ir a la cárcel.\n",
            "The cat was so shy it didn't want to jump on the bed. = El gato era tan tímido que no quería saltar sobre la cama.\n",
            "You're not jealous because I am, but you are also not. = No están celosos porque yo voy, sino también no.\n",
            "I want you to listen. = Quiero que escuches.\n",
            "I'm not in the mood for now. = No estoy de humor para ahora.\n",
            "Tom went and picked us up. = Tom salió y nos cogieron.\n",
            "I will have him call you at home. = Voy a enfermerte en casa.\n",
            "He looked up at the sky. = Él miró al seto por el cielo.\n",
            "My son sleeps under the couch. = Mi hijo duerme bajo el sofá.\n",
            "She is busy sewing little gifts. = Ella está ocupada cosiendo rellenos pequeños.\n",
            "I didn't feel up for a good fight. = No me apeteció una buena pelea.\n",
            "There's a very good coffee shop downtown\n",
            "\n",
            "[20400 | 8341.90] loss=0.98 avg=0.82\n",
            "[20401 | 8344.99] loss=0.71 avg=0.82\n",
            "[20402 | 8348.09] loss=0.91 avg=0.82\n",
            "[20403 | 8351.20] loss=0.66 avg=0.82\n",
            "[20404 | 8354.30] loss=0.81 avg=0.82\n",
            "[20405 | 8357.41] loss=0.74 avg=0.82\n",
            "[20406 | 8360.52] loss=0.95 avg=0.82\n",
            "[20407 | 8363.61] loss=0.83 avg=0.82\n",
            "[20408 | 8366.70] loss=0.97 avg=0.82\n",
            "[20409 | 8369.79] loss=0.87 avg=0.83\n",
            "[20410 | 8372.88] loss=0.66 avg=0.82\n",
            "[20411 | 8375.97] loss=0.86 avg=0.82\n",
            "[20412 | 8379.07] loss=0.71 avg=0.82\n",
            "[20413 | 8382.15] loss=0.76 avg=0.82\n",
            "[20414 | 8385.23] loss=0.76 avg=0.82\n",
            "[20415 | 8388.34] loss=0.78 avg=0.82\n",
            "[20416 | 8391.42] loss=0.97 avg=0.82\n",
            "[20417 | 8394.51] loss=0.86 avg=0.82\n",
            "[20418 | 8397.60] loss=0.93 avg=0.82\n",
            "[20419 | 8400.70] loss=0.73 avg=0.82\n",
            "[20420 | 8403.79] loss=0.92 avg=0.82\n",
            "[20421 | 8406.90] loss=0.88 avg=0.82\n",
            "[20422 | 8410.00] loss=0.93 avg=0.83\n",
            "[20423 | 8413.07] loss=0.87 avg=0.83\n",
            "[20424 | 8416.10] loss=0.89 avg=0.83\n",
            "[20425 | 8419.18] loss=0.86 avg=0.83\n",
            "[20426 | 8422.26] loss=0.85 avg=0.83\n",
            "[20427 | 8425.37] loss=0.89 avg=0.83\n",
            "[20428 | 8428.48] loss=0.87 avg=0.83\n",
            "[20429 | 8431.58] loss=0.84 avg=0.83\n",
            "[20430 | 8434.69] loss=0.72 avg=0.83\n",
            "[20431 | 8437.80] loss=0.73 avg=0.83\n",
            "[20432 | 8440.90] loss=0.74 avg=0.83\n",
            "[20433 | 8444.01] loss=0.78 avg=0.83\n",
            "[20434 | 8447.08] loss=0.99 avg=0.83\n",
            "[20435 | 8450.17] loss=0.80 avg=0.83\n",
            "[20436 | 8453.27] loss=0.85 avg=0.83\n",
            "[20437 | 8456.38] loss=0.85 avg=0.83\n",
            "[20438 | 8459.47] loss=0.92 avg=0.83\n",
            "[20439 | 8462.56] loss=0.77 avg=0.83\n",
            "[20440 | 8465.64] loss=0.69 avg=0.83\n",
            "[20441 | 8468.72] loss=0.76 avg=0.83\n",
            "[20442 | 8471.81] loss=0.62 avg=0.82\n",
            "[20443 | 8474.90] loss=0.90 avg=0.82\n",
            "[20444 | 8477.98] loss=0.80 avg=0.82\n",
            "[20445 | 8481.04] loss=0.66 avg=0.82\n",
            "[20446 | 8484.11] loss=0.63 avg=0.82\n",
            "[20447 | 8487.18] loss=0.85 avg=0.82\n",
            "[20448 | 8490.26] loss=0.76 avg=0.82\n",
            "[20449 | 8493.33] loss=0.69 avg=0.82\n",
            "[20450 | 8496.40] loss=0.75 avg=0.82\n",
            "[20451 | 8499.46] loss=0.71 avg=0.82\n",
            "[20452 | 8502.53] loss=0.89 avg=0.82\n",
            "[20453 | 8505.63] loss=0.70 avg=0.82\n",
            "[20454 | 8508.70] loss=0.68 avg=0.82\n",
            "[20455 | 8511.78] loss=0.79 avg=0.81\n",
            "[20456 | 8514.87] loss=0.95 avg=0.82\n",
            "[20457 | 8517.96] loss=0.69 avg=0.81\n",
            "[20458 | 8521.04] loss=0.94 avg=0.82\n",
            "[20459 | 8524.12] loss=0.88 avg=0.82\n",
            "[20460 | 8527.20] loss=0.98 avg=0.82\n",
            "[20461 | 8530.26] loss=0.78 avg=0.82\n",
            "[20462 | 8533.33] loss=0.67 avg=0.82\n",
            "[20463 | 8536.38] loss=0.79 avg=0.82\n",
            "[20464 | 8539.46] loss=0.77 avg=0.82\n",
            "[20465 | 8542.52] loss=0.91 avg=0.82\n",
            "[20466 | 8545.59] loss=0.79 avg=0.82\n",
            "[20467 | 8548.65] loss=0.92 avg=0.82\n",
            "[20468 | 8551.71] loss=0.60 avg=0.82\n",
            "[20469 | 8554.76] loss=0.70 avg=0.81\n",
            "[20470 | 8557.81] loss=0.68 avg=0.81\n",
            "[20471 | 8560.84] loss=0.96 avg=0.81\n",
            "[20472 | 8563.91] loss=0.89 avg=0.82\n",
            "[20473 | 8566.98] loss=0.83 avg=0.82\n",
            "[20474 | 8570.05] loss=0.65 avg=0.81\n",
            "[20475 | 8573.14] loss=0.98 avg=0.82\n",
            "[20476 | 8576.21] loss=0.87 avg=0.82\n",
            "[20477 | 8579.29] loss=0.81 avg=0.82\n",
            "[20478 | 8582.35] loss=0.69 avg=0.81\n",
            "[20479 | 8585.42] loss=0.76 avg=0.81\n",
            "[20480 | 8588.50] loss=0.84 avg=0.81\n",
            "[20481 | 8591.57] loss=0.84 avg=0.81\n",
            "[20482 | 8594.63] loss=0.90 avg=0.82\n",
            "[20483 | 8597.71] loss=0.76 avg=0.81\n",
            "[20484 | 8600.80] loss=0.75 avg=0.81\n",
            "[20485 | 8603.89] loss=0.74 avg=0.81\n",
            "[20486 | 8606.96] loss=0.90 avg=0.81\n",
            "[20487 | 8610.05] loss=0.88 avg=0.81\n",
            "[20488 | 8613.14] loss=0.67 avg=0.81\n",
            "[20489 | 8616.20] loss=0.78 avg=0.81\n",
            "[20490 | 8619.27] loss=0.71 avg=0.81\n",
            "[20491 | 8622.36] loss=0.71 avg=0.81\n",
            "[20492 | 8625.45] loss=0.70 avg=0.81\n",
            "[20493 | 8628.55] loss=0.89 avg=0.81\n",
            "[20494 | 8631.63] loss=0.81 avg=0.81\n",
            "[20495 | 8634.73] loss=0.88 avg=0.81\n",
            "[20496 | 8637.83] loss=0.97 avg=0.81\n",
            "[20497 | 8640.92] loss=0.85 avg=0.81\n",
            "[20498 | 8644.00] loss=0.88 avg=0.81\n",
            "[20499 | 8647.10] loss=0.77 avg=0.81\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "?\n",
            "She took care of him wherever she went. = Ella cuidó de él donde se iba.\n",
            "This box is too heavy. = Esta caja es demasiado pesada.\n",
            "The room was cleaned by Tom and Mary. = La habitación fue limpiada por Tom y María.\n",
            "I'll have to explain it to Tom. = Tom tendré que echarle una explicación a que le conteste.\n",
            "We made him work even more. = Le hicimos que trabajase aún más.\n",
            "I have to buy a new saxophone. = Tengo que comprar un saxofón nuevo.\n",
            "This is very bad. = Esto es muy malo.\n",
            "I thought you were Canadian. = Pensé que eras canadiense.\n",
            "Tom is now in prison. = Tom ahora está en prisión.\n",
            "They are playing basketball. = Ellos están jugando al baloncesto.\n",
            "They don't like me. = No les gusto a los autobacs.\n",
            "We have more in common than I expected. = Tenemos más en común de lo que se esperaba.\n",
            "I hate my husband. = Odio a mi marido.\n",
            "Don't get drunk. Just drink some water. = No te cleanses, solo beba de agua.\n",
            "There's an orange on the table. = Hay una naranja sobre el escritorio.\n",
            "He knows a lot about animals. = Él sabe mucho acerca de animales.\n",
            "I have to look for my keys. = Tengo que buscar mis llaves.\n",
            "The dog jumped over the fence. = El perro saltó sobre la cerca.\n",
            "I want another one! = ¡Quiero otro!\n",
            "I'm not sure about it. = Estoy lo wronga.\n",
            "Tom can't cook well. = Tom no puede cocinar bien.\n",
            "I want to ask Tom to sign my book. = Quiero pedirle a Tom que firmaré mi libro.\n",
            "This dog is purebred. = Este perro es clínico.\n",
            "Do you mind if I open the window? = ¿Te molesta si abro la ventana?\n",
            "Who's that woman? = ¿Quién es aquella mujer?\n",
            "That's a stupid rumor. = Ese es un rumor tonta.\n",
            "Let's sing. = Cantemos.\n",
            "Let's not get too excited. = No nos emocionamos demasiado!\n",
            "Tom always puts himself first. = Tom siempre se pone en primer lugar.\n",
            "When do you think he'll come? = ¿Cuándo crees que vendrá?\n",
            "Tom is in his room. = Tom está en su habitación.\n",
            "It was an ugly battle. = Era una lucha deteña.\n",
            "She lives in a small house on the floor. = Ella vive en una casa pequeña en el piso.\n",
            "This is the last time. = Es la última vez.\n",
            "Do you want me to explain it to you. = ¿Desea te explicarle?\n",
            "The children went to school in spite of the heat. = Los niños fueron a clases a pesar de el calor.\n",
            "Tom is not married. = Tom no está casado.\n",
            "Tom doesn't want to talk to Mary anymore. = Tom ya no quiere hablar con Mary.\n",
            "I want you to keep me informed. = Quiero que me shanne enseñando.\n",
            "Did you invite him? = ¿Le has invitado?\n",
            "We want to tell you something important. = Queremos decirte algo importante.\n",
            "You are my teacher. = Tú eres mi profesor.\n",
            "The train is at the station. = El tren está en la estación.\n",
            "You can't hide forever. = ¡No te puedes esconder para otro eterno!\n",
            "He's going to come over tomorrow, won't he? = Él va a pasar a pasar mañana, ¿no?\n",
            "I just want to let you know that I think you're the most beautiful woman alive. = Sólo quiero hacerte saber que que que os pienso pusieras las mujeras viviennes actuales.\n",
            "My wife is a vegetarian. = Mi mujer es vegetariano.\n",
            "My wife used to smoke. = Mi esposa soltó fumar.\n",
            "The only\n",
            "\n",
            "[20500 | 8687.87] loss=0.83 avg=0.81\n",
            "[20501 | 8690.97] loss=0.65 avg=0.81\n",
            "[20502 | 8694.07] loss=0.72 avg=0.81\n",
            "[20503 | 8697.15] loss=0.63 avg=0.81\n",
            "[20504 | 8700.25] loss=0.66 avg=0.81\n",
            "[20505 | 8703.33] loss=0.78 avg=0.81\n",
            "[20506 | 8706.41] loss=0.71 avg=0.81\n",
            "[20507 | 8709.51] loss=0.80 avg=0.81\n",
            "[20508 | 8712.59] loss=1.03 avg=0.81\n",
            "[20509 | 8715.68] loss=0.77 avg=0.81\n",
            "[20510 | 8718.77] loss=0.97 avg=0.81\n",
            "[20511 | 8721.86] loss=0.90 avg=0.81\n",
            "[20512 | 8724.97] loss=0.96 avg=0.81\n",
            "[20513 | 8728.06] loss=0.69 avg=0.81\n",
            "[20514 | 8731.14] loss=0.78 avg=0.81\n",
            "[20515 | 8734.21] loss=0.73 avg=0.81\n",
            "[20516 | 8737.28] loss=0.77 avg=0.81\n",
            "[20517 | 8740.36] loss=0.85 avg=0.81\n",
            "[20518 | 8743.45] loss=0.80 avg=0.81\n",
            "[20519 | 8746.53] loss=0.66 avg=0.81\n",
            "[20520 | 8749.62] loss=0.77 avg=0.81\n",
            "[20521 | 8752.70] loss=0.74 avg=0.81\n",
            "[20522 | 8755.79] loss=0.85 avg=0.81\n",
            "[20523 | 8758.88] loss=0.72 avg=0.81\n",
            "[20524 | 8761.97] loss=0.89 avg=0.81\n",
            "[20525 | 8765.05] loss=0.92 avg=0.81\n",
            "[20526 | 8768.14] loss=0.89 avg=0.81\n",
            "[20527 | 8771.21] loss=0.72 avg=0.81\n",
            "[20528 | 8774.32] loss=0.84 avg=0.81\n",
            "[20529 | 8777.38] loss=0.95 avg=0.81\n",
            "[20530 | 8780.47] loss=0.91 avg=0.81\n",
            "[20531 | 8783.56] loss=0.87 avg=0.81\n",
            "[20532 | 8786.65] loss=0.80 avg=0.81\n",
            "[20533 | 8789.74] loss=0.84 avg=0.81\n",
            "[20534 | 8792.83] loss=0.86 avg=0.81\n",
            "[20535 | 8795.92] loss=0.69 avg=0.81\n",
            "[20536 | 8799.01] loss=0.84 avg=0.81\n",
            "[20537 | 8802.10] loss=0.73 avg=0.81\n",
            "[20538 | 8805.19] loss=0.80 avg=0.81\n",
            "[20539 | 8808.27] loss=0.94 avg=0.81\n",
            "[20540 | 8811.35] loss=0.68 avg=0.81\n",
            "[20541 | 8814.41] loss=0.63 avg=0.81\n",
            "[20542 | 8817.49] loss=0.93 avg=0.81\n",
            "[20543 | 8820.58] loss=0.80 avg=0.81\n",
            "[20544 | 8823.67] loss=0.89 avg=0.81\n",
            "[20545 | 8826.75] loss=0.70 avg=0.81\n",
            "[20546 | 8829.82] loss=0.81 avg=0.81\n",
            "[20547 | 8832.91] loss=0.68 avg=0.81\n",
            "[20548 | 8836.00] loss=0.84 avg=0.81\n",
            "[20549 | 8839.08] loss=0.91 avg=0.81\n",
            "[20550 | 8842.16] loss=0.97 avg=0.81\n",
            "[20551 | 8845.23] loss=0.87 avg=0.81\n",
            "[20552 | 8848.32] loss=0.92 avg=0.81\n",
            "[20553 | 8851.42] loss=0.87 avg=0.81\n",
            "[20554 | 8854.51] loss=0.87 avg=0.81\n",
            "[20555 | 8857.61] loss=0.86 avg=0.81\n",
            "[20556 | 8860.68] loss=0.88 avg=0.82\n",
            "[20557 | 8863.78] loss=0.76 avg=0.81\n",
            "[20558 | 8866.88] loss=0.73 avg=0.81\n",
            "[20559 | 8869.99] loss=0.68 avg=0.81\n",
            "[20560 | 8873.10] loss=0.97 avg=0.81\n",
            "[20561 | 8876.21] loss=0.86 avg=0.81\n",
            "[20562 | 8879.30] loss=0.80 avg=0.81\n",
            "[20563 | 8882.39] loss=0.89 avg=0.81\n",
            "[20564 | 8885.48] loss=0.71 avg=0.81\n",
            "[20565 | 8888.56] loss=0.88 avg=0.81\n",
            "[20566 | 8891.64] loss=0.89 avg=0.82\n",
            "[20567 | 8894.72] loss=0.93 avg=0.82\n",
            "[20568 | 8897.79] loss=0.91 avg=0.82\n",
            "[20569 | 8900.87] loss=0.76 avg=0.82\n",
            "[20570 | 8903.93] loss=1.01 avg=0.82\n",
            "[20571 | 8907.02] loss=0.83 avg=0.82\n",
            "[20572 | 8910.10] loss=0.90 avg=0.82\n",
            "[20573 | 8913.18] loss=0.75 avg=0.82\n",
            "[20574 | 8916.28] loss=0.67 avg=0.82\n",
            "[20575 | 8919.36] loss=0.82 avg=0.82\n",
            "[20576 | 8922.45] loss=0.97 avg=0.82\n",
            "[20577 | 8925.53] loss=0.96 avg=0.82\n",
            "[20578 | 8928.63] loss=0.93 avg=0.82\n",
            "[20579 | 8931.73] loss=0.87 avg=0.82\n",
            "[20580 | 8934.81] loss=0.74 avg=0.82\n",
            "[20581 | 8937.89] loss=0.84 avg=0.82\n",
            "[20582 | 8940.98] loss=1.01 avg=0.82\n",
            "[20583 | 8944.04] loss=0.80 avg=0.82\n",
            "[20584 | 8947.13] loss=0.83 avg=0.82\n",
            "[20585 | 8950.22] loss=0.79 avg=0.82\n",
            "[20586 | 8953.32] loss=0.92 avg=0.82\n",
            "[20587 | 8956.40] loss=0.72 avg=0.82\n",
            "[20588 | 8959.50] loss=0.80 avg=0.82\n",
            "[20589 | 8962.58] loss=0.84 avg=0.82\n",
            "[20590 | 8965.67] loss=0.92 avg=0.82\n",
            "[20591 | 8968.77] loss=0.93 avg=0.82\n",
            "[20592 | 8971.86] loss=0.92 avg=0.83\n",
            "[20593 | 8974.97] loss=0.75 avg=0.82\n",
            "[20594 | 8978.07] loss=0.82 avg=0.82\n",
            "[20595 | 8981.16] loss=0.96 avg=0.83\n",
            "[20596 | 8984.25] loss=0.75 avg=0.83\n",
            "[20597 | 8987.35] loss=0.70 avg=0.82\n",
            "[20598 | 8990.44] loss=0.85 avg=0.82\n",
            "[20599 | 8993.53] loss=0.77 avg=0.82\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "aa en el trabajo.\n",
            "You know a lot about Tom. = Existen mucho de Tom.\n",
            "I didn't go on a diet. = No fui a dieta.\n",
            "I'm sure it wasn't planned. = Estoy seguro de que no era planeado.\n",
            "It took them some time to get used to each other. = Tardaron un poco de tiempo en acostumbrarse el uno al otro.\n",
            "He was a great leader. = Él fue un gran líder.\n",
            "I'm busy at the moment. = En este momento estoy ocupado.\n",
            "We're not that far. = No estamos demasiado lejos.\n",
            "Tom tried to get Mary to join the team. = Tom trató de conseguir que Mary se una una equipa al equipo.\n",
            "He was absent from school yesterday. = Ayer él absenceé a clase.\n",
            "I would like to talk to one of your guests. = Me gustaría hablar con uno de sus invitados.\n",
            "Tom gave Mary a hug and a kiss. = Tom le dio un abrazo a Mary e a besar.\n",
            "Tom hasn't finished yet. = Tom no ha terminado todo.\n",
            "You look fine. = Estás muy bien.\n",
            "The boy started crying. = El chico empezó a llorar.\n",
            "If I told you, you would be stupid. = Si te lo contaras, os fuésemos estúpido.\n",
            "I can't sleep. = No puedo dormir.\n",
            "You're very good at tennis. = Eres muy bueno para el tenis.\n",
            "What can she do? = ¿Qué puede usted hacer?\n",
            "I'm not that kind of guy. = No soy esa clase de tipo.\n",
            "Tom doesn't like it when Mary looks at him. = A Tom no le gustó que Mary lo mira.\n",
            "Do you think so? = ¿Te parezca?\n",
            "Tom is going to come over. = Tom va a pasará.\n",
            "Let me pay for it myself. = Déjame pagarlo mismo.\n",
            "How was school today? = ¿Cómo estuvo una clase hoy?\n",
            "Tom said he didn't have enough money. = Tomás dijo que no tenía dinero suficiente.\n",
            "Tom's car is old. = El carro de Tom es viejo.\n",
            "Can you tell me what this is? = ¿Puede decirme qué es esto?\n",
            "She was crying. = Ella estaba llorando.\n",
            "It is no use asking me for money. = No tiene caso pedirme dinero.\n",
            "Don't even think of trying to hide it. = Ni se te ocurra trataremos.\n",
            "Tom took off his leather jacket. = Tomás se quitó su chaqueta de cuero.\n",
            "It's important. = Es importante.\n",
            "This book is mine. = Este libro es mío.\n",
            "I don't have much time. = No tengo mucho tiempo.\n",
            "I'm not saying it's not possible. = No trato de que no sea posible.\n",
            "Tom's new girlfriend took a shine to him. = La nueva novia de Tom se le lusa de él.\n",
            "Tom is very handsome. = Tom es muy atractivo.\n",
            "Tom doesn't think that Mary was wearing a red blouse. = Tom no piensa que Mary llevaba una blusa roja.\n",
            "It is good to see you again. = Qué bueno verte de nuevo.\n",
            "My mom thinks I'm an alien. = Mi mamá piensa que yo soy un alien.\n",
            "We'll find somebody else. = Hemos encontrarnar a alguien más.\n",
            "She must have known her place. = Debe saber su lugar.\n",
            "I have done that many times. = Lo he hecho muchas veces.\n",
            "I'm already a man. = Ya soy un hombre.\n",
            "You may use my bicycle. = Puedes usar mi bicicleta.\n",
            "They're in danger. = Ellos están en peligro.\n",
            "I would like to visit Spain someday. = Quisiera visitar España algún día.\n",
            "When I was in college, I tried to get a grip on my anger. = Cuando estaba en la universidad, me intenté apretar el enfado.\n",
            "She has no regret about losing the child. = Ell\n",
            "\n",
            "[20600 | 9034.05] loss=0.79 avg=0.82\n",
            "[20601 | 9037.14] loss=0.90 avg=0.82\n",
            "[20602 | 9040.23] loss=0.75 avg=0.82\n",
            "[20603 | 9043.34] loss=0.82 avg=0.82\n",
            "[20604 | 9046.44] loss=0.67 avg=0.82\n",
            "[20605 | 9049.53] loss=0.72 avg=0.82\n",
            "[20606 | 9052.61] loss=0.75 avg=0.82\n",
            "[20607 | 9055.67] loss=0.91 avg=0.82\n",
            "[20608 | 9058.74] loss=0.75 avg=0.82\n",
            "[20609 | 9061.83] loss=0.80 avg=0.82\n",
            "[20610 | 9064.92] loss=0.68 avg=0.82\n",
            "[20611 | 9068.01] loss=0.72 avg=0.82\n",
            "[20612 | 9071.08] loss=0.80 avg=0.82\n",
            "[20613 | 9074.15] loss=0.72 avg=0.82\n",
            "[20614 | 9077.22] loss=0.77 avg=0.82\n",
            "[20615 | 9080.30] loss=0.83 avg=0.82\n",
            "[20616 | 9083.38] loss=0.81 avg=0.82\n",
            "[20617 | 9086.47] loss=0.75 avg=0.82\n",
            "[20618 | 9089.57] loss=0.79 avg=0.82\n",
            "[20619 | 9092.66] loss=0.66 avg=0.81\n",
            "[20620 | 9095.73] loss=0.91 avg=0.81\n",
            "[20621 | 9098.79] loss=0.88 avg=0.82\n",
            "[20622 | 9101.88] loss=0.82 avg=0.82\n",
            "[20623 | 9104.95] loss=0.99 avg=0.82\n",
            "[20624 | 9108.02] loss=0.76 avg=0.82\n",
            "[20625 | 9111.10] loss=0.75 avg=0.82\n",
            "[20626 | 9114.20] loss=0.92 avg=0.82\n",
            "[20627 | 9117.28] loss=0.91 avg=0.82\n",
            "[20628 | 9120.36] loss=0.98 avg=0.82\n",
            "[20629 | 9123.45] loss=0.89 avg=0.82\n",
            "[20630 | 9126.52] loss=0.67 avg=0.82\n",
            "[20631 | 9129.61] loss=0.73 avg=0.82\n",
            "[20632 | 9132.69] loss=0.67 avg=0.82\n",
            "[20633 | 9135.77] loss=0.84 avg=0.82\n",
            "[20634 | 9138.84] loss=0.78 avg=0.82\n",
            "[20635 | 9141.94] loss=0.92 avg=0.82\n",
            "[20636 | 9145.00] loss=0.79 avg=0.82\n",
            "[20637 | 9148.08] loss=0.69 avg=0.82\n",
            "[20638 | 9151.18] loss=0.97 avg=0.82\n",
            "[20639 | 9154.25] loss=0.82 avg=0.82\n",
            "[20640 | 9157.34] loss=0.90 avg=0.82\n",
            "[20641 | 9160.42] loss=0.95 avg=0.82\n",
            "[20642 | 9163.50] loss=0.95 avg=0.82\n",
            "[20643 | 9166.58] loss=0.95 avg=0.82\n",
            "[20644 | 9169.67] loss=0.74 avg=0.82\n",
            "[20645 | 9172.73] loss=0.77 avg=0.82\n",
            "[20646 | 9175.81] loss=0.86 avg=0.82\n",
            "[20647 | 9178.87] loss=0.80 avg=0.82\n",
            "[20648 | 9181.92] loss=0.88 avg=0.82\n",
            "[20649 | 9185.00] loss=0.88 avg=0.82\n",
            "[20650 | 9188.08] loss=0.79 avg=0.82\n",
            "[20651 | 9191.17] loss=0.70 avg=0.82\n",
            "[20652 | 9194.25] loss=0.75 avg=0.82\n",
            "[20653 | 9197.32] loss=0.80 avg=0.82\n",
            "[20654 | 9200.39] loss=0.78 avg=0.82\n",
            "[20655 | 9203.48] loss=0.83 avg=0.82\n",
            "[20656 | 9206.56] loss=0.80 avg=0.82\n",
            "[20657 | 9209.63] loss=0.91 avg=0.82\n",
            "[20658 | 9212.71] loss=0.73 avg=0.82\n",
            "[20659 | 9215.78] loss=0.84 avg=0.82\n",
            "[20660 | 9218.89] loss=0.72 avg=0.82\n",
            "[20661 | 9221.97] loss=0.77 avg=0.82\n",
            "[20662 | 9225.07] loss=0.94 avg=0.82\n",
            "[20663 | 9228.15] loss=0.62 avg=0.82\n",
            "[20664 | 9231.24] loss=0.79 avg=0.82\n",
            "[20665 | 9234.33] loss=0.92 avg=0.82\n",
            "[20666 | 9237.42] loss=0.74 avg=0.82\n",
            "[20667 | 9240.53] loss=0.93 avg=0.82\n",
            "[20668 | 9243.62] loss=0.67 avg=0.82\n",
            "[20669 | 9246.73] loss=0.90 avg=0.82\n",
            "[20670 | 9249.82] loss=0.80 avg=0.82\n",
            "[20671 | 9252.92] loss=0.75 avg=0.82\n",
            "[20672 | 9256.00] loss=0.87 avg=0.82\n",
            "[20673 | 9259.08] loss=0.62 avg=0.82\n",
            "[20674 | 9262.18] loss=0.92 avg=0.82\n",
            "[20675 | 9265.26] loss=0.75 avg=0.82\n",
            "[20676 | 9268.34] loss=0.87 avg=0.82\n",
            "[20677 | 9271.44] loss=0.73 avg=0.82\n",
            "[20678 | 9274.54] loss=0.72 avg=0.81\n",
            "[20679 | 9277.64] loss=0.83 avg=0.81\n",
            "[20680 | 9280.73] loss=0.97 avg=0.82\n",
            "[20681 | 9283.82] loss=0.80 avg=0.82\n",
            "[20682 | 9286.92] loss=0.92 avg=0.82\n",
            "[20683 | 9290.00] loss=0.74 avg=0.82\n",
            "[20684 | 9293.11] loss=0.64 avg=0.81\n",
            "[20685 | 9296.20] loss=0.82 avg=0.81\n",
            "[20686 | 9299.30] loss=1.00 avg=0.82\n",
            "[20687 | 9302.39] loss=0.68 avg=0.82\n",
            "[20688 | 9305.49] loss=0.76 avg=0.81\n",
            "[20689 | 9308.59] loss=0.61 avg=0.81\n",
            "[20690 | 9311.69] loss=0.75 avg=0.81\n",
            "[20691 | 9314.78] loss=0.97 avg=0.81\n",
            "[20692 | 9317.87] loss=0.92 avg=0.81\n",
            "[20693 | 9320.96] loss=0.65 avg=0.81\n",
            "[20694 | 9324.05] loss=0.78 avg=0.81\n",
            "[20695 | 9327.15] loss=0.70 avg=0.81\n",
            "[20696 | 9330.26] loss=0.86 avg=0.81\n",
            "[20697 | 9333.35] loss=0.71 avg=0.81\n",
            "[20698 | 9336.42] loss=0.85 avg=0.81\n",
            "[20699 | 9339.49] loss=0.74 avg=0.81\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Tom.\n",
            "When I told Tom about your accident, he wrote a letter to you. = Al vez que le dijiste a Tom una carta sobre tu accidente, él le escribió una carta.\n",
            "You can't run from the past. = No puedes huir del pasado.\n",
            "You can't blame them. = No pueden culpablele.\n",
            "Tom couldn't get Mary to tell him where John was. = Tom no pudo conseguir que Mary le dijera donde él era John.\n",
            "I'll see you tomorrow. = Te veré mañana.\n",
            "Who do you want to talk to? = ¿Con quién quieres hablar?\n",
            "She is a very shy girl. = Es una niña muy tía.\n",
            "She took over the business after her husband's death. = Ella se hizo cargo del negocio después de la muerte de su esposo.\n",
            "I have little money in my wallet. = Tengo poco dinero en mi monedero.\n",
            "I got it for free. = La conseguí lo libre.\n",
            "I'll be waiting for you. = Te estaré esperándolo.\n",
            "I can't believe it. = No puedo creerlo.\n",
            "That's none of your concern. = Eso no es asunto tuyo.\n",
            "What's there to break? = ¿Qué hay nada que romper?\n",
            "If I were a bird, I would fly to you. = Si yo fuera un pájaro volando hasta ti.\n",
            "When I got to the station, the train was gone. = Cuando llegué a la estación, el tren había desaparecido.\n",
            "Tom is playing with Mary's cat. = Tomás está jugando con el gato de Mary.\n",
            "It's good to see you again. = Que gusto verte de nuevo.\n",
            "We're eating popcorn. = Estamos comiendo pochoclo.\n",
            "How much does he expect to receive in return for his gift? = ¿Cuánto espera que recibir su obsequio?\n",
            "This is too large. = Esto es demasiado grande.\n",
            "He likes to play baseball. = Le gusta jugar al béisbol.\n",
            "You are the tallest man I know. = Tú eres el hombre más alto que conozco.\n",
            "The ship sailed for Hawaii. = El barco navegó a Hawaiʻi.\n",
            "I have a sharp pain here. = Tengo un agudo dolor aquí.\n",
            "Is that all? = ¿Es todo?\n",
            "How long do Christmas trees last? = ¿Cuánto duran los árboles de Navidades?\n",
            "You're always asking me to do your homework. = Tú siempre me estás pidiendo que yo haga tu tarea.\n",
            "I'm an only child. = Soy hijo único.\n",
            "Where do you think Tom would be? = ¿Dónde crees que sería Tom?\n",
            "When does the last train leave? = ¿A dónde sale el último tren?\n",
            "His mother was worried about his bad health. = Su madre estaba inquieta la mala salud de él.\n",
            "I wish I could be in Paris now. = Desearía poder estar en París en este momento.\n",
            "Tom died at the age of ninety. = Tom murió a la edad de 90 años.\n",
            "The more I thought about it, the less I liked it. = Cuanto más consideraba sobre eso, menos la agrada.\n",
            "What time is the bus? = ¿A qué hora es el autobús?\n",
            "They arrived too early. = Ellos llegaron demasiado temprano.\n",
            "I would like to get home by sunset. = Me gustaría volver a casa al sol supimos.\n",
            "I have a cousin. = Tengo una prima.\n",
            "She gave him something cold to drink. = Ella le dio algo frío para beber.\n",
            "He has learned to be patient. = A él se le aprendió a ser paciente.\n",
            "That's a magnificent view. = Esa es una magnífica vista.\n",
            "We should learn from his mistakes. = Deberíamos aprender de sus errores.\n",
            "He is not as tall as his brother. = No es tan\n",
            "\n",
            "[20700 | 9380.14] loss=0.91 avg=0.81\n",
            "[20701 | 9383.23] loss=0.95 avg=0.81\n",
            "[20702 | 9386.32] loss=0.94 avg=0.81\n",
            "[20703 | 9389.40] loss=0.82 avg=0.81\n",
            "[20704 | 9392.48] loss=0.92 avg=0.82\n",
            "[20705 | 9395.57] loss=0.70 avg=0.81\n",
            "[20706 | 9398.64] loss=0.73 avg=0.81\n",
            "[20707 | 9401.73] loss=0.90 avg=0.81\n",
            "[20708 | 9404.83] loss=0.88 avg=0.82\n",
            "[20709 | 9407.92] loss=0.82 avg=0.82\n",
            "[20710 | 9411.00] loss=0.79 avg=0.81\n",
            "[20711 | 9414.09] loss=0.77 avg=0.81\n",
            "[20712 | 9417.16] loss=0.80 avg=0.81\n",
            "[20713 | 9420.25] loss=0.93 avg=0.82\n",
            "[20714 | 9423.35] loss=0.81 avg=0.82\n",
            "[20715 | 9426.44] loss=0.96 avg=0.82\n",
            "[20716 | 9429.53] loss=0.87 avg=0.82\n",
            "[20717 | 9432.61] loss=0.88 avg=0.82\n",
            "[20718 | 9435.72] loss=0.96 avg=0.82\n",
            "[20719 | 9438.80] loss=0.72 avg=0.82\n",
            "[20720 | 9441.89] loss=0.83 avg=0.82\n",
            "[20721 | 9444.98] loss=0.87 avg=0.82\n",
            "[20722 | 9448.07] loss=0.83 avg=0.82\n",
            "[20723 | 9451.17] loss=0.85 avg=0.82\n",
            "[20724 | 9454.25] loss=0.76 avg=0.82\n",
            "[20725 | 9457.36] loss=0.68 avg=0.82\n",
            "[20726 | 9460.45] loss=0.74 avg=0.82\n",
            "[20727 | 9463.53] loss=0.67 avg=0.82\n",
            "[20728 | 9466.63] loss=0.77 avg=0.81\n",
            "[20729 | 9469.72] loss=0.82 avg=0.81\n",
            "[20730 | 9472.82] loss=0.81 avg=0.81\n",
            "[20731 | 9475.90] loss=0.81 avg=0.81\n",
            "[20732 | 9478.98] loss=0.69 avg=0.81\n",
            "[20733 | 9482.07] loss=1.00 avg=0.82\n",
            "[20734 | 9485.15] loss=0.77 avg=0.81\n",
            "[20735 | 9488.24] loss=0.82 avg=0.81\n",
            "[20736 | 9491.32] loss=0.67 avg=0.81\n",
            "[20737 | 9494.40] loss=0.93 avg=0.81\n",
            "[20738 | 9497.46] loss=0.68 avg=0.81\n",
            "[20739 | 9500.55] loss=0.74 avg=0.81\n",
            "[20740 | 9503.65] loss=0.83 avg=0.81\n",
            "[20741 | 9506.74] loss=0.82 avg=0.81\n",
            "[20742 | 9509.85] loss=0.86 avg=0.81\n",
            "[20743 | 9512.95] loss=0.66 avg=0.81\n",
            "[20744 | 9516.03] loss=0.70 avg=0.81\n",
            "[20745 | 9519.12] loss=0.72 avg=0.81\n",
            "[20746 | 9522.22] loss=0.73 avg=0.81\n",
            "[20747 | 9525.31] loss=0.86 avg=0.81\n",
            "[20748 | 9528.41] loss=0.79 avg=0.81\n",
            "[20749 | 9531.50] loss=0.66 avg=0.81\n",
            "[20750 | 9534.58] loss=0.98 avg=0.81\n",
            "[20751 | 9537.67] loss=0.81 avg=0.81\n",
            "[20752 | 9540.77] loss=0.82 avg=0.81\n",
            "[20753 | 9543.87] loss=0.87 avg=0.81\n",
            "[20754 | 9546.95] loss=0.91 avg=0.81\n",
            "[20755 | 9550.04] loss=0.73 avg=0.81\n",
            "[20756 | 9553.14] loss=0.61 avg=0.81\n",
            "[20757 | 9556.23] loss=0.80 avg=0.81\n",
            "[20758 | 9559.34] loss=0.70 avg=0.81\n",
            "[20759 | 9562.43] loss=0.93 avg=0.81\n",
            "[20760 | 9565.53] loss=0.81 avg=0.81\n",
            "[20761 | 9568.62] loss=0.88 avg=0.81\n",
            "[20762 | 9571.72] loss=0.74 avg=0.81\n",
            "[20763 | 9574.80] loss=0.79 avg=0.81\n",
            "[20764 | 9577.91] loss=0.93 avg=0.81\n",
            "[20765 | 9580.99] loss=0.93 avg=0.81\n",
            "[20766 | 9584.09] loss=0.76 avg=0.81\n",
            "[20767 | 9587.20] loss=0.82 avg=0.81\n",
            "[20768 | 9590.29] loss=0.98 avg=0.81\n",
            "[20769 | 9593.41] loss=0.85 avg=0.81\n",
            "[20770 | 9596.51] loss=0.73 avg=0.81\n",
            "[20771 | 9599.57] loss=0.80 avg=0.81\n",
            "[20772 | 9602.66] loss=0.94 avg=0.81\n",
            "[20773 | 9605.76] loss=0.63 avg=0.81\n",
            "[20774 | 9608.85] loss=0.70 avg=0.81\n",
            "[20775 | 9611.94] loss=0.86 avg=0.81\n",
            "[20776 | 9615.01] loss=0.94 avg=0.81\n",
            "[20777 | 9618.05] loss=0.70 avg=0.81\n",
            "[20778 | 9621.14] loss=0.99 avg=0.81\n",
            "[20779 | 9624.23] loss=0.68 avg=0.81\n",
            "[20780 | 9627.34] loss=0.85 avg=0.81\n",
            "[20781 | 9630.44] loss=0.81 avg=0.81\n",
            "[20782 | 9633.53] loss=0.72 avg=0.81\n",
            "[20783 | 9636.61] loss=0.77 avg=0.81\n",
            "[20784 | 9639.70] loss=0.84 avg=0.81\n",
            "[20785 | 9642.79] loss=0.71 avg=0.81\n",
            "[20786 | 9645.90] loss=0.67 avg=0.81\n",
            "[20787 | 9649.01] loss=0.92 avg=0.81\n",
            "[20788 | 9652.11] loss=0.91 avg=0.81\n",
            "[20789 | 9655.21] loss=0.77 avg=0.81\n",
            "[20790 | 9658.29] loss=0.94 avg=0.81\n",
            "[20791 | 9661.39] loss=0.87 avg=0.81\n",
            "[20792 | 9664.48] loss=0.96 avg=0.81\n",
            "[20793 | 9667.59] loss=0.92 avg=0.81\n",
            "[20794 | 9670.68] loss=0.69 avg=0.81\n",
            "[20795 | 9673.75] loss=0.84 avg=0.81\n",
            "[20796 | 9676.83] loss=0.89 avg=0.81\n",
            "[20797 | 9679.94] loss=0.81 avg=0.81\n",
            "[20798 | 9683.04] loss=0.63 avg=0.81\n",
            "[20799 | 9686.14] loss=0.73 avg=0.81\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " going to give away our house? = ¿Nos conseguiremos mi casa?\n",
            "I think that's too much. = Creo que es demasiado.\n",
            "Tom couldn't believe his eyes. = Tom no podía creer lo que miraba.\n",
            "Tom didn't look happy. = Tom no se veía feliz.\n",
            "What's your real goal in life? = ¿Cuál es tu verdadera objetivo en la vida?\n",
            "Your house is nice, but mine is even better. = Tu casa está bien, pero la mía la es todo mejor.\n",
            "This must be Tom's mother. = Esta debe ser la madre de Tom.\n",
            "I can't believe you're actually going to say that. = No puedo creer que realmente vas a decir eso.\n",
            "Don't call me again. = No me llamés de nuevo.\n",
            "It would be easy for her. = Sería fácil para ella.\n",
            "The doctor had to operate because of the patient's illness. = El doctor tuvo que operar por el enfermedad del paciente.\n",
            "She asked me whether she could use the telephone. = Ella me preguntó si podía usar el teléfono.\n",
            "How's the weather in New York? = ¿Cómo es el clima en Nueva York?\n",
            "You're the only person I know who has ever been married. = Sois las únicas persona que conozco que se ha estado casada.\n",
            "Tom and Mary look exactly the same as they were when they were children. = Tom y Mary se ven igual de lo mismo cuando eran niños.\n",
            "Let's take it easy for a while. = Tomémonoslo con poco.\n",
            "Did you do that today? = ¿Has hecho eso hoy?\n",
            "This is the very best way to do it. = Esta es justo la mejor manera de hacerlo.\n",
            "Please give him my best regards. = Tengan saludos a mi mejor salud.\n",
            "Have you ever broken your watch again? = ¿Te has quebrado el reloj otra vez?\n",
            "We're on strike again tomorrow. = Hoy de nuevo.\n",
            "She is a good girl. = Ella es una niña buena.\n",
            "I need to talk to you. = Necesito hablar contigo.\n",
            "It is no exaggeration to call him a genius. = No es una exageración llamarte.\n",
            "Can I have some? = ¿Puedo darme alguna?\n",
            "I don't know what happened, but it didn't go well. = No sé qué pasó, pero no salió bien.\n",
            "A lot of people think I'm crazy. = Mucha gente piensa que estoy loca.\n",
            "I never imagined I'd feel this way about you. = Nunca pensé que me sentiría de esta manera sobre ti.\n",
            "I don't understand what you are trying to say. = No entiendo lo que estás tratando de decir.\n",
            "There are two snakes in the bathroom. = Hay dos tiburones en el baño.\n",
            "What am I supposed to do? = ¿Qué se supone que tengo que hacer?\n",
            "I'm going to miss your cooking. = Voy a echar de menos tus platos.\n",
            "We have to be there Monday. = Tenemos que estar allí el lunes.\n",
            "She is not always honest. = No es siempre honesto.\n",
            "This is the girl I told you about the other day. = Esta es la chica de uno de veces.\n",
            "Tom doesn't understand what you want. = Tom no entiende lo que queréis.\n",
            "The sky is getting dark. = El cielo se está oscureciendo.\n",
            "I can't get on the plane. = No puedo subir al avión.\n",
            "Tom is in there with Mary. = Tom está allí con Mary.\n",
            "I like going on walks. = Me gusta dar las coradas.\n",
            "They said yes. = Dijeron que sí.\n",
            "She is going to start at noon today. = Ella está a partir de mediodía hoy.\n",
            "I didn't want to alarm you. = No desaba alarmarte.\n",
            "That's not the real goal. = Esa no es la verdadera meta.\n",
            "Do it right now. = Hazlo ya mismo.\n",
            "It's been a\n",
            "\n",
            "[20800 | 9727.27] loss=0.78 avg=0.81\n",
            "[20801 | 9730.35] loss=0.79 avg=0.81\n",
            "[20802 | 9733.43] loss=0.89 avg=0.81\n",
            "[20803 | 9736.50] loss=0.83 avg=0.81\n",
            "[20804 | 9739.59] loss=0.89 avg=0.81\n",
            "[20805 | 9742.68] loss=0.65 avg=0.81\n",
            "[20806 | 9745.78] loss=0.85 avg=0.81\n",
            "[20807 | 9748.89] loss=0.71 avg=0.81\n",
            "[20808 | 9752.00] loss=0.95 avg=0.81\n",
            "[20809 | 9755.10] loss=0.85 avg=0.81\n",
            "[20810 | 9758.20] loss=0.97 avg=0.81\n",
            "[20811 | 9761.30] loss=0.79 avg=0.81\n",
            "[20812 | 9764.42] loss=0.87 avg=0.81\n",
            "[20813 | 9767.52] loss=0.89 avg=0.81\n",
            "[20814 | 9770.63] loss=0.82 avg=0.81\n",
            "[20815 | 9773.70] loss=0.81 avg=0.81\n",
            "[20816 | 9776.79] loss=0.83 avg=0.81\n",
            "[20817 | 9779.88] loss=0.80 avg=0.81\n",
            "[20818 | 9782.96] loss=0.86 avg=0.82\n",
            "[20819 | 9786.05] loss=0.68 avg=0.81\n",
            "[20820 | 9789.13] loss=0.79 avg=0.81\n",
            "[20821 | 9792.21] loss=0.71 avg=0.81\n",
            "[20822 | 9795.30] loss=0.63 avg=0.81\n",
            "[20823 | 9798.40] loss=0.71 avg=0.81\n",
            "[20824 | 9801.45] loss=0.69 avg=0.81\n",
            "[20825 | 9804.53] loss=0.88 avg=0.81\n",
            "[20826 | 9807.61] loss=0.88 avg=0.81\n",
            "[20827 | 9810.71] loss=0.86 avg=0.81\n",
            "[20828 | 9813.81] loss=0.74 avg=0.81\n",
            "[20829 | 9816.91] loss=0.71 avg=0.81\n",
            "[20830 | 9820.01] loss=0.74 avg=0.81\n",
            "[20831 | 9823.11] loss=0.86 avg=0.81\n",
            "[20832 | 9826.20] loss=0.83 avg=0.81\n",
            "[20833 | 9829.29] loss=0.73 avg=0.81\n",
            "[20834 | 9832.38] loss=0.64 avg=0.81\n",
            "[20835 | 9835.48] loss=0.73 avg=0.81\n",
            "[20836 | 9838.59] loss=0.72 avg=0.80\n",
            "[20837 | 9841.68] loss=0.54 avg=0.80\n",
            "[20838 | 9844.77] loss=0.68 avg=0.80\n",
            "[20839 | 9847.86] loss=0.86 avg=0.80\n",
            "[20840 | 9850.95] loss=0.72 avg=0.80\n",
            "[20841 | 9854.04] loss=0.95 avg=0.80\n",
            "[20842 | 9857.13] loss=0.79 avg=0.80\n",
            "[20843 | 9860.22] loss=0.65 avg=0.80\n",
            "[20844 | 9863.30] loss=0.73 avg=0.80\n",
            "[20845 | 9866.39] loss=0.65 avg=0.80\n",
            "[20846 | 9869.50] loss=0.89 avg=0.80\n",
            "[20847 | 9872.60] loss=0.95 avg=0.80\n",
            "[20848 | 9875.69] loss=0.83 avg=0.80\n",
            "[20849 | 9878.78] loss=0.93 avg=0.80\n",
            "[20850 | 9881.86] loss=0.85 avg=0.80\n",
            "[20851 | 9884.95] loss=0.78 avg=0.80\n",
            "[20852 | 9888.05] loss=0.71 avg=0.80\n",
            "[20853 | 9891.15] loss=0.76 avg=0.80\n",
            "[20854 | 9894.23] loss=0.82 avg=0.80\n",
            "[20855 | 9897.32] loss=1.03 avg=0.80\n",
            "[20856 | 9900.41] loss=1.04 avg=0.81\n",
            "[20857 | 9903.49] loss=0.82 avg=0.81\n",
            "[20858 | 9906.57] loss=0.99 avg=0.81\n",
            "[20859 | 9909.66] loss=0.79 avg=0.81\n",
            "[20860 | 9912.75] loss=0.85 avg=0.81\n",
            "[20861 | 9915.83] loss=0.86 avg=0.81\n",
            "[20862 | 9918.93] loss=0.69 avg=0.81\n",
            "[20863 | 9922.01] loss=0.79 avg=0.81\n",
            "[20864 | 9925.11] loss=0.85 avg=0.81\n",
            "[20865 | 9928.20] loss=0.92 avg=0.81\n",
            "[20866 | 9931.28] loss=0.77 avg=0.81\n",
            "[20867 | 9934.37] loss=0.71 avg=0.81\n",
            "[20868 | 9937.46] loss=0.87 avg=0.81\n",
            "[20869 | 9940.55] loss=0.86 avg=0.81\n",
            "[20870 | 9943.64] loss=0.75 avg=0.81\n",
            "[20871 | 9946.74] loss=0.76 avg=0.81\n",
            "[20872 | 9949.84] loss=0.73 avg=0.81\n",
            "[20873 | 9952.92] loss=0.83 avg=0.81\n",
            "[20874 | 9956.02] loss=0.68 avg=0.81\n",
            "[20875 | 9959.12] loss=0.74 avg=0.81\n",
            "[20876 | 9962.21] loss=0.60 avg=0.80\n",
            "[20877 | 9965.31] loss=0.72 avg=0.80\n",
            "[20878 | 9968.42] loss=0.87 avg=0.80\n",
            "[20879 | 9971.51] loss=0.91 avg=0.80\n",
            "[20880 | 9974.59] loss=0.64 avg=0.80\n",
            "[20881 | 9977.67] loss=0.74 avg=0.80\n",
            "[20882 | 9980.76] loss=0.88 avg=0.80\n",
            "[20883 | 9983.86] loss=0.75 avg=0.80\n",
            "[20884 | 9986.95] loss=0.76 avg=0.80\n",
            "[20885 | 9990.04] loss=0.71 avg=0.80\n",
            "[20886 | 9993.14] loss=0.74 avg=0.80\n",
            "[20887 | 9996.23] loss=0.77 avg=0.80\n",
            "[20888 | 9999.31] loss=0.92 avg=0.80\n",
            "[20889 | 10002.41] loss=0.68 avg=0.80\n",
            "[20890 | 10005.51] loss=0.72 avg=0.80\n",
            "[20891 | 10008.59] loss=0.90 avg=0.80\n",
            "[20892 | 10011.68] loss=0.70 avg=0.80\n",
            "[20893 | 10014.78] loss=0.77 avg=0.80\n",
            "[20894 | 10017.87] loss=0.73 avg=0.80\n",
            "[20895 | 10020.97] loss=0.86 avg=0.80\n",
            "[20896 | 10024.07] loss=0.69 avg=0.80\n",
            "[20897 | 10027.17] loss=0.69 avg=0.80\n",
            "[20898 | 10030.27] loss=0.94 avg=0.80\n",
            "[20899 | 10033.37] loss=0.67 avg=0.80\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " six of the most dangerous crimes, while the majority of the residents were merely victims and some had committed crimes themselves. = Los cánceranos de alborzo no recibidamente encontraron cada vez más peligrosos de las crimees más peligrosas.\n",
            "Are you trying to steal this? = ¿Estás intentando robar esto?\n",
            "Tom bought two dozen pencils. = Tom compró dos docenas de lápices.\n",
            "I am a student. = Soy un estudiante.\n",
            "We haven't spoken for three hours. = Hace tres horas que no hemos hablado.\n",
            "Tom wanted to find out what Mary wanted him to do. = Tom quería averiguar en qué quería que hiciera Mary.\n",
            "This should be easy. = Esto debería ser fácil.\n",
            "Don't forget your ice. = No olvides tus hielbas.\n",
            "Tom's eyesight isn't as good as it used to be. = Los ojos de Tom no son tan buenos como antes.\n",
            "I was born in Osaka. = Nací en Osaka.\n",
            "She was told that the milk came from China. = Se le dijo que le jugaba la leche de China.\n",
            "You don't realize how lucky you're having it. = No te da cuenta de cuán aparentarlo.\n",
            "I am sorry I did not think of something. = Lamento no haber pensado algo.\n",
            "Please look after the children. = Por favor cuida de atrás.\n",
            "Tom didn't do well on any of the tests. = A Tom no le fue bien en cualquier alguno de las pruebas.\n",
            "Tom asked me not to drive too fast. = Tom me pidió que no conduciera demasiado rápido.\n",
            "I'll tell you what the problem is. = Os diré cuál es el problema.\n",
            "I did it by myself. = Lo hice por mí mismo.\n",
            "If you're busy, I can do it. = Si estás ocupado, puedo hacerlo.\n",
            "I can't tell you why it worked, but I can tell you why it didn't. = No te puedo decir por qué funcionó, pero la podría comprender que no.\n",
            "Tom didn't come. = Tom no vino.\n",
            "Are you interested in politics? = ¿Te interesa la política?\n",
            "You have two choices. = Tienes dos opciones.\n",
            "Tom says that's impossible. = Tomás dice que es imposible.\n",
            "You'll get used to it. = Se acostumbraráis a ello.\n",
            "This mustn't be exposed to the sun. = No hay que exponer esto al sol.\n",
            "It was raining heavily in Osaka. = Llovía muy fuerte en Osaka.\n",
            "I can't imagine her doing that. = No sé que ella haya hecho eso.\n",
            "We've come up against some resistance. = Hemos llegado a nuestro resoluente.\n",
            "Let's finish it right now. = Terminemlo ahora mismo.\n",
            "I have to go get it. = Tengo que ir a cogerlo.\n",
            "They will kill time reading this magazine. = Ellos pasarán tiempo leyendo esta revista.\n",
            "He is at his desk, collecting papers. = Se está en su escritorio, recogiendo papeles.\n",
            "I don't know about you, but I'm starving! = No tengo de usted, pero yo no tengo por vuestra hambre.\n",
            "We should be careful not to make him angry. = Debemos ser cuidadoso no hacerlo animado.\n",
            "Tom doesn't think that's the answer. = Tom no cree que esa es la respuesta.\n",
            "Can I go alone? = ¿Puedo ir solo?\n",
            "Can we just take a minute? = ¿Podemos samos un minuto?\n",
            "I am more nervous than happy. = Estoy más nerviosa que feliz.\n",
            "He is always in a hurry. = Siempre está áspero.\n",
            "Tom wasn't hungry after lunch. = Tom no tenía hambre después de almorzar.\n",
            "I thought you were dead. = Pensé que estabas muerto.\n",
            "I didn't know who else to ask. = No sabía a quién más preguntarle.\n",
            "I don't go out much. = No voy por much\n",
            "\n",
            "[20900 | 10074.06] loss=0.75 avg=0.80\n",
            "[20901 | 10077.16] loss=0.76 avg=0.80\n",
            "[20902 | 10080.24] loss=0.70 avg=0.80\n",
            "[20903 | 10083.33] loss=1.08 avg=0.80\n",
            "[20904 | 10086.43] loss=0.77 avg=0.80\n",
            "[20905 | 10089.52] loss=0.81 avg=0.80\n",
            "[20906 | 10092.62] loss=0.63 avg=0.80\n",
            "[20907 | 10095.67] loss=0.86 avg=0.80\n",
            "[20908 | 10098.76] loss=0.78 avg=0.80\n",
            "[20909 | 10101.85] loss=0.80 avg=0.80\n",
            "[20910 | 10104.94] loss=0.72 avg=0.80\n",
            "[20911 | 10108.02] loss=0.70 avg=0.79\n",
            "[20912 | 10111.09] loss=0.74 avg=0.79\n",
            "[20913 | 10114.15] loss=0.82 avg=0.79\n",
            "[20914 | 10117.25] loss=0.82 avg=0.79\n",
            "[20915 | 10120.32] loss=0.79 avg=0.79\n",
            "[20916 | 10123.42] loss=0.84 avg=0.80\n",
            "[20917 | 10126.51] loss=0.83 avg=0.80\n",
            "[20918 | 10129.60] loss=0.73 avg=0.80\n",
            "[20919 | 10132.68] loss=0.95 avg=0.80\n",
            "[20920 | 10135.77] loss=0.91 avg=0.80\n",
            "[20921 | 10138.82] loss=0.76 avg=0.80\n",
            "[20922 | 10141.91] loss=0.87 avg=0.80\n",
            "[20923 | 10145.01] loss=0.90 avg=0.80\n",
            "[20924 | 10148.13] loss=0.77 avg=0.80\n",
            "[20925 | 10151.22] loss=0.92 avg=0.80\n",
            "[20926 | 10154.31] loss=0.80 avg=0.80\n",
            "[20927 | 10157.40] loss=0.73 avg=0.80\n",
            "[20928 | 10160.49] loss=0.91 avg=0.80\n",
            "[20929 | 10163.59] loss=0.76 avg=0.80\n",
            "[20930 | 10166.69] loss=0.90 avg=0.80\n",
            "[20931 | 10169.78] loss=0.83 avg=0.80\n",
            "[20932 | 10172.87] loss=0.94 avg=0.80\n",
            "[20933 | 10175.97] loss=0.81 avg=0.80\n",
            "[20934 | 10179.05] loss=0.76 avg=0.80\n",
            "[20935 | 10182.16] loss=0.79 avg=0.80\n",
            "[20936 | 10185.25] loss=0.72 avg=0.80\n",
            "[20937 | 10188.35] loss=0.91 avg=0.80\n",
            "[20938 | 10191.44] loss=0.92 avg=0.80\n",
            "[20939 | 10194.53] loss=0.90 avg=0.80\n",
            "[20940 | 10197.64] loss=0.77 avg=0.80\n",
            "[20941 | 10200.72] loss=0.85 avg=0.80\n",
            "[20942 | 10203.82] loss=0.93 avg=0.81\n",
            "[20943 | 10206.91] loss=0.89 avg=0.81\n",
            "[20944 | 10210.00] loss=0.71 avg=0.81\n",
            "[20945 | 10213.11] loss=0.75 avg=0.81\n",
            "[20946 | 10216.16] loss=0.93 avg=0.81\n",
            "[20947 | 10219.24] loss=0.81 avg=0.81\n",
            "[20948 | 10222.33] loss=0.80 avg=0.81\n",
            "[20949 | 10225.43] loss=0.70 avg=0.81\n",
            "[20950 | 10228.54] loss=1.02 avg=0.81\n",
            "[20951 | 10231.64] loss=0.87 avg=0.81\n",
            "[20952 | 10234.73] loss=0.89 avg=0.81\n",
            "[20953 | 10237.81] loss=0.72 avg=0.81\n",
            "[20954 | 10240.90] loss=0.87 avg=0.81\n",
            "[20955 | 10244.00] loss=0.84 avg=0.81\n",
            "[20956 | 10247.10] loss=0.82 avg=0.81\n",
            "[20957 | 10250.19] loss=0.66 avg=0.81\n",
            "[20958 | 10253.28] loss=0.80 avg=0.81\n",
            "[20959 | 10256.38] loss=0.90 avg=0.81\n",
            "[20960 | 10259.49] loss=0.73 avg=0.81\n",
            "[20961 | 10262.59] loss=0.68 avg=0.81\n",
            "[20962 | 10265.67] loss=0.99 avg=0.81\n",
            "[20963 | 10268.77] loss=0.90 avg=0.81\n",
            "[20964 | 10271.88] loss=0.82 avg=0.81\n",
            "[20965 | 10274.97] loss=0.63 avg=0.81\n",
            "[20966 | 10278.08] loss=0.80 avg=0.81\n",
            "[20967 | 10281.16] loss=0.70 avg=0.81\n",
            "[20968 | 10284.27] loss=0.88 avg=0.81\n",
            "[20969 | 10287.36] loss=0.73 avg=0.81\n",
            "[20970 | 10290.46] loss=0.87 avg=0.81\n",
            "[20971 | 10293.56] loss=0.80 avg=0.81\n",
            "[20972 | 10296.66] loss=0.90 avg=0.81\n",
            "[20973 | 10299.75] loss=0.90 avg=0.81\n",
            "[20974 | 10302.83] loss=0.81 avg=0.81\n",
            "[20975 | 10305.90] loss=0.69 avg=0.81\n",
            "[20976 | 10308.97] loss=0.92 avg=0.81\n",
            "[20977 | 10312.04] loss=0.88 avg=0.81\n",
            "[20978 | 10315.14] loss=0.76 avg=0.81\n",
            "[20979 | 10318.23] loss=0.81 avg=0.81\n",
            "[20980 | 10321.32] loss=0.85 avg=0.81\n",
            "[20981 | 10324.41] loss=0.95 avg=0.81\n",
            "[20982 | 10327.51] loss=0.93 avg=0.81\n",
            "[20983 | 10330.61] loss=0.91 avg=0.81\n",
            "[20984 | 10333.70] loss=0.89 avg=0.81\n",
            "[20985 | 10336.81] loss=0.78 avg=0.81\n",
            "[20986 | 10339.90] loss=0.72 avg=0.81\n",
            "[20987 | 10342.99] loss=0.77 avg=0.81\n",
            "[20988 | 10346.08] loss=0.81 avg=0.81\n",
            "[20989 | 10349.17] loss=0.92 avg=0.81\n",
            "[20990 | 10352.26] loss=0.86 avg=0.81\n",
            "[20991 | 10355.34] loss=0.69 avg=0.81\n",
            "[20992 | 10358.43] loss=0.82 avg=0.81\n",
            "[20993 | 10361.53] loss=0.65 avg=0.81\n",
            "[20994 | 10364.62] loss=0.95 avg=0.81\n",
            "[20995 | 10367.71] loss=0.86 avg=0.81\n",
            "[20996 | 10370.79] loss=0.77 avg=0.81\n",
            "[20997 | 10373.87] loss=0.90 avg=0.81\n",
            "[20998 | 10376.95] loss=0.69 avg=0.81\n",
            "[20999 | 10380.03] loss=0.97 avg=0.81\n",
            "Saving /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-21000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " sus nueces.\n",
            "I think you're wrong. = Yo pienso que estás equivocado.\n",
            "Tom was disappointed in his son. = Tom estaba decepcionado de su hijo.\n",
            "You are very smart. = Eres muy listo.\n",
            "They're so busy. = Ellos están muy ocupados.\n",
            "I didn't quite understand the problem. = No he entendido el problema.\n",
            "Tom was surprised that Mary said that. = Tom se sorprendió de que Mary diga eso.\n",
            "Tom was caught in a lie. = Tom fue atrapado en una mentira.\n",
            "Where's your father now? = ¿Dónde está ahora tu padre?\n",
            "The police found out where to look. = La policía descubrió dónde buscar.\n",
            "I want to take it easy. = Quiero tomarlo bien.\n",
            "This book is mine. = Este libro es mío.\n",
            "You seem like a nice guy. = Te ves como un buen tipo.\n",
            "It's raining again. = Está lloviendo otra vez.\n",
            "I hate to be alone at home on a nice day. = Odio pasarme solo a casa con un bonito día.\n",
            "Tom likes singing. = A Tom le gusta cantar.\n",
            "She made up that story. = Ella se inventó esa historia.\n",
            "I hope we'll never have to do that again. = Espero que nunca tengamos que hacer eso nueve.\n",
            "Tom has a lot of friends in Boston. = Tom tiene muchos amigos en Boston.\n",
            "I want to go home. = Quiero ir a mi casa.\n",
            "I've never met you in person. = No te he encontrado nunca en persona.\n",
            "She likes oranges. = A ella le gustan las naranjas.\n",
            "I can't believe Tom made good use of my time. = No puedo creer que Tom me hiciera buena uso mi tiempo.\n",
            "There is a little boy standing outside crying. = Hay un niño que está parada afuera.\n",
            "We have to change something. = Tenemos que cambiar algo.\n",
            "My grandmother lived to be ninety-five. = Mi abuela vivió hasta los noventa.\n",
            "Tom is good at what he does. = Tom es bueno en lo que hace.\n",
            "Are you having a nice time? = ¿Estás pasando un buen rato?\n",
            "When the telephone rang, the maid ran to answer it. = Cuando sonó el teléfono, la bailazar corrisó al que retribuía.\n",
            "I went to the supermarket and bought a bag to take home. = Fui al supermercado y compré un paquete para llevar a casa.\n",
            "I'd like to sit by the window. = Quisiera sentarme en la ventana.\n",
            "Tom was able to fix the sink this morning. = Tom fue capaz de arreglar la letrina esta mañana.\n",
            "Don't let Tom block your path. = No dejes que Tom seas la raya a tu ruego.\n",
            "Can't we find a compromise? = ¿No podemos encontrar un compromiso?\n",
            "What were you doing? = ¿Qué estabas haciendo?\n",
            "I was there last year. = Yo estuve allá el año pasado.\n",
            "His parents were farmers. = Sus padres eran granjeros.\n",
            "They're here. = Están aquí.\n",
            "Tom couldn't bear the pain any more so he shot himself. = Tom no podía soportar más el dolor, así que se pegó un tiro.\n",
            "She's afraid of cats. = Ella le tiene miedo de gatos.\n",
            "How do I know which car to buy? = ¿Cómo sé cuál es el coche?\n",
            "I have a friend who lives in Japan now. = Tengo un amigo que vive en Japón ahora.\n",
            "Tom and Mary don't know why John decided to stay in Boston all these years. = Tom y Mary no sabe por qué decidiera ante John en que John decidió permanecer en Boston.\n",
            "We both got in. = Obstrebamos a la casa.\n",
            "You were here first. = Tú estuviste aquí primero.\n",
            "That's not what I meant. = Eso no es lo que quise decir.\n",
            "It's not\n",
            "\n",
            "[21000 | 10436.66] loss=0.91 avg=0.81\n",
            "[21001 | 10439.75] loss=0.96 avg=0.82\n",
            "[21002 | 10442.84] loss=0.82 avg=0.82\n",
            "[21003 | 10445.92] loss=0.72 avg=0.81\n",
            "[21004 | 10449.02] loss=1.04 avg=0.82\n",
            "[21005 | 10452.12] loss=0.80 avg=0.82\n",
            "[21006 | 10455.19] loss=0.94 avg=0.82\n",
            "[21007 | 10458.26] loss=0.94 avg=0.82\n",
            "[21008 | 10461.36] loss=0.87 avg=0.82\n",
            "[21009 | 10464.46] loss=0.77 avg=0.82\n",
            "[21010 | 10467.55] loss=0.79 avg=0.82\n",
            "[21011 | 10470.64] loss=1.00 avg=0.82\n",
            "[21012 | 10473.73] loss=0.73 avg=0.82\n",
            "[21013 | 10476.83] loss=0.71 avg=0.82\n",
            "[21014 | 10479.92] loss=0.84 avg=0.82\n",
            "[21015 | 10483.02] loss=0.67 avg=0.82\n",
            "[21016 | 10486.13] loss=0.91 avg=0.82\n",
            "[21017 | 10489.22] loss=0.84 avg=0.82\n",
            "[21018 | 10492.31] loss=0.70 avg=0.82\n",
            "[21019 | 10495.40] loss=0.88 avg=0.82\n",
            "[21020 | 10498.51] loss=0.73 avg=0.82\n",
            "[21021 | 10501.62] loss=0.77 avg=0.82\n",
            "[21022 | 10504.73] loss=0.90 avg=0.82\n",
            "[21023 | 10507.83] loss=0.81 avg=0.82\n",
            "[21024 | 10510.94] loss=0.84 avg=0.82\n",
            "[21025 | 10514.04] loss=0.91 avg=0.82\n",
            "[21026 | 10517.13] loss=0.84 avg=0.82\n",
            "[21027 | 10520.23] loss=0.93 avg=0.82\n",
            "[21028 | 10523.33] loss=0.84 avg=0.82\n",
            "[21029 | 10526.41] loss=0.88 avg=0.82\n",
            "[21030 | 10529.50] loss=0.78 avg=0.82\n",
            "[21031 | 10532.57] loss=0.76 avg=0.82\n",
            "[21032 | 10535.64] loss=0.85 avg=0.82\n",
            "[21033 | 10538.73] loss=0.85 avg=0.82\n",
            "[21034 | 10541.81] loss=1.00 avg=0.82\n",
            "[21035 | 10544.90] loss=0.81 avg=0.82\n",
            "[21036 | 10548.00] loss=0.81 avg=0.82\n",
            "[21037 | 10551.07] loss=0.81 avg=0.82\n",
            "[21038 | 10554.16] loss=0.85 avg=0.82\n",
            "[21039 | 10557.26] loss=0.87 avg=0.82\n",
            "[21040 | 10560.37] loss=0.76 avg=0.82\n",
            "[21041 | 10563.46] loss=0.79 avg=0.82\n",
            "[21042 | 10566.55] loss=0.79 avg=0.82\n",
            "[21043 | 10569.65] loss=0.96 avg=0.82\n",
            "[21044 | 10572.76] loss=0.76 avg=0.82\n",
            "[21045 | 10575.85] loss=0.69 avg=0.82\n",
            "[21046 | 10578.95] loss=0.85 avg=0.82\n",
            "[21047 | 10582.04] loss=0.86 avg=0.82\n",
            "[21048 | 10585.13] loss=0.78 avg=0.82\n",
            "[21049 | 10588.24] loss=0.77 avg=0.82\n",
            "[21050 | 10591.34] loss=0.94 avg=0.82\n",
            "[21051 | 10594.44] loss=0.76 avg=0.82\n",
            "[21052 | 10597.53] loss=0.75 avg=0.82\n",
            "[21053 | 10600.62] loss=0.81 avg=0.82\n",
            "[21054 | 10603.72] loss=0.76 avg=0.82\n",
            "[21055 | 10606.80] loss=0.69 avg=0.82\n",
            "[21056 | 10609.89] loss=0.83 avg=0.82\n",
            "[21057 | 10613.01] loss=0.75 avg=0.82\n",
            "[21058 | 10616.10] loss=0.80 avg=0.82\n",
            "[21059 | 10619.19] loss=0.79 avg=0.82\n",
            "[21060 | 10622.27] loss=0.91 avg=0.82\n",
            "[21061 | 10625.36] loss=0.94 avg=0.82\n",
            "[21062 | 10628.46] loss=0.81 avg=0.82\n",
            "[21063 | 10631.57] loss=0.69 avg=0.82\n",
            "[21064 | 10634.66] loss=0.76 avg=0.82\n",
            "[21065 | 10637.76] loss=0.80 avg=0.82\n",
            "[21066 | 10640.87] loss=0.78 avg=0.82\n",
            "[21067 | 10643.97] loss=0.88 avg=0.82\n",
            "[21068 | 10647.07] loss=0.82 avg=0.82\n",
            "[21069 | 10650.17] loss=0.99 avg=0.82\n",
            "[21070 | 10653.28] loss=0.67 avg=0.82\n",
            "[21071 | 10656.39] loss=0.73 avg=0.82\n",
            "[21072 | 10659.49] loss=0.76 avg=0.82\n",
            "[21073 | 10662.59] loss=0.81 avg=0.82\n",
            "[21074 | 10665.69] loss=0.65 avg=0.81\n",
            "[21075 | 10668.79] loss=0.72 avg=0.81\n",
            "[21076 | 10671.89] loss=0.86 avg=0.81\n",
            "[21077 | 10674.98] loss=0.78 avg=0.81\n",
            "[21078 | 10678.08] loss=0.86 avg=0.81\n",
            "[21079 | 10681.18] loss=0.83 avg=0.81\n",
            "[21080 | 10684.29] loss=0.82 avg=0.81\n",
            "[21081 | 10687.40] loss=0.88 avg=0.82\n",
            "[21082 | 10690.50] loss=0.74 avg=0.81\n",
            "[21083 | 10693.60] loss=0.72 avg=0.81\n",
            "[21084 | 10696.70] loss=0.85 avg=0.81\n",
            "[21085 | 10699.80] loss=0.96 avg=0.82\n",
            "[21086 | 10702.89] loss=0.71 avg=0.81\n",
            "[21087 | 10706.00] loss=0.75 avg=0.81\n",
            "[21088 | 10709.09] loss=0.87 avg=0.81\n",
            "[21089 | 10712.20] loss=0.73 avg=0.81\n",
            "[21090 | 10715.29] loss=0.78 avg=0.81\n",
            "[21091 | 10718.40] loss=0.84 avg=0.81\n",
            "[21092 | 10721.51] loss=0.71 avg=0.81\n",
            "[21093 | 10724.60] loss=0.93 avg=0.81\n",
            "[21094 | 10727.69] loss=0.71 avg=0.81\n",
            "[21095 | 10730.79] loss=0.75 avg=0.81\n",
            "[21096 | 10733.88] loss=0.77 avg=0.81\n",
            "[21097 | 10736.97] loss=0.92 avg=0.81\n",
            "[21098 | 10740.06] loss=0.77 avg=0.81\n",
            "[21099 | 10743.15] loss=0.74 avg=0.81\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " en ajeno.\n",
            "I am going to wait for her. = La esperaré.\n",
            "I am interested in Chinese architecture. = Estoy interesado en la arquitectura china.\n",
            "When was the last time you fed the cows? = ¿Cuándo fue la última vez que alimentaste a las vacas?\n",
            "I just felt like hearing the sound of your voice. = Solo me sentí que estaba escuchando tu voz.\n",
            "I really want to get married. = Realmente quiero casarme.\n",
            "Tom seems lonely without Mary. = Tom parece solo sin Mary.\n",
            "Why do you want me to tell you this? = ¿Por qué quieres que te cuente esto?\n",
            "He was an honest man. = Era un hombre honrado.\n",
            "Do you want me to answer? = ¿Querés que conteste?\n",
            "I know what to do first. = Sé qué hacer primero.\n",
            "That's not a real telephone. = Ese no es un teléfono real.\n",
            "She doesn't give orders. = Ella no da órdenes.\n",
            "Please turn off the light while I'm taking a bath. = Apagá la luz mientras me bañé, por favor.\n",
            "I love that shirt. = Me encanta esa camisa.\n",
            "We've had a lot of snow this year. = Hemos tenido mucha nieve este año.\n",
            "He's very popular among students. = Es muy popular entre los alumútros.\n",
            "That was the least interesting of all the choices. = Era la menor of toda la alternativa.\n",
            "Please make yourself at home. = Por favor, sentí como en casa.\n",
            "I am sure he would be happy to see you. = Estoy seguro de que él iba a gustar verte.\n",
            "The house I remember so well is the one I found today. = La casa que me recuerdo tan bien cuya age que encontré hoy.\n",
            "There is no sense in doing that. = No tiene sentido hacer eso.\n",
            "He lives very alone. = Él vive muy solitude.\n",
            "Tom thought about what Mary had told him. = Tom pensó acerca de lo que Mary les dijo.\n",
            "Tom said he could not get Mary on the phone. = Tom dijo que no pudo a Mary al teléfono.\n",
            "The boy seems to be rich. = Parece que el niño no parece rico.\n",
            "I'd like to think about it. = Quisiera pensármelo.\n",
            "You did it. = Lo hiciste.\n",
            "I have to clean the entire house. = Tengo que limpiar la casa entera.\n",
            "He is a writer. = Es escritor.\n",
            "That was just an enormous bluff. = Eso fue sólo una gran blanca.\n",
            "A woman's beauty depends on her looks. = Las bellecas de las femenas dependen de sus verdes.\n",
            "Are you sure you don't want anything? = ¿Estás segura de que no quieres nada?\n",
            "Mary was wearing a black skirt. = Mary vestía una chaqueta negra de negro.\n",
            "All is still. = Todo está en calma.\n",
            "I have no time to write. Could you send it by email? = No tengo tiempo para escribir. ¿Podría enviarlo por correo electrónico?\n",
            "The dog is very tame. = El perro es muy difícil.\n",
            "What's so special? = ¿Qué es tan especial?\n",
            "There were so many people. = Había nadie de personas.\n",
            "I can't hear you very well. = No te puedo escuchar bien.\n",
            "I was born in 1975. = Nací en 1975.\n",
            "My mother bought me this toy when I was twelve years old. = Mi madre me compró este juguete cuando yo tenía doce años.\n",
            "She was at the station. = Ella estaba en la estación.\n",
            "It is said that the Stone was brought to this place by an angel. = Se dice que la Piscina fue trajido hasta este lugar por un ángel.\n",
            "The boy ran away and ran back to the house. = El chico huyó y se fue corriendo hacia la casa.\n",
            "He tried to solve the problem. = Intentó resolver el problema.\n",
            "I don't\n",
            "\n",
            "[21100 | 10783.88] loss=0.85 avg=0.81\n",
            "[21101 | 10786.97] loss=0.95 avg=0.81\n",
            "[21102 | 10790.07] loss=0.89 avg=0.81\n",
            "[21103 | 10793.17] loss=0.91 avg=0.81\n",
            "[21104 | 10796.28] loss=0.90 avg=0.82\n",
            "[21105 | 10799.38] loss=0.91 avg=0.82\n",
            "[21106 | 10802.49] loss=0.97 avg=0.82\n",
            "[21107 | 10805.59] loss=0.73 avg=0.82\n",
            "[21108 | 10808.68] loss=0.92 avg=0.82\n",
            "[21109 | 10811.78] loss=0.81 avg=0.82\n",
            "[21110 | 10814.87] loss=0.81 avg=0.82\n",
            "[21111 | 10817.92] loss=0.80 avg=0.82\n",
            "[21112 | 10821.01] loss=0.87 avg=0.82\n",
            "[21113 | 10824.11] loss=0.75 avg=0.82\n",
            "[21114 | 10827.21] loss=0.82 avg=0.82\n",
            "[21115 | 10830.31] loss=0.84 avg=0.82\n",
            "[21116 | 10833.42] loss=0.82 avg=0.82\n",
            "[21117 | 10836.52] loss=0.98 avg=0.82\n",
            "[21118 | 10839.62] loss=0.93 avg=0.82\n",
            "[21119 | 10842.72] loss=0.74 avg=0.82\n",
            "[21120 | 10845.82] loss=0.92 avg=0.82\n",
            "[21121 | 10848.92] loss=0.82 avg=0.82\n",
            "[21122 | 10852.01] loss=0.73 avg=0.82\n",
            "[21123 | 10855.11] loss=0.70 avg=0.82\n",
            "[21124 | 10858.21] loss=0.95 avg=0.82\n",
            "[21125 | 10861.31] loss=0.78 avg=0.82\n",
            "[21126 | 10864.42] loss=0.93 avg=0.82\n",
            "[21127 | 10867.51] loss=0.89 avg=0.82\n",
            "[21128 | 10870.61] loss=0.84 avg=0.82\n",
            "[21129 | 10873.70] loss=0.71 avg=0.82\n",
            "[21130 | 10876.78] loss=0.67 avg=0.82\n",
            "[21131 | 10879.89] loss=0.69 avg=0.82\n",
            "[21132 | 10882.98] loss=0.88 avg=0.82\n",
            "[21133 | 10886.07] loss=0.87 avg=0.82\n",
            "[21134 | 10889.18] loss=0.74 avg=0.82\n",
            "[21135 | 10892.28] loss=0.72 avg=0.82\n",
            "[21136 | 10895.38] loss=0.72 avg=0.82\n",
            "[21137 | 10898.47] loss=0.78 avg=0.82\n",
            "[21138 | 10901.57] loss=0.72 avg=0.82\n",
            "[21139 | 10904.67] loss=0.67 avg=0.81\n",
            "[21140 | 10907.78] loss=0.81 avg=0.81\n",
            "[21141 | 10910.89] loss=0.76 avg=0.81\n",
            "[21142 | 10913.99] loss=0.91 avg=0.81\n",
            "[21143 | 10917.11] loss=0.92 avg=0.81\n",
            "[21144 | 10920.21] loss=0.84 avg=0.82\n",
            "[21145 | 10923.32] loss=0.74 avg=0.81\n",
            "[21146 | 10926.42] loss=0.77 avg=0.81\n",
            "[21147 | 10929.52] loss=0.87 avg=0.81\n",
            "[21148 | 10932.63] loss=0.99 avg=0.82\n",
            "[21149 | 10935.72] loss=0.66 avg=0.81\n",
            "[21150 | 10938.82] loss=0.87 avg=0.82\n",
            "[21151 | 10941.92] loss=0.76 avg=0.81\n",
            "[21152 | 10945.03] loss=0.66 avg=0.81\n",
            "[21153 | 10948.13] loss=0.67 avg=0.81\n",
            "[21154 | 10951.25] loss=0.74 avg=0.81\n",
            "[21155 | 10954.35] loss=0.78 avg=0.81\n",
            "[21156 | 10957.46] loss=0.77 avg=0.81\n",
            "[21157 | 10960.55] loss=0.80 avg=0.81\n",
            "[21158 | 10963.65] loss=0.92 avg=0.81\n",
            "[21159 | 10966.75] loss=0.94 avg=0.81\n",
            "[21160 | 10969.86] loss=0.96 avg=0.81\n",
            "[21161 | 10972.96] loss=0.96 avg=0.82\n",
            "[21162 | 10976.06] loss=0.83 avg=0.82\n",
            "[21163 | 10979.15] loss=0.79 avg=0.82\n",
            "[21164 | 10982.26] loss=1.07 avg=0.82\n",
            "[21165 | 10985.35] loss=0.67 avg=0.82\n",
            "[21166 | 10988.45] loss=0.66 avg=0.82\n",
            "[21167 | 10991.54] loss=0.84 avg=0.82\n",
            "[21168 | 10994.63] loss=0.88 avg=0.82\n",
            "[21169 | 10997.72] loss=0.81 avg=0.82\n",
            "[21170 | 11000.83] loss=0.90 avg=0.82\n",
            "[21171 | 11003.92] loss=0.83 avg=0.82\n",
            "[21172 | 11007.02] loss=0.86 avg=0.82\n",
            "[21173 | 11010.11] loss=0.83 avg=0.82\n",
            "[21174 | 11013.18] loss=0.73 avg=0.82\n",
            "[21175 | 11016.29] loss=0.83 avg=0.82\n",
            "[21176 | 11019.38] loss=0.93 avg=0.82\n",
            "[21177 | 11022.46] loss=0.70 avg=0.82\n",
            "[21178 | 11025.53] loss=0.73 avg=0.82\n",
            "[21179 | 11028.61] loss=0.66 avg=0.81\n",
            "[21180 | 11031.68] loss=0.79 avg=0.81\n",
            "[21181 | 11034.77] loss=0.81 avg=0.81\n",
            "[21182 | 11037.87] loss=0.94 avg=0.82\n",
            "[21183 | 11040.95] loss=0.97 avg=0.82\n",
            "[21184 | 11044.03] loss=0.80 avg=0.82\n",
            "[21185 | 11047.09] loss=0.77 avg=0.82\n",
            "[21186 | 11050.15] loss=0.89 avg=0.82\n",
            "[21187 | 11053.21] loss=0.82 avg=0.82\n",
            "[21188 | 11056.28] loss=0.74 avg=0.82\n",
            "[21189 | 11059.34] loss=0.79 avg=0.82\n",
            "[21190 | 11062.43] loss=0.93 avg=0.82\n",
            "[21191 | 11065.53] loss=0.74 avg=0.82\n",
            "[21192 | 11068.61] loss=0.75 avg=0.82\n",
            "[21193 | 11071.70] loss=0.95 avg=0.82\n",
            "[21194 | 11074.76] loss=0.76 avg=0.82\n",
            "[21195 | 11077.85] loss=0.93 avg=0.82\n",
            "[21196 | 11080.93] loss=0.67 avg=0.82\n",
            "[21197 | 11084.02] loss=0.69 avg=0.81\n",
            "[21198 | 11087.10] loss=0.94 avg=0.82\n",
            "[21199 | 11090.17] loss=0.78 avg=0.82\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " lñarse la muerte.\n",
            "He didn't tell me. = Él no me lo dijo.\n",
            "I'm eating lunch, Dad. = Estoy almorzando, papá.\n",
            "He went swimming in the river. = Se fue a nadar al río.\n",
            "Tom always uses the same dictionary on any subject that he researches. = Tom siempre usa el mismo diccionario en cualquier tema del que estudia.\n",
            "Tom should be up soon. = Tom debería estar andado.\n",
            "I have to help my mother. = Tengo que ayudar a mamá.\n",
            "Tom told Mary that he did not love her. = Tom le dijo a Mary que no la amaba.\n",
            "You'd better ask someone else. = Es que buscar a otra persona.\n",
            "Tom is a very good table tennis player. = Tom casi es muy buen jugador de ping-pong.\n",
            "I just got a letter from Tom. = Acabo de recibirle una carta de Tom.\n",
            "It is our duty to help them. = Es nuestro deber ayudarle.\n",
            "My house is here. = Mi casa está aquí.\n",
            "The house is dirty. = La casa está sucia.\n",
            "His old car is parked in front of his house. = Su antiguo vehículo está parado frente a su casa.\n",
            "We're almost out of time. = Muriéramos casi nieto.\n",
            "The woman is wearing a skirt. = La mujer lleva una falda.\n",
            "The dog followed its master tightly. = El perro siguió firmemente con lo suha.\n",
            "Tom wasn't able to finish all his homework quickly enough. = Tom no fue capaz de terminar toda su tarea suficientemente rápido.\n",
            "I'm not going to tell you where it is. = No te voy a decir dónde está.\n",
            "Tom isn't able to do it by himself. = Tom no puede hacerlo por sí solo.\n",
            "Tom bought a red car. = Tom compró un auto rojo.\n",
            "He got up at six every morning. = Él se levantó todos los días convenciendo.\n",
            "She is wearing glasses. = Ella lleva gafas.\n",
            "I have four computers. = Tengo cuatro computadores.\n",
            "He doesn't like him. = Le no le gusta.\n",
            "It's my sister's bicycle. = Es la bicicleta de mi hermana.\n",
            "Tom went into the kitchen looking for Mary. = Tom entró a la cocina a buscar a Mary.\n",
            "All our efforts resulted in nothing. = Todos nuestros intentos resultaron nada.\n",
            "Can we talk? = ¿Podemos hablar?\n",
            "Tom always tells the truth. = Tom siempre dice la verdad.\n",
            "I feel the same way as you do. = Me siento igual que tiene.\n",
            "I'm not sure. = No estoy seguro.\n",
            "I think it's time for me to write my father a letter. = Creo que es hora de que me escriba una carta a mi papá.\n",
            "What do you mean? = ¿Qué quiere decir?\n",
            "He was a good king. = Fue un buen rey.\n",
            "I wanted to make sure that Tom was in his room. = Yo quería asegurarme de que Tom estaba en su habitación.\n",
            "Do you have any advice for me? = ¿Tenéis algún consejo para mí?\n",
            "I'm looking forward to playing tennis with Tom. = Estoy deseando jugar tenis con Tom.\n",
            "I think it's going to be a warm and friendly holiday. = Creo que voy a tener unas flores y amistosa de regoción.\n",
            "This soup is too salty to eat. = Esta sopa está muy salada como para comer.\n",
            "That is not true. = Eso no es verdad.\n",
            "I will stay here for a couple of days. = Me quedaré un par de días a un par de tenerme.\n",
            "That is not true. = Eso no es verdad.\n",
            "I'm trying. = Lo estoy intentando.\n",
            "This is our friend. = Este es nuestro amigo.\n",
            "This dictionary deals with words. = Este diccionario trata a palabras.\n",
            "Why is Tom always so serious? = ¿\n",
            "\n",
            "[21200 | 11130.67] loss=0.70 avg=0.81\n",
            "[21201 | 11133.75] loss=0.84 avg=0.81\n",
            "[21202 | 11136.84] loss=0.82 avg=0.81\n",
            "[21203 | 11139.92] loss=0.89 avg=0.82\n",
            "[21204 | 11143.00] loss=0.77 avg=0.81\n",
            "[21205 | 11146.09] loss=0.71 avg=0.81\n",
            "[21206 | 11149.16] loss=0.83 avg=0.81\n",
            "[21207 | 11152.25] loss=0.73 avg=0.81\n",
            "[21208 | 11155.33] loss=0.92 avg=0.81\n",
            "[21209 | 11158.43] loss=0.84 avg=0.81\n",
            "[21210 | 11161.52] loss=0.79 avg=0.81\n",
            "[21211 | 11164.61] loss=0.94 avg=0.82\n",
            "[21212 | 11167.69] loss=0.94 avg=0.82\n",
            "[21213 | 11170.78] loss=0.74 avg=0.82\n",
            "[21214 | 11173.88] loss=0.69 avg=0.81\n",
            "[21215 | 11176.95] loss=0.73 avg=0.81\n",
            "[21216 | 11180.03] loss=0.69 avg=0.81\n",
            "[21217 | 11183.12] loss=0.74 avg=0.81\n",
            "[21218 | 11186.21] loss=0.79 avg=0.81\n",
            "[21219 | 11189.30] loss=0.69 avg=0.81\n",
            "[21220 | 11192.40] loss=0.78 avg=0.81\n",
            "[21221 | 11195.48] loss=0.89 avg=0.81\n",
            "[21222 | 11198.56] loss=0.62 avg=0.81\n",
            "[21223 | 11201.64] loss=0.79 avg=0.81\n",
            "[21224 | 11204.72] loss=0.91 avg=0.81\n",
            "[21225 | 11207.81] loss=0.70 avg=0.81\n",
            "[21226 | 11210.91] loss=0.65 avg=0.81\n",
            "[21227 | 11214.01] loss=0.81 avg=0.81\n",
            "[21228 | 11217.10] loss=0.63 avg=0.81\n",
            "[21229 | 11220.18] loss=0.75 avg=0.80\n",
            "[21230 | 11223.27] loss=0.77 avg=0.80\n",
            "[21231 | 11226.36] loss=0.81 avg=0.80\n",
            "[21232 | 11229.45] loss=0.94 avg=0.81\n",
            "[21233 | 11232.55] loss=0.71 avg=0.81\n",
            "[21234 | 11235.63] loss=0.93 avg=0.81\n",
            "[21235 | 11238.71] loss=0.95 avg=0.81\n",
            "[21236 | 11241.80] loss=0.81 avg=0.81\n",
            "[21237 | 11244.89] loss=0.90 avg=0.81\n",
            "[21238 | 11248.00] loss=0.78 avg=0.81\n",
            "[21239 | 11251.09] loss=0.62 avg=0.81\n",
            "[21240 | 11254.19] loss=0.65 avg=0.80\n",
            "[21241 | 11257.28] loss=0.86 avg=0.81\n",
            "[21242 | 11260.36] loss=0.74 avg=0.80\n",
            "[21243 | 11263.43] loss=0.79 avg=0.80\n",
            "[21244 | 11266.53] loss=0.77 avg=0.80\n",
            "[21245 | 11269.62] loss=0.85 avg=0.80\n",
            "[21246 | 11272.71] loss=0.92 avg=0.81\n",
            "[21247 | 11275.80] loss=0.87 avg=0.81\n",
            "[21248 | 11278.89] loss=0.89 avg=0.81\n",
            "[21249 | 11282.01] loss=0.92 avg=0.81\n",
            "[21250 | 11285.11] loss=0.92 avg=0.81\n",
            "[21251 | 11288.22] loss=0.90 avg=0.81\n",
            "[21252 | 11291.30] loss=0.66 avg=0.81\n",
            "[21253 | 11294.38] loss=0.82 avg=0.81\n",
            "[21254 | 11297.49] loss=0.85 avg=0.81\n",
            "[21255 | 11300.56] loss=0.76 avg=0.81\n",
            "[21256 | 11303.63] loss=0.73 avg=0.81\n",
            "[21257 | 11306.71] loss=0.78 avg=0.81\n",
            "[21258 | 11309.79] loss=0.93 avg=0.81\n",
            "[21259 | 11312.88] loss=0.75 avg=0.81\n",
            "[21260 | 11315.97] loss=0.83 avg=0.81\n",
            "[21261 | 11319.07] loss=0.62 avg=0.81\n",
            "[21262 | 11322.17] loss=0.99 avg=0.81\n",
            "[21263 | 11325.23] loss=0.80 avg=0.81\n",
            "[21264 | 11328.30] loss=0.89 avg=0.81\n",
            "[21265 | 11331.38] loss=0.71 avg=0.81\n",
            "[21266 | 11334.46] loss=0.71 avg=0.81\n",
            "[21267 | 11337.57] loss=0.94 avg=0.81\n",
            "[21268 | 11340.66] loss=0.87 avg=0.81\n",
            "[21269 | 11343.74] loss=0.75 avg=0.81\n",
            "[21270 | 11346.83] loss=0.83 avg=0.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pENG5_kYWYgt"
      },
      "source": [
        "### Especificar los checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c35c67fa-1c17-44a5-db66-6fc09c4a77cf",
        "id": "BMA8JAZqWYgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# modelo tuneado con traducciones de inglés a español\n",
        "checkpoint = checkpoint_dir + '/run1_spa-eng'\n",
        "checkpoint_num = '45000'\n",
        "model_checkpoint_path = 'model_checkpoint_path: \"' + checkpoint + '/model-' + checkpoint_num + '\"'\n",
        "\n",
        "with open('gpt-2/models/345M/checkpoint', \"wt\") as file:\n",
        "    print(model_checkpoint_path)\n",
        "    file.write(model_checkpoint_path)\n",
        "with open('gpt-2/models/345M/counter', \"wt\") as file:\n",
        "    file.write(f'{checkpoint_num}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_checkpoint_path: \"/content/drive/My Drive/Colab Notebooks/checkpoints/run1_spa-eng/model-45000\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e9796956-30f6-44d0-888e-35be9dbbbde3",
        "id": "-sSDW9HnAMxT",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "ejemplo = \"Who did you go to the movies with? =\" #@param {type : 'string'}\n",
        "numero_de_muestras = 1 #@param {type : 'number'}\n",
        "temperature=0.1 #@param {type : 'number'}\n",
        "#@markdown La temperatura controla el grado de aleatoriedad (0 = determinista)\n",
        "top_k=40 #@param {type : 'integer'}\n",
        "#@markdown Número de candidatos considerados en el beam search (0 = \"greedy\", funciona bien con 40)\n",
        "top_p=0.1 #@param {type : 'number'}\n",
        "#@markdown Controla la diversidad. (0 = valor por defecto, funciona bien con 0.9)\n",
        "texts = interact_model(prompt=ejemplo,\n",
        "                       model_name='345M',\n",
        "                       nsamples=numero_de_muestras,\n",
        "                       temperature=temperature,\n",
        "                       top_k=top_k,\n",
        "                       top_p=top_p)\n",
        "display(HTML('<p><b><i>' + ejemplo + '</b></i></p>'))\n",
        "display(HTML('<p>' + texts[0][:texts[0].find('.')+1] + '</p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>Who did you go to the movies with? =</b></i></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p> ¿Este es mienda con?\n",
              "I'm not going to hurt you.</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GbnbEd02Efe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}