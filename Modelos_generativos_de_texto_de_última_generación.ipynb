{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Modelos generativos de texto de última generación",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teticio/aventuras-con-textos/blob/master/Modelos_generativos_de_texto_de_%C3%BAltima_generaci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T25DUNWU0R7t"
      },
      "source": [
        "# Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-14T16:27:26.559794Z",
          "start_time": "2019-08-14T16:27:24.706228Z"
        },
        "colab_type": "code",
        "id": "zZq-8euV0Q-f",
        "outputId": "201918b6-685a-4ca6-d72c-83c81b542212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import selectors\n",
        "import subprocess as sp\n",
        "from IPython.display import clear_output\n",
        "from IPython.core.display import display, HTML\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import get_file\n",
        "\n",
        "sess = tf.Session()\n",
        "checkpoint_dir = os.getcwd().replace('\\\\', '/') + '/checkpoints'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b4eQ2KGyPgUx"
      },
      "source": [
        "# XLNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EItLO1rwgd9Y"
      },
      "source": [
        "### Instalar modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sg2zUAnoZ_IK",
        "outputId": "92fd654d-e072-4549-c9eb-0a753a0ab719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!test -d XLnet-gen || git clone --quiet https://github.com/rusiaaman/XLnet-gen.git\n",
        "!pip install -q -r XLnet-gen/requirements.txt\n",
        "!test -e cased_L-24_H-1024_A-16.zip || wget -q https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip\n",
        "!test -d xlnet_cased_L-24_H-1024_A-16 || unzip -q cased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.0MB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 19.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xq_iYW7ogjw7"
      },
      "source": [
        "### Generar texto\n",
        "\n",
        "Requiere `pip install tensorflow==1.14.0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2JBzCdVv6Q-z",
        "outputId": "0787d36c-b9a0-4e19-c551-f8b826052f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "ejemplo = 'Sadly, however, before she could get to a phone to tell anyone- about it, a terribly stupid catastrophe occurred, and the idea was lost forever.' #@param {type : 'string'}\n",
        "with open('test', 'wt') as file:\n",
        "    file.write(ejemplo)\n",
        "command = [sys.executable, 'XLnet-gen/language_generation.py',\n",
        "           '--model_config_path=xlnet_cased_L-24_H-1024_A-16/xlnet_config.json',\n",
        "           '--init_checkpoint=xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt',\n",
        "           '--spiece_model_file=xlnet_cased_L-24_H-1024_A-16/spiece.model',\n",
        "           '--input_file=test',\n",
        "           '--max_mem_length=512',\n",
        "           '--num_toks_pred=512',\n",
        "           '--num_samples=1',\n",
        "           '--top_p=0.9']\n",
        "p = sp.Popen(command, stderr=sp.PIPE)\n",
        "if p.returncode != 0:\n",
        "    print(p.communicate()[1].decode())\n",
        "with open('test.xlnet', 'rt') as file:\n",
        "    for line in file.readlines():\n",
        "        display(HTML('<p>' + line + '</p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======Example 0=================\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>Sadly, however, before she could get to a phone to tell anyone- about it, a terribly stupid catastrophe occurred, and the idea was lost forever.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======Example 0 SAMPLE 0======\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>Now she was stuck with the dreadful notion of seeing herself impure and not even knowing what she had imagined. Sure, it was only a whiff of impure air she had been breathing for that whole time, but what could she have actually imagined? She had no idea how to think of anything.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>After everyone had spoken, she turned to take the stairs down to her apartment and watched the people leave. The crowd were already in a hurry to get out of the restaurant. Since she was alone, she thought she would try to a lie. With each successive thing she wanted to tell people, she found it hard to think of a lie.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>A big pro at lying like her father was, she needed to think of something else that would have an easy target in the eyes of those around her. After all, this was her last chance to tell her father about her friend and everyone else about her boyfriend and the wedding plans they had on the horizon. At the moment, it was more important to avoid things than face her fears.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>For that reason, she started to think about something else. Possibly something that would involve going to a dungeon, torture, or some other horrible horrible thing that would hurt her. It would also need to be intimate with people, which her father would probably be very uncomfortable with. When she thought about the possibility, she was appalled by how terribly destructive and scary it all could be.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>Next, she thought about another rather horrible idea that was \"on the thoughts\" of the other people in the restaurant. These men were already together. She decided it would be easy for them to discover that there were two women in the party, and take their chances with them. She knew she could prevent this. In fact, she was preparing herself mentally to use her abilities to prevent it. If anything, it might also make things even more difficult for her dad.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>She thought about how hard it was not to see herself in the same light as the woman who had been in the photo. In fact, it was hard to imagine the woman as the person who had been in the photo, because she had no idea what it would be like. It seemed much more difficult than she had thought it would be.\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>She had to have some kind of defense for herself. If anyone did discover that she was a female, it would mean a lot to her father. The thought of that made her want to scream, but the words didn't come easily to her. They didn't come\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>==================================\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mw1_5HFpPHXA"
      },
      "source": [
        "# GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4-gN6fB1Rr7h"
      },
      "source": [
        "### Instalar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jqpWEas7zw7P",
        "outputId": "07b1387f-cbd7-48e3-e9d4-fc0971cffe0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!test -d gpt-2 || git clone --quiet https://github.com/nshepperd/gpt-2.git\n",
        "!pip install -q -r gpt-2/requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 81kB 5.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 604kB 15.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 21.2MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Iij49z25SpD"
      },
      "source": [
        "### Importar los módulos de GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-14T16:27:32.626001Z",
          "start_time": "2019-08-14T16:27:32.230217Z"
        },
        "colab_type": "code",
        "id": "iMdg--TD5DDG",
        "outputId": "aecf45a8-e587-486f-fd12-7bd9078b3d3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "if not 'gpt-2' in sys.path: # hack\n",
        "    sys.path += ['gpt-2']\n",
        "if not 'gpt-2/src' in sys.path: # hack\n",
        "    sys.path += ['gpt-2/src']\n",
        "sys.argv = ['train.py', '--dataset=../train.txt', '--model_name=345M', '--memory_saving_gradients']    \n",
        "import train\n",
        "import model, sample, encoder"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0814 16:36:13.342616 140046045587328 deprecation_wrapper.py:119] From gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0814 16:36:13.365304 140046045587328 deprecation_wrapper.py:119] From gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jkpW4YIFRnWf"
      },
      "source": [
        "### Descargar los pesos del modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r8bVBQsB_oBk",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('gpt-2/models/345M'):\n",
        "    sp.call([sys.executable, 'download_model.py', '345M'], cwd='gpt-2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z2dTNhrkROWv"
      },
      "source": [
        "### Definir la función para generar textos\n",
        "\n",
        "Basado en https://github.com/openai/gpt-2/blob/master/src/interactive_conditional_samples.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-14T16:27:38.283016Z",
          "start_time": "2019-08-14T16:27:38.269628Z"
        },
        "colab_type": "code",
        "id": "lJsfjW3fASYu",
        "colab": {}
      },
      "source": [
        "def interact_model(\n",
        "    prompt,\n",
        "    model_name='117M',\n",
        "    seed=None,\n",
        "    nsamples=1,\n",
        "    batch_size=1,\n",
        "    length=None,\n",
        "    temperature=1,\n",
        "    top_k=0,\n",
        "    top_p=0.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Interactively run the model\n",
        "    :model_name=117M : String, which model to use\n",
        "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
        "     results\n",
        "    :nsamples=1 : Number of samples to return total\n",
        "    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
        "    :length=None : Number of tokens in generated text, if None (default), is\n",
        "     determined by model hyperparameters\n",
        "    :temperature=1 : Float value controlling randomness in boltzmann\n",
        "     distribution. Lower temperature results in less random completions. As the\n",
        "     temperature approaches zero, the model will become deterministic and\n",
        "     repetitive. Higher temperature results in more random completions.\n",
        "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "     considered for each step (token), resulting in deterministic completions,\n",
        "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "     special setting meaning no restrictions. 40 generally is a good value.\n",
        "    :top_p=0.0 : Float value controlling diversity. Implements nucleus sampling,\n",
        "     overriding top_k if set to a value > 0. A good setting is 0.9.\n",
        "    \"\"\"\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    cwd = os.getcwd()\n",
        "    os.chdir(cwd + '/gpt-2') # hack\n",
        "    raw_text = prompt\n",
        "    texts = []\n",
        "    \n",
        "    try:\n",
        "        enc = encoder.get_encoder(model_name)\n",
        "        hparams = model.default_hparams()\n",
        "        with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
        "            hparams.override_from_dict(json.load(f))\n",
        "\n",
        "        if length is None:\n",
        "            length = hparams.n_ctx // 2\n",
        "        elif length > hparams.n_ctx:\n",
        "            raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k, top_p=top_p\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        context_tokens = enc.encode(raw_text)\n",
        "        for _ in range(nsamples // batch_size):\n",
        "            out = sess.run(output, feed_dict={\n",
        "                context: [context_tokens for _ in range(batch_size)]\n",
        "            })[:, len(context_tokens):]\n",
        "            for i in range(batch_size):\n",
        "                texts += [enc.decode(out[i])]\n",
        "                \n",
        "    except:\n",
        "        os.chdir(cwd) # hack\n",
        "        raise\n",
        "    os.chdir(cwd)\n",
        "    return texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6yskrmEFWNIl"
      },
      "source": [
        "### Especificar los checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-14T16:27:40.915241Z",
          "start_time": "2019-08-14T16:27:40.910447Z"
        },
        "colab_type": "code",
        "id": "4D3vkm1iWK4k",
        "outputId": "a27a6cf5-04a9-4343-adf5-8764e0385ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# modelo pre-entrenado\n",
        "checkpoint_num = ''\n",
        "model_checkpoint_path = 'model_checkpoint_path: \"model.ckpt\"'\n",
        "\n",
        "with open('gpt-2/models/345M/checkpoint', \"wt\") as file:\n",
        "    print(model_checkpoint_path)\n",
        "    file.write(model_checkpoint_path)\n",
        "with open('gpt-2/models/345M/counter', \"wt\") as file:\n",
        "    file.write(f'{checkpoint_num}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_checkpoint_path: \"model.ckpt\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EDVL323ARG34"
      },
      "source": [
        "### Generar muestras con el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gq6s-tH1ljXu",
        "outputId": "8ff49c27-4314-413f-ed7e-c43b5f77bd6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ejemplo = \"You're in a desert, walking along in the sand, when all of a sudden you look down and see a tortoise, Leon. It's crawling toward you. You reach down, you flip the tortoise over on its back. The tortoise lays on its back, its belly baking in the hot sun, beating its legs trying to turn itself over, but it can\\u2019t, not without your help. But you\\u2019re not helping. Why is that?\" #@param {type : 'string'}\n",
        "numero_de_muestras = 3 #@param {type : 'number'}\n",
        "temperature=1 #@param {type : 'number'}\n",
        "#@markdown La temperatura controla el grado de aleatoriedad (0 = determinista)\n",
        "top_k=40 #@param {type : 'integer'}\n",
        "#@markdown Número de candidatos considerados en el beam search (0 = \"greedy\", funciona bien con 40)\n",
        "top_p=0.9 #@param {type : 'number'}\n",
        "#@markdown Controla la diversidad. (0 = valor por defecto, funciona bien con 0.9)\n",
        "texts = interact_model(prompt=ejemplo,\n",
        "                       model_name='345M',\n",
        "                       nsamples=numero_de_muestras,\n",
        "                       temperature=temperature,\n",
        "                       top_k=top_k,\n",
        "                       top_p=top_p)\n",
        "for i, text in enumerate(texts):\n",
        "    display(HTML('<p>' + \"=\" * 40 + \" SAMPLE \" + str(i+1) + \" \" + \"=\" * 40 + '</p>'))\n",
        "    display(HTML('<p>' + ('<b><i>' + ejemplo + '</b></i>' + text).replace('\\n', '<br>') + '</p>'))\n",
        "    display(HTML('<p>' + \"=\" * 80 + '</p>'))\n",
        "    display(HTML('<p><br></p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======================================== SAMPLE 1 ========================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>You're in a desert, walking along in the sand, when all of a sudden you look down and see a tortoise, Leon. It's crawling toward you. You reach down, you flip the tortoise over on its back. The tortoise lays on its back, its belly baking in the hot sun, beating its legs trying to turn itself over, but it can’t, not without your help. But you’re not helping. Why is that?</b></i>’ The tortoise is silent. It stares up at you.<br><br><br>For this, you apologize. You tell it to move on. You apologize again. The tortoise no longer can sit still. It moves on, dragging its leg.<br><br><br>It looks up. The tortoise looks up at you with a bright grin.<br><br><br>***<br><br><br>For days, the last images from one of my least favorite movies from my childhood are burned in my mind. I can't remember the title of the movie, as I could think of it at any moment. But the trailer comes out, and the whole world swells with shame. The movie exists, in my mind. Everything in the movie is an expression of my experiences. People I've known ask me why I thought those movies, and I'd tell them there's nothing wrong with them. Then, once they've done so, they can't remember what they were expecting.<br><br><br>I didn't even know for sure what was supposed to happen with the kids at the end. Many of my memories from that movie overlap in ways that make sense. But no matter what kids, students, or the \"real\" world watches, or doesn't see, they'll understand something about the movie.<br><br>Twenty years ago, the Academy of Motion Picture Arts and Sciences approved Shame, my first feature film. The parents were giving me a lot of grief for casting two white actors in blackface as human beggars. But for me, it was an honor and an adventure, a delight in the chase, ever closer to seeing people really feel for one another.<br><br><br>About the orgy-loving moon girl It is no accident that the film, screened for a student audience, was nominated for an Oscar. She is naked, she is shameless, but she is not only an attractive person. As she peels a pack of lemons and sacrifices them to insects like bats and scorpions, a detail that was only intended to provoke laughs, I recognized with joy that its tragic content was beautiful, even uplifting. \"The End of the World as We Know It\" is also a moment of transcendence. It is no accident that the movie, screened for a student audience, was nominated for an Oscar. She is naked, she is shameless, but she is not only an attractive person. As she peels a pack of lemons and sacrifices them to insects like bats and scorpions, a detail that was only intended to provoke laughs, I recognized with joy that its tragic</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>================================================================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><br></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======================================== SAMPLE 2 ========================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>You're in a desert, walking along in the sand, when all of a sudden you look down and see a tortoise, Leon. It's crawling toward you. You reach down, you flip the tortoise over on its back. The tortoise lays on its back, its belly baking in the hot sun, beating its legs trying to turn itself over, but it can’t, not without your help. But you’re not helping. Why is that?</b></i> Is there something wrong’with you? Liger A You’are telling the truth. Hard enough that you have to take every breath out of it. Leon A It's your turn. Harder than you think. Harder than you're willing to go. You hold it. Liger A You. Hurry! Hurry, hurry’do not stop. Behind you’a beaver, a burrowing lizard, spots you. The burrowing lizard just as it was always kind enough to curl up in a ball next to you and stretch out an arm to shake your hand. Then it climbs up on its hind legs. Liger A It raises its right arm. It opens a small door over your shoulder. She turns the knob on the door to the room below. Three dead animals are scattered around, running through a ravine. Liger A More dead bodies, crawling through. There's blood everywhere. Then, like a train, something drops to the ground from above, throws back some body parts, and spills them all out over the place. Another little kid with a shirtless and stupid pose. The burrowing lizard. The burrowing lizard raises its right arm, takes out its fist, and goes for the butt. But Leon D turns around. Come back. Leon D Come back, coward! Liger A Do not try to beat them over the head, look down, and tell me you are fighting them, leaving them to die, it is better that than walking around surrounded by them? Big smile. \"Good idea\" makes you dizzy. Big smile, large slow smile, friend the burrowing lizard gets a drag and takes a bite out of his own butthole. Liger A Yes, it has laid down some zombie bodies and laid them out with all that dying away. He has killed quite a number of them, but that is more to attract the carnivorous insects that once ate their own. Big smile. \"I will teach you to live\", they say from beneath you. Leon D Oughta fuckin' bother’move out’or else! Liger A Do not. Leon D You are an alien. ’You’will’be going to another planet with monsters. So you understand not to bother, because it could just as well take you with it. I assume you know how far away this planet is, maybe a million kilometers. Liger A You've got a course? That's this planet's orbit. More than</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>================================================================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><br></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======================================== SAMPLE 3 ========================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>You're in a desert, walking along in the sand, when all of a sudden you look down and see a tortoise, Leon. It's crawling toward you. You reach down, you flip the tortoise over on its back. The tortoise lays on its back, its belly baking in the hot sun, beating its legs trying to turn itself over, but it can’t, not without your help. But you’re not helping. Why is that?</b></i> Because ’I don't give a shit about a tortoise, I've been around for forty years and I've fought wars’ But, though Leon is wounded, you don't give a shit because ’I'm a coward, we need help, they're coming. They're crawling up the side of a hill, and out in the distance you see a red and blue and white patch’ And suddenly, you start running with Leon. I've seen tanks burn before, but this is hot’ But again, you run after them, running against the never ending dangers of the desert. You see a red, yellow and white rectangle ’this is the Death Star’ and suddenly, something terrible happens. A giant cylindrical object is burning through the sky. As you watch it burn, an explosion’ the blob of smoke ’clears the horizon as it floats up to us, a huge, blazing white globe’ charging towards us. And you watch it right behind you on that gigantic globe. You rush toward it, aim, blast. But not, because you don't care. You're watching the Death Star do one amazing thing, and now, the heroes’and the galaxy’are going down’<|endoftext|>People change, jobs change, progress stagnates. The topic of video game marketing, the platform that supposedly gave rise to entire forms of entertainment, seems and always has been relegated to the back of main headlines and unreported blogs. And if you're looking to game makers in particular, this is becoming a problem because there's still a bit of quality control to this idea that a publication that hopes to expand a market, is going to put their whole company at risk for another outlet to copy and paste to their liking, instead of producing something unique to their audience.<br><br>It's easy to get into this territory. Why should an animated RPG be your exclusive lifestyle knowledge, compared to the world of Minecraft or Portal? I've heard it's because game makers don't always know how to sell their product well enough to encourage the hardcore gamers that make the title popular, and are sometimes unable to convince people to spend a month or more just for that core multiplayer experience.<br><br><br>It's not that all game designers are nukers. Many are commercial designers of diverse styles and language tones who want to sell their designs across platforms, including still-useable games like Minecraft. If you want to compare your game design to a 5 year old game you did on two</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>================================================================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><br></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b3PU6QOz0Pva"
      },
      "source": [
        "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "45QUnzzAnDbT"
      },
      "source": [
        "### Contestar una pregunta sobre un texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-DsXfTZBra-M",
        "outputId": "625d7ae1-9cca-45bd-c138-16a12d992d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        }
      },
      "source": [
        "contexto = 'The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer \\n\\\n",
        "Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in \\n\\\n",
        "Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried \\n\\\n",
        "the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started \\n\\\n",
        "ahead of the 1936 Summer Olympics. \\n\\\n",
        "\\n\\\n",
        "After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the \\n\\\n",
        "Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was \\n\\\n",
        "following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing \\n\\\n",
        "ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of \\n\\\n",
        "Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the \\n\\\n",
        "event. \\n\\\n",
        "\\n\\\n",
        "Q: What was the theme \\n\\\n",
        "A: “one world, one dream”. \\n\\\n",
        "Q: What was the length of the race? \\n\\\n",
        "A: 137,000 km \\n\\\n",
        "Q: Was it larger than previous ones? \\n\\\n",
        "A: No \\n\\\n",
        "Q: Where did the race begin? \\n\\\n",
        "A: Olympia, Greece \\n\\\n",
        "Q: Is there anything notable about that place? \\n\\\n",
        "A: birthplace of Olympic Games \\n\\\n",
        "Q: Where did they go after? \\n\\\n",
        "A: Athens \\n\\\n",
        "Q: How many days was the race? \\n\\\n",
        "A: seven \\n\\\n",
        "Q: Did they visit any notable landmarks? \\n\\\n",
        "A: Panathinaiko Stadium \\n'\n",
        "\n",
        "pregunta = 'Q: And did they climb any mountains? \\n\\\n",
        "A:'\n",
        "\n",
        "display(HTML('<p>' + (contexto + '<b><i>' + pregunta + '</b></i>').replace('\\n', '<br>') + '</p>'))\n",
        "texts = interact_model(prompt=contexto + pregunta,model_name='345M', temperature=0.001)\n",
        "answer = texts[0][:texts[0].find('Q')]\n",
        "display(HTML('<p>' + answer + '</p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer <br>Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in <br>Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried <br>the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started <br>ahead of the 1936 Summer Olympics. <br><br>After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the <br>Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was <br>following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing <br>ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of <br>Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the <br>event. <br><br>Q: What was the theme <br>A: “one world, one dream”. <br>Q: What was the length of the race? <br>A: 137,000 km <br>Q: Was it larger than previous ones? <br>A: No <br>Q: Where did the race begin? <br>A: Olympia, Greece <br>Q: Is there anything notable about that place? <br>A: birthplace of Olympic Games <br>Q: Where did they go after? <br>A: Athens <br>Q: How many days was the race? <br>A: seven <br>Q: Did they visit any notable landmarks? <br>A: Panathinaiko Stadium <br><b><i>Q: And did they climb any mountains? <br>A:</b></i></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p> Mount Everest \n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "alhg-bLiKQbD",
        "outputId": "127f2876-7c26-4cd3-8855-bcf1a8e67d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "contexto = 'Tom goes everywhere with Catherine Green, a 54-year-old secretary. He moves around her office at work and goes \\n\\\n",
        "shopping with her. ”Most people don’t seem to mind Tom,” says Catherine, who thinks he is wonderful. ”He’s my \\n\\\n",
        "fourth child,” she says. She may think of him and treat him that way as her son. He moves around buying his food, \\n\\\n",
        "paying his health bills and his taxes, but in fact Tom is a dog. \\n\\\n",
        "\\n\\\n",
        "Catherine and Tom live in Sweden, a country where everyone is expected to lead an orderly life according to rules \\n\\\n",
        "laid down by the government, which also provides a high level of care for its people. This level of care \\n\\\n",
        "costs money. \\n\\\n",
        "\\n\\\n",
        "People in Sweden pay taxes on everything, so aren’t surprised to find that owning a dog means more \\n\\\n",
        "taxes. Some people are paying as much as 500 Swedish kronor in taxes a year for the right to keep their dog, which \\n\\\n",
        "is spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. However, most \\n\\\n",
        "such treatment is expensive, so owners often decide to offer health and even life for their dog. \\n\\\n",
        "\\n\\\n",
        "In Sweden dog owners must pay for any damage their dog does. A Swedish Kennel Club official explains what this means: \\n\\\n",
        "if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay \\n\\\n",
        "for any damage done to the car, even if your dog has been killed in the accident. \\n\\\n",
        "\\n\\\n",
        "Q: How old is Catherine? \\n\\\n",
        "A: 54 \\n'\n",
        "\n",
        "pregunta = 'Q: where does she live? \\n\\\n",
        "A:'\n",
        "\n",
        "display(HTML('<p>' + (contexto + '<b><i>' + pregunta + '</b></i>').replace('\\n', '<br>') + '</p>'))\n",
        "texts = interact_model(prompt=contexto + pregunta, model_name='345M', temperature=0.001)\n",
        "answer = texts[0][:texts[0].find('Q')]\n",
        "display(HTML('<p>' + answer + '</p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>Tom goes everywhere with Catherine Green, a 54-year-old secretary. He moves around her office at work and goes <br>shopping with her. ”Most people don’t seem to mind Tom,” says Catherine, who thinks he is wonderful. ”He’s my <br>fourth child,” she says. She may think of him and treat him that way as her son. He moves around buying his food, <br>paying his health bills and his taxes, but in fact Tom is a dog. <br><br>Catherine and Tom live in Sweden, a country where everyone is expected to lead an orderly life according to rules <br>laid down by the government, which also provides a high level of care for its people. This level of care <br>costs money. <br><br>People in Sweden pay taxes on everything, so aren’t surprised to find that owning a dog means more <br>taxes. Some people are paying as much as 500 Swedish kronor in taxes a year for the right to keep their dog, which <br>is spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. However, most <br>such treatment is expensive, so owners often decide to offer health and even life for their dog. <br><br>In Sweden dog owners must pay for any damage their dog does. A Swedish Kennel Club official explains what this means: <br>if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay <br>for any damage done to the car, even if your dog has been killed in the accident. <br><br>Q: How old is Catherine? <br>A: 54 <br><b><i>Q: where does she live? <br>A:</b></i></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p> in Sweden. She lives in a small apartment in a small town in the north of Sweden. She is a single mother of two children. She is a member of the Swedish Kennel Club.\n",
              "\n",
              "</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "76OckKscm3W5"
      },
      "source": [
        "### Resumir un texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gPtPeYo9iGWn",
        "outputId": "45b7f823-cc2b-41a9-8237-aae8fc974f11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "contexto ='1. Introduction \\n\\\n",
        "\\n\\\n",
        "Machine learning systems now excel (in expectation) at \\n\\\n",
        "tasks they are trained for by using a combination of large \\n\\\n",
        "datasets, high-capacity models, and supervised learning \\n\\\n",
        "(Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei \\n\\\n",
        "et al., 2016). Yet these systems are brittle and sensitive to \\n\\\n",
        "slight changes in the data distribution (Recht et al., 2018) \\n\\\n",
        "and task specification (Kirkpatrick et al., 2017). Current \\n\\\n",
        "systems are better characterized as narrow experts rather than \\n\\\n",
        "competent generalists. We would like to move towards more \\n\\\n",
        "general systems which can perform many tasks – eventually \\n\\\n",
        "without the need to manually create and label a training \\n\\\n",
        "dataset for each one. \\n\\\n",
        "\\n\\\n",
        "The dominant approach to creating ML systems is to collect a \\n\\\n",
        "dataset of training examples demonstrating correct \\n\\\n",
        "behavior for a desired task, train a system to imitate these \\n\\\n",
        "behaviors, and then test its performance on independent \\n\\\n",
        "and identically distributed (IID) held-out examples. This \\n\\\n",
        "has served well to make progress on narrow experts. But \\n\\\n",
        "the often erratic behavior of captioning models (Lake et al., \\n\\\n",
        "2017), reading comprehension systems (Jia & Liang, 2017), \\n\\\n",
        "and image classifiers (Alcorn et al., 2018) on the diversity \\n\\\n",
        "and variety of possible inputs highlights some of the shortcomings \\n\\\n",
        "of this approach. \\n\\\n",
        "\\n\\\n",
        "Our suspicion is that the prevalence of single task training \\n\\\n",
        "on single domain datasets is a major contributor to the lack \\n\\\n",
        "of generalization observed in current systems. Progress \\n\\\n",
        "towards robust systems with current architectures is likely \\n\\\n",
        "to require training and measuring performance on a wide \\n\\\n",
        "range of domains and tasks. Recently, several benchmarks \\n\\\n",
        "have been proposed such as GLUE (Wang et al., 2018) and \\n\\\n",
        "decaNLP (McCann et al., 2018) to begin studying this. \\n\\\n",
        "\\n\\\n",
        "Multitask learning (Caruana, 1997) is a promising framework for \\n\\\n",
        "improving general performance. However, multitask training in NLP \\n\\\n",
        "is still nascent. Recent work reports modest performance \\n\\\n",
        "improvements (Yogatama et al., \\n\\\n",
        "2019) and the two most ambitious efforts to date have \\n\\\n",
        "trained on a total of 10 and 17 (dataset, objective) \\n\\\n",
        "pairs respectively (McCann et al., 2018) (Bowman et al., \\n\\\n",
        "2018). From a meta-learning perspective, each (dataset, \\n\\\n",
        "objective) pair is a single training example sampled \\n\\\n",
        "from the distribution of datasets and objectives. Current \\n\\\n",
        "ML systems need hundreds to thousands of examples to \\n\\\n",
        "induce functions which generalize well. This suggests that \\n\\\n",
        "multitask training many need just as many effective training \\n\\\n",
        "pairs to realize its promise with current approaches. It will \\n\\\n",
        "be very difficult to continue to scale the creation of datasets \\n\\\n",
        "and the design of objectives to the degree that may be required  \\n\\\n",
        "to brute force our way there with current techniques. \\n\\\n",
        "This motivates exploring additional setups for performing \\n\\\n",
        "multitask learning. \\n'\n",
        "\n",
        "pregunta = 'TL;DR:' # Too Long; Didn't Read (¡resúmelo por favor!)\n",
        "\n",
        "display(HTML('<p>' + (contexto + '<b><i>' + pregunta + '</b></i>').replace('\\n', '<br>') + '</p>'))\n",
        "texts = interact_model(prompt=contexto + pregunta, model_name='345M', temperature=0.001)\n",
        "answer = texts[0][:texts[0].find('.')+1]\n",
        "display(HTML('<p>' + answer + '</p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>1. Introduction <br><br>Machine learning systems now excel (in expectation) at <br>tasks they are trained for by using a combination of large <br>datasets, high-capacity models, and supervised learning <br>(Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei <br>et al., 2016). Yet these systems are brittle and sensitive to <br>slight changes in the data distribution (Recht et al., 2018) <br>and task specification (Kirkpatrick et al., 2017). Current <br>systems are better characterized as narrow experts rather than <br>competent generalists. We would like to move towards more <br>general systems which can perform many tasks – eventually <br>without the need to manually create and label a training <br>dataset for each one. <br><br>The dominant approach to creating ML systems is to collect a <br>dataset of training examples demonstrating correct <br>behavior for a desired task, train a system to imitate these <br>behaviors, and then test its performance on independent <br>and identically distributed (IID) held-out examples. This <br>has served well to make progress on narrow experts. But <br>the often erratic behavior of captioning models (Lake et al., <br>2017), reading comprehension systems (Jia & Liang, 2017), <br>and image classifiers (Alcorn et al., 2018) on the diversity <br>and variety of possible inputs highlights some of the shortcomings <br>of this approach. <br><br>Our suspicion is that the prevalence of single task training <br>on single domain datasets is a major contributor to the lack <br>of generalization observed in current systems. Progress <br>towards robust systems with current architectures is likely <br>to require training and measuring performance on a wide <br>range of domains and tasks. Recently, several benchmarks <br>have been proposed such as GLUE (Wang et al., 2018) and <br>decaNLP (McCann et al., 2018) to begin studying this. <br><br>Multitask learning (Caruana, 1997) is a promising framework for <br>improving general performance. However, multitask training in NLP <br>is still nascent. Recent work reports modest performance <br>improvements (Yogatama et al., <br>2019) and the two most ambitious efforts to date have <br>trained on a total of 10 and 17 (dataset, objective) <br>pairs respectively (McCann et al., 2018) (Bowman et al., <br>2018). From a meta-learning perspective, each (dataset, <br>objective) pair is a single training example sampled <br>from the distribution of datasets and objectives. Current <br>ML systems need hundreds to thousands of examples to <br>induce functions which generalize well. This suggests that <br>multitask training many need just as many effective training <br>pairs to realize its promise with current approaches. It will <br>be very difficult to continue to scale the creation of datasets <br>and the design of objectives to the degree that may be required  <br>to brute force our way there with current techniques. <br>This motivates exploring additional setups for performing <br>multitask learning. <br><b><i>TL;DR:</b></i></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p> We propose a novel approach to training ML systems which \n",
              "allows for the creation of large datasets and large \n",
              "objectives.</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AI9Ys9-gQKbh"
      },
      "source": [
        "### Descargar un texto para fine-tunear el modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-14T16:30:22.760420Z",
          "start_time": "2019-08-14T16:30:11.116409Z"
        },
        "colab_type": "code",
        "id": "vGi73ipH0lMn",
        "outputId": "15877ffd-3a78-4b53-c880-03b986469f44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "if os.path.exists('./train.txt'):\n",
        "    os.remove('./train.txt')\n",
        "\n",
        "# The Hitchhikers Guide to the Galaxy\n",
        "#get_file(os.getcwd() + '/train.txt', origin='https://docs.google.com/uc?export=download&id=1yQgoz5QmvbxQzN2TZWWjvniPqDhGoC8W')\n",
        "\n",
        "# Lord of the Rings : The Fellowship of the Ring\n",
        "#get_file(os.getcwd() + '/train.txt', origin='https://docs.google.com/uc?export=download&id=1kZOHpaPV2ml8rT9l_bbWPt06H6qz-kcm')\n",
        "\n",
        "# Game of Thrones: A Song of Fire and Ice\n",
        "#get_file(os.getcwd() + '/train.txt', origin='https://docs.google.com/uc?export=download&id=1saZmZA07QAqkG-afxlHDPEXNhDNV8Qka')\n",
        "\n",
        "# Cien Años de Soledad (no funciona muy bien al no haber sido entrenado en español el modelo...)\n",
        "#get_file(os.getcwd() + '/train.txt', origin='https://docs.google.com/uc?export=download&id=1XDJmGP9MtOLQSujO5Voicp1wyekHchpm')\n",
        "\n",
        "# Harry Potter and the Goblet of Fire\n",
        "#get_file(os.getcwd() + '/train.txt', origin='https://docs.google.com/uc?export=download&id=10OhbIQHNJrtBiKer8tP_LbxjASqItNzZ')\n",
        "\n",
        "# Trump's pearls of wisdom (http://www.trumptwitterarchive.com/archive)\n",
        "get_file(os.getcwd() + '/train.txt', origin='https://docs.google.com/uc?export=download&id=1d7g95QNkpvC4bwy5xtsC33JYb-AxvJ-q')\n",
        "\n",
        "encoding = 'cp1252' if os.name == 'nt' else 'utf-8'\n",
        "raw_text = ''\n",
        "with open('train.txt', 'r', encoding=encoding, errors='backslashreplace') as f:\n",
        "    raw_text += f.read()\n",
        "with open('train.txt', 'w', encoding=encoding, errors='backslashreplace') as f:\n",
        "    f.write(raw_text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://docs.google.com/uc?export=download&id=1d7g95QNkpvC4bwy5xtsC33JYb-AxvJ-q\n",
            "3121152/Unknown - 1s 0us/step"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "joro1L73Qc8i"
      },
      "source": [
        "### Crear directorio para guardar los checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-14T16:30:27.118211Z",
          "start_time": "2019-08-14T16:30:27.114590Z"
        },
        "colab_type": "code",
        "id": "TyyVsOx0lHhx",
        "outputId": "ad5d22e7-69b4-4bfe-98cb-77532be77fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "try: # estamos en Google Colab?\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    checkpoint_dir = '/content/drive/My Drive/Colab Notebooks/checkpoints'\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xAC8hmQbQiqt"
      },
      "source": [
        "### Entrenar el modelo\n",
        "\n",
        "https://github.com/nshepperd/gpt-2.git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-14T16:31:14.762380Z",
          "start_time": "2019-08-14T16:30:29.972619Z"
        },
        "colab_type": "code",
        "id": "VPFHeGYW6sDk",
        "outputId": "a5fb73d9-35a2-4222-de34-634610ca1b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train.CHECKPOINT_DIR = checkpoint_dir\n",
        "tf.reset_default_graph()\n",
        "cwd = os.getcwd()\n",
        "os.chdir(cwd + '/gpt-2') # hack\n",
        "try:\n",
        "    train.main()\n",
        "except:\n",
        "    os.chdir(cwd) # hack\n",
        "    raise\n",
        "os.chdir(cwd) # hack     "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0814 16:38:04.139084 140046045587328 deprecation_wrapper.py:119] From gpt-2/train.py:87: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0814 16:38:04.140685 140046045587328 deprecation_wrapper.py:119] From gpt-2/train.py:90: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0814 16:38:15.765946 140046045587328 deprecation.py:323] From gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0814 16:38:15.784390 140046045587328 deprecation.py:323] From gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0814 16:38:15.787425 140046045587328 deprecation.py:323] From gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0814 16:38:15.801182 140046045587328 deprecation_wrapper.py:119] From gpt-2/train.py:120: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0814 16:38:31.279970 140046045587328 deprecation_wrapper.py:119] From gpt-2/train.py:143: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0814 16:38:31.285289 140046045587328 deprecation_wrapper.py:119] From gpt-2/train.py:146: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0814 16:38:31.287990 140046045587328 deprecation_wrapper.py:119] From gpt-2/train.py:148: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0814 16:38:31.620051 140046045587328 deprecation_wrapper.py:119] From gpt-2/train.py:151: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0814 16:38:44.093391 140046045587328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint models/345M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:05<00:00,  5.31s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 808214 tokens\n",
            "Training...\n",
            "[1 | 12.65] loss=3.42 avg=3.42\n",
            "[2 | 14.48] loss=3.19 avg=3.30\n",
            "[3 | 16.30] loss=3.35 avg=3.32\n",
            "[4 | 18.15] loss=3.04 avg=3.25\n",
            "[5 | 20.02] loss=3.31 avg=3.26\n",
            "[6 | 21.88] loss=3.32 avg=3.27\n",
            "[7 | 23.76] loss=3.34 avg=3.28\n",
            "[8 | 25.67] loss=2.93 avg=3.23\n",
            "[9 | 27.57] loss=3.07 avg=3.21\n",
            "[10 | 29.45] loss=2.97 avg=3.19\n",
            "[11 | 31.31] loss=3.65 avg=3.23\n",
            "[12 | 33.14] loss=3.52 avg=3.26\n",
            "[13 | 34.98] loss=3.30 avg=3.26\n",
            "[14 | 36.78] loss=3.28 avg=3.26\n",
            "[15 | 38.56] loss=3.21 avg=3.26\n",
            "[16 | 40.34] loss=3.86 avg=3.30\n",
            "[17 | 42.11] loss=3.01 avg=3.28\n",
            "[18 | 43.88] loss=3.31 avg=3.28\n",
            "[19 | 45.63] loss=3.51 avg=3.30\n",
            "[20 | 47.37] loss=3.29 avg=3.30\n",
            "[21 | 49.10] loss=3.31 avg=3.30\n",
            "[22 | 50.84] loss=3.54 avg=3.31\n",
            "[23 | 52.56] loss=3.41 avg=3.31\n",
            "[24 | 54.30] loss=3.13 avg=3.31\n",
            "[25 | 56.03] loss=3.19 avg=3.30\n",
            "[26 | 57.76] loss=3.27 avg=3.30\n",
            "[27 | 59.49] loss=3.71 avg=3.32\n",
            "[28 | 61.20] loss=3.82 avg=3.34\n",
            "[29 | 62.91] loss=3.15 avg=3.33\n",
            "[30 | 64.64] loss=3.08 avg=3.32\n",
            "[31 | 66.36] loss=3.01 avg=3.31\n",
            "[32 | 68.07] loss=3.27 avg=3.31\n",
            "[33 | 69.78] loss=3.28 avg=3.31\n",
            "[34 | 71.49] loss=3.46 avg=3.31\n",
            "[35 | 73.22] loss=3.48 avg=3.32\n",
            "[36 | 74.94] loss=3.11 avg=3.31\n",
            "[37 | 76.67] loss=3.06 avg=3.30\n",
            "[38 | 78.40] loss=2.67 avg=3.28\n",
            "[39 | 80.14] loss=3.77 avg=3.30\n",
            "[40 | 81.87] loss=3.06 avg=3.29\n",
            "[41 | 83.60] loss=3.53 avg=3.30\n",
            "[42 | 85.34] loss=3.64 avg=3.31\n",
            "[43 | 87.08] loss=2.97 avg=3.30\n",
            "[44 | 88.83] loss=3.05 avg=3.29\n",
            "[45 | 90.58] loss=3.51 avg=3.30\n",
            "[46 | 92.31] loss=2.50 avg=3.27\n",
            "[47 | 94.07] loss=3.56 avg=3.28\n",
            "[48 | 95.85] loss=2.98 avg=3.27\n",
            "[49 | 97.63] loss=3.17 avg=3.27\n",
            "[50 | 99.39] loss=3.45 avg=3.28\n",
            "[51 | 101.17] loss=3.52 avg=3.28\n",
            "[52 | 102.95] loss=3.24 avg=3.28\n",
            "[53 | 104.72] loss=3.03 avg=3.27\n",
            "[54 | 106.50] loss=2.89 avg=3.27\n",
            "[55 | 108.27] loss=3.76 avg=3.28\n",
            "[56 | 110.06] loss=2.94 avg=3.27\n",
            "[57 | 111.83] loss=3.07 avg=3.26\n",
            "[58 | 113.61] loss=3.08 avg=3.26\n",
            "[59 | 115.38] loss=3.56 avg=3.27\n",
            "[60 | 117.15] loss=2.84 avg=3.26\n",
            "[61 | 118.92] loss=2.81 avg=3.25\n",
            "[62 | 120.70] loss=3.04 avg=3.24\n",
            "[63 | 122.47] loss=3.31 avg=3.25\n",
            "[64 | 124.24] loss=3.67 avg=3.25\n",
            "[65 | 126.01] loss=3.75 avg=3.26\n",
            "[66 | 127.78] loss=3.25 avg=3.26\n",
            "[67 | 129.55] loss=3.11 avg=3.26\n",
            "[68 | 131.32] loss=3.13 avg=3.26\n",
            "[69 | 133.07] loss=3.30 avg=3.26\n",
            "[70 | 134.83] loss=3.46 avg=3.26\n",
            "[71 | 136.59] loss=3.50 avg=3.27\n",
            "[72 | 138.34] loss=3.41 avg=3.27\n",
            "[73 | 140.08] loss=2.89 avg=3.26\n",
            "[74 | 141.83] loss=3.03 avg=3.26\n",
            "[75 | 143.57] loss=3.46 avg=3.26\n",
            "[76 | 145.32] loss=3.59 avg=3.27\n",
            "[77 | 147.06] loss=3.37 avg=3.27\n",
            "[78 | 148.78] loss=3.11 avg=3.27\n",
            "[79 | 150.52] loss=2.84 avg=3.26\n",
            "[80 | 152.25] loss=3.02 avg=3.26\n",
            "[81 | 153.98] loss=3.08 avg=3.25\n",
            "[82 | 155.72] loss=2.83 avg=3.24\n",
            "[83 | 157.45] loss=2.95 avg=3.24\n",
            "[84 | 159.19] loss=3.18 avg=3.24\n",
            "[85 | 160.92] loss=3.51 avg=3.24\n",
            "[86 | 162.67] loss=2.88 avg=3.24\n",
            "[87 | 164.41] loss=3.58 avg=3.24\n",
            "[88 | 166.15] loss=3.37 avg=3.25\n",
            "[89 | 167.89] loss=3.21 avg=3.24\n",
            "[90 | 169.64] loss=2.85 avg=3.24\n",
            "[91 | 171.40] loss=2.74 avg=3.23\n",
            "[92 | 173.15] loss=3.17 avg=3.23\n",
            "[93 | 174.90] loss=3.16 avg=3.23\n",
            "[94 | 176.65] loss=3.24 avg=3.23\n",
            "[95 | 178.41] loss=3.01 avg=3.22\n",
            "[96 | 180.18] loss=3.22 avg=3.22\n",
            "[97 | 181.95] loss=2.96 avg=3.22\n",
            "[98 | 183.70] loss=2.94 avg=3.22\n",
            "[99 | 185.47] loss=3.38 avg=3.22\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "The first issue that came before the Congress was that the Congress of United States has the absolute right to decide its own course and to negotiate with the United States Government. It is not the position of the Government of India that its policy to do not allow any violation of the bilateral agreement that has been agreed to by both of them was made in vain for over three quarters of a century. This has had an adverse impact on the bilateral relations.\n",
            "This is important, as I have said many times before, and I have given it my attention. My Government will continue to do exactly what is right in all areas and I have given it my full attention and commitment.\n",
            "Pakistan is a partner in many areas and as per the agreement, it is the Government of China, which has expressed willingness to do what India has asked them to do in a manner which I have not been able to do in the past.\n",
            "As well as this, the Chinese Government has promised, I know for a fact, that it will be able to take full advantage of the agreements that are in place. Pakistan is therefore in a unique position for doing the right thing.\n",
            "It is my understanding that in terms of India and the Chinese Government, there will be the opportunity at hand that we all dream to have during today's bilateral meetings.\n",
            "The situation in Pakistan that exists today has had an adverse effect on bilateral relations. This is no longer a partnership that was supposed to happen in 2000 because of the fact that there was nothing to it when it first emerged in 2002 when the first bilateral agreement was reached.\n",
            "So we are still working to make up lost ground. One of the things that I have suggested is that we should use this in the context of negotiations as a way to increase the likelihood of reaching a peaceful resolution to the problem and the chances of reaching a negotiated solution.\n",
            "This will be a difficult negotiation because of the political nature of it as well as the technical nature of it. This is not a mere exercise in diplomatic procedure or a mere exercise in diplomatic talks.\n",
            "It has been an exercise in terms of political negotiations, it has been an exercise in terms of technical negotiations, the two sides come together and try to achieve a result which is a solution to the problem of terrorism, it has been an exercise in terms of political negotiations.\n",
            "The question is how big is this? Is there someone that will come to the table who knows exactly what will come out? How do this negotiations reach an agreement that will satisfy their respective sides if they have not reached an agreement already.\n",
            "The key here is that the agreement reached will be a solution so that the problem of terrorism can be solved so that the lives of Pakistanis and Indians will be not only safe but also prosperous as well.\n",
            "When we see the situation today where terror attacks continue to take place, we can only continue to work to make sure that all human lives and interests of India and Pakistan are safeguarded.\n",
            "So yes the situation in the region is very difficult. That is why we are currently engaged in bilateral talks.\n",
            "It is important that we not leave things unresolved. There is no reason why Pakistan must not meet its commitments. I have talked to Pakistan Prime Minister that the country is going to be a peaceful place. They have made great strides in making sure that they are the best friend that they can have on this planet for many years to come.\n",
            "I have been very pleased with the level of engagement which we are seeing from you in terms of both countries.\n",
            "You and your Government have made a big step with the development of your economy. There is a huge amount that is being done and you should continue to do all that you can to ensure that development gets continued and to make sure those things are done.\n",
            "There is still so much work to be done in Pakistan and Pakistan is going in the right direction to make sure that no harm arises from what could be terrorism.\n",
            "We have made great strides in Pakistan and India. Both of us have got ahead on this very complicated thing where it's really all about trying to solve a very complicated situation.\n",
            "As well as the things I have mentioned.\n",
            "There is still so much to be done but we shall make sure that the issues of terrorism are resolved as soon as possible and as soon as possible as you are taking the same steps the two countries should be doing the same.\n",
            "You have always been an outstanding ally. My priority as Leader of the Opposition in the House of Commons is to work with you so that our relationship continues to improve.\n",
            "I look forward to the Prime Minister and I hope that he and the Prime Minister of India meet tomorrow.\n",
            "I have just returned from a visit to Pakistan. It was very pleasant. Had great time. Thank you to all the people of Pakistan and to the people of the world for the continued hospitality. It is an honor to address the Pakistani people.\n",
            "Thank you. I was joined by Prime Minister Sheikh Yusuf Raza Gilani and his family. All the people of Pakistan express the highest appreciation both\n",
            "\n",
            "[100 | 212.26] loss=3.10 avg=3.22\n",
            "[101 | 214.01] loss=3.62 avg=3.22\n",
            "[102 | 215.76] loss=3.26 avg=3.22\n",
            "[103 | 217.49] loss=3.10 avg=3.22\n",
            "[104 | 219.22] loss=3.41 avg=3.22\n",
            "[105 | 220.95] loss=2.77 avg=3.22\n",
            "[106 | 222.70] loss=3.28 avg=3.22\n",
            "[107 | 224.43] loss=3.37 avg=3.22\n",
            "[108 | 226.18] loss=3.55 avg=3.23\n",
            "[109 | 227.92] loss=2.72 avg=3.22\n",
            "[110 | 229.65] loss=3.03 avg=3.21\n",
            "[111 | 231.39] loss=3.03 avg=3.21\n",
            "[112 | 233.13] loss=3.13 avg=3.21\n",
            "[113 | 234.86] loss=3.42 avg=3.21\n",
            "[114 | 236.59] loss=3.52 avg=3.22\n",
            "[115 | 238.32] loss=3.29 avg=3.22\n",
            "[116 | 240.06] loss=3.44 avg=3.22\n",
            "[117 | 241.80] loss=3.21 avg=3.22\n",
            "[118 | 243.52] loss=3.01 avg=3.22\n",
            "[119 | 245.23] loss=2.91 avg=3.21\n",
            "[120 | 246.97] loss=3.18 avg=3.21\n",
            "[121 | 248.70] loss=2.96 avg=3.21\n",
            "[122 | 250.43] loss=2.89 avg=3.21\n",
            "[123 | 252.16] loss=2.74 avg=3.20\n",
            "[124 | 253.89] loss=3.33 avg=3.20\n",
            "[125 | 255.64] loss=3.73 avg=3.21\n",
            "[126 | 257.37] loss=3.50 avg=3.21\n",
            "[127 | 259.10] loss=2.98 avg=3.21\n",
            "[128 | 260.84] loss=2.89 avg=3.21\n",
            "[129 | 262.56] loss=2.72 avg=3.20\n",
            "[130 | 264.29] loss=2.63 avg=3.19\n",
            "[131 | 266.03] loss=3.09 avg=3.19\n",
            "[132 | 267.75] loss=2.90 avg=3.19\n",
            "[133 | 269.51] loss=2.48 avg=3.18\n",
            "[134 | 271.23] loss=3.20 avg=3.18\n",
            "[135 | 272.96] loss=3.27 avg=3.18\n",
            "[136 | 274.69] loss=3.05 avg=3.18\n",
            "[137 | 276.42] loss=3.13 avg=3.18\n",
            "[138 | 278.15] loss=2.98 avg=3.17\n",
            "[139 | 279.88] loss=3.38 avg=3.18\n",
            "[140 | 281.61] loss=3.15 avg=3.17\n",
            "[141 | 283.35] loss=2.85 avg=3.17\n",
            "[142 | 285.08] loss=3.11 avg=3.17\n",
            "[143 | 286.81] loss=3.55 avg=3.18\n",
            "[144 | 288.55] loss=3.46 avg=3.18\n",
            "[145 | 290.28] loss=3.06 avg=3.18\n",
            "[146 | 292.01] loss=3.31 avg=3.18\n",
            "[147 | 293.75] loss=3.41 avg=3.18\n",
            "[148 | 295.48] loss=3.14 avg=3.18\n",
            "[149 | 297.23] loss=2.91 avg=3.18\n",
            "[150 | 298.95] loss=3.50 avg=3.18\n",
            "[151 | 300.70] loss=2.99 avg=3.18\n",
            "[152 | 302.44] loss=3.11 avg=3.18\n",
            "[153 | 304.17] loss=2.81 avg=3.17\n",
            "[154 | 305.91] loss=3.10 avg=3.17\n",
            "[155 | 307.64] loss=3.07 avg=3.17\n",
            "[156 | 309.37] loss=3.42 avg=3.17\n",
            "[157 | 311.10] loss=2.82 avg=3.17\n",
            "[158 | 312.83] loss=3.19 avg=3.17\n",
            "[159 | 314.56] loss=2.75 avg=3.17\n",
            "[160 | 316.30] loss=3.73 avg=3.17\n",
            "[161 | 318.03] loss=3.26 avg=3.17\n",
            "[162 | 319.76] loss=2.89 avg=3.17\n",
            "[163 | 321.49] loss=3.22 avg=3.17\n",
            "[164 | 323.22] loss=3.04 avg=3.17\n",
            "[165 | 324.95] loss=2.75 avg=3.16\n",
            "[166 | 326.68] loss=2.92 avg=3.16\n",
            "[167 | 328.41] loss=3.35 avg=3.16\n",
            "[168 | 330.15] loss=3.22 avg=3.16\n",
            "[169 | 331.87] loss=3.17 avg=3.16\n",
            "[170 | 333.61] loss=3.39 avg=3.17\n",
            "[171 | 335.34] loss=3.23 avg=3.17\n",
            "[172 | 337.09] loss=3.11 avg=3.17\n",
            "[173 | 338.82] loss=3.27 avg=3.17\n",
            "[174 | 340.55] loss=3.36 avg=3.17\n",
            "[175 | 342.29] loss=3.06 avg=3.17\n",
            "[176 | 344.02] loss=3.56 avg=3.17\n",
            "[177 | 345.77] loss=3.14 avg=3.17\n",
            "[178 | 347.50] loss=2.72 avg=3.17\n",
            "[179 | 349.24] loss=3.37 avg=3.17\n",
            "[180 | 350.99] loss=3.28 avg=3.17\n",
            "[181 | 352.74] loss=3.21 avg=3.17\n",
            "[182 | 354.47] loss=3.14 avg=3.17\n",
            "[183 | 356.20] loss=3.43 avg=3.17\n",
            "[184 | 357.94] loss=3.45 avg=3.18\n",
            "[185 | 359.67] loss=3.33 avg=3.18\n",
            "[186 | 361.41] loss=2.83 avg=3.18\n",
            "[187 | 363.14] loss=3.64 avg=3.18\n",
            "[188 | 364.87] loss=3.26 avg=3.18\n",
            "[189 | 366.63] loss=3.04 avg=3.18\n",
            "[190 | 368.36] loss=2.99 avg=3.18\n",
            "[191 | 370.10] loss=3.19 avg=3.18\n",
            "[192 | 371.83] loss=3.10 avg=3.18\n",
            "[193 | 373.56] loss=3.31 avg=3.18\n",
            "[194 | 375.30] loss=3.45 avg=3.18\n",
            "[195 | 377.03] loss=3.16 avg=3.18\n",
            "[196 | 378.76] loss=3.54 avg=3.19\n",
            "[197 | 380.49] loss=3.20 avg=3.19\n",
            "[198 | 382.22] loss=3.23 avg=3.19\n",
            "[199 | 383.96] loss=2.73 avg=3.18\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " it was called in the 1980s in the US to beat the Soviet invasion of Afghanistan. That ended a lot of horrible things. Now it's a disaster.\"\n",
            "\n",
            "\n",
            "Donald Trump\n",
            "\n",
            "On US President Barack Obama's decision not to retaliate for a Russian cyberattack against Hillary Clinton's Democratic presidential campaign:\n",
            "\n",
            "\"He knows who did it. A lot of people who know something about Russia. But he knows who did it. You don't tell people when you don't have something. And so people think he says he doesn't know.\"\n",
            "\n",
            "\n",
            "Donald Trump\n",
            "\n",
            "If you want to stop Hillary Clinton's campaign from winning in 2016, then Hillary Clinton should be running away from Trump\n",
            "\n",
            "\"If Hillary Clinton runs away from Trump, then she's just saying that's her goal and she never has.\"\n",
            "\n",
            "Donald Trump\n",
            "\n",
            "In an interview with The Washington Post, Donald Trump says Clinton's negative polls are \"unfortunate\" and points out she is \"definitely going down\"\n",
            "\n",
            "\"So many polls and this guy I think he knows, I think he knows a lot but he can't say exactly what I can do. But he can't say what she's gonna do either.\"\n",
            "\n",
            "Hillary Clinton\n",
            "\n",
            "Hillary Clinton is going away quickly on her presidential campaign - which is why she is not only losing but losing badly\n",
            "\n",
            "\"What does this mean for a campaign? She's out. She's gone. You're out. She's out. She's never gonna be President. She needs to be thrown out.\"\n",
            "\n",
            "\n",
            "Donald Trump<|endoftext|>The Obama administration is threatening to withhold $20 billion from China over human rights violations and \"gross human rights crimes\" in the South China Sea through a controversial agreement the U.S. is seeking to ratify with China.\n",
            "\n",
            "Trump has vowed to get tough with China if it can't improve its human rights record. Yet China may have a better case that the U.S. has a better case.\n",
            "\n",
            "\n",
            "The Obama administration is threatening to withhold $20 billion from China over human rights abuses and \"gross humanrights crimes\" in the South China Sea through a controversial agreement the U.S. is seeking to ratify with Beijing.\n",
            "\n",
            "The Obama administration has already withdrawn $13 billion from China over claims in the Spratly Islands and the Triton Reef in the Paracel Sea -- money it wanted to spend but has been repeatedly denied.\n",
            "\n",
            "The U.S. is threatening to cut off an additional $5 billion, and it has promised to do this one year sooner than the current agreement.\n",
            "\n",
            "The new agreement has nothing at all to do with China. It's simply part of what China has been asking all along: \"better\" on human rights.\n",
            "\n",
            "Just as in the early days of the trade war between the U.S. and China, China is seeking to ratify the deal with the U.S. as soon as possible.\n",
            "\n",
            "\n",
            "We've long known that U.S. demands on China are unacceptable. Now we know why we are being forced into one.\n",
            "\n",
            "China has repeatedly called President Trump's demands to improve its human rights record a \"red line\" that will cause the U.S. to drop its trade deals with the country. China claims the U.S. is trying to give China \"special treatment\" on trade matters.\n",
            "\n",
            "What the new trade agreement would do\n",
            "\n",
            "China insists that China will give itself \"special treatment\" on its trade agreements. China is also demanding that any trade deals be approved by the U.S.\n",
            "\n",
            "\n",
            "The Trump administration is insisting on the same conditions that got the U.S. into the trade war with China. But in both cases the deal is a complete joke:\n",
            "\n",
            "\n",
            "The Obama administration's demand from China to grant it \"special treatment\" on trade is also a red line for both the U.S. and China--a request that's being met by both men.\n",
            "\n",
            "\n",
            "Under the \"special treatment\" policy China's \"customs enforcement officials\" get as much as 15% of their orders directly from the U.S. in the form of lower duties. This has been approved by the Obama administration to go along with existing duties paid via China in the form of lower tariffs and tariff relief\n",
            "\n",
            "\n",
            "What would be achieved through the agreement\n",
            "\n",
            "A \"normalization of relations\" would benefit all involved both by easing trade barriers and giving China a chance to renegotiate for the better. China still has to agree (and enforce) to what the U.S. demands (including on human rights) is.\n",
            "\n",
            "One of the conditions a deal must be approved by Congress by January 15th is that any trade agreements, including those with China, can only take place in \"a normal manner.\" To the best of our knowledge all the negotiations taking place at the same time as the trade deal is totally illegal.\n",
            "\n",
            "\n",
            "As this deal is being negotiated between China and the U.S. it would not have been possible to achieve\n",
            "\n",
            "[200 | 407.85] loss=3.07 avg=3.18\n",
            "[201 | 409.57] loss=3.24 avg=3.18\n",
            "[202 | 411.30] loss=2.99 avg=3.18\n",
            "[203 | 413.03] loss=3.07 avg=3.18\n",
            "[204 | 414.76] loss=3.37 avg=3.18\n",
            "[205 | 416.49] loss=3.66 avg=3.19\n",
            "[206 | 418.22] loss=3.25 avg=3.19\n",
            "[207 | 419.96] loss=3.23 avg=3.19\n",
            "[208 | 421.69] loss=3.74 avg=3.19\n",
            "[209 | 423.40] loss=3.02 avg=3.19\n",
            "[210 | 425.14] loss=2.98 avg=3.19\n",
            "[211 | 426.87] loss=2.74 avg=3.18\n",
            "[212 | 428.58] loss=3.45 avg=3.19\n",
            "[213 | 430.31] loss=2.58 avg=3.18\n",
            "[214 | 432.03] loss=3.00 avg=3.18\n",
            "[215 | 433.76] loss=2.81 avg=3.17\n",
            "[216 | 435.49] loss=3.44 avg=3.18\n",
            "[217 | 437.21] loss=2.99 avg=3.17\n",
            "[218 | 438.95] loss=3.41 avg=3.18\n",
            "[219 | 440.68] loss=3.28 avg=3.18\n",
            "[220 | 442.42] loss=2.89 avg=3.17\n",
            "[221 | 444.15] loss=3.35 avg=3.18\n",
            "[222 | 445.91] loss=2.87 avg=3.17\n",
            "[223 | 447.64] loss=3.26 avg=3.17\n",
            "[224 | 449.37] loss=3.18 avg=3.17\n",
            "[225 | 451.12] loss=3.17 avg=3.17\n",
            "[226 | 452.87] loss=2.64 avg=3.17\n",
            "[227 | 454.62] loss=3.28 avg=3.17\n",
            "[228 | 456.35] loss=3.27 avg=3.17\n",
            "[229 | 458.10] loss=2.52 avg=3.16\n",
            "[230 | 459.85] loss=2.75 avg=3.16\n",
            "[231 | 461.61] loss=3.23 avg=3.16\n",
            "[232 | 463.36] loss=3.15 avg=3.16\n",
            "[233 | 465.11] loss=2.92 avg=3.16\n",
            "[234 | 466.86] loss=2.66 avg=3.15\n",
            "[235 | 468.61] loss=2.88 avg=3.15\n",
            "[236 | 470.37] loss=3.20 avg=3.15\n",
            "[237 | 472.12] loss=3.12 avg=3.15\n",
            "[238 | 473.85] loss=3.66 avg=3.15\n",
            "[239 | 475.58] loss=2.88 avg=3.15\n",
            "[240 | 477.32] loss=2.93 avg=3.15\n",
            "[241 | 479.07] loss=2.84 avg=3.15\n",
            "[242 | 480.80] loss=2.82 avg=3.14\n",
            "[243 | 482.53] loss=2.79 avg=3.14\n",
            "[244 | 484.26] loss=3.36 avg=3.14\n",
            "[245 | 485.99] loss=3.34 avg=3.14\n",
            "[246 | 487.72] loss=3.01 avg=3.14\n",
            "[247 | 489.46] loss=3.30 avg=3.14\n",
            "[248 | 491.21] loss=3.45 avg=3.15\n",
            "[249 | 492.94] loss=3.18 avg=3.15\n",
            "[250 | 494.69] loss=2.63 avg=3.14\n",
            "[251 | 496.42] loss=3.04 avg=3.14\n",
            "[252 | 498.16] loss=3.52 avg=3.14\n",
            "[253 | 499.89] loss=2.88 avg=3.14\n",
            "[254 | 501.65] loss=2.90 avg=3.14\n",
            "[255 | 503.38] loss=3.32 avg=3.14\n",
            "[256 | 505.11] loss=3.24 avg=3.14\n",
            "[257 | 506.84] loss=3.30 avg=3.14\n",
            "[258 | 508.58] loss=2.84 avg=3.14\n",
            "[259 | 510.32] loss=2.58 avg=3.13\n",
            "[260 | 512.05] loss=3.41 avg=3.14\n",
            "[261 | 513.78] loss=3.17 avg=3.14\n",
            "[262 | 515.52] loss=3.14 avg=3.14\n",
            "[263 | 517.25] loss=2.50 avg=3.13\n",
            "[264 | 518.98] loss=3.19 avg=3.13\n",
            "[265 | 520.71] loss=3.56 avg=3.14\n",
            "[266 | 522.44] loss=2.66 avg=3.13\n",
            "[267 | 524.17] loss=2.73 avg=3.13\n",
            "[268 | 525.90] loss=3.07 avg=3.13\n",
            "[269 | 527.63] loss=3.15 avg=3.13\n",
            "[270 | 529.36] loss=3.11 avg=3.13\n",
            "[271 | 531.11] loss=3.24 avg=3.13\n",
            "[272 | 532.84] loss=3.40 avg=3.13\n",
            "[273 | 534.59] loss=2.74 avg=3.13\n",
            "[274 | 536.32] loss=3.02 avg=3.12\n",
            "[275 | 538.05] loss=3.21 avg=3.13\n",
            "[276 | 539.78] loss=3.17 avg=3.13\n",
            "[277 | 541.51] loss=2.50 avg=3.12\n",
            "[278 | 543.24] loss=3.48 avg=3.12\n",
            "[279 | 544.97] loss=3.26 avg=3.12\n",
            "[280 | 546.70] loss=2.87 avg=3.12\n",
            "[281 | 548.43] loss=3.18 avg=3.12\n",
            "[282 | 550.16] loss=3.45 avg=3.13\n",
            "[283 | 551.88] loss=3.15 avg=3.13\n",
            "[284 | 553.62] loss=2.83 avg=3.12\n",
            "[285 | 555.35] loss=3.07 avg=3.12\n",
            "[286 | 557.08] loss=2.89 avg=3.12\n",
            "[287 | 558.81] loss=3.30 avg=3.12\n",
            "[288 | 560.55] loss=3.26 avg=3.12\n",
            "[289 | 562.28] loss=3.45 avg=3.13\n",
            "[290 | 564.01] loss=2.65 avg=3.12\n",
            "[291 | 565.74] loss=2.86 avg=3.12\n",
            "[292 | 567.48] loss=3.13 avg=3.12\n",
            "[293 | 569.21] loss=2.73 avg=3.12\n",
            "[294 | 570.95] loss=3.17 avg=3.12\n",
            "[295 | 572.69] loss=3.39 avg=3.12\n",
            "[296 | 574.42] loss=2.76 avg=3.12\n",
            "[297 | 576.16] loss=3.48 avg=3.12\n",
            "[298 | 577.89] loss=3.50 avg=3.12\n",
            "[299 | 579.62] loss=3.10 avg=3.12\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�’s’s campaign “to ”destroy ” Donald Trump's ”idea” by saying that ‘you’ve lost interest’s of your supporters’ by simply not taking your bullshit seriously.”\n",
            "#NeverTrump #MakeAmericaGreatAgain\n",
            "#NeverTrump\n",
            "@EricGewirtz  is making this up.\n",
            "@EricGewirtz   You are a smart guy.\n",
            "@mikelucales  Your idea is a shame.\n",
            "@dannymike  How many of you are out of work?\n",
            "@loudmicky  I'm an ex-Marine and I know the truth\n",
            "@miketowhite   Thanks.\n",
            "@carljones    I'm sure.\n",
            "I know many people will never join your party. But for those people who do join them must learn to treat people fairly. If you want people to join you in droves you must always act fair - even when it is inconvenient.\n",
            "Do Not Be a Dick. #NeverTrump @CarmenJGillespie@DougMacFarlane@JebBush@JohnPerry @BrentScowcroft\n",
            "Just when you thought we were done with Donald Trump.  In the final debate @greta was right again.  We're getting there.\n",
            "The #NeverTrump crowd has never understood that Donald Trump is not a very good communicator.\n",
            "They should do their own @MSNBC show like @dannypayne or @Greta if I were @GovWalker.  I'd have a lot better ideas.\n",
            "@BrentScowcroft  Do you know that I am one of the best at this?\n",
            "@LOUISBOBEYARD: @realDonaldTrump  He would be no good for the country. #Trump2014 #BTS #Trump2016\n",
            "The @PantsOnFire  @Laughtrack  Twitter account has a really cute joke.\n",
            "@carljones  This is a terrible guy for the job and a total loser.\n",
            "@KathyPeloski @KHOU7 News.\n",
            "@tweetedevs A good thing to do is to never do it.\n",
            "@tweetedevs    @KHOU7 News  has great news.\n",
            "@tweetedevs @KHOU7 News  is great.\n",
            "@jamesflynn   I believe @loudmicky. I don't think he has the guts to go and win.\n",
            "@michaelkantzer @MannyMakesTV\n",
            "@nprboyce   Thank you.\n",
            "@piersmorgan @Mancave   Trump is my #NoLimpinMan - he's got the guts.\n",
            "@HJTK    Thanks.\n",
            "@fawxinj  I'm a real estate tycoon and so am @piersmorgan and @cameron_chow on @CNBC.\n",
            "@tav_marsh It depends.\n",
            "@Greta  I can appreciate that.\n",
            "@nflynn     I don't want to talk to you.  I'm the one who should talk to.\n",
            "@jamesflynn     I don't want to. You're doing great.\n",
            "@brianwolczyska @Greta I'm sure I do.\n",
            "@Nathaniele_Barrett @Greta @McDonaldKris @dylanjmccool   @natthewcain  @Greta  @Mancave  @BrentScowcroft @BrentJames_Dennis1 @thegreatamerican1  @piersmorgan @loudmicky  I can appreciate your passion.\n",
            "@Greta  It depends.\n",
            "@Greta  .@michaelkantzer just wants to make sure @piersmorgan is there.\n",
            "@CameronB_Chow    I will never forget the guy.\n",
            "@TheGreatOne    This is a total waste of time.\n",
            "@Mancave  @Mancave @natthewcain @BrentJames_Dennis1  @piersmorgan @Loudmicky  My job is to win.\n",
            "@nidawitness   I have never met this guy.\n",
            "@Greta  I am not going to waste my time on this guy.\n",
            "#PantsOnFire  @piersmorgan @michaelkantzer I have a better plan.\n",
            ".@JamesFlynn    I would never think of doing that.\n",
            "@michaelkantzer I will be watching the @PantsOnFire  Twitter and other social accounts on all of today's programs.\n",
            "@piersmorgan @\n",
            "\n",
            "[300 | 603.40] loss=3.25 avg=3.12\n",
            "[301 | 605.15] loss=3.02 avg=3.12\n",
            "[302 | 606.88] loss=3.18 avg=3.12\n",
            "[303 | 608.61] loss=3.16 avg=3.12\n",
            "[304 | 610.34] loss=3.14 avg=3.12\n",
            "[305 | 612.07] loss=2.83 avg=3.12\n",
            "[306 | 613.80] loss=3.06 avg=3.12\n",
            "[307 | 615.54] loss=3.00 avg=3.12\n",
            "[308 | 617.27] loss=2.86 avg=3.12\n",
            "[309 | 619.02] loss=2.76 avg=3.11\n",
            "[310 | 620.77] loss=3.15 avg=3.11\n",
            "[311 | 622.50] loss=3.00 avg=3.11\n",
            "[312 | 624.23] loss=3.23 avg=3.11\n",
            "[313 | 625.96] loss=2.80 avg=3.11\n",
            "[314 | 627.72] loss=3.33 avg=3.11\n",
            "[315 | 629.45] loss=3.01 avg=3.11\n",
            "[316 | 631.18] loss=3.25 avg=3.11\n",
            "[317 | 632.92] loss=3.08 avg=3.11\n",
            "[318 | 634.64] loss=3.40 avg=3.12\n",
            "[319 | 636.38] loss=2.64 avg=3.11\n",
            "[320 | 638.11] loss=3.35 avg=3.11\n",
            "[321 | 639.84] loss=3.63 avg=3.12\n",
            "[322 | 641.58] loss=2.67 avg=3.11\n",
            "[323 | 643.31] loss=2.92 avg=3.11\n",
            "[324 | 645.05] loss=2.96 avg=3.11\n",
            "[325 | 646.78] loss=3.00 avg=3.11\n",
            "[326 | 648.52] loss=2.81 avg=3.11\n",
            "[327 | 650.25] loss=3.16 avg=3.11\n",
            "[328 | 651.98] loss=2.81 avg=3.10\n",
            "[329 | 653.71] loss=3.36 avg=3.11\n",
            "[330 | 655.44] loss=3.44 avg=3.11\n",
            "[331 | 657.17] loss=2.81 avg=3.11\n",
            "[332 | 658.90] loss=2.90 avg=3.10\n",
            "[333 | 660.66] loss=3.36 avg=3.11\n",
            "[334 | 662.39] loss=2.50 avg=3.10\n",
            "[335 | 664.12] loss=2.90 avg=3.10\n",
            "[336 | 665.85] loss=3.01 avg=3.10\n",
            "[337 | 667.59] loss=2.77 avg=3.09\n",
            "[338 | 669.31] loss=2.90 avg=3.09\n",
            "[339 | 671.05] loss=3.18 avg=3.09\n",
            "[340 | 672.79] loss=3.08 avg=3.09\n",
            "[341 | 674.54] loss=3.37 avg=3.10\n",
            "[342 | 676.29] loss=3.00 avg=3.09\n",
            "[343 | 678.05] loss=3.18 avg=3.10\n",
            "[344 | 679.79] loss=2.74 avg=3.09\n",
            "[345 | 681.52] loss=3.04 avg=3.09\n",
            "[346 | 683.26] loss=3.21 avg=3.09\n",
            "[347 | 684.99] loss=3.37 avg=3.10\n",
            "[348 | 686.72] loss=3.22 avg=3.10\n",
            "[349 | 688.45] loss=3.04 avg=3.10\n",
            "[350 | 690.18] loss=3.12 avg=3.10\n",
            "[351 | 691.91] loss=3.00 avg=3.10\n",
            "[352 | 693.65] loss=2.96 avg=3.09\n",
            "[353 | 695.40] loss=3.48 avg=3.10\n",
            "[354 | 697.13] loss=3.24 avg=3.10\n",
            "[355 | 698.89] loss=3.36 avg=3.10\n",
            "[356 | 700.62] loss=3.05 avg=3.10\n",
            "[357 | 702.35] loss=3.37 avg=3.10\n",
            "[358 | 704.09] loss=3.56 avg=3.11\n",
            "[359 | 705.84] loss=3.19 avg=3.11\n",
            "[360 | 707.57] loss=3.07 avg=3.11\n",
            "[361 | 709.31] loss=2.93 avg=3.11\n",
            "[362 | 711.06] loss=3.18 avg=3.11\n",
            "[363 | 712.80] loss=3.15 avg=3.11\n",
            "[364 | 714.54] loss=3.34 avg=3.11\n",
            "[365 | 716.27] loss=3.13 avg=3.11\n",
            "[366 | 718.01] loss=2.62 avg=3.11\n",
            "[367 | 719.73] loss=3.00 avg=3.10\n",
            "[368 | 721.49] loss=2.84 avg=3.10\n",
            "[369 | 723.24] loss=3.30 avg=3.10\n",
            "[370 | 724.97] loss=2.92 avg=3.10\n",
            "[371 | 726.70] loss=3.47 avg=3.11\n",
            "[372 | 728.46] loss=2.92 avg=3.10\n",
            "[373 | 730.18] loss=2.58 avg=3.10\n",
            "[374 | 731.93] loss=2.27 avg=3.09\n",
            "[375 | 733.67] loss=3.11 avg=3.09\n",
            "[376 | 735.39] loss=2.92 avg=3.09\n",
            "[377 | 737.13] loss=2.73 avg=3.09\n",
            "[378 | 738.86] loss=2.74 avg=3.08\n",
            "[379 | 740.61] loss=2.85 avg=3.08\n",
            "[380 | 742.36] loss=3.12 avg=3.08\n",
            "[381 | 744.12] loss=3.34 avg=3.08\n",
            "[382 | 745.87] loss=3.06 avg=3.08\n",
            "[383 | 747.62] loss=3.17 avg=3.08\n",
            "[384 | 749.37] loss=2.75 avg=3.08\n",
            "[385 | 751.11] loss=3.16 avg=3.08\n",
            "[386 | 752.85] loss=3.27 avg=3.08\n",
            "[387 | 754.60] loss=3.38 avg=3.09\n",
            "[388 | 756.33] loss=2.99 avg=3.08\n",
            "[389 | 758.09] loss=3.24 avg=3.09\n",
            "[390 | 759.83] loss=3.10 avg=3.09\n",
            "[391 | 761.58] loss=2.84 avg=3.08\n",
            "[392 | 763.32] loss=2.70 avg=3.08\n",
            "[393 | 765.05] loss=3.03 avg=3.08\n",
            "[394 | 766.78] loss=3.00 avg=3.08\n",
            "[395 | 768.53] loss=2.91 avg=3.08\n",
            "[396 | 770.27] loss=3.45 avg=3.08\n",
            "[397 | 772.00] loss=3.44 avg=3.08\n",
            "[398 | 773.74] loss=2.86 avg=3.08\n",
            "[399 | 775.49] loss=2.77 avg=3.08\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "king of the US\" - \"the U.S. is now paying hundreds of billions of dollars to Iraq and Syria for the fight against ISIS [Al-Qaeda and its assorted cousins]\" https://t.co/0DQzLfj2lh\n",
            "If you think Trump is a friend to your community you are a liar - https://t.co/lVqY1K4v7x . @kcraigcalm  I just won!\n",
            "We really have to start treating people fairly! https://t.co/8JQc4WKcwH\n",
            "It was a horrible day when it happened but I'm just happy its over! #HappyVeteranDay\n",
            "Good afternoon! https://t.co/j7x3VlW5D4\n",
            "I just won the #1 ranking in the USA! https://t.co/W8W2T6gKt0\n",
            "We are looking into why no one at the White House ever told us about the many deaths of our brave veterans. https://t.co/kVcHVrX6E\n",
            "Great rally in West Virginia tonight. Thank you. https://t.co/uTqhxkL6mO\n",
            "Trump Tower is now the tallest building in Manhattan. Great job!\n",
            "Congratulations to @IvankaTrump on being named one of Time Magazine's 50 most beautiful women of the year! https://t.co/pOiKHfUmVx\n",
            "Good luck to @DHSgov as we prepare to open an 800 mile expansion to connect the new @USAToport and VA in Houston! https://t.co/JtJ5VnFQzW\n",
            "Congratulations to @IvankaTrump on being named one of Time Magazine's 50 most beautiful women of the year! https://t.co/pOiKHfUmVx\n",
            "#CelebApprentice returns tomorrow at 8pm ET on SyFy!\n",
            "As an American citizen and member of the Armed Forces I stand before you today because you have shown the world that you are one of us. America will never ever be stopped. We WILL NEVER BE OWEED! #CelebApprentice\n",
            "I will not be traveling to Dallas tonight but will be visiting San Antonio at some point tomorrow. Good luck!\n",
            "https://t.co/Z6qQ6jYd1a\n",
            "It was a great day yesterday but a great day for @IvankaTrump and the #CelebApprentice @SyFy! #CelebApprenticehttps://t.co/f9t7u9yC6t\n",
            "Today is an honor day - but the United States will never be stopped by fear &amp; intimidation. https://t.co/xW8nSfY7Y6\n",
            "The great news about the @DallasGOP is that @DonaldJTrumpJr. showed courage and passion to work @NRA members and supporters for the first in their lives! https://t.co/7aQe1b8Z9S\n",
            "I just w/ the @DallasGOP are doing a great job in getting new gun owners &amp; safety at the convention on May 4th. https://t.co/1CXR4yXJtX\n",
            "I am honored to have been chosen #CelebApprentice. Thank you for your support! #DallasGOP https://t.co/ejD8aXzVnA\n",
            "I just watched the #DallasGOP on @SyFy. https://t.co/1aXvVX4Q7c\n",
            "I am honored that Donald Trump Jr. and the DallasGOP will be meeting to ensure good safety &amp; safety for all shooting victims. #DallasGOP https://t.co/5gxV7b1F4v\n",
            "I have just watched the #DallasGOP on @SyFy. https://t.co/wFwEQpqh7C\n",
            "The DallasGOP is really important to us. We will be showing up tomorrow to see that we are safe from the dangers of shooting victims and getting better safety! https://t.co/qWqQ9V6Fv3\n",
            "We just met with @NRA in Houston TX. #DallasGOP A great meeting. Great people &amp; great opportunities to work together. https://t.co/f8VfMn5u0A\n",
            "I just met with the DallasGOP in Houston TX. Thank you in advance! #DallasGOPhttps://t.co/m5hYqFQ8gA\n",
            "The DallasGOP is making life safer for the people. Many were shot during the shooting yesterday in Dallas. We will be showing up to ensure that safety! https://t.co/dRiY9fXpWU\n",
            "Just saw the Dallas\n",
            "\n",
            "[400 | 799.33] loss=2.81 avg=3.08\n",
            "[401 | 801.04] loss=3.42 avg=3.08\n",
            "[402 | 802.77] loss=3.40 avg=3.08\n",
            "[403 | 804.48] loss=2.66 avg=3.08\n",
            "[404 | 806.19] loss=3.00 avg=3.08\n",
            "[405 | 807.92] loss=2.90 avg=3.08\n",
            "[406 | 809.65] loss=3.16 avg=3.08\n",
            "[407 | 811.36] loss=3.00 avg=3.08\n",
            "[408 | 813.09] loss=3.21 avg=3.08\n",
            "[409 | 814.82] loss=3.63 avg=3.08\n",
            "[410 | 816.55] loss=3.40 avg=3.09\n",
            "[411 | 818.28] loss=3.14 avg=3.09\n",
            "[412 | 820.01] loss=2.62 avg=3.08\n",
            "[413 | 821.74] loss=2.92 avg=3.08\n",
            "[414 | 823.47] loss=2.87 avg=3.08\n",
            "[415 | 825.22] loss=2.81 avg=3.08\n",
            "[416 | 826.97] loss=2.99 avg=3.07\n",
            "[417 | 828.71] loss=2.76 avg=3.07\n",
            "[418 | 830.44] loss=3.37 avg=3.07\n",
            "[419 | 832.17] loss=3.13 avg=3.07\n",
            "[420 | 833.91] loss=2.79 avg=3.07\n",
            "[421 | 835.64] loss=3.23 avg=3.07\n",
            "[422 | 837.38] loss=2.77 avg=3.07\n",
            "[423 | 839.11] loss=2.94 avg=3.07\n",
            "[424 | 840.85] loss=2.99 avg=3.07\n",
            "[425 | 842.60] loss=2.63 avg=3.06\n",
            "[426 | 844.35] loss=2.82 avg=3.06\n",
            "[427 | 846.10] loss=3.24 avg=3.06\n",
            "[428 | 847.83] loss=2.73 avg=3.06\n",
            "[429 | 849.57] loss=3.23 avg=3.06\n",
            "[430 | 851.30] loss=2.93 avg=3.06\n",
            "[431 | 853.05] loss=2.72 avg=3.06\n",
            "[432 | 854.80] loss=3.37 avg=3.06\n",
            "[433 | 856.55] loss=2.88 avg=3.06\n",
            "[434 | 858.30] loss=3.00 avg=3.06\n",
            "[435 | 860.03] loss=3.02 avg=3.06\n",
            "[436 | 861.78] loss=3.00 avg=3.06\n",
            "[437 | 863.51] loss=3.09 avg=3.06\n",
            "[438 | 865.26] loss=3.20 avg=3.06\n",
            "[439 | 867.01] loss=2.74 avg=3.05\n",
            "[440 | 868.74] loss=3.12 avg=3.06\n",
            "[441 | 870.47] loss=3.07 avg=3.06\n",
            "[442 | 872.20] loss=3.03 avg=3.06\n",
            "[443 | 873.93] loss=3.20 avg=3.06\n",
            "[444 | 875.66] loss=2.97 avg=3.06\n",
            "[445 | 877.40] loss=3.07 avg=3.06\n",
            "[446 | 879.13] loss=3.21 avg=3.06\n",
            "[447 | 880.86] loss=3.21 avg=3.06\n",
            "[448 | 882.59] loss=3.30 avg=3.06\n",
            "[449 | 884.32] loss=2.73 avg=3.06\n",
            "[450 | 886.05] loss=3.12 avg=3.06\n",
            "[451 | 887.78] loss=2.84 avg=3.06\n",
            "[452 | 889.51] loss=2.73 avg=3.05\n",
            "[453 | 891.24] loss=2.80 avg=3.05\n",
            "[454 | 892.97] loss=2.99 avg=3.05\n",
            "[455 | 894.70] loss=3.39 avg=3.05\n",
            "[456 | 896.44] loss=2.04 avg=3.04\n",
            "[457 | 898.18] loss=2.68 avg=3.04\n",
            "[458 | 899.94] loss=2.66 avg=3.04\n",
            "[459 | 901.67] loss=2.74 avg=3.03\n",
            "[460 | 903.42] loss=3.11 avg=3.03\n",
            "[461 | 905.15] loss=2.94 avg=3.03\n",
            "[462 | 906.89] loss=2.91 avg=3.03\n",
            "[463 | 908.62] loss=2.87 avg=3.03\n",
            "[464 | 910.35] loss=3.26 avg=3.03\n",
            "[465 | 912.09] loss=2.76 avg=3.03\n",
            "[466 | 913.83] loss=3.27 avg=3.03\n",
            "[467 | 915.56] loss=2.77 avg=3.03\n",
            "[468 | 917.30] loss=3.32 avg=3.03\n",
            "[469 | 919.03] loss=2.95 avg=3.03\n",
            "[470 | 920.76] loss=3.15 avg=3.03\n",
            "[471 | 922.50] loss=3.44 avg=3.04\n",
            "[472 | 924.22] loss=3.07 avg=3.04\n",
            "[473 | 925.96] loss=2.76 avg=3.03\n",
            "[474 | 927.69] loss=2.67 avg=3.03\n",
            "[475 | 929.42] loss=3.21 avg=3.03\n",
            "[476 | 931.15] loss=3.49 avg=3.04\n",
            "[477 | 932.88] loss=3.48 avg=3.04\n",
            "[478 | 934.61] loss=2.61 avg=3.04\n",
            "[479 | 936.34] loss=2.94 avg=3.04\n",
            "[480 | 938.08] loss=2.97 avg=3.04\n",
            "[481 | 939.80] loss=3.14 avg=3.04\n",
            "[482 | 941.54] loss=2.85 avg=3.03\n",
            "[483 | 943.29] loss=3.03 avg=3.03\n",
            "[484 | 945.01] loss=3.50 avg=3.04\n",
            "[485 | 946.75] loss=2.94 avg=3.04\n",
            "[486 | 948.48] loss=2.95 avg=3.04\n",
            "[487 | 950.21] loss=3.28 avg=3.04\n",
            "[488 | 951.93] loss=2.61 avg=3.04\n",
            "[489 | 953.67] loss=3.34 avg=3.04\n",
            "[490 | 955.39] loss=2.83 avg=3.04\n",
            "[491 | 957.12] loss=2.81 avg=3.03\n",
            "[492 | 958.85] loss=2.51 avg=3.03\n",
            "[493 | 960.58] loss=3.00 avg=3.03\n",
            "[494 | 962.31] loss=2.75 avg=3.03\n",
            "[495 | 964.04] loss=3.07 avg=3.03\n",
            "[496 | 965.78] loss=3.00 avg=3.03\n",
            "[497 | 967.51] loss=3.14 avg=3.03\n",
            "[498 | 969.24] loss=2.88 avg=3.03\n",
            "[499 | 970.97] loss=2.92 avg=3.02\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "co@fbi.gov\n",
            "Happy New Year everyone!” “@HuckabeeNBC”   I will not be missed! We will fix the #1 failing law enforcement agency and we will win!\n",
            "Today is the 12th anniversary of the Sandy Hook School Shooting in Newtown, CT...\n",
            "Thank you to the families of my wife @MelaniaTrump and children @MELANIATRUMP.\n",
            "“The best is yet to come.” – Don Draper\n",
            "When I see a person who is strong enough to win I congratulate them. When we see bad people we don't want them.\n",
            "Thank you to @FLORIDAFIVE for your continued love and support.#FloridaFITN\n",
            "“Don’t be surprised if @FoxNews does so again during the presidential cycle.”  @DanScavino   #FLGovGaryJohnson https://t.co/8eBXWOZgV7\n",
            "I am going to have a serious conversation about gun legislation @10News on @foxandfriends on Saturday morning. Thanks!!!\n",
            ".@MELANIATRUMP @realDonaldTrump The only man who has said it, Hillary Clinton... https://t.co/X6XqZHpY8f\n",
            "I am not a fan of Hillary who is too smart and doesn’t have brains but is she a woman on the world stage.\n",
            "@WendyMcMullin@ChrisCuomo  @ChrisChristie  Good talking!\n",
            ".@ChrisChristie is a great man. Thanks!\n",
            "If Hillary doesn’t do well in the upcoming NH primary I will be voting for Green Party candidate Jill Stein.\n",
            "It’s a shame because I will be voting for Trump!\n",
            ".@BernieSanders campaign is doing well in NH. New Hampshire just closed. We have to keep winin in NJ.\n",
            ".@PeteSwindell’s @CNN commentary is terrible. You only win if you are right.\n",
            ".@ChrisChristie is a big supporter of @GovChristie. He is smart and has great heart.\n",
            "Congratulations to @GOP on its victory. They have put all of their energy into bringing the great job numbers this year.\n",
            "Congratulations! #MakeAmericaGreatAgain #Trump2016 #MakeAmericaGreatAgain2016 https://t.co/c4zCKvD6nJ\n",
            "Why is Chris Hayes always on the Trump train, even though he has nothing but problems &amp; failed policies?\n",
            "My campaign will be the best in history. Our economy is on the comeback &amp; our vets are proud and so are all of our citizens. I will bring us back!\n",
            "Thank you @HuckabeeNBC and @foxandfriends. I will be interviewed on @midweek with @MELANIATRUMP  I will be on the show from 3 PM to 5 PM!\n",
            "I will be speaking at the #MakeAmericaGreatAgain Trump Victory in NJ. Thank you for supporting me! We will bring our jobs back!\n",
            "#TrumpNewJersey will attract over 3500 construction jobs and thousands more retail and hotel jobs https://t.co/BZ9pGtYd7K\n",
            "Hillary Clinton’s approval rating has gone from 43% to 49% in just eight days. If she loses we will have a new president in 4. We are doing really poorly!\n",
            "My speech at the #MakeAmericaGreatAgain #Trump2016 #Trump2016 on the campaign trail &amp; in the Oval Office. https://t.co/y6sN5vBkNX #MAGA\n",
            "I have been a great supporter of @MELANIATRUMP for many years. She should be justly remembered as being a true advocate for our military &amp; our great people!\n",
            "Thank you to my wonderful children &amp; family @CJReynolds and @DrewForest (also great!).#Trump2016 https://t.co/aYzQ2GX2Tp\n",
            "Trump is now saying that he would be the anti-Hillary Clinton! So will it be the Anti-Hillary Clinton? https://t.co/dvS1qNnCqz\n",
            "How will Clinton and Bush handle ISIS? https://t.co/9yQyqhP8Lq\n",
            "Thanks @MELANIATRUMP. We are now going to be the biggest economy in the world! And we are doing so at a record pace!\n",
            "Our great people have come together in such a way that we are now doing so much better than we expected.\n",
            "Thank you @FoxNews for the endorsement. You are doing very well! I am thinking about doing the same on Monday Night Football!\n",
            "Thank you @CNN.’s great ratings are a great sign. We need to be smart and smart fast! I am going\n",
            "\n",
            "[500 | 994.98] loss=2.84 avg=3.02\n",
            "[501 | 996.73] loss=3.45 avg=3.03\n",
            "[502 | 998.48] loss=3.47 avg=3.03\n",
            "[503 | 1000.23] loss=3.30 avg=3.03\n",
            "[504 | 1001.96] loss=2.68 avg=3.03\n",
            "[505 | 1003.71] loss=2.80 avg=3.03\n",
            "[506 | 1005.46] loss=3.09 avg=3.03\n",
            "[507 | 1007.19] loss=3.06 avg=3.03\n",
            "[508 | 1008.92] loss=3.18 avg=3.03\n",
            "[509 | 1010.65] loss=2.97 avg=3.03\n",
            "[510 | 1012.38] loss=3.19 avg=3.03\n",
            "[511 | 1014.13] loss=3.14 avg=3.03\n",
            "[512 | 1015.86] loss=3.15 avg=3.03\n",
            "[513 | 1017.59] loss=2.69 avg=3.03\n",
            "[514 | 1019.32] loss=2.49 avg=3.03\n",
            "[515 | 1021.06] loss=2.55 avg=3.02\n",
            "[516 | 1022.78] loss=2.97 avg=3.02\n",
            "[517 | 1024.52] loss=3.32 avg=3.02\n",
            "[518 | 1026.25] loss=3.20 avg=3.02\n",
            "[519 | 1027.98] loss=3.13 avg=3.03\n",
            "[520 | 1029.71] loss=2.85 avg=3.02\n",
            "[521 | 1031.45] loss=3.34 avg=3.03\n",
            "[522 | 1033.18] loss=3.06 avg=3.03\n",
            "[523 | 1034.91] loss=3.48 avg=3.03\n",
            "[524 | 1036.64] loss=2.96 avg=3.03\n",
            "[525 | 1038.38] loss=2.66 avg=3.03\n",
            "[526 | 1040.11] loss=2.73 avg=3.02\n",
            "[527 | 1041.84] loss=2.86 avg=3.02\n",
            "[528 | 1043.58] loss=2.67 avg=3.02\n",
            "[529 | 1045.31] loss=2.79 avg=3.02\n",
            "[530 | 1047.04] loss=3.11 avg=3.02\n",
            "[531 | 1048.76] loss=2.54 avg=3.01\n",
            "[532 | 1050.49] loss=3.03 avg=3.01\n",
            "[533 | 1052.22] loss=3.07 avg=3.01\n",
            "[534 | 1053.94] loss=3.20 avg=3.02\n",
            "[535 | 1055.68] loss=2.50 avg=3.01\n",
            "[536 | 1057.41] loss=3.24 avg=3.01\n",
            "[537 | 1059.14] loss=3.29 avg=3.02\n",
            "[538 | 1060.87] loss=3.45 avg=3.02\n",
            "[539 | 1062.60] loss=2.97 avg=3.02\n",
            "[540 | 1064.33] loss=2.67 avg=3.02\n",
            "[541 | 1066.07] loss=3.18 avg=3.02\n",
            "[542 | 1067.80] loss=2.40 avg=3.01\n",
            "[543 | 1069.53] loss=3.08 avg=3.01\n",
            "[544 | 1071.26] loss=3.07 avg=3.01\n",
            "[545 | 1072.99] loss=3.25 avg=3.02\n",
            "[546 | 1074.72] loss=2.88 avg=3.01\n",
            "[547 | 1076.45] loss=3.34 avg=3.02\n",
            "[548 | 1078.19] loss=2.58 avg=3.01\n",
            "[549 | 1079.92] loss=3.18 avg=3.01\n",
            "[550 | 1081.65] loss=3.10 avg=3.02\n",
            "[551 | 1083.37] loss=3.15 avg=3.02\n",
            "[552 | 1085.10] loss=2.77 avg=3.01\n",
            "[553 | 1086.84] loss=2.92 avg=3.01\n",
            "[554 | 1088.57] loss=2.91 avg=3.01\n",
            "[555 | 1090.30] loss=2.74 avg=3.01\n",
            "[556 | 1092.03] loss=3.19 avg=3.01\n",
            "[557 | 1093.76] loss=2.98 avg=3.01\n",
            "[558 | 1095.50] loss=2.84 avg=3.01\n",
            "[559 | 1097.23] loss=2.64 avg=3.01\n",
            "[560 | 1098.96] loss=3.12 avg=3.01\n",
            "[561 | 1100.69] loss=2.84 avg=3.00\n",
            "[562 | 1102.42] loss=3.11 avg=3.01\n",
            "[563 | 1104.16] loss=3.12 avg=3.01\n",
            "[564 | 1105.89] loss=3.03 avg=3.01\n",
            "[565 | 1107.62] loss=2.89 avg=3.01\n",
            "[566 | 1109.35] loss=3.26 avg=3.01\n",
            "[567 | 1111.09] loss=2.78 avg=3.01\n",
            "[568 | 1112.82] loss=2.95 avg=3.01\n",
            "[569 | 1114.56] loss=2.99 avg=3.01\n",
            "[570 | 1116.29] loss=2.91 avg=3.00\n",
            "[571 | 1118.03] loss=2.85 avg=3.00\n",
            "[572 | 1119.76] loss=3.34 avg=3.01\n",
            "[573 | 1121.50] loss=2.70 avg=3.00\n",
            "[574 | 1123.23] loss=3.18 avg=3.01\n",
            "[575 | 1124.96] loss=2.40 avg=3.00\n",
            "[576 | 1126.71] loss=3.16 avg=3.00\n",
            "[577 | 1128.46] loss=2.70 avg=3.00\n",
            "[578 | 1130.19] loss=2.65 avg=2.99\n",
            "[579 | 1131.92] loss=2.58 avg=2.99\n",
            "[580 | 1133.65] loss=2.53 avg=2.99\n",
            "[581 | 1135.38] loss=2.91 avg=2.98\n",
            "[582 | 1137.11] loss=3.10 avg=2.99\n",
            "[583 | 1138.85] loss=2.83 avg=2.98\n",
            "[584 | 1140.58] loss=2.69 avg=2.98\n",
            "[585 | 1142.31] loss=3.44 avg=2.99\n",
            "[586 | 1144.04] loss=2.66 avg=2.98\n",
            "[587 | 1145.77] loss=2.63 avg=2.98\n",
            "[588 | 1147.50] loss=3.07 avg=2.98\n",
            "[589 | 1149.23] loss=2.93 avg=2.98\n",
            "[590 | 1150.97] loss=2.57 avg=2.98\n",
            "[591 | 1152.70] loss=2.52 avg=2.97\n",
            "[592 | 1154.43] loss=2.87 avg=2.97\n",
            "[593 | 1156.16] loss=2.44 avg=2.96\n",
            "[594 | 1157.89] loss=2.95 avg=2.96\n",
            "[595 | 1159.62] loss=3.30 avg=2.97\n",
            "[596 | 1161.35] loss=2.61 avg=2.96\n",
            "[597 | 1163.08] loss=2.92 avg=2.96\n",
            "[598 | 1164.84] loss=3.49 avg=2.97\n",
            "[599 | 1166.57] loss=3.03 avg=2.97\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " very smart as it turns out.\n",
            "Thank you @davidpatel!\n",
            "I will be interviewed on @CNN  at 9pm ET at 8 AM.\n",
            "The reason I am winning in 2016 is that I have people who are doing one thing and doing it very well-- and that is getting the job done!\n",
            "@JimmieFerguson Thank you. #MAGA\n",
            "I am very proud of @JimmieFerguson and believe him when he says he has always believed in him (and he's right).\n",
            "If you are a big time political donor don't get mad or complain. Keep fighting!\n",
            "@paulbarned #MAGA\n",
            "I am also hearing that a lot of folks are saying I have a great opportunity to get the highest number of delegates since Reagan.\n",
            "The polls are right over a 100% win for Trump. So what? I won't be president.\n",
            "What I would do now if elected would be to find a way to get the Republicans the numbers so they have the votes to win-- I mean super delegate. https://t.co/Xo9Tqx2z7Q\n",
            "@HazelCadie  @davidpatel  Good job\n",
            "@joeymcdoom @davidpatel  Thank you Joe.\n",
            ".@Jerry_McDoom  Thanks Dave.\n",
            ".@bobbynorris  Good job on @foxandfriends.\n",
            "I think the election is a major blowout but I am not sure yet.\n",
            "After many years I am proud to be associated with @TheRealBenSasse @FoxNews #VoteTrump #VoteTrump2016\n",
            "This is one of the biggest stories in politics &amp; media. It will be a big win for conservatives!\n",
            "@NataliePatel Thanks for the nice comment @JimmieFerguson\n",
            "I am very proud of @JimmieFerguson and believe him when he says he has always believed in him. https://t.co/ZVxo3FgF7H\n",
            "We are all watching CNN/Fox now. What will everyone else see this Friday? #Trump2016\n",
            ".@davidpatel  I am.  https://t.co/xHgTJ3y8Cc\n",
            ".@David_Patel I am.  https://t.co/6nQi3X5CfC\n",
            ".@piersmorgan  I am. #MakeAmericaGreatAgain #Trump2016\n",
            ".@piersmorgan  Thanks! #MakeAmericaGreatAgain\n",
            "Just found out that a major media outlet, CNN , is now saying that I have a BIG win. I do!\n",
            ".@hughhughhowell  Who's the boss? You have no authority to make the decision.\n",
            ".@foxandfriends  I am. #MAGA https://t.co/W3Cg9KrKqW\n",
            ".@FoxBusiness  Good morning David.  #MakeAmericaGreatAgain #Trump2016\n",
            "How could my poll numbers be that bad with so much fake news on CNN. You are being completely dishonest!\n",
            ".@mike-jerry  Thank you! #MAGA https://t.co/Xo9Tqx2z7Q\n",
            "Just got back from Michigan where we had a big win. We had some very big wins. @Trump_Racetrack #Michigan2Trump2016\n",
            "Just landed in Detroit, Michigan just had another fantastic crowd of thousands.\n",
            "I will be on @FoxNewsSunday talking the Michigan primary results and the debate with Jeb! #MakeAmericaGreatAgain #Trump2016\n",
            ".@BillKristol  Thank you Bill. #trump2016 #MakeAmericaGreatAgain #Trump2016\n",
            "If #CrookedHillary won the debate I would've never been able to win the #GOPDebate #MakeAmericaGreatAgain #Trump2016\n",
            "The media is failing miserably at #CrookedHillary. She lost the debate. She should NEVER have won it!\n",
            "I will be on @meetthepress Sunday at 7 &amp; 10 A.M. with @MariaBartiromo. #MakeAmericaGreatAgain #Trump2016\n",
            "Crooked Hillary was unable to beat Marco Rubio. She is totally weak and ineffective.\n",
            "Hillary lied when she said that I won the last debate. In fact I easily could of been third on stage against a very smart person. #MakeAmericaGreatAgain\n",
            "Crooked Hillary will need a big win over Bernie Sanders in the primaries as well as Super Tuesday in order to win. #MakeAmericaGreatAgain #Trump2016\n",
            "Ted Cruz is very smart but I am just far more talented. I like to beat people with smart people.\n",
            "Thank you to @EricTrump for the great endorsement! #MakeAmericaGreatAgain #Trump2016 https://t.co/4pYU6xKwE1\n",
            "\"My motto is \"Keep the Promise\n",
            "\n",
            "[600 | 1190.33] loss=3.34 avg=2.97\n",
            "[601 | 1192.09] loss=3.17 avg=2.98\n",
            "[602 | 1193.82] loss=3.19 avg=2.98\n",
            "[603 | 1195.55] loss=3.31 avg=2.98\n",
            "[604 | 1197.28] loss=3.01 avg=2.98\n",
            "[605 | 1199.01] loss=3.27 avg=2.98\n",
            "[606 | 1200.74] loss=3.04 avg=2.98\n",
            "[607 | 1202.47] loss=2.94 avg=2.98\n",
            "[608 | 1204.20] loss=3.34 avg=2.99\n",
            "[609 | 1205.93] loss=3.15 avg=2.99\n",
            "[610 | 1207.66] loss=2.79 avg=2.99\n",
            "[611 | 1209.40] loss=3.08 avg=2.99\n",
            "[612 | 1211.13] loss=3.04 avg=2.99\n",
            "[613 | 1212.88] loss=2.61 avg=2.99\n",
            "[614 | 1214.61] loss=3.07 avg=2.99\n",
            "[615 | 1216.34] loss=2.70 avg=2.98\n",
            "[616 | 1218.07] loss=3.18 avg=2.99\n",
            "[617 | 1219.80] loss=2.72 avg=2.98\n",
            "[618 | 1221.52] loss=3.21 avg=2.98\n",
            "[619 | 1223.26] loss=3.19 avg=2.99\n",
            "[620 | 1224.99] loss=3.30 avg=2.99\n",
            "[621 | 1226.72] loss=2.54 avg=2.99\n",
            "[622 | 1228.45] loss=2.75 avg=2.98\n",
            "[623 | 1230.18] loss=2.81 avg=2.98\n",
            "[624 | 1231.91] loss=2.98 avg=2.98\n",
            "[625 | 1233.64] loss=2.57 avg=2.98\n",
            "[626 | 1235.37] loss=3.03 avg=2.98\n",
            "[627 | 1237.11] loss=2.88 avg=2.98\n",
            "[628 | 1238.85] loss=3.04 avg=2.98\n",
            "[629 | 1240.57] loss=3.27 avg=2.98\n",
            "[630 | 1242.30] loss=3.20 avg=2.98\n",
            "[631 | 1244.04] loss=2.57 avg=2.98\n",
            "[632 | 1245.78] loss=3.14 avg=2.98\n",
            "[633 | 1247.51] loss=3.29 avg=2.98\n",
            "[634 | 1249.25] loss=3.52 avg=2.99\n",
            "[635 | 1250.98] loss=3.31 avg=2.99\n",
            "[636 | 1252.71] loss=2.76 avg=2.99\n",
            "[637 | 1254.45] loss=3.12 avg=2.99\n",
            "[638 | 1256.18] loss=2.45 avg=2.99\n",
            "[639 | 1257.91] loss=2.81 avg=2.98\n",
            "[640 | 1259.65] loss=2.74 avg=2.98\n",
            "[641 | 1261.38] loss=3.08 avg=2.98\n",
            "[642 | 1263.11] loss=2.77 avg=2.98\n",
            "[643 | 1264.84] loss=2.56 avg=2.98\n",
            "[644 | 1266.57] loss=2.83 avg=2.97\n",
            "[645 | 1268.30] loss=3.01 avg=2.97\n",
            "[646 | 1270.03] loss=3.14 avg=2.98\n",
            "[647 | 1271.76] loss=3.38 avg=2.98\n",
            "[648 | 1273.49] loss=3.13 avg=2.98\n",
            "[649 | 1275.22] loss=2.75 avg=2.98\n",
            "[650 | 1276.95] loss=3.09 avg=2.98\n",
            "[651 | 1278.68] loss=2.91 avg=2.98\n",
            "[652 | 1280.41] loss=2.68 avg=2.98\n",
            "[653 | 1282.15] loss=3.07 avg=2.98\n",
            "[654 | 1283.88] loss=3.21 avg=2.98\n",
            "[655 | 1285.61] loss=3.22 avg=2.98\n",
            "[656 | 1287.34] loss=2.98 avg=2.98\n",
            "[657 | 1289.07] loss=2.89 avg=2.98\n",
            "[658 | 1290.80] loss=2.55 avg=2.98\n",
            "[659 | 1292.53] loss=2.65 avg=2.97\n",
            "[660 | 1294.26] loss=3.14 avg=2.98\n",
            "[661 | 1295.99] loss=3.11 avg=2.98\n",
            "[662 | 1297.73] loss=2.72 avg=2.97\n",
            "[663 | 1299.46] loss=2.57 avg=2.97\n",
            "[664 | 1301.19] loss=2.73 avg=2.97\n",
            "[665 | 1302.92] loss=2.77 avg=2.97\n",
            "[666 | 1304.65] loss=2.86 avg=2.96\n",
            "[667 | 1306.38] loss=2.83 avg=2.96\n",
            "[668 | 1308.13] loss=2.84 avg=2.96\n",
            "[669 | 1309.87] loss=2.94 avg=2.96\n",
            "[670 | 1311.60] loss=2.35 avg=2.96\n",
            "[671 | 1313.33] loss=2.83 avg=2.95\n",
            "[672 | 1315.07] loss=3.03 avg=2.96\n",
            "[673 | 1316.80] loss=2.42 avg=2.95\n",
            "[674 | 1318.53] loss=2.95 avg=2.95\n",
            "[675 | 1320.27] loss=2.79 avg=2.95\n",
            "[676 | 1322.00] loss=2.93 avg=2.95\n",
            "[677 | 1323.74] loss=2.78 avg=2.95\n",
            "[678 | 1325.47] loss=3.14 avg=2.95\n",
            "[679 | 1327.20] loss=3.09 avg=2.95\n",
            "[680 | 1328.93] loss=2.85 avg=2.95\n",
            "[681 | 1330.67] loss=3.03 avg=2.95\n",
            "[682 | 1332.40] loss=2.26 avg=2.94\n",
            "[683 | 1334.15] loss=3.03 avg=2.94\n",
            "[684 | 1335.90] loss=2.83 avg=2.94\n",
            "[685 | 1337.65] loss=2.49 avg=2.94\n",
            "[686 | 1339.41] loss=2.90 avg=2.94\n",
            "[687 | 1341.14] loss=3.29 avg=2.94\n",
            "[688 | 1342.87] loss=3.13 avg=2.94\n",
            "[689 | 1344.60] loss=2.30 avg=2.94\n",
            "[690 | 1346.33] loss=3.38 avg=2.94\n",
            "[691 | 1348.06] loss=3.23 avg=2.94\n",
            "[692 | 1349.79] loss=3.21 avg=2.95\n",
            "[693 | 1351.52] loss=3.00 avg=2.95\n",
            "[694 | 1353.25] loss=2.63 avg=2.94\n",
            "[695 | 1354.98] loss=3.00 avg=2.94\n",
            "[696 | 1356.72] loss=3.07 avg=2.95\n",
            "[697 | 1358.46] loss=2.95 avg=2.95\n",
            "[698 | 1360.20] loss=3.08 avg=2.95\n",
            "[699 | 1361.95] loss=3.22 avg=2.95\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\"We're not looking to see any changes. They're making changes. We're not looking to see any changes.\n",
            "In the past three months alone the @nyjets have won 11 games against the @futsalids! @nyjets A+\n",
            "“I think you have a job and then at some point you have to stop and consider the job.” – Donald Trump\n",
            "In the @NYDailyNews exclusive with @MarthaTMarks on #Superbowl I am talking about my Super Bowl #MVP resume @MarthaTMarks  @MarthaMacaronst @MelanieCarson\n",
            "In @NYPost a lot of people are questioning your Super Bowl #MVP resume @MarthaMacaronst @MelanieCarson   @MarthaMacaronst @MaggieKLN\n",
            "I want you to believe that the @NYDailyNews interview with @MelanieCarson was the first time in the history of @MarthaMacaronst that I said @MelanieCarson was the best I had ever met—only I didn’t realize I had’t said that!\n",
            "@MarthaMacaronst @MelanieCarson I just want you to stop and consider the job. You are doing a great job.\n",
            "The @NYDailyNews interview with @MarthaMacaronst was the first time in my history that I said @MelanieCarson was the best. You had a lot of great things to say...\n",
            "“You think about the game when you're running a business. Make a plan for the rest of your life.” – Think Like a Champion\n",
            "I want you to make a choice about what kind of person you want to be.” – Think Like a Champion\n",
            "“It isn’t about winning or losing. It is all about getting what you want out of life.\n",
            "“If you’re not looking for a winner your life will be a dead one.” – Think Like a Champion\n",
            "“What do you see as a mistake in life?” — John Wooden\n",
            "“The best business plan is for your own success. There are so many ways to find success.\n",
            "“Never forget to look within. If you're looking around you're going to miss something.” – Think Like a Champion\n",
            "The @NYDailyNews interview with Marlee Matlin was the first time in history that I said @MelanieCarson was the BEST I had ever met--only the @NYDailyNews had me wrong!\n",
            "@MarthaMacaronst @MelanieCarson  @MelanieCarson @mariegraham So that’s what you were born with?\n",
            "@mariegraham  @MelanieCarson @melanieCarson @marieegraham  @marieograham So you’re not a winner?\n",
            "“Do the right thing.” — Steve Jobs\n",
            "“Do the right thing always.” - The Art of the Deal\n",
            "Do my interviews and @MarthaMacaronst interviews. @MarthaMacaronst @MelanieCarson is one of the best of every generation.\n",
            "A good business plan gives you the opportunity to learn something new everyday. Focus on where you’re going.\n",
            "Don’t believe the hype. You’re the business you’re the business and it's always your business.\n",
            "The last couple weeks have been a real roller coaster ride for me. Now I think the end comes but I still have lots of work to do!\n",
            "@larsbaker22  @MarthaMacaronst @MelanieCarson So? I didn’t really know her.\n",
            "@larsbaker22  @MarthaMacaronst @MelanieCarson What happened and why was it such a big deal?\n",
            "@JameelGangI ❤️” — (@scoocsco) June 1, 2012 @JoeHegley  @MarthaMacaronst @MelanieCarson  A terrific one-two\n",
            "Do your interviews and @MarthaMacaronst interviews. @MarthaMacaronst @MelanieCarson is one of the best of every generation.\n",
            "@MelanieCarson @marieegraham   @marieagraham You’re talking about the same kid and the same team!\n",
            "@larsbaker22  @MarthaMacaronst @MarthaMacaronst You’re talking ”well” about the same person.\n",
            "I know nothing about Mela. I can just tell it's the same person.\n",
            "@larsbaker22  @MelanieCarson @MarthaMacaronst A really smart guy who will grow into a great one!\n",
            "@mike_keemers @MelanieCarson What’s your response when you\n",
            "\n",
            "[700 | 1386.06] loss=3.31 avg=2.95\n",
            "[701 | 1387.77] loss=2.70 avg=2.95\n",
            "[702 | 1389.53] loss=2.98 avg=2.95\n",
            "[703 | 1391.28] loss=3.14 avg=2.95\n",
            "[704 | 1393.02] loss=2.70 avg=2.95\n",
            "[705 | 1394.75] loss=3.08 avg=2.95\n",
            "[706 | 1396.46] loss=3.01 avg=2.95\n",
            "[707 | 1398.19] loss=3.10 avg=2.95\n",
            "[708 | 1399.92] loss=3.27 avg=2.96\n",
            "[709 | 1401.65] loss=2.97 avg=2.96\n",
            "[710 | 1403.36] loss=2.79 avg=2.96\n",
            "[711 | 1405.10] loss=2.62 avg=2.95\n",
            "[712 | 1406.83] loss=2.64 avg=2.95\n",
            "[713 | 1408.55] loss=3.42 avg=2.95\n",
            "[714 | 1410.31] loss=3.04 avg=2.95\n",
            "[715 | 1412.04] loss=2.79 avg=2.95\n",
            "[716 | 1413.78] loss=3.04 avg=2.95\n",
            "[717 | 1415.52] loss=3.56 avg=2.96\n",
            "[718 | 1417.25] loss=3.13 avg=2.96\n",
            "[719 | 1418.98] loss=3.63 avg=2.97\n",
            "[720 | 1420.71] loss=2.89 avg=2.97\n",
            "[721 | 1422.45] loss=2.93 avg=2.97\n",
            "[722 | 1424.17] loss=2.69 avg=2.96\n",
            "[723 | 1425.91] loss=3.31 avg=2.97\n",
            "[724 | 1427.63] loss=3.05 avg=2.97\n",
            "[725 | 1429.37] loss=2.91 avg=2.97\n",
            "[726 | 1431.10] loss=3.12 avg=2.97\n",
            "[727 | 1432.85] loss=2.06 avg=2.96\n",
            "[728 | 1434.58] loss=3.19 avg=2.96\n",
            "[729 | 1436.31] loss=3.08 avg=2.96\n",
            "[730 | 1438.04] loss=3.10 avg=2.97\n",
            "[731 | 1439.77] loss=3.02 avg=2.97\n",
            "[732 | 1441.50] loss=2.67 avg=2.96\n",
            "[733 | 1443.23] loss=2.87 avg=2.96\n",
            "[734 | 1444.96] loss=3.28 avg=2.97\n",
            "[735 | 1446.70] loss=3.47 avg=2.97\n",
            "[736 | 1448.43] loss=3.14 avg=2.97\n",
            "[737 | 1450.18] loss=3.31 avg=2.98\n",
            "[738 | 1451.91] loss=3.15 avg=2.98\n",
            "[739 | 1453.64] loss=2.93 avg=2.98\n",
            "[740 | 1455.37] loss=3.04 avg=2.98\n",
            "[741 | 1457.09] loss=2.57 avg=2.97\n",
            "[742 | 1458.82] loss=3.08 avg=2.97\n",
            "[743 | 1460.55] loss=2.88 avg=2.97\n",
            "[744 | 1462.29] loss=2.88 avg=2.97\n",
            "[745 | 1464.03] loss=2.83 avg=2.97\n",
            "[746 | 1465.76] loss=2.72 avg=2.97\n",
            "[747 | 1467.49] loss=2.94 avg=2.97\n",
            "[748 | 1469.22] loss=2.83 avg=2.97\n",
            "[749 | 1470.95] loss=3.21 avg=2.97\n",
            "[750 | 1472.68] loss=2.85 avg=2.97\n",
            "[751 | 1474.41] loss=2.62 avg=2.96\n",
            "[752 | 1476.14] loss=3.03 avg=2.97\n",
            "[753 | 1477.87] loss=3.11 avg=2.97\n",
            "[754 | 1479.60] loss=3.07 avg=2.97\n",
            "[755 | 1481.34] loss=2.96 avg=2.97\n",
            "[756 | 1483.07] loss=2.96 avg=2.97\n",
            "[757 | 1484.82] loss=2.52 avg=2.96\n",
            "[758 | 1486.55] loss=3.16 avg=2.97\n",
            "[759 | 1488.28] loss=3.21 avg=2.97\n",
            "[760 | 1490.01] loss=3.36 avg=2.97\n",
            "[761 | 1491.74] loss=2.68 avg=2.97\n",
            "[762 | 1493.47] loss=3.31 avg=2.97\n",
            "[763 | 1495.20] loss=3.28 avg=2.97\n",
            "[764 | 1496.94] loss=2.74 avg=2.97\n",
            "[765 | 1498.67] loss=3.05 avg=2.97\n",
            "[766 | 1500.40] loss=2.71 avg=2.97\n",
            "[767 | 1502.13] loss=1.86 avg=2.96\n",
            "[768 | 1503.86] loss=2.87 avg=2.96\n",
            "[769 | 1505.59] loss=3.31 avg=2.96\n",
            "[770 | 1507.32] loss=2.72 avg=2.96\n",
            "[771 | 1509.05] loss=2.87 avg=2.96\n",
            "[772 | 1510.79] loss=2.54 avg=2.95\n",
            "[773 | 1512.52] loss=2.92 avg=2.95\n",
            "[774 | 1514.27] loss=3.16 avg=2.96\n",
            "[775 | 1516.01] loss=3.13 avg=2.96\n",
            "[776 | 1517.74] loss=2.85 avg=2.96\n",
            "[777 | 1519.47] loss=2.87 avg=2.96\n",
            "[778 | 1521.21] loss=2.77 avg=2.95\n",
            "[779 | 1522.94] loss=3.33 avg=2.96\n",
            "[780 | 1524.68] loss=3.01 avg=2.96\n",
            "[781 | 1526.41] loss=3.16 avg=2.96\n",
            "[782 | 1528.15] loss=2.67 avg=2.96\n",
            "[783 | 1529.88] loss=2.77 avg=2.96\n",
            "[784 | 1531.61] loss=2.37 avg=2.95\n",
            "[785 | 1533.34] loss=2.75 avg=2.95\n",
            "[786 | 1535.07] loss=2.88 avg=2.95\n",
            "[787 | 1536.81] loss=3.24 avg=2.95\n",
            "[788 | 1538.53] loss=2.91 avg=2.95\n",
            "[789 | 1540.28] loss=3.20 avg=2.95\n",
            "[790 | 1542.01] loss=3.13 avg=2.95\n",
            "[791 | 1543.74] loss=3.33 avg=2.96\n",
            "[792 | 1545.47] loss=3.33 avg=2.96\n",
            "[793 | 1547.20] loss=2.88 avg=2.96\n",
            "[794 | 1548.94] loss=3.28 avg=2.96\n",
            "[795 | 1550.66] loss=2.45 avg=2.96\n",
            "[796 | 1552.40] loss=2.87 avg=2.96\n",
            "[797 | 1554.13] loss=2.79 avg=2.96\n",
            "[798 | 1555.86] loss=2.95 avg=2.96\n",
            "[799 | 1557.59] loss=3.14 avg=2.96\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " https://t.co/xmH6cRQxNQ\n",
            "The New York Times just gave the Trump National Golf Club Doonbeg an \"A.\" Very nice and very competitive. I will play a lot!\n",
            "“Donald Trump: Trump Organization 'will compete with others on every front'” https://t.co/Z9yS5pJqH\n",
            "When will the Democrats give us a complete shutdown of government?\n",
            "Why haven’t the Democrats gone after the dishonest Republicans who lied about Planned Parenthood? Their leader now talks about \"the Gang\" (i.e. CR)!\n",
            "The U.S. is being hit hard by a massive earthquake from Mexico.\n",
            "“Terrific speech on Mexico earthquake disaster by @realDonaldTrump’s entourage at World Trade Center https://t.co/mG9K4Kxq3T\n",
            "The Democratic Leader in the House just went crazy trying to negotiate with Republicans on DACA. So do we!\n",
            "The New York Times just gives the Trump Organization an \"A.\" Very nice and very competitive. I will play a lot!\n",
            "“@TrumpVine: 'America First' at NYC @TrumpTowerNY’s new luxury hotel &amp; luxury condominium” https://t.co/P6mRr2F7i\n",
            "It's time to #MakeAmericaGreatAgain (or what remains of it)! #TrumpVine #TrumpRanch\n",
            "“Donald Trump’s career has taken huge shifts over many years as a real estate developer.” http://t.co/QGnBmQqb4V\n",
            "The Democrats are working crazy on DACA. I will save DACA for next year- I just won’t have to say “say”-I will be president!\n",
            "The U.S. is being hit badly by Mexico. Will be hit hard again!\n",
            "The new Trump Towers in Mexico City are stunning!\n",
            "Mexico is spending much more than it was paying under the Bush years - $25 Billion over 4 years- but the Democrats want to continue this. What they are really doing is a disaster!\n",
            "The Democrats want to continue the Obama illegal amnesty? They now want to keep DACA! What about the border?\n",
            "Just finished a great speech at the NYC @TrumpTowerNY https://t.co/y3BdGZh7D1\n",
            "@Trump_Wings  Thank you for your kind words about my @Telemundo/NBC interview  I would not have made the same comment  if I was a TV network owner!\n",
            "We are not going into Mexico and they are going to be fine - we are sending millions of dollars home (and we will be bringing in many others) - just very sad!\n",
            "This story was taken out of context by a very weak and biased reporter with the Fake News Media.\n",
            "The media said that when the Obama Administration decided to take advantage of vulnerable immigrants we needed to build the Wall! The only way we are going to come up with the money is with the Wall!\n",
            "The Democrats want to continue the Obama illegal amnesty? They now want to keep DACA! What they are really doing is a disaster!\n",
            "“The world is better off when the elite are accountable to the people.” - Frederick Douglass\n",
            "@TrumpNyc’s new luxury hotel @TrumpFerryNY’s design by @RocZoli brings a modern sophistication to the city’s most spectacular skyline\n",
            "@TrumpWorldNewYork” great reviews!\n",
            "“It’s a great honor to join @IvankaTrump”  @DonaldJTrumpJr and @TrumpJr19 as they present the inaugural @MissUSA Pageant at Trump National #Doral https://t.co/XB6aXkIqE7\n",
            "@TrumpLasVegas http://t.co/Hn4dT5nH9e\n",
            "#MakeAmericaGreatAgain!\n",
            "We've all thought of Trump Tower as a masterpiece! Will be magnificent soon!\n",
            "Can you see the new Trump Tower in Las Vegas?  http://t.co/W9ZWfqh2Bv\n",
            "This story was taken out of context by a very weak and biased reporter with the Fake News Media.\n",
            "The Democrat leader in the House is saying the exact same thing as the Democrats (or more importantly Obama) who are supposed to be the leaders of this country!\n",
            "@BrentGlobe @foxnews  I guess everyone is now talking about Trump Trump Tower’’’s amazing views!\n",
            "Obama ‘s Attorney General has a history of saying and doing things contrary to how he actually is thinking about things. Don't have a clue!\n",
            "The Democrat leader in the House has said that the Obama ‘administration’s illegal amnesty program is hurting the\n",
            "\n",
            "[800 | 1581.50] loss=3.01 avg=2.96\n",
            "[801 | 1583.21] loss=3.25 avg=2.96\n",
            "[802 | 1584.95] loss=2.97 avg=2.96\n",
            "[803 | 1586.66] loss=2.71 avg=2.96\n",
            "[804 | 1588.39] loss=3.42 avg=2.96\n",
            "[805 | 1590.13] loss=2.73 avg=2.96\n",
            "[806 | 1591.86] loss=2.78 avg=2.96\n",
            "[807 | 1593.60] loss=2.84 avg=2.96\n",
            "[808 | 1595.33] loss=3.20 avg=2.96\n",
            "[809 | 1597.07] loss=3.09 avg=2.96\n",
            "[810 | 1598.78] loss=2.68 avg=2.96\n",
            "[811 | 1600.49] loss=2.93 avg=2.96\n",
            "[812 | 1602.22] loss=2.82 avg=2.96\n",
            "[813 | 1603.93] loss=2.22 avg=2.95\n",
            "[814 | 1605.67] loss=3.06 avg=2.95\n",
            "[815 | 1607.38] loss=2.55 avg=2.95\n",
            "[816 | 1609.11] loss=3.24 avg=2.95\n",
            "[817 | 1610.82] loss=2.52 avg=2.95\n",
            "[818 | 1612.55] loss=2.50 avg=2.94\n",
            "[819 | 1614.27] loss=3.08 avg=2.94\n",
            "[820 | 1615.99] loss=2.61 avg=2.94\n",
            "[821 | 1617.73] loss=2.82 avg=2.94\n",
            "[822 | 1619.46] loss=2.92 avg=2.94\n",
            "[823 | 1621.19] loss=3.15 avg=2.94\n",
            "[824 | 1622.92] loss=2.86 avg=2.94\n",
            "[825 | 1624.64] loss=2.46 avg=2.93\n",
            "[826 | 1626.37] loss=2.79 avg=2.93\n",
            "[827 | 1628.10] loss=3.16 avg=2.93\n",
            "[828 | 1629.84] loss=3.04 avg=2.94\n",
            "[829 | 1631.57] loss=3.07 avg=2.94\n",
            "[830 | 1633.29] loss=3.26 avg=2.94\n",
            "[831 | 1635.03] loss=2.66 avg=2.94\n",
            "[832 | 1636.76] loss=2.50 avg=2.93\n",
            "[833 | 1638.48] loss=3.13 avg=2.94\n",
            "[834 | 1640.22] loss=3.06 avg=2.94\n",
            "[835 | 1641.95] loss=3.31 avg=2.94\n",
            "[836 | 1643.68] loss=3.43 avg=2.95\n",
            "[837 | 1645.41] loss=3.15 avg=2.95\n",
            "[838 | 1647.14] loss=2.83 avg=2.95\n",
            "[839 | 1648.90] loss=3.02 avg=2.95\n",
            "[840 | 1650.63] loss=2.32 avg=2.94\n",
            "[841 | 1652.36] loss=2.47 avg=2.94\n",
            "[842 | 1654.12] loss=3.19 avg=2.94\n",
            "[843 | 1655.86] loss=3.23 avg=2.94\n",
            "[844 | 1657.59] loss=2.34 avg=2.94\n",
            "[845 | 1659.32] loss=2.39 avg=2.93\n",
            "[846 | 1661.06] loss=3.10 avg=2.93\n",
            "[847 | 1662.79] loss=3.12 avg=2.93\n",
            "[848 | 1664.53] loss=2.59 avg=2.93\n",
            "[849 | 1666.26] loss=2.92 avg=2.93\n",
            "[850 | 1667.99] loss=3.06 avg=2.93\n",
            "[851 | 1669.75] loss=2.74 avg=2.93\n",
            "[852 | 1671.49] loss=2.31 avg=2.92\n",
            "[853 | 1673.24] loss=2.99 avg=2.92\n",
            "[854 | 1674.99] loss=2.54 avg=2.92\n",
            "[855 | 1676.72] loss=2.95 avg=2.92\n",
            "[856 | 1678.45] loss=2.86 avg=2.92\n",
            "[857 | 1680.18] loss=3.05 avg=2.92\n",
            "[858 | 1681.92] loss=2.82 avg=2.92\n",
            "[859 | 1683.66] loss=3.20 avg=2.92\n",
            "[860 | 1685.41] loss=3.57 avg=2.93\n",
            "[861 | 1687.15] loss=2.37 avg=2.92\n",
            "[862 | 1688.88] loss=2.72 avg=2.92\n",
            "[863 | 1690.63] loss=2.47 avg=2.92\n",
            "[864 | 1692.37] loss=3.27 avg=2.92\n",
            "[865 | 1694.10] loss=2.89 avg=2.92\n",
            "[866 | 1695.85] loss=2.48 avg=2.92\n",
            "[867 | 1697.60] loss=2.82 avg=2.91\n",
            "[868 | 1699.35] loss=2.83 avg=2.91\n",
            "[869 | 1701.08] loss=2.73 avg=2.91\n",
            "[870 | 1702.84] loss=2.93 avg=2.91\n",
            "[871 | 1704.59] loss=3.17 avg=2.91\n",
            "[872 | 1706.32] loss=2.55 avg=2.91\n",
            "[873 | 1708.05] loss=2.68 avg=2.91\n",
            "[874 | 1709.78] loss=2.58 avg=2.91\n",
            "[875 | 1711.52] loss=2.75 avg=2.90\n",
            "[876 | 1713.25] loss=2.67 avg=2.90\n",
            "[877 | 1714.98] loss=2.66 avg=2.90\n",
            "[878 | 1716.72] loss=2.76 avg=2.90\n",
            "[879 | 1718.45] loss=2.76 avg=2.90\n",
            "[880 | 1720.18] loss=2.84 avg=2.90\n",
            "[881 | 1721.91] loss=3.01 avg=2.90\n",
            "[882 | 1723.67] loss=2.54 avg=2.89\n",
            "[883 | 1725.43] loss=3.06 avg=2.90\n",
            "[884 | 1727.16] loss=3.05 avg=2.90\n",
            "[885 | 1728.90] loss=3.28 avg=2.90\n",
            "[886 | 1730.64] loss=2.48 avg=2.90\n",
            "[887 | 1732.39] loss=3.18 avg=2.90\n",
            "[888 | 1734.12] loss=3.05 avg=2.90\n",
            "[889 | 1735.87] loss=2.58 avg=2.90\n",
            "[890 | 1737.60] loss=2.89 avg=2.90\n",
            "[891 | 1739.34] loss=3.04 avg=2.90\n",
            "[892 | 1741.07] loss=3.09 avg=2.90\n",
            "[893 | 1742.82] loss=2.97 avg=2.90\n",
            "[894 | 1744.57] loss=2.69 avg=2.90\n",
            "[895 | 1746.32] loss=2.32 avg=2.89\n",
            "[896 | 1748.07] loss=3.07 avg=2.90\n",
            "[897 | 1749.80] loss=2.93 avg=2.90\n",
            "[898 | 1751.53] loss=2.91 avg=2.90\n",
            "[899 | 1753.26] loss=2.81 avg=2.90\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "g3nE\n",
            "\"America is not at war with you, or Canada or China. We are at war with the media and with our politicians. If you continue to tell the truth the truth will win. The election is not over because you cannot win when you have such a corrupt media.\"\n",
            "#TrumpVlog – #VanityFair #NeverBackDown https://t.co/q2sP6WvWg5\n",
            ".@GolangNews: \"The real genius of Vladimir Putin is his ability to shape his country's behavior in a way that's consistent with its geopolitical goals.\" #TrumpVlog\n",
            "I would like to remind everyone that The Apprentice is airing again tonight on NBC - on February 13th! #Trump2016\n",
            ".@daslat: \"I do think it's a really important election, which is why we must be so ready. Because the only thing that will beat Trump is time.\" #Trump2016\n",
            "Just watched @nbc prime time show - \"Donald Trump: Hillary Clinton 'Unfit to Be Prime Minister'\" by @TuckerCarlson. It is an even worse show than this one...\n",
            "Wow @billmaher just called me. Very unfair!\n",
            "Don't forget that at 9:00am on Sunday February 12th there won't be 4 hours of debate on @CNN. #Trump2016\n",
            "Don't forget that CNN is owned by Time Warner--and they have taken tremendous stock in my opponent. Time Warner\n",
            "Join my campaign tomorrow! http://t.co/5J6WUi4cL8 #Trump2016\n",
            "Join \"Donald Trump: I'm Coming For You\" at http://t.co/xQXeO1vBvX. #Trump2016 #JoinTheRevolution\n",
            "The Clinton campaign hired paid trolls to attack me on twitter in an effort to deny me my chance to win. Don't ever let this happen to you\n",
            "Congratulations to Mike Huckabee - You would have done well in Arkansas!\n",
            "This year's Republican National Convention will be the best ever.  It will give the party the best opportunity to reclaim the White House since 1988!\n",
            ".@MikeHuckabee said yesterday that he voted for #Cleveland1st.  Very true!\n",
            "\"Donald Trump: Obama's 'The Fed Wants To Destroy You' Is A Completely False Report #Trump2016\" http://t.co/0mRXhYxjY5 via @WSJ\n",
            "Why should @FoxNews and its many stations give me more ratings (or less) if I cancel their show?  They are too biased and too corrupt!\n",
            "@TK_Trump Thanks.\n",
            "Via @Citizen_Times:  \"Donald Trump to give speech in Toronto on Canada in June\" http://t.co/j7QeBQ4xnW #Trump2016\n",
            "I will be doing a major speech tonight in Toronto outlining my ideas for the American Dream!    http://t.co/j7QeBQ4xnW #Trump2016\n",
            "Via @GretawireTV @DavidBlanchflower                      \"Donald Trump for President is running for mayor of Los Angeles...\"\n",
            ".@billmaher @nbc had me take a hit on @nbc last week - but now they are covering me as a really interesting person.\n",
            "Great to be in @nbc's 5pm hour - they are a total waste of airtime &amp; air time - far higher!\n",
            "Via @ABCNews: \"Donald Trump: I'm Not Going to Do A Debate With Hillary\" http://t.co/z4hq0eKvB5 via @abcchicago\n",
            ".@billmaher @nbc had me take a hit on @nbc last week - but now they are covered as a really interesting person.\n",
            "\"Donald Trump says Hillary Clinton 'hasn't produced a single fact-check'\" http://t.co/5QeI6vRzYB via @BreitbartNews\n",
            "I will be speaking at 12:50 pm today at the Republican National Convention in Cleveland Ohio. Will be discussing economic and jobs at the Republican National Convention\n",
            "The Clinton campaign did a terrific job today- it was absolutely dishonest for them to do an ad on me. You see they didn't want to air it!\n",
            "Why can't @billmaher @nbc &amp; @mcuban be honest?  They should focus on their failed campaigns &amp; not talk about The Donald!\n",
            "Why can't @mcuban and @billmaher @nbc/abc spend more time attacking me than they do their failing campaigns?  Who wants to see my name on The Big Show.\n",
            "#SaveOurPensionTime #MakeAmericaGreatAgain #Trump2016 http://t.co/r\n",
            "\n",
            "[900 | 1777.09] loss=3.34 avg=2.90\n",
            "[901 | 1778.82] loss=3.00 avg=2.90\n",
            "[902 | 1780.54] loss=2.66 avg=2.90\n",
            "[903 | 1782.28] loss=2.83 avg=2.90\n",
            "[904 | 1783.99] loss=3.35 avg=2.90\n",
            "[905 | 1785.71] loss=2.55 avg=2.90\n",
            "[906 | 1787.42] loss=2.73 avg=2.90\n",
            "[907 | 1789.13] loss=3.17 avg=2.90\n",
            "[908 | 1790.85] loss=3.14 avg=2.90\n",
            "[909 | 1792.58] loss=2.89 avg=2.90\n",
            "[910 | 1794.29] loss=3.32 avg=2.91\n",
            "[911 | 1796.01] loss=2.75 avg=2.90\n",
            "[912 | 1797.74] loss=2.84 avg=2.90\n",
            "[913 | 1799.47] loss=2.55 avg=2.90\n",
            "[914 | 1801.21] loss=2.76 avg=2.90\n",
            "[915 | 1802.94] loss=2.64 avg=2.90\n",
            "[916 | 1804.67] loss=2.91 avg=2.90\n",
            "[917 | 1806.40] loss=2.85 avg=2.90\n",
            "[918 | 1808.13] loss=2.83 avg=2.90\n",
            "[919 | 1809.86] loss=2.49 avg=2.89\n",
            "[920 | 1811.59] loss=2.64 avg=2.89\n",
            "[921 | 1813.32] loss=2.94 avg=2.89\n",
            "[922 | 1815.05] loss=3.27 avg=2.89\n",
            "[923 | 1816.78] loss=2.87 avg=2.89\n",
            "[924 | 1818.51] loss=2.76 avg=2.89\n",
            "[925 | 1820.25] loss=2.95 avg=2.89\n",
            "[926 | 1822.00] loss=2.55 avg=2.89\n",
            "[927 | 1823.73] loss=2.68 avg=2.89\n",
            "[928 | 1825.46] loss=2.17 avg=2.88\n",
            "[929 | 1827.19] loss=3.27 avg=2.88\n",
            "[930 | 1828.92] loss=2.97 avg=2.88\n",
            "[931 | 1830.66] loss=3.18 avg=2.89\n",
            "[932 | 1832.39] loss=3.08 avg=2.89\n",
            "[933 | 1834.12] loss=2.36 avg=2.88\n",
            "[934 | 1835.86] loss=3.37 avg=2.89\n",
            "[935 | 1837.59] loss=2.94 avg=2.89\n",
            "[936 | 1839.34] loss=3.09 avg=2.89\n",
            "[937 | 1841.07] loss=2.96 avg=2.89\n",
            "[938 | 1842.80] loss=2.92 avg=2.89\n",
            "[939 | 1844.53] loss=2.88 avg=2.89\n",
            "[940 | 1846.27] loss=2.79 avg=2.89\n",
            "[941 | 1848.00] loss=2.90 avg=2.89\n",
            "[942 | 1849.73] loss=3.05 avg=2.89\n",
            "[943 | 1851.46] loss=2.66 avg=2.89\n",
            "[944 | 1853.22] loss=2.79 avg=2.89\n",
            "[945 | 1854.95] loss=2.76 avg=2.89\n",
            "[946 | 1856.69] loss=3.09 avg=2.89\n",
            "[947 | 1858.43] loss=2.93 avg=2.89\n",
            "[948 | 1860.17] loss=3.07 avg=2.89\n",
            "[949 | 1861.91] loss=2.72 avg=2.89\n",
            "[950 | 1863.65] loss=2.45 avg=2.89\n",
            "[951 | 1865.38] loss=3.11 avg=2.89\n",
            "[952 | 1867.14] loss=2.99 avg=2.89\n",
            "[953 | 1868.88] loss=2.53 avg=2.89\n",
            "[954 | 1870.61] loss=2.77 avg=2.88\n",
            "[955 | 1872.35] loss=2.66 avg=2.88\n",
            "[956 | 1874.10] loss=2.84 avg=2.88\n",
            "[957 | 1875.85] loss=3.03 avg=2.88\n",
            "[958 | 1877.60] loss=2.30 avg=2.88\n",
            "[959 | 1879.35] loss=2.00 avg=2.87\n",
            "[960 | 1881.10] loss=3.49 avg=2.87\n",
            "[961 | 1882.85] loss=3.32 avg=2.88\n",
            "[962 | 1884.60] loss=2.77 avg=2.88\n",
            "[963 | 1886.35] loss=3.14 avg=2.88\n",
            "[964 | 1888.11] loss=2.84 avg=2.88\n",
            "[965 | 1889.86] loss=2.44 avg=2.88\n",
            "[966 | 1891.61] loss=2.90 avg=2.88\n",
            "[967 | 1893.34] loss=3.10 avg=2.88\n",
            "[968 | 1895.08] loss=2.74 avg=2.88\n",
            "[969 | 1896.82] loss=3.11 avg=2.88\n",
            "[970 | 1898.56] loss=3.10 avg=2.88\n",
            "[971 | 1900.29] loss=2.78 avg=2.88\n",
            "[972 | 1902.05] loss=2.79 avg=2.88\n",
            "[973 | 1903.79] loss=3.20 avg=2.88\n",
            "[974 | 1905.53] loss=2.85 avg=2.88\n",
            "[975 | 1907.26] loss=2.56 avg=2.88\n",
            "[976 | 1908.99] loss=3.37 avg=2.88\n",
            "[977 | 1910.72] loss=2.81 avg=2.88\n",
            "[978 | 1912.45] loss=2.97 avg=2.88\n",
            "[979 | 1914.18] loss=2.50 avg=2.88\n",
            "[980 | 1915.91] loss=2.63 avg=2.88\n",
            "[981 | 1917.65] loss=3.18 avg=2.88\n",
            "[982 | 1919.38] loss=2.84 avg=2.88\n",
            "[983 | 1921.12] loss=2.93 avg=2.88\n",
            "[984 | 1922.87] loss=3.22 avg=2.88\n",
            "[985 | 1924.60] loss=3.07 avg=2.89\n",
            "[986 | 1926.34] loss=3.11 avg=2.89\n",
            "[987 | 1928.08] loss=2.89 avg=2.89\n",
            "[988 | 1929.81] loss=2.97 avg=2.89\n",
            "[989 | 1931.55] loss=2.38 avg=2.88\n",
            "[990 | 1933.29] loss=2.49 avg=2.88\n",
            "[991 | 1935.02] loss=3.09 avg=2.88\n",
            "[992 | 1936.76] loss=2.69 avg=2.88\n",
            "[993 | 1938.51] loss=2.95 avg=2.88\n",
            "[994 | 1940.27] loss=3.03 avg=2.88\n",
            "[995 | 1942.01] loss=2.74 avg=2.88\n",
            "[996 | 1943.75] loss=3.08 avg=2.88\n",
            "[997 | 1945.50] loss=2.45 avg=2.88\n",
            "[998 | 1947.25] loss=2.48 avg=2.88\n",
            "[999 | 1949.01] loss=3.12 avg=2.88\n",
            "Saving /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-1000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " lied around this week. If I don't tell you in the first couple of days I'll regret it.\n",
            "Why aren't the Committees looking into the @McCarthyFile. It is a farce created by Dems in order to smear Trump - is why the Dems keep getting so many votes.\n",
            "Why isn't the Senate investigating the Hillary Clinton Hillary Diane Rodham ClintonFBI's 37 secret pages of memos about Russia, Clintons and Uranium One Internal poll shows Coffman in dead heat with Democratic challenger Clinton releases video encouraging people to vote MORE campaign? Why isn't the House hearing Russian collusion today?\n",
            "In order to protect Hillary Clinton they have Comey (and others) delete documents &amp; destroy evidence—but the failing @nytimes is doing a great job at covering it up. Very dishonest!\n",
            "Will soon be making a few more decisions on ObamaCare and the Republicans can start to do a complete turnaround if they get the credit that is due. ObamaCare has always been a disaster\n",
            "#Trump2016  http://t.co/ZWZ1jw8cL\n",
            "Do you believe the failing @nytimes has been covering up the investigation of Crooked Clinton all along for years-even during Crooked Hillary's tenure as Secretary\n",
            ".@nytimes is a very unfair &amp; unfair and very unfair newspaper. Even with the way it is covered it is a total disgrace!\n",
            ".@nytimes  Just to shut me up. What was their problem getting caught in a scandal?\n",
            ".@nbc is very unfair to me &amp; their ratings are now down to record lows. They got lucky in that Hillary won!\n",
            "This is not me!\n",
            "Crooked Clinton was very unqualified for the job by the way in many areas but her careless comments &amp; bad judgment were to blame. I would never do that.\n",
            ".@nbc will do a good program when they start with me in the debates. A fantastic network!\n",
            "Crooked Clinton admitted very much she could be investigated for treason but there was a big deal about her deleted records. I don't think so!\n",
            "I will be making my final decision on the future of ObamaCare shortly. I have yet to make a decision and I won't.\n",
            "Crooked Clinton has spent so much time talking about the scandal about her emails it may be a big factor in the election\n",
            "The media and the Dems are trying to distract from Hillary Clinton's disastrous decision to fire her husband. I told you so!\n",
            "With ObamaCare and all of its disaster she was right to do it - it could be the death of her. Crooked is a disaster!\n",
            "Trump National Doral is a tremendous addition to the South Beach skyline. Great hotels and a great course!\n",
            "The press and the Dems should focus on the real issues: health care premiums deductibles and illegal immigration.\n",
            "Crooked Clinton spent a lot of time on the 9 to 5 while I was busy doing all the big deals. She even got away with a crime. I said get the hell out of my country!\n",
            "Great to see that the New York Times is doing a very nice &amp; important story on my decision to fire Bob Mueller.\n",
            "Crooked Clinton spent too much time on the 9 to 5 and I stayed up all night. I'm not proud of it nor am I going to let her down!\n",
            "I am very proud of the @nytimes. They are a great newspaper.\n",
            ".@nbc today is done. They have treated me extremely unfairly and unfairly. The @nytimes/ABCNews Poll is trending in an anti-Trump direction.\n",
            "Many of the illegal immigrants who have entered our country are criminals with no intention of ever leaving. These people have been criminals in every way &amp; must be properly addressed!\n",
            "Great to see the House vote yesterday on the massive Border Security and Border Wall. Border security and wall will be done! I'm talking about many billions and we're getting it done!\n",
            "Great meeting with the @SenOrrinB_Nomination at the @WhiteHouse. Thank you to @VP Pence!https://t.co/p1W6Q5Wd1W\n",
            "We are finally bringing the people the truth. Don’t believe the lies and be prepared to see for yourself. #TruthIsTelling https://t.co/UZG8iEI8YZ\n",
            "\"If you’re looking for a strategy for doing your job look no further than the Trump way.\" - Donald J. Trump\n",
            "My daughter Ivanka along with her husband Jared are spending the weekend at my Washington D.C. White House w/ @VP Pence &amp; family. We are honored to be hosting these top political leaders! #MAGA\n",
            "We have been working feverishly on Border Security &amp; Wall. Both are priorities for the next Administration. We will make it happen!\n",
            "Just spoke with President of Mexico &amp; President of the United States @VP @México @\n",
            "\n",
            "[1000 | 1987.05] loss=2.43 avg=2.87\n",
            "[1001 | 1988.83] loss=2.70 avg=2.87\n",
            "[1002 | 1990.62] loss=2.78 avg=2.87\n",
            "[1003 | 1992.40] loss=3.11 avg=2.87\n",
            "[1004 | 1994.17] loss=3.06 avg=2.87\n",
            "[1005 | 1995.92] loss=3.08 avg=2.88\n",
            "[1006 | 1997.67] loss=3.04 avg=2.88\n",
            "[1007 | 1999.41] loss=2.97 avg=2.88\n",
            "[1008 | 2001.14] loss=2.88 avg=2.88\n",
            "[1009 | 2002.88] loss=2.82 avg=2.88\n",
            "[1010 | 2004.62] loss=2.92 avg=2.88\n",
            "[1011 | 2006.35] loss=3.00 avg=2.88\n",
            "[1012 | 2008.09] loss=3.04 avg=2.88\n",
            "[1013 | 2009.82] loss=3.10 avg=2.88\n",
            "[1014 | 2011.54] loss=3.02 avg=2.89\n",
            "[1015 | 2013.30] loss=2.76 avg=2.88\n",
            "[1016 | 2015.05] loss=2.87 avg=2.88\n",
            "[1017 | 2016.78] loss=2.72 avg=2.88\n",
            "[1018 | 2018.51] loss=2.39 avg=2.88\n",
            "[1019 | 2020.24] loss=3.12 avg=2.88\n",
            "[1020 | 2021.98] loss=3.03 avg=2.88\n",
            "[1021 | 2023.69] loss=3.14 avg=2.88\n",
            "[1022 | 2025.42] loss=2.72 avg=2.88\n",
            "[1023 | 2027.13] loss=2.86 avg=2.88\n",
            "[1024 | 2028.84] loss=2.72 avg=2.88\n",
            "[1025 | 2030.55] loss=2.97 avg=2.88\n",
            "[1026 | 2032.26] loss=2.33 avg=2.88\n",
            "[1027 | 2034.00] loss=3.13 avg=2.88\n",
            "[1028 | 2035.73] loss=3.09 avg=2.88\n",
            "[1029 | 2037.46] loss=3.11 avg=2.88\n",
            "[1030 | 2039.19] loss=3.10 avg=2.89\n",
            "[1031 | 2040.92] loss=3.32 avg=2.89\n",
            "[1032 | 2042.65] loss=2.96 avg=2.89\n",
            "[1033 | 2044.38] loss=3.20 avg=2.89\n",
            "[1034 | 2046.12] loss=3.33 avg=2.90\n",
            "[1035 | 2047.87] loss=2.59 avg=2.89\n",
            "[1036 | 2049.60] loss=2.85 avg=2.89\n",
            "[1037 | 2051.34] loss=2.79 avg=2.89\n",
            "[1038 | 2053.07] loss=2.79 avg=2.89\n",
            "[1039 | 2054.81] loss=2.60 avg=2.89\n",
            "[1040 | 2056.56] loss=2.88 avg=2.89\n",
            "[1041 | 2058.32] loss=2.72 avg=2.89\n",
            "[1042 | 2060.07] loss=3.00 avg=2.89\n",
            "[1043 | 2061.82] loss=2.81 avg=2.89\n",
            "[1044 | 2063.58] loss=2.60 avg=2.88\n",
            "[1045 | 2065.31] loss=3.01 avg=2.89\n",
            "[1046 | 2067.05] loss=3.25 avg=2.89\n",
            "[1047 | 2068.78] loss=3.34 avg=2.89\n",
            "[1048 | 2070.52] loss=3.06 avg=2.90\n",
            "[1049 | 2072.25] loss=2.62 avg=2.89\n",
            "[1050 | 2073.99] loss=3.26 avg=2.90\n",
            "[1051 | 2075.74] loss=2.95 avg=2.90\n",
            "[1052 | 2077.49] loss=2.73 avg=2.90\n",
            "[1053 | 2079.25] loss=2.25 avg=2.89\n",
            "[1054 | 2080.98] loss=2.50 avg=2.89\n",
            "[1055 | 2082.71] loss=2.59 avg=2.88\n",
            "[1056 | 2084.44] loss=3.30 avg=2.89\n",
            "[1057 | 2086.17] loss=2.70 avg=2.88\n",
            "[1058 | 2087.90] loss=2.74 avg=2.88\n",
            "[1059 | 2089.65] loss=3.18 avg=2.89\n",
            "[1060 | 2091.38] loss=2.71 avg=2.88\n",
            "[1061 | 2093.11] loss=2.67 avg=2.88\n",
            "[1062 | 2094.86] loss=1.94 avg=2.87\n",
            "[1063 | 2096.62] loss=3.09 avg=2.88\n",
            "[1064 | 2098.37] loss=2.84 avg=2.87\n",
            "[1065 | 2100.10] loss=2.68 avg=2.87\n",
            "[1066 | 2101.83] loss=2.62 avg=2.87\n",
            "[1067 | 2103.58] loss=2.79 avg=2.87\n",
            "[1068 | 2105.34] loss=2.74 avg=2.87\n",
            "[1069 | 2107.06] loss=3.11 avg=2.87\n",
            "[1070 | 2108.80] loss=2.66 avg=2.87\n",
            "[1071 | 2110.53] loss=3.04 avg=2.87\n",
            "[1072 | 2112.26] loss=3.32 avg=2.87\n",
            "[1073 | 2113.99] loss=2.72 avg=2.87\n",
            "[1074 | 2115.72] loss=2.72 avg=2.87\n",
            "[1075 | 2117.45] loss=3.16 avg=2.87\n",
            "[1076 | 2119.18] loss=3.16 avg=2.88\n",
            "[1077 | 2120.92] loss=2.94 avg=2.88\n",
            "[1078 | 2122.65] loss=2.68 avg=2.88\n",
            "[1079 | 2124.38] loss=2.65 avg=2.87\n",
            "[1080 | 2126.12] loss=2.75 avg=2.87\n",
            "[1081 | 2127.83] loss=3.10 avg=2.87\n",
            "[1082 | 2129.56] loss=3.06 avg=2.88\n",
            "[1083 | 2131.30] loss=2.53 avg=2.87\n",
            "[1084 | 2133.01] loss=3.04 avg=2.87\n",
            "[1085 | 2134.75] loss=2.54 avg=2.87\n",
            "[1086 | 2136.46] loss=3.04 avg=2.87\n",
            "[1087 | 2138.19] loss=2.51 avg=2.87\n",
            "[1088 | 2139.92] loss=2.94 avg=2.87\n",
            "[1089 | 2141.64] loss=2.73 avg=2.87\n",
            "[1090 | 2143.36] loss=3.13 avg=2.87\n",
            "[1091 | 2145.07] loss=2.43 avg=2.87\n",
            "[1092 | 2146.78] loss=2.63 avg=2.86\n",
            "[1093 | 2148.49] loss=2.59 avg=2.86\n",
            "[1094 | 2150.20] loss=2.85 avg=2.86\n",
            "[1095 | 2151.94] loss=3.16 avg=2.86\n",
            "[1096 | 2153.66] loss=2.49 avg=2.86\n",
            "[1097 | 2155.38] loss=2.82 avg=2.86\n",
            "[1098 | 2157.09] loss=2.71 avg=2.86\n",
            "[1099 | 2158.80] loss=2.58 avg=2.86\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ackk\n",
            "“President Trump was correct when he said our immigration laws are too tough.” @TuckerCarlson\n",
            "Our nation is an immigration nation. We must make our country a place where no one breaks laws. Let’s protect American workers and law-abiding Americans!\n",
            "\"We’ll need an immigration agenda much tougher than Obamacare.\" Our country depends on immigration. If’s not doing well I can blame the Democrats!\n",
            "The NFL is having a really hard time at football fans....really dislike the way we are being paid and abused by the league. The NFL should apologize to all...\n",
            "...and stop making us pay so much money. @NFL should have the right to stop playing games if they want. We must stop paying them peanuts!\n",
            "...The NFL is a total waste of time and money. NFL ratings are way down the lowest in many years. Would be nice if the league would fix the game rather than playing games!\n",
            "Thank you! https://t.co/fZ7VmFdLr0https://t.co/W3kUqNzw3p\n",
            "I will be on Fox &amp; Friends Weekend at 7:00 AM EST.\n",
            "I will be on @mcuban @jasoninthehouse for the 7:00 A.M. call. @FoxNews #TuckerCarlson\n",
            "Thank you @FoxNews - so much to talk about today! https://t.co/4hZi8rLnjH\n",
            ".@FLGovScott @MarkBDaSilva @FLMarthaMcSally &amp; @ChrisMcDaniel  You guys were terrific on @FoxNews tonight. It was a great evening!\n",
            "Wow - @FLGovScott got slammed by @JoeBiden on @FoxNews  - bad for Florida!\n",
            "Wow - @FLGovScott did the most for the people of my state. His support means so much for my campaign for @DHSgov. I love you all!\n",
            "Join me right now at 9:00 A.M. EST for a Town Hall with Rick Scott of Florida on @FoxNews. #Trump2016https://t.co/cWjhVk3Bq2 https://t.co/xF2qHZH7Xo\n",
            "WEDNESDAY\n",
            "Watch @megynkelly now on @FoxNews  https://t.co/YjXc4tQ1Ww\n",
            "Watch @megynkelly now on @FoxNews  https://t.co/XBXV6QxW2h\n",
            "Join me at Trump National Washington DC on Thursday at 7:30 P.M. on @FoxNews! Tickets: https://t.co/KpVuYh1gx1 https://t.co/Km1yjTqb9A\n",
            "Join me on Wednesday, March 18th at 7:30 P.M. on @FoxNews! Tickets: https://t.co/L8vfYj9XVc https://t.co/fkvU1U7t6M\n",
            "This will be a historic night in the history of our country. Join me for a historic rally today in Washington DC! Tickets: https://t.co/IzU5qKzXoM\n",
            "Wow - @FLGovScott &amp; @MarcoRubio @FLGovCapitol @ScottWalker @JebBush @ScottWalkerForPresident  Thank you Marco &amp; @JebBush! https://t.co/xHqXKjGfUH\n",
            "Thank you! #Trump2016https://t.co/KpVuYgkUQJ https://t.co/wK4MvqK7ZG\n",
            "#VoteTrump @HillaryClinton @joelpollak https://t.co/u3LQVgxEZy https://t.co/gjw2vXgNp3\n",
            "#VoteTrump @JohnPalinUSA https://t.co/Yp9gv7Fv1M\n",
            "Join me tomorrow at 9:00 A.M. at the @MarineCorps &amp; @DHSgov Center in Washington DC. Join hundreds of thousands of registered voters all over America in the final days of the election. Tickets: https://t.co/KpVuYgkUQJ\n",
            "Thank you so much to all of the great supporters in the D.C. area tonight. Thank you! #Trump2016 https://t.co/J0fLfWvj1G\n",
            "Thank you! #Trump2016https://t.co/I2tO1qeXvK https://t.co/wG7VZ9zmZb\n",
            "Thank you. #VoteTrump\n",
            "\n",
            "[1100 | 2182.74] loss=2.54 avg=2.85\n",
            "[1101 | 2184.46] loss=2.84 avg=2.85\n",
            "[1102 | 2186.18] loss=3.43 avg=2.86\n",
            "[1103 | 2187.89] loss=3.22 avg=2.86\n",
            "[1104 | 2189.61] loss=3.04 avg=2.86\n",
            "[1105 | 2191.32] loss=2.68 avg=2.86\n",
            "[1106 | 2193.03] loss=3.08 avg=2.86\n",
            "[1107 | 2194.75] loss=3.29 avg=2.87\n",
            "[1108 | 2196.46] loss=2.63 avg=2.87\n",
            "[1109 | 2198.18] loss=2.98 avg=2.87\n",
            "[1110 | 2199.89] loss=2.90 avg=2.87\n",
            "[1111 | 2201.60] loss=2.84 avg=2.87\n",
            "[1112 | 2203.32] loss=2.64 avg=2.87\n",
            "[1113 | 2205.03] loss=3.09 avg=2.87\n",
            "[1114 | 2206.75] loss=2.96 avg=2.87\n",
            "[1115 | 2208.47] loss=2.76 avg=2.87\n",
            "[1116 | 2210.18] loss=2.58 avg=2.86\n",
            "[1117 | 2211.90] loss=2.87 avg=2.86\n",
            "[1118 | 2213.61] loss=2.73 avg=2.86\n",
            "[1119 | 2215.32] loss=2.59 avg=2.86\n",
            "[1120 | 2217.05] loss=2.79 avg=2.86\n",
            "[1121 | 2218.76] loss=3.09 avg=2.86\n",
            "[1122 | 2220.49] loss=2.58 avg=2.86\n",
            "[1123 | 2222.23] loss=3.28 avg=2.86\n",
            "[1124 | 2223.96] loss=3.04 avg=2.87\n",
            "[1125 | 2225.69] loss=2.68 avg=2.86\n",
            "[1126 | 2227.44] loss=2.70 avg=2.86\n",
            "[1127 | 2229.17] loss=2.46 avg=2.86\n",
            "[1128 | 2230.91] loss=2.55 avg=2.85\n",
            "[1129 | 2232.64] loss=3.08 avg=2.86\n",
            "[1130 | 2234.37] loss=3.17 avg=2.86\n",
            "[1131 | 2236.10] loss=2.64 avg=2.86\n",
            "[1132 | 2237.84] loss=2.52 avg=2.85\n",
            "[1133 | 2239.57] loss=2.85 avg=2.85\n",
            "[1134 | 2241.30] loss=3.00 avg=2.86\n",
            "[1135 | 2243.03] loss=2.92 avg=2.86\n",
            "[1136 | 2244.76] loss=2.83 avg=2.86\n",
            "[1137 | 2246.50] loss=2.52 avg=2.85\n",
            "[1138 | 2248.23] loss=3.19 avg=2.86\n",
            "[1139 | 2249.96] loss=2.83 avg=2.86\n",
            "[1140 | 2251.69] loss=2.91 avg=2.86\n",
            "[1141 | 2253.43] loss=2.88 avg=2.86\n",
            "[1142 | 2255.15] loss=2.54 avg=2.85\n",
            "[1143 | 2256.88] loss=2.66 avg=2.85\n",
            "[1144 | 2258.62] loss=2.62 avg=2.85\n",
            "[1145 | 2260.35] loss=2.75 avg=2.85\n",
            "[1146 | 2262.08] loss=2.52 avg=2.85\n",
            "[1147 | 2263.82] loss=2.80 avg=2.84\n",
            "[1148 | 2265.55] loss=2.16 avg=2.84\n",
            "[1149 | 2267.28] loss=2.53 avg=2.83\n",
            "[1150 | 2268.99] loss=2.76 avg=2.83\n",
            "[1151 | 2270.73] loss=2.87 avg=2.83\n",
            "[1152 | 2272.47] loss=2.75 avg=2.83\n",
            "[1153 | 2274.20] loss=2.47 avg=2.83\n",
            "[1154 | 2275.94] loss=2.80 avg=2.83\n",
            "[1155 | 2277.67] loss=2.79 avg=2.83\n",
            "[1156 | 2279.38] loss=2.67 avg=2.83\n",
            "[1157 | 2281.10] loss=2.38 avg=2.82\n",
            "[1158 | 2282.81] loss=2.74 avg=2.82\n",
            "[1159 | 2284.52] loss=2.58 avg=2.82\n",
            "[1160 | 2286.23] loss=3.01 avg=2.82\n",
            "[1161 | 2287.94] loss=2.50 avg=2.82\n",
            "[1162 | 2289.65] loss=2.51 avg=2.82\n",
            "[1163 | 2291.36] loss=2.71 avg=2.81\n",
            "[1164 | 2293.09] loss=2.57 avg=2.81\n",
            "[1165 | 2294.80] loss=2.84 avg=2.81\n",
            "[1166 | 2296.52] loss=2.39 avg=2.81\n",
            "[1167 | 2298.23] loss=2.58 avg=2.81\n",
            "[1168 | 2299.94] loss=2.27 avg=2.80\n",
            "[1169 | 2301.68] loss=2.99 avg=2.80\n",
            "[1170 | 2303.40] loss=3.03 avg=2.80\n",
            "[1171 | 2305.11] loss=2.59 avg=2.80\n",
            "[1172 | 2306.82] loss=2.72 avg=2.80\n",
            "[1173 | 2308.53] loss=2.78 avg=2.80\n",
            "[1174 | 2310.24] loss=2.61 avg=2.80\n",
            "[1175 | 2311.96] loss=2.82 avg=2.80\n",
            "[1176 | 2313.67] loss=2.84 avg=2.80\n",
            "[1177 | 2315.38] loss=3.04 avg=2.80\n",
            "[1178 | 2317.09] loss=2.51 avg=2.80\n",
            "[1179 | 2318.80] loss=2.92 avg=2.80\n",
            "[1180 | 2320.52] loss=2.65 avg=2.80\n",
            "[1181 | 2322.23] loss=2.37 avg=2.80\n",
            "[1182 | 2323.94] loss=2.65 avg=2.79\n",
            "[1183 | 2325.66] loss=3.08 avg=2.80\n",
            "[1184 | 2327.37] loss=2.79 avg=2.80\n",
            "[1185 | 2329.08] loss=2.73 avg=2.80\n",
            "[1186 | 2330.80] loss=2.92 avg=2.80\n",
            "[1187 | 2332.51] loss=2.64 avg=2.80\n",
            "[1188 | 2334.23] loss=2.11 avg=2.79\n",
            "[1189 | 2335.94] loss=2.79 avg=2.79\n",
            "[1190 | 2337.65] loss=2.82 avg=2.79\n",
            "[1191 | 2339.38] loss=2.87 avg=2.79\n",
            "[1192 | 2341.09] loss=2.53 avg=2.79\n",
            "[1193 | 2342.80] loss=2.97 avg=2.79\n",
            "[1194 | 2344.52] loss=2.91 avg=2.79\n",
            "[1195 | 2346.23] loss=2.74 avg=2.79\n",
            "[1196 | 2347.95] loss=2.33 avg=2.78\n",
            "[1197 | 2349.66] loss=2.81 avg=2.79\n",
            "[1198 | 2351.37] loss=3.26 avg=2.79\n",
            "[1199 | 2353.08] loss=3.07 avg=2.79\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " on: https://t.co/lx1Hq2pI0Q\n",
            "Thank you @Lithuania! #MakeAmericaGreatAgain #TrumpTrain2016 https://t.co/mK4nJ0s9hP\n",
            "As soon as I have more time I will be releasing my tax returns. Big story-they show big numbers but I think they can be faked!\n",
            "Wow just out from the @CNN Poll. Thank you! Thank you for the nice words. Thank you for having me! #Trump2016 #VoteTrumpNV #MakeAmericaGreatAgain #TrumpTrain2016\n",
            "I will be doing @wolfblitz tonight at 9PM ET on @nbc @CNN.  Enjoy! #Trump2016\n",
            ".@BillKristol got what he wished for-a huge hit in the @CNN poll. I don't see it!\n",
            "I'm in Las Vegas tonight. Will be live tweeting. Thank you to everyone for the nice words on @CNN! #Trump2016#MakeAmericaGreatAgain\n",
            "Just put #TrumpTicket on @Macy's website. The seats are very hard to find. You won't be disappointed!\n",
            "With such poor poll numbers and the new poll report by @CNN just out on me the F.B.I. is now much more involved. Very unsavory!\n",
            "I will be doing Face The Nation this morning. Enjoy the show!\n",
            "Thank you @IvankaTrump for your very kind words. A real champion!#MakeAmericaGreatAgain #Trump2016 https://t.co/LjZLKr7h2Z\n",
            "In the beginning there was no such thing as success #Trump2016 https://t.co/Vf4L9yIzQC\n",
            "Thank you Gary Johnson-the only one who can fix our disastrous trade deals the failing Military and illegal immigration!\n",
            ".@CNN polling has just fallen into the single-digits - terrible reporting/exaggeration of numbers. Really bad reporting by CNN.\n",
            "Thank you @Jill_Kraig. You are an amazing person with a great message! #Trump2016  You will never be forgotten!\n",
            "I agree with @DanaPerino that #Dems do not have a chance. She is a total flunky and/or a lost cause who does not have the energy/feels that she can't win!\n",
            "Great new poll from @CNN just out - I will be on live #CSPAN at 6:05 A.M. See you on #CSPan!\n",
            "@IvankaTrump made history as the first #Apprentice contestant to earn over $5million- she's a winner! Thank you Ivanka!\n",
            "Wow the ratings for the #TrumpTone has gone down a massive 51% since I announced. Not good! Time for a change and will end!\n",
            "Congratulations to Gary Johnson - he will be our next president in 2016. Thank you for your support!\n",
            "I am happy to announce that I am now endorsing Gary Johnson for President of the United States. Gary has the courage &amp; passion to #MakeAmericaGreatAgain!\n",
            "#MakeAmericaGreatAgain #Trump2016 https://t.co/xE2jzH4Uyj\n",
            "#MakeAmericaGreatAgain #VoteTrumpNV #USA https://t.co/sDtLn4zYW2\n",
            "#VoteTrumpNV  Thank you! #AmericaFirst #MakeAmericaGreatAgain https://t.co/1C9x6J0Yv7\n",
            "Join me in #NewYork tomorrow morning for a #MakeAmericaGreatAgain #ElectionConvention! Tickets: https://t.co/8E1yEq4H2E\n",
            "Will be on @foxandfriends at 7:15 a.m. Enjoy!\n",
            "In any event I am proud of @Nathalie_Dowd. You had a terrific campaign - the press called it \"The Great National Convention!\" Go for it!\n",
            "Great evening in New Hampshire. Get out and vote!\n",
            "I will be appearing tonight on @foxandfriends - 5 minutes. Enjoy!\n",
            "\"If people are going to have fun go where the fun is. And if you're going to have fun where is the fun? There are no empty seats at the Trump Tower.\" --Donald J. Trump\n",
            "Thank you to our amazing &amp; loyal @NewYorkUnion members. #NYTWeeklyGuildSummit https://t.co/1EyGpMnFv5\n",
            "Today I am honored to welcome @nyu_union members to Trump Tower for a Guild Dinner. This will be the first meeting of our union so we are always talking! https://t.co/qF1sH1qYm8\n",
            "I am so proud that @nyu_union members are making a deal to stay with me in New York City. Many union members have just given $1\n",
            "\n",
            "[1200 | 2376.92] loss=3.06 avg=2.80\n",
            "[1201 | 2378.62] loss=2.84 avg=2.80\n",
            "[1202 | 2380.34] loss=2.83 avg=2.80\n",
            "[1203 | 2382.07] loss=2.50 avg=2.79\n",
            "[1204 | 2383.80] loss=2.56 avg=2.79\n",
            "[1205 | 2385.52] loss=2.82 avg=2.79\n",
            "[1206 | 2387.25] loss=2.71 avg=2.79\n",
            "[1207 | 2388.98] loss=2.67 avg=2.79\n",
            "[1208 | 2390.71] loss=2.74 avg=2.79\n",
            "[1209 | 2392.44] loss=2.72 avg=2.79\n",
            "[1210 | 2394.17] loss=3.15 avg=2.79\n",
            "[1211 | 2395.90] loss=3.03 avg=2.79\n",
            "[1212 | 2397.64] loss=2.59 avg=2.79\n",
            "[1213 | 2399.37] loss=2.53 avg=2.79\n",
            "[1214 | 2401.10] loss=2.87 avg=2.79\n",
            "[1215 | 2402.84] loss=2.93 avg=2.79\n",
            "[1216 | 2404.57] loss=2.88 avg=2.79\n",
            "[1217 | 2406.30] loss=2.60 avg=2.79\n",
            "[1218 | 2408.04] loss=2.67 avg=2.79\n",
            "[1219 | 2409.77] loss=2.98 avg=2.79\n",
            "[1220 | 2411.51] loss=2.67 avg=2.79\n",
            "[1221 | 2413.24] loss=2.61 avg=2.79\n",
            "[1222 | 2414.98] loss=2.93 avg=2.79\n",
            "[1223 | 2416.71] loss=3.01 avg=2.79\n",
            "[1224 | 2418.44] loss=2.76 avg=2.79\n",
            "[1225 | 2420.17] loss=2.59 avg=2.79\n",
            "[1226 | 2421.90] loss=3.09 avg=2.79\n",
            "[1227 | 2423.63] loss=2.73 avg=2.79\n",
            "[1228 | 2425.36] loss=3.24 avg=2.80\n",
            "[1229 | 2427.08] loss=3.00 avg=2.80\n",
            "[1230 | 2428.81] loss=2.50 avg=2.80\n",
            "[1231 | 2430.54] loss=2.81 avg=2.80\n",
            "[1232 | 2432.25] loss=2.78 avg=2.80\n",
            "[1233 | 2433.96] loss=2.66 avg=2.79\n",
            "[1234 | 2435.69] loss=2.84 avg=2.79\n",
            "[1235 | 2437.40] loss=2.65 avg=2.79\n",
            "[1236 | 2439.13] loss=2.21 avg=2.79\n",
            "[1237 | 2440.84] loss=2.70 avg=2.79\n",
            "[1238 | 2442.55] loss=2.80 avg=2.79\n",
            "[1239 | 2444.26] loss=2.75 avg=2.79\n",
            "[1240 | 2445.97] loss=2.91 avg=2.79\n",
            "[1241 | 2447.69] loss=2.94 avg=2.79\n",
            "[1242 | 2449.41] loss=2.75 avg=2.79\n",
            "[1243 | 2451.12] loss=3.02 avg=2.79\n",
            "[1244 | 2452.85] loss=2.86 avg=2.79\n",
            "[1245 | 2454.56] loss=2.66 avg=2.79\n",
            "[1246 | 2456.27] loss=2.55 avg=2.79\n",
            "[1247 | 2457.99] loss=2.74 avg=2.79\n",
            "[1248 | 2459.69] loss=2.65 avg=2.79\n",
            "[1249 | 2461.41] loss=2.80 avg=2.79\n",
            "[1250 | 2463.13] loss=2.70 avg=2.79\n",
            "[1251 | 2464.84] loss=3.03 avg=2.79\n",
            "[1252 | 2466.56] loss=3.55 avg=2.80\n",
            "[1253 | 2468.28] loss=2.64 avg=2.79\n",
            "[1254 | 2469.99] loss=2.47 avg=2.79\n",
            "[1255 | 2471.71] loss=2.74 avg=2.79\n",
            "[1256 | 2473.42] loss=2.23 avg=2.78\n",
            "[1257 | 2475.14] loss=3.15 avg=2.79\n",
            "[1258 | 2476.85] loss=2.74 avg=2.79\n",
            "[1259 | 2478.56] loss=2.56 avg=2.79\n",
            "[1260 | 2480.28] loss=2.65 avg=2.78\n",
            "[1261 | 2482.00] loss=2.87 avg=2.78\n",
            "[1262 | 2483.71] loss=2.43 avg=2.78\n",
            "[1263 | 2485.42] loss=2.64 avg=2.78\n",
            "[1264 | 2487.13] loss=2.75 avg=2.78\n",
            "[1265 | 2488.84] loss=3.22 avg=2.78\n",
            "[1266 | 2490.55] loss=3.03 avg=2.79\n",
            "[1267 | 2492.26] loss=2.73 avg=2.79\n",
            "[1268 | 2493.97] loss=2.46 avg=2.78\n",
            "[1269 | 2495.69] loss=2.82 avg=2.78\n",
            "[1270 | 2497.42] loss=2.52 avg=2.78\n",
            "[1271 | 2499.14] loss=1.91 avg=2.77\n",
            "[1272 | 2500.86] loss=3.23 avg=2.78\n",
            "[1273 | 2502.57] loss=3.10 avg=2.78\n",
            "[1274 | 2504.28] loss=3.04 avg=2.78\n",
            "[1275 | 2506.00] loss=2.75 avg=2.78\n",
            "[1276 | 2507.73] loss=2.43 avg=2.78\n",
            "[1277 | 2509.46] loss=3.20 avg=2.78\n",
            "[1278 | 2511.19] loss=2.55 avg=2.78\n",
            "[1279 | 2512.90] loss=2.63 avg=2.78\n",
            "[1280 | 2514.62] loss=2.51 avg=2.78\n",
            "[1281 | 2516.33] loss=2.88 avg=2.78\n",
            "[1282 | 2518.04] loss=2.80 avg=2.78\n",
            "[1283 | 2519.74] loss=2.62 avg=2.78\n",
            "[1284 | 2521.45] loss=2.51 avg=2.77\n",
            "[1285 | 2523.17] loss=2.93 avg=2.77\n",
            "[1286 | 2524.88] loss=2.43 avg=2.77\n",
            "[1287 | 2526.59] loss=2.67 avg=2.77\n",
            "[1288 | 2528.30] loss=2.46 avg=2.77\n",
            "[1289 | 2530.01] loss=2.99 avg=2.77\n",
            "[1290 | 2531.72] loss=2.33 avg=2.76\n",
            "[1291 | 2533.44] loss=2.95 avg=2.77\n",
            "[1292 | 2535.15] loss=3.07 avg=2.77\n",
            "[1293 | 2536.88] loss=2.49 avg=2.77\n",
            "[1294 | 2538.60] loss=3.02 avg=2.77\n",
            "[1295 | 2540.31] loss=2.72 avg=2.77\n",
            "[1296 | 2542.03] loss=2.81 avg=2.77\n",
            "[1297 | 2543.76] loss=2.78 avg=2.77\n",
            "[1298 | 2545.47] loss=2.75 avg=2.77\n",
            "[1299 | 2547.19] loss=3.01 avg=2.77\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "://t.co/3Cm5qW6pJq\n",
            "Will the media investigate and report the many conflicts of interest created by the Clintons who have been involved in lots of deals that would be of great interest to others? Big conflicts of interest - and no wonder Crooked Hillary doesn't want our help!\n",
            "I will be interviewed by @FoxNews' @meghannsayer tonight at 9 PM. Enjoy!\n",
            "I am in Florida with my wife Melania and our son Barron. https://t.co/XHm5YjUuAa\n",
            "Thank you South Carolina! #MAGARoverTrump #MakeAmericaGreatAgain https://t.co/k3hVzdGq2Q https://t.co/1yTlD9VY9P\n",
            "Thank you South Carolina! We are going to MAKE AMERICA GREAT AGAIN! https://t.co/fQ4Ld4qO2K\n",
            "#TrumpTours#MakeAmericaGreatAgain https://t.co/KrjYq8Q6eV https://t.co/rK9pjKmW4h\n",
            "I am in South Carolina with @DonaldJTrumpJr and will be going to more of the great places of the state tomorrow night. Enjoy! https://t.co/Xhm5YjUuAa\n",
            "I will be interviewed by @GStephanopoulos on @FoxNews tonight at 9 pm. Enjoy!\n",
            "#MakeAmericaGreatAgain #Trump2017 https://t.co/cwYg4LXa9G\n",
            "#MakeAmericaGreatAgain #TrumpTrain https://t.co/yP2LQdVZHn\n",
            "New York @TrumpNationalPark features a 150000 foot glass staircase that takes guests on its first 15000 feet! https://t.co/VuDdPxj4i4\n",
            "#MakeAmericaGreatAgain We will Make America Great Again! https://t.co/dKdOgCw2Q8\n",
            "I am in South Carolina with my wife Melania. Join me in Myrtle Beach tomorrow! https://t.co/x4tF5kI2F6\n",
            "#TrumpTours We will make America great! This journey won't take us far but will unite us... https://t.co/gBZHlJY6U6\n",
            "“#IWantItAll” https://t.co/Xhm5YjUuAa\n",
            "Join me tonight in South Carolina: New Hampshire! This will be a fantastic evening. I look forward to being with all of you!\n",
            "My team and I will have lunch at Trump National Golf Club Washington DC at 7:00pm in advance of our arrival in Myrtle Beach. Tickets: https://t.co/k3hVzdGq2Q\n",
            "My very successful campaign has energized a new generation of Republican voters and made them more conservative than they have ever been before. https://t.co/3Cm5qW6pJq\n",
            "Join the Trump Revolution!  https://t.co/j5mNzq3eYp\n",
            "Join the Trump Revolution:  https://t.co/K9xWG0UZWp https://t.co/dB9WUW6p7P\n",
            "Thank you Washington D.C.! Join me in Myrtle Beach at 7pm: https://t.co/Vw0xhZzQZ5\n",
            "Join me tomorrow night at 7pm: https://t.co/4bVw9BkvDk\n",
            "Join the Trump Revolution:  https://t.co/lN2ZvGm3mR\n",
            "Join the Trump revolution:  https://t.co/zPmFbqPwEt\n",
            "Join me tomorrow night in Myrtle Beach: 7 pm: https://t.co/Qwq1Bp3Q4f\n",
            "My @foxandfriends interview discussing the @TrumpTours New York location of the New York Trump National Golf Club Washington D.C. and all of the events planned today! https://t.co/2F1tJkD3Kv\n",
            "Join the Trump Revolution: https://t.co/1Zl2MmN8L5\n",
            "The only choice is: #MakeAmericaGreatAgain🇺🇸 https://t.co/h2Tm9LKXhP\n",
            "Newt Gingrich today said that we have an \"unprecedented\" opportunity to get rid of an administration that's been so careless and dishonest...not good!\n",
            "Will be in Myrtle Beach South Carolina tonight! Watch on: https://t.co/v2KrJlYyVc\n",
            "Join the Trump Revolution: https://t.co\n",
            "\n",
            "[1300 | 2570.99] loss=2.69 avg=2.77\n",
            "[1301 | 2572.72] loss=2.39 avg=2.77\n",
            "[1302 | 2574.45] loss=2.33 avg=2.76\n",
            "[1303 | 2576.18] loss=3.06 avg=2.77\n",
            "[1304 | 2577.89] loss=2.64 avg=2.76\n",
            "[1305 | 2579.61] loss=1.93 avg=2.76\n",
            "[1306 | 2581.33] loss=2.63 avg=2.75\n",
            "[1307 | 2583.06] loss=2.85 avg=2.76\n",
            "[1308 | 2584.79] loss=2.57 avg=2.75\n",
            "[1309 | 2586.52] loss=2.69 avg=2.75\n",
            "[1310 | 2588.25] loss=3.13 avg=2.76\n",
            "[1311 | 2589.99] loss=2.50 avg=2.75\n",
            "[1312 | 2591.72] loss=2.89 avg=2.76\n",
            "[1313 | 2593.45] loss=2.86 avg=2.76\n",
            "[1314 | 2595.18] loss=2.94 avg=2.76\n",
            "[1315 | 2596.91] loss=2.94 avg=2.76\n",
            "[1316 | 2598.65] loss=2.89 avg=2.76\n",
            "[1317 | 2600.38] loss=2.50 avg=2.76\n",
            "[1318 | 2602.10] loss=2.25 avg=2.75\n",
            "[1319 | 2603.83] loss=2.92 avg=2.76\n",
            "[1320 | 2605.56] loss=3.02 avg=2.76\n",
            "[1321 | 2607.30] loss=2.56 avg=2.76\n",
            "[1322 | 2609.01] loss=2.44 avg=2.75\n",
            "[1323 | 2610.73] loss=2.40 avg=2.75\n",
            "[1324 | 2612.46] loss=3.00 avg=2.75\n",
            "[1325 | 2614.18] loss=2.81 avg=2.75\n",
            "[1326 | 2615.89] loss=2.96 avg=2.76\n",
            "[1327 | 2617.61] loss=2.73 avg=2.75\n",
            "[1328 | 2619.32] loss=2.19 avg=2.75\n",
            "[1329 | 2621.03] loss=2.32 avg=2.74\n",
            "[1330 | 2622.74] loss=2.36 avg=2.74\n",
            "[1331 | 2624.47] loss=2.83 avg=2.74\n",
            "[1332 | 2626.18] loss=3.09 avg=2.75\n",
            "[1333 | 2627.91] loss=3.23 avg=2.75\n",
            "[1334 | 2629.62] loss=3.07 avg=2.75\n",
            "[1335 | 2631.33] loss=2.36 avg=2.75\n",
            "[1336 | 2633.05] loss=2.58 avg=2.75\n",
            "[1337 | 2634.76] loss=2.78 avg=2.75\n",
            "[1338 | 2636.47] loss=2.95 avg=2.75\n",
            "[1339 | 2638.20] loss=2.64 avg=2.75\n",
            "[1340 | 2639.91] loss=2.51 avg=2.75\n",
            "[1341 | 2641.64] loss=2.48 avg=2.74\n",
            "[1342 | 2643.36] loss=2.54 avg=2.74\n",
            "[1343 | 2645.09] loss=2.86 avg=2.74\n",
            "[1344 | 2646.82] loss=2.96 avg=2.75\n",
            "[1345 | 2648.53] loss=2.75 avg=2.75\n",
            "[1346 | 2650.24] loss=2.47 avg=2.74\n",
            "[1347 | 2651.95] loss=2.47 avg=2.74\n",
            "[1348 | 2653.68] loss=3.17 avg=2.74\n",
            "[1349 | 2655.41] loss=2.61 avg=2.74\n",
            "[1350 | 2657.13] loss=2.84 avg=2.74\n",
            "[1351 | 2658.84] loss=2.87 avg=2.75\n",
            "[1352 | 2660.55] loss=2.63 avg=2.74\n",
            "[1353 | 2662.26] loss=2.24 avg=2.74\n",
            "[1354 | 2663.99] loss=2.63 avg=2.74\n",
            "[1355 | 2665.70] loss=2.93 avg=2.74\n",
            "[1356 | 2667.42] loss=2.38 avg=2.74\n",
            "[1357 | 2669.13] loss=2.61 avg=2.73\n",
            "[1358 | 2670.84] loss=2.85 avg=2.74\n",
            "[1359 | 2672.55] loss=2.74 avg=2.74\n",
            "[1360 | 2674.27] loss=2.64 avg=2.74\n",
            "[1361 | 2675.98] loss=2.95 avg=2.74\n",
            "[1362 | 2677.72] loss=2.69 avg=2.74\n",
            "[1363 | 2679.43] loss=2.77 avg=2.74\n",
            "[1364 | 2681.14] loss=3.08 avg=2.74\n",
            "[1365 | 2682.86] loss=2.83 avg=2.74\n",
            "[1366 | 2684.57] loss=2.62 avg=2.74\n",
            "[1367 | 2686.30] loss=2.53 avg=2.74\n",
            "[1368 | 2688.03] loss=2.74 avg=2.74\n",
            "[1369 | 2689.74] loss=2.39 avg=2.73\n",
            "[1370 | 2691.45] loss=2.69 avg=2.73\n",
            "[1371 | 2693.16] loss=3.03 avg=2.74\n",
            "[1372 | 2694.90] loss=2.46 avg=2.73\n",
            "[1373 | 2696.62] loss=3.16 avg=2.74\n",
            "[1374 | 2698.33] loss=2.51 avg=2.74\n",
            "[1375 | 2700.05] loss=2.64 avg=2.74\n",
            "[1376 | 2701.76] loss=2.75 avg=2.74\n",
            "[1377 | 2703.47] loss=3.11 avg=2.74\n",
            "[1378 | 2705.20] loss=2.92 avg=2.74\n",
            "[1379 | 2706.91] loss=2.74 avg=2.74\n",
            "[1380 | 2708.64] loss=2.45 avg=2.74\n",
            "[1381 | 2710.38] loss=2.80 avg=2.74\n",
            "[1382 | 2712.08] loss=2.47 avg=2.74\n",
            "[1383 | 2713.79] loss=2.62 avg=2.74\n",
            "[1384 | 2715.51] loss=3.11 avg=2.74\n",
            "[1385 | 2717.22] loss=2.53 avg=2.74\n",
            "[1386 | 2718.93] loss=2.31 avg=2.73\n",
            "[1387 | 2720.64] loss=2.47 avg=2.73\n",
            "[1388 | 2722.35] loss=2.17 avg=2.72\n",
            "[1389 | 2724.08] loss=2.59 avg=2.72\n",
            "[1390 | 2725.79] loss=2.58 avg=2.72\n",
            "[1391 | 2727.51] loss=2.81 avg=2.72\n",
            "[1392 | 2729.22] loss=2.92 avg=2.72\n",
            "[1393 | 2730.93] loss=2.54 avg=2.72\n",
            "[1394 | 2732.65] loss=2.83 avg=2.72\n",
            "[1395 | 2734.35] loss=2.71 avg=2.72\n",
            "[1396 | 2736.07] loss=2.62 avg=2.72\n",
            "[1397 | 2737.79] loss=2.62 avg=2.72\n",
            "[1398 | 2739.50] loss=2.80 avg=2.72\n",
            "[1399 | 2741.22] loss=2.76 avg=2.72\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "t  we need @WendyWilliams on @ApprenticeNBC!\n",
            "I'll be on @foxandfriends when it returns at 7:00. Enjoy!\n",
            "Thank you for your kind words during the Republican debate. Very friendly and nice.\n",
            "The real enemy is the person you can be. - John Wayne\n",
            "The Donald J. Trump Signature Collection returns this fall - center-collar shirt &amp; button-up sweater vests. #TrumpPants http://t.co/Hd0pV3Lg\n",
            "My father George is a wonderful guy who can be very tough. My mother is a great person &amp; great mother! She is a great leader.\n",
            "@KirstenPorter  She is wonderful and I love her! http://t.co/Xl3Q1Rq8\n",
            "@laurancarratt  She looks awesome! Also great choice of clothes. http://t.co/VXj7gxuY\n",
            "@carlos_vincerello  He was terrific!\n",
            "@paulcjwatson  He is wonderful!\n",
            "@hugo_kane  He is wonderful.  Thanks.\n",
            ".@LilJon is doing great work.  Great character.                      http://t.co/5mQjB3xo\n",
            "Lol but a lot of people think that I use @VanityFair to promote my book. No I use it to promote @TrumpDoral &amp; travel!\n",
            "I hope @VanityFair never apologizes to me.  What do you think I did just after they sent the letter.\n",
            ".@vanessays  Love the book! Good luck!\n",
            "I hope @VanityFair apologizes to me.  I won't apologize to their subscribers or customers.  The book is good!\n",
            "\"Donald Trump and the Art of the Deal\" is a brilliant and unique look at the life of Donald J. Trump http://t.co/JpBzfPpv\n",
            "\"Trump: 'The Art of the Deal' Is a Great Book\" http://t.co/2RJkIpQ9\n",
            "\"How a Business Deal Can Go Bad\" by Donald J. Trump III http://t.co/3cFfLfHX\n",
            "I'll be interviewed on @greta tonight at 9 PM. Enjoy! #CelebApprentice\n",
            "My speech today to the Palm Beach County Conservative Political Conference in Palm Beach FL: http://t.co/7Tj0U4dY\n",
            "My speech today to the Palm Beach County Conservative Political Conference: http://t.co/5LqnYjXw\n",
            "My speech today to the Palm Beach County Conservative Political Conference: http://t.co/QVxF9LsN\n",
            "My speech today to the Palm Beach County Conservative Political Conference: http://t.co/6QdQ6KXE\n",
            "My @foxandfriends interview discussing my @piersmorgan interview discussing my book TRUMP: A FERRYBACK &amp; MAHON: TOUGH ENOUGH  http://t.co/8W1BkU1Z\n",
            "I'm making $17M per year--I don't have to spend it at Mar-a-Lago.    http://t.co/6M6GkzQS\n",
            "My @foxandfriends interview with Chris Wallace discussing Trump @VanityFair book and Palm Beach's Conservative Political Conference http://t.co/HjbQQ1U7\n",
            "Trump's father: 'I used to go with my father at Mar-a-Lago' http://t.co/7RlqfztF Trump International Golf Links Miami: $100m renovations coming http://t.co/Q0dF6QZD\n",
            "My @CNBCWashington interview w/ @JohnHeilemann discussing Mar-a-Lago &amp; the upcoming interview with @KarlRove http://t.co/QbxIhZJl\n",
            "\"Donald Trump: You Made a Basket of Terrible Decisions\" http://t.co/X9xX6CcG From @NewYorkObserver by @HannahFournier: http://t.co/yU9P0GkX\n",
            "\"Donald Trump: When You Look Back At This Campaign It Will Look All Right\" http://t.co/rYXcUdMZ via @NewYorkObserver by @HannahFournier: http://t.co/X9F7BXZJ\n",
            "This campaign has been a disaster. I could go on for hours but I will not. It is very draining on my mind &amp; body. I love this campaign &\n",
            "\n",
            "[1400 | 2765.17] loss=2.48 avg=2.72\n",
            "[1401 | 2766.87] loss=2.39 avg=2.72\n",
            "[1402 | 2768.60] loss=2.41 avg=2.71\n",
            "[1403 | 2770.34] loss=2.98 avg=2.72\n",
            "[1404 | 2772.07] loss=2.72 avg=2.72\n",
            "[1405 | 2773.80] loss=2.83 avg=2.72\n",
            "[1406 | 2775.53] loss=2.50 avg=2.72\n",
            "[1407 | 2777.24] loss=2.74 avg=2.72\n",
            "[1408 | 2778.96] loss=2.41 avg=2.71\n",
            "[1409 | 2780.67] loss=2.94 avg=2.71\n",
            "[1410 | 2782.38] loss=2.71 avg=2.71\n",
            "[1411 | 2784.09] loss=2.55 avg=2.71\n",
            "[1412 | 2785.80] loss=2.55 avg=2.71\n",
            "[1413 | 2787.51] loss=2.65 avg=2.71\n",
            "[1414 | 2789.23] loss=3.00 avg=2.71\n",
            "[1415 | 2790.94] loss=2.14 avg=2.71\n",
            "[1416 | 2792.65] loss=2.78 avg=2.71\n",
            "[1417 | 2794.35] loss=2.23 avg=2.70\n",
            "[1418 | 2796.07] loss=2.58 avg=2.70\n",
            "[1419 | 2797.78] loss=2.44 avg=2.70\n",
            "[1420 | 2799.49] loss=2.73 avg=2.70\n",
            "[1421 | 2801.20] loss=2.98 avg=2.70\n",
            "[1422 | 2802.91] loss=2.64 avg=2.70\n",
            "[1423 | 2804.63] loss=2.97 avg=2.71\n",
            "[1424 | 2806.34] loss=3.12 avg=2.71\n",
            "[1425 | 2808.05] loss=2.85 avg=2.71\n",
            "[1426 | 2809.77] loss=2.85 avg=2.71\n",
            "[1427 | 2811.51] loss=2.66 avg=2.71\n",
            "[1428 | 2813.22] loss=2.63 avg=2.71\n",
            "[1429 | 2814.93] loss=2.36 avg=2.71\n",
            "[1430 | 2816.65] loss=2.35 avg=2.70\n",
            "[1431 | 2818.36] loss=2.80 avg=2.70\n",
            "[1432 | 2820.08] loss=2.74 avg=2.71\n",
            "[1433 | 2821.79] loss=3.05 avg=2.71\n",
            "[1434 | 2823.51] loss=2.52 avg=2.71\n",
            "[1435 | 2825.22] loss=2.41 avg=2.70\n",
            "[1436 | 2826.93] loss=2.37 avg=2.70\n",
            "[1437 | 2828.64] loss=2.71 avg=2.70\n",
            "[1438 | 2830.36] loss=2.40 avg=2.70\n",
            "[1439 | 2832.07] loss=2.50 avg=2.70\n",
            "[1440 | 2833.77] loss=3.27 avg=2.70\n",
            "[1441 | 2835.49] loss=2.40 avg=2.70\n",
            "[1442 | 2837.20] loss=2.82 avg=2.70\n",
            "[1443 | 2838.91] loss=2.59 avg=2.70\n",
            "[1444 | 2840.63] loss=1.99 avg=2.69\n",
            "[1445 | 2842.34] loss=2.68 avg=2.69\n",
            "[1446 | 2844.05] loss=2.44 avg=2.69\n",
            "[1447 | 2845.76] loss=2.45 avg=2.69\n",
            "[1448 | 2847.47] loss=2.46 avg=2.68\n",
            "[1449 | 2849.18] loss=2.26 avg=2.68\n",
            "[1450 | 2850.89] loss=2.63 avg=2.68\n",
            "[1451 | 2852.61] loss=2.56 avg=2.68\n",
            "[1452 | 2854.32] loss=3.02 avg=2.68\n",
            "[1453 | 2856.03] loss=3.10 avg=2.69\n",
            "[1454 | 2857.74] loss=2.38 avg=2.68\n",
            "[1455 | 2859.45] loss=2.77 avg=2.68\n",
            "[1456 | 2861.16] loss=2.43 avg=2.68\n",
            "[1457 | 2862.87] loss=2.60 avg=2.68\n",
            "[1458 | 2864.58] loss=3.05 avg=2.68\n",
            "[1459 | 2866.29] loss=2.85 avg=2.69\n",
            "[1460 | 2868.00] loss=2.51 avg=2.68\n",
            "[1461 | 2869.72] loss=2.39 avg=2.68\n",
            "[1462 | 2871.43] loss=2.54 avg=2.68\n",
            "[1463 | 2873.14] loss=2.82 avg=2.68\n",
            "[1464 | 2874.86] loss=2.85 avg=2.68\n",
            "[1465 | 2876.57] loss=2.82 avg=2.68\n",
            "[1466 | 2878.28] loss=2.78 avg=2.68\n",
            "[1467 | 2880.00] loss=2.15 avg=2.68\n",
            "[1468 | 2881.71] loss=2.12 avg=2.67\n",
            "[1469 | 2883.43] loss=2.47 avg=2.67\n",
            "[1470 | 2885.15] loss=2.60 avg=2.67\n",
            "[1471 | 2886.86] loss=2.08 avg=2.67\n",
            "[1472 | 2888.58] loss=2.92 avg=2.67\n",
            "[1473 | 2890.28] loss=2.24 avg=2.66\n",
            "[1474 | 2892.00] loss=2.60 avg=2.66\n",
            "[1475 | 2893.71] loss=2.88 avg=2.67\n",
            "[1476 | 2895.42] loss=3.12 avg=2.67\n",
            "[1477 | 2897.13] loss=3.07 avg=2.67\n",
            "[1478 | 2898.86] loss=3.01 avg=2.68\n",
            "[1479 | 2900.57] loss=2.74 avg=2.68\n",
            "[1480 | 2902.28] loss=2.15 avg=2.67\n",
            "[1481 | 2904.00] loss=2.82 avg=2.67\n",
            "[1482 | 2905.71] loss=2.93 avg=2.68\n",
            "[1483 | 2907.42] loss=2.77 avg=2.68\n",
            "[1484 | 2909.13] loss=2.23 avg=2.67\n",
            "[1485 | 2910.84] loss=1.49 avg=2.66\n",
            "[1486 | 2912.55] loss=2.81 avg=2.66\n",
            "[1487 | 2914.26] loss=3.07 avg=2.67\n",
            "[1488 | 2915.97] loss=2.81 avg=2.67\n",
            "[1489 | 2917.71] loss=2.67 avg=2.67\n",
            "[1490 | 2919.42] loss=2.31 avg=2.66\n",
            "[1491 | 2921.13] loss=2.69 avg=2.66\n",
            "[1492 | 2922.84] loss=2.55 avg=2.66\n",
            "[1493 | 2924.55] loss=2.66 avg=2.66\n",
            "[1494 | 2926.27] loss=2.64 avg=2.66\n",
            "[1495 | 2927.99] loss=3.09 avg=2.67\n",
            "[1496 | 2929.70] loss=2.05 avg=2.66\n",
            "[1497 | 2931.44] loss=2.99 avg=2.66\n",
            "[1498 | 2933.15] loss=2.97 avg=2.67\n",
            "[1499 | 2934.86] loss=2.49 avg=2.67\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " in the past couple decades is not the same as it was 40 years ago. — Donald J. Trump (@realDonaldTrump) September 21, 2014\n",
            ".@CNN said they were not talking to me. I said they couldn't be!\n",
            "Trump Tower in NYC - http://t.co/6U1N9QqZT6\n",
            ".@NYDailyNews’s @MaraGabbard says she’s willing to speak with me in order to protect her future. @NYDailyNews is so wrong. We won’t have many more debates.  @MaraGabbard\n",
            "The great news is that my poll numbers haven't dropped at all! I am a major media darling and a popular vote winner!\n",
            "Can you imagine if I lost the popular vote would have been a tie in the Electoral College. Also  all illegal votes cast - although many states voted illegally.\n",
            "I was told to get out of the race by Hillary and the Dems long before the Election &amp; there was NO collusion! So why didn't they start now?\n",
            "“Trump: Russia Colluded on Election” http://t.co/fTZrZT4H8U via @AP\n",
            "The Clinton campaign knew that Putin wanted Trump as their candidate but said nothing. Only when I said 'let’s’d have Russian help in the general election does it look bad.\n",
            "#SOTU #Trump2016 http://t.co/W4HqJpQgvB\n",
            ".@MarianneForDC I love your speech. Thank you! Let's have a great debate! #TrumpTucson\n",
            "It will be a beautiful day in Washington. Will the Democrats hold their nerve? #Trump2016\n",
            "Why isn?t Hillary Clinton indicted for her emails? The FBI just put together a massive new investigation into her server!\n",
            "Why doesn’t Hillary Clinton just release her server? The FBI is re-opening the whole thing, including she’s got nothing to hide!\n",
            "Great news! Hillary Clinton just testified under oath before the House Intel Committee. Not good! They deserve much better!\n",
            "The Federal Reserve just said something about #Apple #iPhone #Siri that has people very excited! What a coincidence!\n",
            ".@FoxNews is doing a very inaccurate report on me. They say I have been calling people \"baddies\".  Actually I am calling some of them “cool kids”!\n",
            ".@megynkelly is being treated terribly by the networks. She should take my call because it will be a really great debate!\n",
            ".@megynkelly is being treated very badly by the networks.  I am very happy she will be my first debate moderator. She has done well.\n",
            "Why does @megynkelly call me \"baddies\" in a debate and so much else in the MSM. She has my complete and total endorsement!\n",
            ".@megynkelly is done as a presidential candidate. She does not have the guts nor the stamina to do the job. She is a disaster. Sorry!\n",
            "Why aren’t the networks changing the title from the debate to the \"Republican debate\" - the biggest since I made the debate a party event?\n",
            "Why can’t @CNN just say I made a \"major announcement\" about a debate moderator? I'm not making that up!\n",
            "@greta @megynkelly is the worst on TV and I hope she takes the plunge and does a \"major announcement\" so people don't think I don't want to debate her!\n",
            "@Cher (@Cher) Thank you! #GOPDebate http://t.co/fIcTZqHJHj\n",
            "@DennisDowds @megynkelly @FoxNews  She is very biased.\n",
            "@Tupamarik @FoxNews  How long until the Dems get intel on me? I have nothing to hide.\n",
            "@pamela_j_van_Riper @GOP debate    Great interview great fun!\n",
            "@barnabas_h                 \"@realDonaldTrump  True. Thanks. Thank you for talking with us. I agree with your decision!\" Thanks!\n",
            "After the big poll the Dems just got an advance copy of the Wall Street Journal which is why they are trying so hard.\n",
            ".@NBC has lost all credibility. They only want to go after you for ratings. They don't want to get tough and nasty. Don't be afraid!\n",
            ".@NBC just got an advance copy of the Wall Street Journal. This should not happen to anyone. Get smart!\n",
            "@d_blu                    \"@realDonaldTrump   Thank you. Thanks. Thanks. @FoxNews is great for your great comments\n",
            "\n",
            "[1500 | 2958.63] loss=2.68 avg=2.67\n",
            "[1501 | 2960.34] loss=2.43 avg=2.66\n",
            "[1502 | 2962.07] loss=2.40 avg=2.66\n",
            "[1503 | 2963.78] loss=3.11 avg=2.67\n",
            "[1504 | 2965.50] loss=2.73 avg=2.67\n",
            "[1505 | 2967.21] loss=2.81 avg=2.67\n",
            "[1506 | 2968.92] loss=2.85 avg=2.67\n",
            "[1507 | 2970.63] loss=2.74 avg=2.67\n",
            "[1508 | 2972.34] loss=3.10 avg=2.67\n",
            "[1509 | 2974.05] loss=2.85 avg=2.68\n",
            "[1510 | 2975.77] loss=2.96 avg=2.68\n",
            "[1511 | 2977.50] loss=2.53 avg=2.68\n",
            "[1512 | 2979.20] loss=2.98 avg=2.68\n",
            "[1513 | 2980.92] loss=2.86 avg=2.68\n",
            "[1514 | 2982.63] loss=2.57 avg=2.68\n",
            "[1515 | 2984.36] loss=2.31 avg=2.68\n",
            "[1516 | 2986.07] loss=3.09 avg=2.68\n",
            "[1517 | 2987.78] loss=2.65 avg=2.68\n",
            "[1518 | 2989.50] loss=2.57 avg=2.68\n",
            "[1519 | 2991.20] loss=2.66 avg=2.68\n",
            "[1520 | 2992.91] loss=2.50 avg=2.68\n",
            "[1521 | 2994.63] loss=2.57 avg=2.68\n",
            "[1522 | 2996.33] loss=2.48 avg=2.67\n",
            "[1523 | 2998.05] loss=2.94 avg=2.68\n",
            "[1524 | 2999.76] loss=2.67 avg=2.68\n",
            "[1525 | 3001.46] loss=2.57 avg=2.68\n",
            "[1526 | 3003.18] loss=2.45 avg=2.67\n",
            "[1527 | 3004.89] loss=2.85 avg=2.68\n",
            "[1528 | 3006.61] loss=2.96 avg=2.68\n",
            "[1529 | 3008.32] loss=2.65 avg=2.68\n",
            "[1530 | 3010.03] loss=3.04 avg=2.68\n",
            "[1531 | 3011.75] loss=3.13 avg=2.69\n",
            "[1532 | 3013.46] loss=2.54 avg=2.69\n",
            "[1533 | 3015.18] loss=2.33 avg=2.68\n",
            "[1534 | 3016.89] loss=3.13 avg=2.69\n",
            "[1535 | 3018.60] loss=2.64 avg=2.69\n",
            "[1536 | 3020.32] loss=2.31 avg=2.68\n",
            "[1537 | 3022.04] loss=2.96 avg=2.68\n",
            "[1538 | 3023.75] loss=2.27 avg=2.68\n",
            "[1539 | 3025.47] loss=2.60 avg=2.68\n",
            "[1540 | 3027.18] loss=3.13 avg=2.68\n",
            "[1541 | 3028.89] loss=2.70 avg=2.68\n",
            "[1542 | 3030.60] loss=2.25 avg=2.68\n",
            "[1543 | 3032.32] loss=3.08 avg=2.68\n",
            "[1544 | 3034.03] loss=1.35 avg=2.67\n",
            "[1545 | 3035.76] loss=2.45 avg=2.67\n",
            "[1546 | 3037.47] loss=2.67 avg=2.67\n",
            "[1547 | 3039.18] loss=2.74 avg=2.67\n",
            "[1548 | 3040.88] loss=2.62 avg=2.67\n",
            "[1549 | 3042.60] loss=2.38 avg=2.67\n",
            "[1550 | 3044.31] loss=2.78 avg=2.67\n",
            "[1551 | 3046.02] loss=2.87 avg=2.67\n",
            "[1552 | 3047.73] loss=2.57 avg=2.67\n",
            "[1553 | 3049.44] loss=2.71 avg=2.67\n",
            "[1554 | 3051.16] loss=2.51 avg=2.67\n",
            "[1555 | 3052.87] loss=2.84 avg=2.67\n",
            "[1556 | 3054.58] loss=2.36 avg=2.67\n",
            "[1557 | 3056.29] loss=2.42 avg=2.66\n",
            "[1558 | 3058.01] loss=2.78 avg=2.66\n",
            "[1559 | 3059.72] loss=2.45 avg=2.66\n",
            "[1560 | 3061.43] loss=1.94 avg=2.65\n",
            "[1561 | 3063.15] loss=3.08 avg=2.66\n",
            "[1562 | 3064.86] loss=2.92 avg=2.66\n",
            "[1563 | 3066.57] loss=2.63 avg=2.66\n",
            "[1564 | 3068.28] loss=3.11 avg=2.67\n",
            "[1565 | 3069.99] loss=2.71 avg=2.67\n",
            "[1566 | 3071.70] loss=2.13 avg=2.66\n",
            "[1567 | 3073.42] loss=2.54 avg=2.66\n",
            "[1568 | 3075.13] loss=2.16 avg=2.65\n",
            "[1569 | 3076.85] loss=2.43 avg=2.65\n",
            "[1570 | 3078.56] loss=2.73 avg=2.65\n",
            "[1571 | 3080.27] loss=2.34 avg=2.65\n",
            "[1572 | 3081.98] loss=2.72 avg=2.65\n",
            "[1573 | 3083.70] loss=2.55 avg=2.65\n",
            "[1574 | 3085.42] loss=2.80 avg=2.65\n",
            "[1575 | 3087.14] loss=2.84 avg=2.65\n",
            "[1576 | 3088.85] loss=2.74 avg=2.65\n",
            "[1577 | 3090.56] loss=2.44 avg=2.65\n",
            "[1578 | 3092.28] loss=2.70 avg=2.65\n",
            "[1579 | 3093.99] loss=2.17 avg=2.65\n",
            "[1580 | 3095.70] loss=2.82 avg=2.65\n",
            "[1581 | 3097.42] loss=2.47 avg=2.65\n",
            "[1582 | 3099.13] loss=2.87 avg=2.65\n",
            "[1583 | 3100.84] loss=3.03 avg=2.65\n",
            "[1584 | 3102.55] loss=2.75 avg=2.65\n",
            "[1585 | 3104.26] loss=3.48 avg=2.66\n",
            "[1586 | 3105.97] loss=2.58 avg=2.66\n",
            "[1587 | 3107.69] loss=2.98 avg=2.67\n",
            "[1588 | 3109.40] loss=2.59 avg=2.66\n",
            "[1589 | 3111.11] loss=2.53 avg=2.66\n",
            "[1590 | 3112.83] loss=2.33 avg=2.66\n",
            "[1591 | 3114.54] loss=2.67 avg=2.66\n",
            "[1592 | 3116.25] loss=2.90 avg=2.66\n",
            "[1593 | 3117.96] loss=2.73 avg=2.66\n",
            "[1594 | 3119.67] loss=2.82 avg=2.66\n",
            "[1595 | 3121.38] loss=2.33 avg=2.66\n",
            "[1596 | 3123.09] loss=2.82 avg=2.66\n",
            "[1597 | 3124.81] loss=2.21 avg=2.66\n",
            "[1598 | 3126.52] loss=2.36 avg=2.66\n",
            "[1599 | 3128.23] loss=2.51 avg=2.65\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "! You and I have had great success together. Congratulations on building your business on an A++ basis. https://t.co/jL0Rtj1QyW\n",
            "Great news out of China concerning my decision to not attend the Ritz Carlton reception which had to be cancelled given the terrible safety &amp; security situation at this year's Ritz Carlton reception. Thanks. https://t.co/UxQWj6WKgC\n",
            "The World Cup soccer games have been held in the United States in such bad shape! Not one major country in the world has been able to produce a soccer game of this magnitude. Great damage - far fewer people in stadiums!\n",
            "Great news from South Korea. Very proud of your leaders. Our country is going to work with you as best as we can to support a strong and prosperous North Korea!\n",
            "So many Americans have suffered under Obama - our country is in such trouble. Just had a very hard day at the office!\n",
            "I will be interviewed by @EricBranson now at a news conference in Pennsylvania.  @EricBranson is the owner of the @NYDailyNews.\n",
            "People are being shot in California at a very high level; and the death toll in San Bernardino continues to rise. Many terrorists would love to do this. Will the media pay attention? Also a lot of bad things going on!\n",
            "Just heard that @nytimes newsroom with its total lack of respect for my country is being sued by a couple at what should be a very negotiated settlement. U.S.A. is a very different story!\n",
            ".@brithumeaux12 is a fantastic woman and has done a fantastic job in representing her on this. @NYDailyNews thanks!\n",
            ".@NYDailyNews is a very talented paper with a great future ahead. Just called and wish them a happy and healthy life!\n",
            ".@NYDailyNews offers me my very first opportunity to do the #NewYearsEve with my family in a long time. A great thing for both of us. https://t.co/X2V9JqNhQF\n",
            "Thank you to all of the people that have been so well wishing my friends and family at the @Ritz Carlton &amp; Mar-a-Lago. I am going to a great hotel today!\n",
            "As we have had multiple days of snow in California a lot of people are now in a really bad state of mind and some people are even losing the ability to see - not good - things. Not good but not as bad!\n",
            "Snow in Northern California with possibly more to come in Southern and Central California. Will be a much tougher winter than this. All people with vision problems please stay away from vehicles.\n",
            "\"Do your own homework. That's how I learned what to avoid.\" -- Albert Einstein\n",
            "When does the \"Trump effect\" start to show up and who takes the lead? https://t.co/xV8Y0QiLf6\n",
            "So many people feel very comfortable and entitled that many are leaving the U.S. without saying goodbye. It's so sad!\n",
            "The reason the U.S. spent 3.7 TRILLION DOLLARS during Obama is not because of the jobs. It's because the American people want jobs.\n",
            "We are losing our industrial base to China. We must get our act together quickly!\n",
            "My daughter Ivanka was named one of Time Magazine 'Man of The Year' winners. Congratulations! https://t.co/tAo6aS1bZt\n",
            "We are at war with radical Islamic terrorism and we must totally destroy the evil that has taken over large parts of the Islamic world. This is a global emergency!\n",
            "Great speech last night in the South Carolina Tea Party Convention - thanks. https://t.co/gCxwYqyIz2\n",
            "Big news today - the White House said it is ending the practice of briefing the media on classified matters. So much for briefing!\n",
            "What will be the next big story out of Washington if I am not the POTUS? That would be the horrible and totally illegal Russia story with phony collusion by the Democrats and the Fake Dossier used by the Democrats, for the sake of \"political\" purposes!\n",
            "Very disappointed in my friend @VicMcMahon--He would have won a lot of the Republican Primary debates. He is very weak on crime weak on borders and big for the USA!\n",
            "The United States has taken in more people than in the entire Vietnam War!\n",
            "Thank you so much - it meant a lot to me. https://t.co/tQH9LJqg5b\n",
            "Congratulations to @the_beast on his re-election as Chair of FITN! We will be very proud of him and his family.\n",
            "Congrats to Mark Levin for his very courageous decision to step forward to share his heart and mind on the need for Trump reform as a President. We\n",
            "\n",
            "[1600 | 3152.28] loss=2.85 avg=2.66\n",
            "[1601 | 3153.98] loss=2.57 avg=2.65\n",
            "[1602 | 3155.70] loss=2.46 avg=2.65\n",
            "[1603 | 3157.42] loss=2.29 avg=2.65\n",
            "[1604 | 3159.13] loss=2.00 avg=2.64\n",
            "[1605 | 3160.85] loss=2.89 avg=2.65\n",
            "[1606 | 3162.56] loss=2.23 avg=2.64\n",
            "[1607 | 3164.27] loss=2.70 avg=2.64\n",
            "[1608 | 3165.98] loss=2.49 avg=2.64\n",
            "[1609 | 3167.70] loss=2.60 avg=2.64\n",
            "[1610 | 3169.40] loss=2.61 avg=2.64\n",
            "[1611 | 3171.12] loss=2.41 avg=2.64\n",
            "[1612 | 3172.82] loss=2.38 avg=2.63\n",
            "[1613 | 3174.53] loss=3.32 avg=2.64\n",
            "[1614 | 3176.25] loss=1.56 avg=2.63\n",
            "[1615 | 3177.96] loss=2.77 avg=2.63\n",
            "[1616 | 3179.67] loss=2.86 avg=2.63\n",
            "[1617 | 3181.38] loss=2.81 avg=2.64\n",
            "[1618 | 3183.09] loss=1.98 avg=2.63\n",
            "[1619 | 3184.80] loss=2.48 avg=2.63\n",
            "[1620 | 3186.52] loss=2.63 avg=2.63\n",
            "[1621 | 3188.23] loss=2.91 avg=2.63\n",
            "[1622 | 3189.94] loss=2.86 avg=2.63\n",
            "[1623 | 3191.65] loss=2.57 avg=2.63\n",
            "[1624 | 3193.37] loss=3.03 avg=2.64\n",
            "[1625 | 3195.08] loss=2.51 avg=2.64\n",
            "[1626 | 3196.79] loss=2.70 avg=2.64\n",
            "[1627 | 3198.50] loss=2.84 avg=2.64\n",
            "[1628 | 3200.21] loss=3.03 avg=2.64\n",
            "[1629 | 3201.92] loss=2.31 avg=2.64\n",
            "[1630 | 3203.64] loss=2.77 avg=2.64\n",
            "[1631 | 3205.35] loss=3.09 avg=2.64\n",
            "[1632 | 3207.06] loss=3.05 avg=2.65\n",
            "[1633 | 3208.77] loss=2.75 avg=2.65\n",
            "[1634 | 3210.48] loss=2.74 avg=2.65\n",
            "[1635 | 3212.20] loss=2.25 avg=2.65\n",
            "[1636 | 3213.93] loss=2.70 avg=2.65\n",
            "[1637 | 3215.64] loss=2.70 avg=2.65\n",
            "[1638 | 3217.35] loss=3.09 avg=2.65\n",
            "[1639 | 3219.07] loss=2.61 avg=2.65\n",
            "[1640 | 3220.78] loss=2.64 avg=2.65\n",
            "[1641 | 3222.50] loss=3.11 avg=2.66\n",
            "[1642 | 3224.21] loss=2.37 avg=2.65\n",
            "[1643 | 3225.93] loss=2.27 avg=2.65\n",
            "[1644 | 3227.64] loss=2.30 avg=2.65\n",
            "[1645 | 3229.35] loss=2.55 avg=2.64\n",
            "[1646 | 3231.07] loss=2.06 avg=2.64\n",
            "[1647 | 3232.78] loss=2.61 avg=2.64\n",
            "[1648 | 3234.51] loss=2.78 avg=2.64\n",
            "[1649 | 3236.22] loss=2.99 avg=2.64\n",
            "[1650 | 3237.93] loss=2.55 avg=2.64\n",
            "[1651 | 3239.65] loss=2.69 avg=2.64\n",
            "[1652 | 3241.36] loss=3.03 avg=2.65\n",
            "[1653 | 3243.07] loss=2.58 avg=2.65\n",
            "[1654 | 3244.78] loss=2.57 avg=2.65\n",
            "[1655 | 3246.50] loss=2.78 avg=2.65\n",
            "[1656 | 3248.21] loss=3.18 avg=2.65\n",
            "[1657 | 3249.92] loss=2.45 avg=2.65\n",
            "[1658 | 3251.63] loss=2.76 avg=2.65\n",
            "[1659 | 3253.34] loss=2.95 avg=2.65\n",
            "[1660 | 3255.05] loss=2.34 avg=2.65\n",
            "[1661 | 3256.76] loss=2.56 avg=2.65\n",
            "[1662 | 3258.47] loss=2.62 avg=2.65\n",
            "[1663 | 3260.18] loss=2.91 avg=2.65\n",
            "[1664 | 3261.88] loss=2.90 avg=2.65\n",
            "[1665 | 3263.59] loss=3.12 avg=2.66\n",
            "[1666 | 3265.31] loss=3.11 avg=2.66\n",
            "[1667 | 3267.02] loss=2.52 avg=2.66\n",
            "[1668 | 3268.72] loss=2.16 avg=2.66\n",
            "[1669 | 3270.44] loss=2.63 avg=2.66\n",
            "[1670 | 3272.17] loss=2.83 avg=2.66\n",
            "[1671 | 3273.88] loss=2.66 avg=2.66\n",
            "[1672 | 3275.59] loss=2.69 avg=2.66\n",
            "[1673 | 3277.30] loss=2.28 avg=2.66\n",
            "[1674 | 3279.02] loss=3.27 avg=2.66\n",
            "[1675 | 3280.73] loss=2.60 avg=2.66\n",
            "[1676 | 3282.45] loss=2.23 avg=2.66\n",
            "[1677 | 3284.16] loss=2.53 avg=2.66\n",
            "[1678 | 3285.88] loss=2.07 avg=2.65\n",
            "[1679 | 3287.59] loss=2.71 avg=2.65\n",
            "[1680 | 3289.31] loss=2.66 avg=2.65\n",
            "[1681 | 3291.02] loss=2.76 avg=2.65\n",
            "[1682 | 3292.75] loss=3.13 avg=2.66\n",
            "[1683 | 3294.45] loss=2.39 avg=2.65\n",
            "[1684 | 3296.17] loss=2.98 avg=2.66\n",
            "[1685 | 3297.88] loss=1.94 avg=2.65\n",
            "[1686 | 3299.59] loss=2.75 avg=2.65\n",
            "[1687 | 3301.30] loss=2.70 avg=2.65\n",
            "[1688 | 3303.01] loss=2.94 avg=2.65\n",
            "[1689 | 3304.73] loss=2.34 avg=2.65\n",
            "[1690 | 3306.44] loss=2.81 avg=2.65\n",
            "[1691 | 3308.15] loss=2.81 avg=2.65\n",
            "[1692 | 3309.85] loss=2.72 avg=2.65\n",
            "[1693 | 3311.58] loss=2.68 avg=2.65\n",
            "[1694 | 3313.29] loss=2.71 avg=2.66\n",
            "[1695 | 3315.00] loss=2.55 avg=2.65\n",
            "[1696 | 3316.71] loss=2.67 avg=2.65\n",
            "[1697 | 3318.42] loss=2.54 avg=2.65\n",
            "[1698 | 3320.14] loss=2.50 avg=2.65\n",
            "[1699 | 3321.84] loss=2.86 avg=2.65\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " It could happen to you!\n",
            "Today in the West Wing was a great day for our country.\n",
            "Today it was my great honor to welcome President Xi of China to the @WhiteHouse. https://t.co/3g9uCbM0mX https://t.co/mU1q4F9GpJ\n",
            "Congratulations to @SenTedCruz on being the Republican Nominee @GOP Nominee @RNC convention &amp; taking on @MittRomney- and @BarackObama. We will all be better for this victory! https://t.co/J6hG4cG2B3\n",
            ".@RepKarenTandy I have known you &amp; your family for so many years. You are an amazing woman who never lets you down!\n",
            "Tonight at 10 P.M. we will make a historic choice in November. Together we will rebuild our economy and our future for all Americans. Together we will MAKE AMERICA GREAT AGAIN!\n",
            "The new White House Situation Room shows the great progress that @Cabinet is making. This will be a very productive Presidency!\n",
            "Congratulations to our new Attorney General @JennyBethSanders! I will be working with her all night to support her effort for a more fair and just society.\n",
            "Thank you @JennyBethSanders – for your kind words. Watch this space for news and analysis...\n",
            "Just spoke with my friend and Senator @TedCruz. I'm grateful for his support.\n",
            "I applaud President Obama and Nancy Pelosi for choosing the great men who will be doing a fabulous job at the Executive Branch.\n",
            "I also applaud Chairman Graham and Nancy Pelosi for choosing the talented leadership of Chairman Mulvaney as his Deputy. These are the two great Senators who will help MAKE AMERICA GREAT AGAIN!\n",
            "Just arrived at Andrews Air Force Base @ Andrews for a beautiful ceremony with the great men who will be doing a fantastic job for us. https://t.co/KdXfXvQ2y6 https://t.co/3Lq4LvjQZo\n",
            "@kimjfreed  @realDonaldTrump  Thank you for your nice words about this great man &amp; all of his success. He is a truly great person. I will be there much longer than usual soon...\n",
            "Thank you @KatyTurNBC! See you soon.\n",
            ".@katecraig824 @foxnews   As expected I totally agree with your assessment.\n",
            "@katecraig824 @FoxNews  We did not work out a deal on health care.\n",
            ".@foxandfriends in 5 minutes- a special. Enjoy.\n",
            "“Trump's Twitter account is an instant success: In a month he has gained nearly 4 million followers by sharing his thoughts” http://t.co/oM6zKkZh0G via @DailyBeast\n",
            ".@dennisrodman  Dennis, thanks for the beautiful compliment last night. You have never been so right about me. #Trump2016\n",
            "@sophie_yikes  Sophie is a great reporter. Really good reporter.\n",
            "“I was actually quite impressed with the coverage by this reporter. He went out of his way to say what was true and what wasn't about the President” http://t.co/2iKQJYqQoJ  Via @BreitbartNews\n",
            "“There were a great many of the same stories from the previous administration about people involved in the Trump administration” http://t.co/v2UJK3dqE4 via @Newsmax_Media #DailyBeast\n",
            "Via @dailycaller by @mboyle1: “Trump to @KatyTurNBC: ‘I’m Your Champion’ ‘I’ll Keep Your Dreams Alive’ http://t.co/6lY3Pf2U1Y\n",
            ".@TheBrodyFile Thanks--really exciting.\n",
            "Great to meet all in New York tonight. Great future ahead.\n",
            "Today @BarackObama had a very bad day:  He lost another job and went to Cuba.  Now Obama wants to sell it off...http://t.co/rWJ9wqx6jU\n",
            "I can't believe Hillary just gave her speech to a group of people for two hours. She didn't even say a word. I thought she was being very nice...\n",
            "Just arrived at Trump Tower--just outside! Time for our #MakeAmericaGreatAgain Rally! https://t.co/5V8vzm5v8P https://t.co/9mvOd0vK8p\n",
            ".@billmaher’s latest @NBC interview where he states that \"President Obama\" is the problem in a more moderate manner is unfair and false. This will work--but not at the expense of\n",
            "\n",
            "[1700 | 3345.74] loss=2.56 avg=2.65\n",
            "[1701 | 3347.45] loss=2.75 avg=2.65\n",
            "[1702 | 3349.16] loss=2.98 avg=2.66\n",
            "[1703 | 3350.88] loss=2.60 avg=2.66\n",
            "[1704 | 3352.59] loss=2.39 avg=2.65\n",
            "[1705 | 3354.30] loss=2.60 avg=2.65\n",
            "[1706 | 3356.02] loss=2.72 avg=2.65\n",
            "[1707 | 3357.73] loss=2.33 avg=2.65\n",
            "[1708 | 3359.45] loss=2.76 avg=2.65\n",
            "[1709 | 3361.16] loss=2.61 avg=2.65\n",
            "[1710 | 3362.88] loss=2.81 avg=2.65\n",
            "[1711 | 3364.59] loss=2.75 avg=2.65\n",
            "[1712 | 3366.30] loss=2.62 avg=2.65\n",
            "[1713 | 3368.02] loss=1.66 avg=2.64\n",
            "[1714 | 3369.73] loss=2.73 avg=2.64\n",
            "[1715 | 3371.44] loss=2.98 avg=2.65\n",
            "[1716 | 3373.15] loss=2.63 avg=2.65\n",
            "[1717 | 3374.87] loss=2.48 avg=2.65\n",
            "[1718 | 3376.57] loss=2.87 avg=2.65\n",
            "[1719 | 3378.29] loss=2.39 avg=2.65\n",
            "[1720 | 3380.00] loss=2.81 avg=2.65\n",
            "[1721 | 3381.71] loss=2.21 avg=2.64\n",
            "[1722 | 3383.42] loss=2.77 avg=2.64\n",
            "[1723 | 3385.13] loss=2.66 avg=2.64\n",
            "[1724 | 3386.84] loss=2.55 avg=2.64\n",
            "[1725 | 3388.55] loss=2.51 avg=2.64\n",
            "[1726 | 3390.27] loss=2.64 avg=2.64\n",
            "[1727 | 3391.98] loss=2.09 avg=2.64\n",
            "[1728 | 3393.69] loss=2.06 avg=2.63\n",
            "[1729 | 3395.40] loss=2.40 avg=2.63\n",
            "[1730 | 3397.11] loss=2.77 avg=2.63\n",
            "[1731 | 3398.82] loss=2.50 avg=2.63\n",
            "[1732 | 3400.53] loss=2.86 avg=2.63\n",
            "[1733 | 3402.25] loss=2.86 avg=2.63\n",
            "[1734 | 3403.95] loss=2.77 avg=2.63\n",
            "[1735 | 3405.67] loss=2.72 avg=2.64\n",
            "[1736 | 3407.38] loss=2.34 avg=2.63\n",
            "[1737 | 3409.10] loss=2.59 avg=2.63\n",
            "[1738 | 3410.81] loss=2.83 avg=2.63\n",
            "[1739 | 3412.52] loss=3.27 avg=2.64\n",
            "[1740 | 3414.24] loss=2.73 avg=2.64\n",
            "[1741 | 3415.95] loss=2.63 avg=2.64\n",
            "[1742 | 3417.66] loss=2.73 avg=2.64\n",
            "[1743 | 3419.37] loss=2.84 avg=2.64\n",
            "[1744 | 3421.09] loss=2.14 avg=2.64\n",
            "[1745 | 3422.81] loss=2.98 avg=2.64\n",
            "[1746 | 3424.52] loss=2.63 avg=2.64\n",
            "[1747 | 3426.23] loss=2.61 avg=2.64\n",
            "[1748 | 3427.95] loss=3.28 avg=2.65\n",
            "[1749 | 3429.66] loss=2.55 avg=2.65\n",
            "[1750 | 3431.38] loss=2.03 avg=2.64\n",
            "[1751 | 3433.09] loss=1.99 avg=2.63\n",
            "[1752 | 3434.81] loss=2.68 avg=2.64\n",
            "[1753 | 3436.53] loss=2.61 avg=2.63\n",
            "[1754 | 3438.23] loss=2.94 avg=2.64\n",
            "[1755 | 3439.95] loss=3.05 avg=2.64\n",
            "[1756 | 3441.65] loss=2.80 avg=2.64\n",
            "[1757 | 3443.37] loss=2.23 avg=2.64\n",
            "[1758 | 3445.08] loss=3.17 avg=2.64\n",
            "[1759 | 3446.81] loss=2.27 avg=2.64\n",
            "[1760 | 3448.52] loss=2.69 avg=2.64\n",
            "[1761 | 3450.23] loss=2.63 avg=2.64\n",
            "[1762 | 3451.94] loss=2.98 avg=2.64\n",
            "[1763 | 3453.66] loss=2.30 avg=2.64\n",
            "[1764 | 3455.37] loss=2.91 avg=2.64\n",
            "[1765 | 3457.08] loss=2.66 avg=2.64\n",
            "[1766 | 3458.79] loss=3.24 avg=2.65\n",
            "[1767 | 3460.50] loss=2.65 avg=2.65\n",
            "[1768 | 3462.22] loss=2.42 avg=2.65\n",
            "[1769 | 3463.93] loss=2.96 avg=2.65\n",
            "[1770 | 3465.64] loss=2.82 avg=2.65\n",
            "[1771 | 3467.35] loss=2.76 avg=2.65\n",
            "[1772 | 3469.06] loss=2.54 avg=2.65\n",
            "[1773 | 3470.78] loss=2.18 avg=2.65\n",
            "[1774 | 3472.49] loss=2.29 avg=2.64\n",
            "[1775 | 3474.20] loss=2.66 avg=2.64\n",
            "[1776 | 3475.91] loss=2.96 avg=2.65\n",
            "[1777 | 3477.63] loss=1.88 avg=2.64\n",
            "[1778 | 3479.34] loss=2.46 avg=2.64\n",
            "[1779 | 3481.05] loss=3.08 avg=2.64\n",
            "[1780 | 3482.77] loss=2.59 avg=2.64\n",
            "[1781 | 3484.48] loss=3.42 avg=2.65\n",
            "[1782 | 3486.19] loss=3.34 avg=2.66\n",
            "[1783 | 3487.91] loss=3.44 avg=2.66\n",
            "[1784 | 3489.62] loss=2.89 avg=2.67\n",
            "[1785 | 3491.34] loss=2.43 avg=2.66\n",
            "[1786 | 3493.05] loss=2.37 avg=2.66\n",
            "[1787 | 3494.76] loss=2.30 avg=2.66\n",
            "[1788 | 3496.48] loss=2.65 avg=2.66\n",
            "[1789 | 3498.20] loss=3.11 avg=2.66\n",
            "[1790 | 3499.90] loss=3.06 avg=2.67\n",
            "[1791 | 3501.62] loss=2.38 avg=2.66\n",
            "[1792 | 3503.33] loss=2.84 avg=2.67\n",
            "[1793 | 3505.04] loss=2.40 avg=2.66\n",
            "[1794 | 3506.76] loss=2.50 avg=2.66\n",
            "[1795 | 3508.46] loss=2.65 avg=2.66\n",
            "[1796 | 3510.18] loss=2.77 avg=2.66\n",
            "[1797 | 3511.89] loss=2.55 avg=2.66\n",
            "[1798 | 3513.60] loss=2.66 avg=2.66\n",
            "[1799 | 3515.32] loss=3.12 avg=2.67\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "; the media is just too corrupt for anyone's taste.  No wonder so many people love @seanhannity. He goes down in history as the worst show in media history (and the beginning).\n",
            "The #FakeNews Media is the enemy &amp; this will be the biggest election story in television history. If media was honest it would be totally out of control. Many stories are made up and inaccurate. Sad when you think it.\n",
            "This is why you need good journalism! You need to hear who's giving money &amp; what they are saying about you. Keep your head down and hope you're right!\n",
            "The failing @NYTimes should have a new managing editor. I will never again use its old name as they publish so much false reporting...they are failing badly!\n",
            "Many people see the #CrookedHillary investigation as a big deal. The @NYTimes is far more concerned with oblivion &amp; destruction. They have the worst reporters in politics!\n",
            "The media is totally biased against Crooked Hillary Clinton-she does a poor job of getting interviews (cont) http://t.co/v6Vh7rIb0x\n",
            "@NYDailyNews is not real in many ways. Its editorial stance is totally one-sided. There is no bias when you take the facts into account. It's @NewYorkDailyNews.\n",
            "The @NYDailyNews will be going out of business-it is a horrible &amp; pathetic piece. It's a disgrace-not good for health or property\n",
            ".@NewYorkDailyNews will be sold soon in the face of poor and very hostile press.\n",
            ".@NewYorkDailyNews has lost its way &amp; needs a change. I’m pleased to see it’s demise. Too much bias &amp; very few new pieces!\n",
            "...must be stopped!\n",
            "The Democrats should focus on getting REPUBLICANS to the polls in order to keep the country safe &amp; to keep it FREE!\n",
            "I hope the press keeps reporting that I support universal background checks. This is an important first step to keeping guns out of the hands of the criminals!\n",
            "The Crooked Hillary investigation &amp; the #BagJob are both an excuse for her terrible record of corruption &amp; abuse. No surprise!\n",
            "Crooked Hillary has become so politically incorrect. She’s been caught laughing on at least seven separate occasions! She should be disqualified from politics!\n",
            "Wow I just learned that Crooked Hillary has a total 'black book' of corruption https://t.co/y4Zcxqyj3P\n",
            "I will be on @seanhannity tonight at 10:00 - 7:00 PM. It's going to be a great night!\n",
            "Greatest Witch Hunt in US history!\n",
            "I am delighted that the US Marshals office in Charlotte NC have located the body of my friend and colleague Jack the Ripper--I will be there soon!\n",
            "We will not allow the Witch Hunt to change with the election result!\n",
            "Will be on @TeamCavuto tonight at 10:45pm at 8:45 PM and it will be terrific.\n",
            "@seanhannity just won a new rating among 18-49 on TV (thanks Sean!) by a full 0.8.\n",
            ".@NYNewsday must be taken down to hell by their readers! A new one is planned in 5-6 weeks!\n",
            "I am pleased to announce that @NYDailyNews will be sold soon and another one will be started soon - they are in huge trouble. No more!\n",
            ".@NYDailyNews will be gone in 5-6 weeks - too much for readers - and that’s coming.\n",
            ".@NYDailyNews is being sold by @NYDailyNews owner Andrew Puzder. Can’t beat it for coverage. No more!\n",
            ".@NYDailyNews has to be completely sold to save itself - bad coverage--and the name is terrible.  Very sad.\n",
            "The Fake News is getting worse by the day. The only ones who can save it are the winners of the Apprentice!\n",
            "#TrumpVlog #POTUS2015 #POTUS2016  https://t.co/QVrOa0rUJl\n",
            "I hope Crooked Hillary gets caught. The FBI will never give up her 33,000 e-mails nor will she be charged - or even convicted.\n",
            "I was the one who started a petition against @NYDailyNews &amp; will be the one who ends  the petition against @NYNewsday. Very unfair! https://t.co/xUOmvj4YWJ\n",
            "Just got caught on Video laughing as I say \"I’ll bet money on John Kasich.\" https://t.co/xlPpv5ZqyJ\n",
            "Just heard from @NYDailyNews that they will close soon - is is going\n",
            "\n",
            "[1800 | 3539.05] loss=2.68 avg=2.67\n",
            "[1801 | 3540.76] loss=2.40 avg=2.66\n",
            "[1802 | 3542.47] loss=2.46 avg=2.66\n",
            "[1803 | 3544.19] loss=2.30 avg=2.66\n",
            "[1804 | 3545.89] loss=2.38 avg=2.65\n",
            "[1805 | 3547.61] loss=2.63 avg=2.65\n",
            "[1806 | 3549.32] loss=2.74 avg=2.66\n",
            "[1807 | 3551.04] loss=2.43 avg=2.65\n",
            "[1808 | 3552.75] loss=2.94 avg=2.66\n",
            "[1809 | 3554.46] loss=2.58 avg=2.66\n",
            "[1810 | 3556.18] loss=2.45 avg=2.65\n",
            "[1811 | 3557.89] loss=1.81 avg=2.64\n",
            "[1812 | 3559.62] loss=2.60 avg=2.64\n",
            "[1813 | 3561.33] loss=2.27 avg=2.64\n",
            "[1814 | 3563.03] loss=2.51 avg=2.64\n",
            "[1815 | 3564.75] loss=2.76 avg=2.64\n",
            "[1816 | 3566.47] loss=2.95 avg=2.64\n",
            "[1817 | 3568.18] loss=2.55 avg=2.64\n",
            "[1818 | 3569.90] loss=2.37 avg=2.64\n",
            "[1819 | 3571.61] loss=2.85 avg=2.64\n",
            "[1820 | 3573.32] loss=2.92 avg=2.64\n",
            "[1821 | 3575.03] loss=2.96 avg=2.65\n",
            "[1822 | 3576.74] loss=2.77 avg=2.65\n",
            "[1823 | 3578.45] loss=2.61 avg=2.65\n",
            "[1824 | 3580.16] loss=3.14 avg=2.65\n",
            "[1825 | 3581.87] loss=2.38 avg=2.65\n",
            "[1826 | 3583.58] loss=2.25 avg=2.65\n",
            "[1827 | 3585.29] loss=2.60 avg=2.65\n",
            "[1828 | 3587.00] loss=2.46 avg=2.64\n",
            "[1829 | 3588.72] loss=2.68 avg=2.64\n",
            "[1830 | 3590.43] loss=2.44 avg=2.64\n",
            "[1831 | 3592.14] loss=2.31 avg=2.64\n",
            "[1832 | 3593.85] loss=2.34 avg=2.64\n",
            "[1833 | 3595.57] loss=2.82 avg=2.64\n",
            "[1834 | 3597.28] loss=2.46 avg=2.64\n",
            "[1835 | 3598.99] loss=2.78 avg=2.64\n",
            "[1836 | 3600.70] loss=2.52 avg=2.64\n",
            "[1837 | 3602.41] loss=2.76 avg=2.64\n",
            "[1838 | 3604.12] loss=2.03 avg=2.63\n",
            "[1839 | 3605.83] loss=2.38 avg=2.63\n",
            "[1840 | 3607.54] loss=2.87 avg=2.63\n",
            "[1841 | 3609.26] loss=2.48 avg=2.63\n",
            "[1842 | 3610.97] loss=2.49 avg=2.63\n",
            "[1843 | 3612.68] loss=2.49 avg=2.63\n",
            "[1844 | 3614.39] loss=2.55 avg=2.63\n",
            "[1845 | 3616.10] loss=2.71 avg=2.63\n",
            "[1846 | 3617.81] loss=2.88 avg=2.63\n",
            "[1847 | 3619.53] loss=2.04 avg=2.62\n",
            "[1848 | 3621.24] loss=2.75 avg=2.63\n",
            "[1849 | 3622.96] loss=2.53 avg=2.62\n",
            "[1850 | 3624.67] loss=2.48 avg=2.62\n",
            "[1851 | 3626.38] loss=2.91 avg=2.63\n",
            "[1852 | 3628.09] loss=2.09 avg=2.62\n",
            "[1853 | 3629.80] loss=2.39 avg=2.62\n",
            "[1854 | 3631.52] loss=2.44 avg=2.62\n",
            "[1855 | 3633.23] loss=2.34 avg=2.61\n",
            "[1856 | 3634.95] loss=2.66 avg=2.61\n",
            "[1857 | 3636.67] loss=2.55 avg=2.61\n",
            "[1858 | 3638.38] loss=1.95 avg=2.61\n",
            "[1859 | 3640.10] loss=2.82 avg=2.61\n",
            "[1860 | 3641.80] loss=3.03 avg=2.61\n",
            "[1861 | 3643.51] loss=2.20 avg=2.61\n",
            "[1862 | 3645.23] loss=1.97 avg=2.60\n",
            "[1863 | 3646.95] loss=2.76 avg=2.60\n",
            "[1864 | 3648.65] loss=2.43 avg=2.60\n",
            "[1865 | 3650.36] loss=2.70 avg=2.60\n",
            "[1866 | 3652.08] loss=2.38 avg=2.60\n",
            "[1867 | 3653.79] loss=2.44 avg=2.60\n",
            "[1868 | 3655.50] loss=2.62 avg=2.60\n",
            "[1869 | 3657.22] loss=2.36 avg=2.60\n",
            "[1870 | 3658.93] loss=2.54 avg=2.60\n",
            "[1871 | 3660.64] loss=2.94 avg=2.60\n",
            "[1872 | 3662.34] loss=3.04 avg=2.60\n",
            "[1873 | 3664.06] loss=2.71 avg=2.61\n",
            "[1874 | 3665.77] loss=2.60 avg=2.61\n",
            "[1875 | 3667.48] loss=2.26 avg=2.60\n",
            "[1876 | 3669.19] loss=2.74 avg=2.60\n",
            "[1877 | 3670.90] loss=2.44 avg=2.60\n",
            "[1878 | 3672.61] loss=2.12 avg=2.60\n",
            "[1879 | 3674.32] loss=2.92 avg=2.60\n",
            "[1880 | 3676.03] loss=2.64 avg=2.60\n",
            "[1881 | 3677.74] loss=2.64 avg=2.60\n",
            "[1882 | 3679.46] loss=2.55 avg=2.60\n",
            "[1883 | 3681.16] loss=2.99 avg=2.60\n",
            "[1884 | 3682.88] loss=2.69 avg=2.61\n",
            "[1885 | 3684.59] loss=2.98 avg=2.61\n",
            "[1886 | 3686.30] loss=2.86 avg=2.61\n",
            "[1887 | 3688.02] loss=2.32 avg=2.61\n",
            "[1888 | 3689.73] loss=2.77 avg=2.61\n",
            "[1889 | 3691.45] loss=2.75 avg=2.61\n",
            "[1890 | 3693.16] loss=2.64 avg=2.61\n",
            "[1891 | 3694.87] loss=2.32 avg=2.61\n",
            "[1892 | 3696.59] loss=2.04 avg=2.60\n",
            "[1893 | 3698.30] loss=2.51 avg=2.60\n",
            "[1894 | 3700.02] loss=2.18 avg=2.60\n",
            "[1895 | 3701.74] loss=2.46 avg=2.60\n",
            "[1896 | 3703.45] loss=2.18 avg=2.59\n",
            "[1897 | 3705.17] loss=2.99 avg=2.60\n",
            "[1898 | 3706.88] loss=2.56 avg=2.60\n",
            "[1899 | 3708.59] loss=2.58 avg=2.60\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Commission of Canada. @RNC Convention now in full motion. Big news to be announced tomorrow.\n",
            "Big news that I have been asked to be at the @RNCinCLE convention in Cleveland on Saturday. Big progress being made.\n",
            "I never asked you to \"be present\" @RNCinCLE? I never asked for my name to appear in any way. Never asked.\n",
            "Just got back from one of the largest Republican Party of any kind of size anywhere in the country. It was a great event. Thank you!\n",
            "Thank you @GovernorPataki! #RNCinCLE https://t.co/Y3C6uQg3Qo\n",
            "Thank you @JohnEdgington! #RNCinCLE https://t.co/pB4T1XmIWX\n",
            "Thank you for so kind words from Senator @JonTaso last month. Great words. The media wanted your endorsement!\n",
            "I am honored that John H. Sununu a long time President of the United States and U.S. Senator for the Great State of New York would like to be our nominee!\n",
            "I just visited President @JohnEdgington in the Governor's Mansion after my conversation with the two great gentlemen that I called to congratulate them. https://t.co/xHZxmT5W8z\n",
            "I am honored that Senator @JonTaso would like to be the Nominations Committee Chairman. His credentials and good judgment are strong. He has my complete &amp; total endorsement!\n",
            "I am honored that the Governor of Ohio has named Governor John H. Kasich (of great accomplishment &amp; vision) as his running mate. Congratulations to Ohio.\n",
            "Congratulations to John Kasich. Strong on Crime Border Military &amp; our Military-A Choice that Everyone Can Vote For!\n",
            "Thank you @JarrettWrightNATIONAL —- https://t.co/UeX1KwUZWk\n",
            "Thank you @DanScavino – an experienced developer! https://t.co/3iG5cQc5U6\n",
            ".@GOP needs to quickly nominate a ticket that represents conservatism with integrity and strength. Make sure you get out &amp; vote on May 3.\n",
            ".@MittRomney wants to raise taxes by $10.3 Trillion. We need to lower taxes.\n",
            "The failing Washington Times was the first to report that the @RNC was considering the \"Be Here Now\" mentality. They go out of their way to trash on good people\n",
            "I've been calling on John Kasich and the RNC. Just heard it was in fact a \"Plan B\" that they could have \"discouraged\" voters.\n",
            "Fantastic job on @foxandfriends by Katie Couric in a great discussion – thank you. https://t.co/n0PzIpFtU7\n",
            "Thank you for your kind words @JohnEdgington. I will be in Kentucky this weekend. Will be an incredible day - hopefully the media can get better reporting!\n",
            "The Washington Post has totally apologized for calling me out before the election &amp; is now a really negative reporter. Don't let them get away with this!\n",
            "I have spent billions of dollars on the border and built and expanded the Military in the Middle East. When are they going to take care of our Border?\n",
            "When I left the Apprentice in January the Washington Post called to say that I \"had very good ratings.\" They are now saying that I have an \"A\" as a result of my record.\n",
            "I love the RNC conference call on immigration. We will build a huge Border Wall and build a great Wall. Bad hombres will be stopped in their tracks.\n",
            "The \"RNC Convention\" which will take place this weekend is the biggest event of the year. I'm going to be there with great people &amp; will be an event unlike any other!\n",
            "Our great @RepAlexMooney has joined the #RNC. She has done an outstanding job. We love Alex and her family. We are proud of her!\n",
            "I have been an enthusiastic supporter of John Kasich and I will be supporting him in the Presidential and General Election. He is a tough guy.\n",
            "#RNCinCLE https://t.co/XU2mE2qjv8\n",
            "The failing Washington Times called me \"The New Establishment.\" This is really just an establishment. They have given me so much negative press and negative ratings!\n",
            "#RNCinCLE We are so pleased to say that @RepAlexMooney has endorsed Kasich. She made this endorsement before John Kasich made it!\n",
            "Thank you @RepBobbyScott! https://t.co/xqNxUELR8Z\n",
            "I am in Texas helping to organize our great Republican Party delegates to defend our Constitution. We are going to MAKE AMERICA GREAT AGAIN! #RNCinCLE https://t.\n",
            "\n",
            "[1900 | 3732.51] loss=2.77 avg=2.60\n",
            "[1901 | 3734.21] loss=2.21 avg=2.59\n",
            "[1902 | 3735.93] loss=2.47 avg=2.59\n",
            "[1903 | 3737.64] loss=2.52 avg=2.59\n",
            "[1904 | 3739.35] loss=2.65 avg=2.59\n",
            "[1905 | 3741.06] loss=1.75 avg=2.58\n",
            "[1906 | 3742.78] loss=2.47 avg=2.58\n",
            "[1907 | 3744.49] loss=2.39 avg=2.58\n",
            "[1908 | 3746.20] loss=2.52 avg=2.58\n",
            "[1909 | 3747.92] loss=2.90 avg=2.58\n",
            "[1910 | 3749.62] loss=2.25 avg=2.58\n",
            "[1911 | 3751.34] loss=3.07 avg=2.58\n",
            "[1912 | 3753.05] loss=2.93 avg=2.59\n",
            "[1913 | 3754.76] loss=2.18 avg=2.58\n",
            "[1914 | 3756.48] loss=2.72 avg=2.59\n",
            "[1915 | 3758.19] loss=2.90 avg=2.59\n",
            "[1916 | 3759.90] loss=2.41 avg=2.59\n",
            "[1917 | 3761.62] loss=2.61 avg=2.59\n",
            "[1918 | 3763.33] loss=2.65 avg=2.59\n",
            "[1919 | 3765.04] loss=2.83 avg=2.59\n",
            "[1920 | 3766.76] loss=2.14 avg=2.59\n",
            "[1921 | 3768.48] loss=2.62 avg=2.59\n",
            "[1922 | 3770.19] loss=2.73 avg=2.59\n",
            "[1923 | 3771.91] loss=2.88 avg=2.59\n",
            "[1924 | 3773.62] loss=2.69 avg=2.59\n",
            "[1925 | 3775.34] loss=2.88 avg=2.59\n",
            "[1926 | 3777.05] loss=2.55 avg=2.59\n",
            "[1927 | 3778.76] loss=2.26 avg=2.59\n",
            "[1928 | 3780.47] loss=2.23 avg=2.59\n",
            "[1929 | 3782.18] loss=2.31 avg=2.58\n",
            "[1930 | 3783.89] loss=2.65 avg=2.58\n",
            "[1931 | 3785.61] loss=2.70 avg=2.59\n",
            "[1932 | 3787.32] loss=2.39 avg=2.58\n",
            "[1933 | 3789.03] loss=2.82 avg=2.59\n",
            "[1934 | 3790.74] loss=2.88 avg=2.59\n",
            "[1935 | 3792.46] loss=2.58 avg=2.59\n",
            "[1936 | 3794.17] loss=2.09 avg=2.58\n",
            "[1937 | 3795.88] loss=2.68 avg=2.59\n",
            "[1938 | 3797.59] loss=2.61 avg=2.59\n",
            "[1939 | 3799.30] loss=2.05 avg=2.58\n",
            "[1940 | 3801.01] loss=2.52 avg=2.58\n",
            "[1941 | 3802.72] loss=2.91 avg=2.58\n",
            "[1942 | 3804.44] loss=2.89 avg=2.59\n",
            "[1943 | 3806.15] loss=2.80 avg=2.59\n",
            "[1944 | 3807.85] loss=2.53 avg=2.59\n",
            "[1945 | 3809.57] loss=2.50 avg=2.59\n",
            "[1946 | 3811.28] loss=2.57 avg=2.59\n",
            "[1947 | 3812.99] loss=2.13 avg=2.58\n",
            "[1948 | 3814.70] loss=2.39 avg=2.58\n",
            "[1949 | 3816.41] loss=2.81 avg=2.58\n",
            "[1950 | 3818.13] loss=2.65 avg=2.58\n",
            "[1951 | 3819.84] loss=2.38 avg=2.58\n",
            "[1952 | 3821.55] loss=2.81 avg=2.58\n",
            "[1953 | 3823.27] loss=2.46 avg=2.58\n",
            "[1954 | 3824.98] loss=2.43 avg=2.58\n",
            "[1955 | 3826.69] loss=2.74 avg=2.58\n",
            "[1956 | 3828.41] loss=2.56 avg=2.58\n",
            "[1957 | 3830.12] loss=2.40 avg=2.58\n",
            "[1958 | 3831.84] loss=2.45 avg=2.58\n",
            "[1959 | 3833.55] loss=2.34 avg=2.58\n",
            "[1960 | 3835.27] loss=1.78 avg=2.57\n",
            "[1961 | 3836.99] loss=2.33 avg=2.57\n",
            "[1962 | 3838.70] loss=2.21 avg=2.56\n",
            "[1963 | 3840.42] loss=2.19 avg=2.56\n",
            "[1964 | 3842.13] loss=1.98 avg=2.55\n",
            "[1965 | 3843.84] loss=2.05 avg=2.55\n",
            "[1966 | 3845.55] loss=1.96 avg=2.54\n",
            "[1967 | 3847.26] loss=2.86 avg=2.55\n",
            "[1968 | 3848.98] loss=2.63 avg=2.55\n",
            "[1969 | 3850.68] loss=2.36 avg=2.54\n",
            "[1970 | 3852.40] loss=2.69 avg=2.55\n",
            "[1971 | 3854.11] loss=2.50 avg=2.55\n",
            "[1972 | 3855.81] loss=2.67 avg=2.55\n",
            "[1973 | 3857.53] loss=3.15 avg=2.55\n",
            "[1974 | 3859.24] loss=2.33 avg=2.55\n",
            "[1975 | 3860.96] loss=1.61 avg=2.54\n",
            "[1976 | 3862.66] loss=2.88 avg=2.54\n",
            "[1977 | 3864.38] loss=2.15 avg=2.54\n",
            "[1978 | 3866.09] loss=2.43 avg=2.54\n",
            "[1979 | 3867.80] loss=2.25 avg=2.54\n",
            "[1980 | 3869.51] loss=2.49 avg=2.54\n",
            "[1981 | 3871.22] loss=2.14 avg=2.53\n",
            "[1982 | 3872.94] loss=2.66 avg=2.53\n",
            "[1983 | 3874.65] loss=2.27 avg=2.53\n",
            "[1984 | 3876.36] loss=2.04 avg=2.53\n",
            "[1985 | 3878.07] loss=2.11 avg=2.52\n",
            "[1986 | 3879.79] loss=2.66 avg=2.52\n",
            "[1987 | 3881.50] loss=2.72 avg=2.52\n",
            "[1988 | 3883.21] loss=2.27 avg=2.52\n",
            "[1989 | 3884.92] loss=2.63 avg=2.52\n",
            "[1990 | 3886.64] loss=2.51 avg=2.52\n",
            "[1991 | 3888.35] loss=2.27 avg=2.52\n",
            "[1992 | 3890.06] loss=2.47 avg=2.52\n",
            "[1993 | 3891.77] loss=2.79 avg=2.52\n",
            "[1994 | 3893.49] loss=3.23 avg=2.53\n",
            "[1995 | 3895.20] loss=2.70 avg=2.53\n",
            "[1996 | 3896.91] loss=2.76 avg=2.53\n",
            "[1997 | 3898.63] loss=2.97 avg=2.54\n",
            "[1998 | 3900.34] loss=2.34 avg=2.54\n",
            "[1999 | 3902.06] loss=2.53 avg=2.54\n",
            "Saving /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-2000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "’s '60 Minutes' report on Trump”\n",
            "I am in California. Tonight! @foxandfriends #MAGA\n",
            "When @gretawire tried to sell his \"bimbo card\" to @megynkelly—she said she didn’t buy the card! So now he's trying to sell all of his cards to her!\n",
            "Will be in California. Tonight at 8pm at @CNN. Tune in!\n",
            "“There is only one constant in your life--your own thought process.”  - George S. Patton\n",
            "Why did @megynkelly ask @RealBenCarson for my autograph at his book signing? He was unhappy that she signed it. Bad form and just plain not her man!\n",
            "The Republicans are looking at the new @CNN schedule to see what better way to spend their time than by attacking the media. #MAGA\n",
            "Trump @gretawire is going nuts over my tweet about @megynkelly. He should apologize for not knowing what I said.\n",
            "I don’t know @RealBenCarson and he doesn’t know me--but is @realDonaldTrump a true genius? Not easy to say but true genius!\n",
            "“When you have confidence in yourself you can do whatever it is you want.”  – Think Like A Champion\n",
            "I’m so glad that @RealBenCarson is back on @gretawire. When his endorsement came out he hated @RealBenCarson and wanted to drop down into the dumps. Ben is back!\n",
            "Wow big media blitz in New York City against @megynkelly. She wants to take me down—I want to take her up—not fair and just plain wrong!\n",
            ".@megynkelly is just another phony reporter who “can’t say anything bad about Trump” on \"Today Show\" -- and she gets a free pass. She’s a disaster without a brain!\n",
            "I just signed my own @NHLPA Players’ Association endorsement--and they want me to do more. I will without a fight!\n",
            "I think this is an important time for America—and that includes our schools! #MAGARatehttps://t.co/v4iYQyW7Qz\n",
            "Great @CNN reporting’s analysis of Trump signing autographs &amp; autographing books: https://t.co/f2lPp6yjz7\n",
            "I want America to succeed. To that end--I want to invest in its future by making it a nation of entrepreneurs &amp; innovators. — Donald J. Trump (@realDonaldTrump) January 11, 2018 Make America Great Again\n",
            "The new ratings for @ESPN @NCAA football are way down. We must rebuild the institutions of our country. We must put America First. #TrumpTrain https://t.co/Xa6mwX7Q7M\n",
            "It’s time for our schools to improve and to help our country to make us great again. #MAGAhttps://t.co/TK3Hx9PtqW https://t.co/cW7KXyVlw2\n",
            "The #POTUS is in California for the @NCAA football game. There will be a lot of action!#MakeAmericaGreatAgain https://t.co/WqW5rq6ZWl\n",
            "So much @CNN coverage of the 'Trump signing' with the 'CNN signees' (who don't sign autographs) wearing shirts saying “He’s a genius”--sad!\n",
            ".@CNN &amp; they make no attempt to check the “headshot photos” they used. I just happen to have a very good eye--looks great-a shame they can’t check!\n",
            ".@CNN did their Headshot photo with the “Trump signees” but @realDonaldTrump was in front of the cameras with his son Barron. A total waste of time. @FoxNews\n",
            "A very big win today for the country. So much progress. The Republicans should just sit back &amp; enjoy the new ratings numbers.  #MAGA\n",
            "Today I will announce the winner and the elimination of the @NCAA men's basketball champion’s tax bracket!\n",
            "This is very exciting. #NCAABowl #Trump2016 https://t.co/nUELUWZqH7\n",
            "Today we celebrated our 20th anniversary of @NCAA Football Championship Subdivision Football. Congratulations to our great players &amp; coaches. https://t.co/gOgqDnVcG5\n",
            "I am in Pennsylvania. Will be doing a lot of signing and traveling and will be doing one last press conference. Tune in! @foxandfriends\n",
            "Today is an important day for America... #FITN #\n",
            "\n",
            "[2000 | 3939.98] loss=2.56 avg=2.54\n",
            "[2001 | 3941.68] loss=2.35 avg=2.53\n",
            "[2002 | 3943.36] loss=1.92 avg=2.53\n",
            "[2003 | 3945.04] loss=2.22 avg=2.53\n",
            "[2004 | 3946.71] loss=3.34 avg=2.53\n",
            "[2005 | 3948.38] loss=2.60 avg=2.53\n",
            "[2006 | 3950.05] loss=2.79 avg=2.54\n",
            "[2007 | 3951.73] loss=2.44 avg=2.54\n",
            "[2008 | 3953.41] loss=2.42 avg=2.53\n",
            "[2009 | 3955.09] loss=2.36 avg=2.53\n",
            "[2010 | 3956.77] loss=2.98 avg=2.54\n",
            "[2011 | 3958.45] loss=2.03 avg=2.53\n",
            "[2012 | 3960.13] loss=2.34 avg=2.53\n",
            "[2013 | 3961.81] loss=2.30 avg=2.53\n",
            "[2014 | 3963.50] loss=2.20 avg=2.52\n",
            "[2015 | 3965.19] loss=2.62 avg=2.53\n",
            "[2016 | 3966.86] loss=2.60 avg=2.53\n",
            "[2017 | 3968.54] loss=2.84 avg=2.53\n",
            "[2018 | 3970.24] loss=2.85 avg=2.53\n",
            "[2019 | 3971.93] loss=2.47 avg=2.53\n",
            "[2020 | 3973.61] loss=2.45 avg=2.53\n",
            "[2021 | 3975.30] loss=2.55 avg=2.53\n",
            "[2022 | 3976.97] loss=2.39 avg=2.53\n",
            "[2023 | 3978.66] loss=2.65 avg=2.53\n",
            "[2024 | 3980.35] loss=2.69 avg=2.53\n",
            "[2025 | 3982.04] loss=1.95 avg=2.53\n",
            "[2026 | 3983.73] loss=1.97 avg=2.52\n",
            "[2027 | 3985.44] loss=2.57 avg=2.52\n",
            "[2028 | 3987.13] loss=2.25 avg=2.52\n",
            "[2029 | 3988.81] loss=2.62 avg=2.52\n",
            "[2030 | 3990.50] loss=2.53 avg=2.52\n",
            "[2031 | 3992.19] loss=2.81 avg=2.52\n",
            "[2032 | 3993.88] loss=2.40 avg=2.52\n",
            "[2033 | 3995.58] loss=2.49 avg=2.52\n",
            "[2034 | 3997.27] loss=2.27 avg=2.52\n",
            "[2035 | 3998.97] loss=2.91 avg=2.52\n",
            "[2036 | 4000.65] loss=2.23 avg=2.52\n",
            "[2037 | 4002.35] loss=2.85 avg=2.52\n",
            "[2038 | 4004.05] loss=2.77 avg=2.53\n",
            "[2039 | 4005.77] loss=1.98 avg=2.52\n",
            "[2040 | 4007.46] loss=2.46 avg=2.52\n",
            "[2041 | 4009.17] loss=2.96 avg=2.52\n",
            "[2042 | 4010.88] loss=2.20 avg=2.52\n",
            "[2043 | 4012.58] loss=2.67 avg=2.52\n",
            "[2044 | 4014.27] loss=2.44 avg=2.52\n",
            "[2045 | 4015.97] loss=2.64 avg=2.52\n",
            "[2046 | 4017.67] loss=2.55 avg=2.52\n",
            "[2047 | 4019.37] loss=2.76 avg=2.53\n",
            "[2048 | 4021.09] loss=3.11 avg=2.53\n",
            "[2049 | 4022.80] loss=2.75 avg=2.53\n",
            "[2050 | 4024.50] loss=3.03 avg=2.54\n",
            "[2051 | 4026.20] loss=2.42 avg=2.54\n",
            "[2052 | 4027.91] loss=2.28 avg=2.53\n",
            "[2053 | 4029.63] loss=2.55 avg=2.53\n",
            "[2054 | 4031.33] loss=2.64 avg=2.54\n",
            "[2055 | 4033.03] loss=2.54 avg=2.54\n",
            "[2056 | 4034.71] loss=2.17 avg=2.53\n",
            "[2057 | 4036.43] loss=2.49 avg=2.53\n",
            "[2058 | 4038.13] loss=2.39 avg=2.53\n",
            "[2059 | 4039.85] loss=2.77 avg=2.53\n",
            "[2060 | 4041.56] loss=2.36 avg=2.53\n",
            "[2061 | 4043.27] loss=2.66 avg=2.53\n",
            "[2062 | 4044.97] loss=2.74 avg=2.53\n",
            "[2063 | 4046.68] loss=2.88 avg=2.54\n",
            "[2064 | 4048.39] loss=2.70 avg=2.54\n",
            "[2065 | 4050.10] loss=2.50 avg=2.54\n",
            "[2066 | 4051.81] loss=2.78 avg=2.54\n",
            "[2067 | 4053.52] loss=2.33 avg=2.54\n",
            "[2068 | 4055.22] loss=2.72 avg=2.54\n",
            "[2069 | 4056.93] loss=2.45 avg=2.54\n",
            "[2070 | 4058.64] loss=2.71 avg=2.54\n",
            "[2071 | 4060.35] loss=2.37 avg=2.54\n",
            "[2072 | 4062.06] loss=2.32 avg=2.54\n",
            "[2073 | 4063.77] loss=2.58 avg=2.54\n",
            "[2074 | 4065.48] loss=2.25 avg=2.54\n",
            "[2075 | 4067.19] loss=2.60 avg=2.54\n",
            "[2076 | 4068.90] loss=2.36 avg=2.53\n",
            "[2077 | 4070.61] loss=2.76 avg=2.54\n",
            "[2078 | 4072.32] loss=2.80 avg=2.54\n",
            "[2079 | 4074.03] loss=2.35 avg=2.54\n",
            "[2080 | 4075.74] loss=2.50 avg=2.54\n",
            "[2081 | 4077.45] loss=2.69 avg=2.54\n",
            "[2082 | 4079.16] loss=2.14 avg=2.53\n",
            "[2083 | 4080.87] loss=2.79 avg=2.54\n",
            "[2084 | 4082.58] loss=2.66 avg=2.54\n",
            "[2085 | 4084.29] loss=2.06 avg=2.53\n",
            "[2086 | 4086.00] loss=3.50 avg=2.54\n",
            "[2087 | 4087.72] loss=2.43 avg=2.54\n",
            "[2088 | 4089.43] loss=2.41 avg=2.54\n",
            "[2089 | 4091.15] loss=2.34 avg=2.54\n",
            "[2090 | 4092.85] loss=2.86 avg=2.54\n",
            "[2091 | 4094.57] loss=2.42 avg=2.54\n",
            "[2092 | 4096.28] loss=2.60 avg=2.54\n",
            "[2093 | 4098.00] loss=2.17 avg=2.54\n",
            "[2094 | 4099.72] loss=2.65 avg=2.54\n",
            "[2095 | 4101.44] loss=1.57 avg=2.53\n",
            "[2096 | 4103.15] loss=2.92 avg=2.53\n",
            "[2097 | 4104.86] loss=2.47 avg=2.53\n",
            "[2098 | 4106.57] loss=2.41 avg=2.53\n",
            "[2099 | 4108.28] loss=2.74 avg=2.53\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "8.00/15 for a package of snacks and drinks.\n",
            "As you know millions of Americans have just received free or cheap food with very real costs to the government. We should never accept a food stamp recipient into our government ever again!\n",
            "On @TheView  @oreillyfactor co host @morningmika:  Trump: \"Thank you for your great support.\"\n",
            "Wow--the @NYDailyNews lost more money in one year than my charity raised in 15 years of paying taxes! What does that tell you?\n",
            "A massive landslide. We are going to Make America Great Again and it will all begin with the great folks of Iowa!\n",
            "@jeffrovsky    I don't know about the charity-I mean deal.\n",
            ".@NYDailyNews - \"Donald Trump to address 2013 @StJude’s to benefit @TrumpGolfLA\" http://t.co/YUOJ0z4k3i\n",
            "@JohnCasper2  Yes you can.\n",
            "@jeffrovsky   I hope so.\n",
            "@DawnDellano  The Foundation is just that - a great organization.\n",
            "@jeffrovsky  Thanks.\n",
            "@jeffrovsky  And @TrumpSoHo are fantastic--great for weddings weddings.\n",
            "@DawnDellano Thank you!\n",
            "@brucehill91 @Morning_Joe @marklevinshow  Mark is great.\n",
            "@JohnCasper2 Thank you.\n",
            "@jeffrovsky  Thanks.\n",
            "@soulatix  Thank you.\n",
            "@brucehill91 @Morning_Joe @marklevinshow  Great job Joe.\n",
            "@jackman_7000 Thank you.\n",
            "@Briarwood_1 Thank you.\n",
            "@paulkraus Thank you.\n",
            "@DawnDellano Thank you.\n",
            "@jeffrovsky Thanks.\n",
            "@brucehill91 Thank you.\n",
            "This is a big deal and many people didn't realize it. @realDonaldTrump Foundation is giving away free hotel room keys to families and friends of those who received food stamps http://t.co/uLgE0mF2zs\n",
            "@marklevinshow I like being right.\n",
            "@jeffrovsky Thanks--just wanted to point that out.\n",
            "@brucehill91  Just thought of it--thanks!\n",
            "@jeffrovsky Thanks--also great.\n",
            "@dawndellano Thank you!\n",
            "@brucehill91 @Morning_Joe @marklevinshow It would be great--and fair - but not a charity!\n",
            "@brucehill91  Thanks, just thought it'd be worthwhile.\n",
            "@BryantKWatson  Thanks!\n",
            "@Misterjamesxxx  Thanks!\n",
            "@LangleyTiger  Thank you.\n",
            "@sparks_cane @Morning_Joe @TheBrodyFile @JoeNBC  Thank you\n",
            "@Briarwood_1 Thank you\n",
            "@Briarwood_1 Thank you\n",
            "Congratulations to my great friend @JoeNBC on his new role as a 'Daily Show'-Joe has never been more popular for a second!\n",
            "@jeffrovsky Thanks--I like the guy.\n",
            "@brucehill91 Thank you. We will.\n",
            "@Cjdunl20 @ThePeteRose_Insider  Thanks Pete.\n",
            "@Briarwood_1 Thanks--a big plus for me!\n",
            "@DawnDellano Thanks very nice.\n",
            "@Cjdunl20 Thank you-I appreciate it!\n",
            "@brucehill91  It will be great--and fair.\n",
            "This is a really great decision for a great cause - a charity for the great people of Iowa! Let's Make America Great Again and we will bring back $1 Billion for the kids! http://t.co/uLgE0mF2zs\n",
            "@gretawire @jaketapper Thank you\n",
            "@RiccardoTorti Thank you\n",
            "@thedrugbreaker Thanks.\n",
            "@rjohnny  Thanks--that's great.\n",
            "@JAG13  Thanks very much.\n",
            "I love being right! Thanks for the nice compliment. http://t.co/uLgE0mF2zs\n",
            "@cjkarpfords @TheBrodyFile Sorry not nice but good to know.\n",
            "@DawnDellano  I hope so!\n",
            "@brucehill91 @Morning_Joe @marklevinshope. Thanks\n",
            "@brucehill91 Hope so--always nice.\n",
            "@jacksbarnes Thank you.\n",
            "@Briarwood_1 Thanks.\n",
            "@Briarwood_1 Thank you.\n",
            "@Cjdunl20 Thank you.\n",
            "@BryantKW\n",
            "\n",
            "[2100 | 4132.19] loss=2.67 avg=2.53\n",
            "[2101 | 4133.89] loss=1.99 avg=2.53\n",
            "[2102 | 4135.61] loss=2.95 avg=2.53\n",
            "[2103 | 4137.32] loss=1.81 avg=2.53\n",
            "[2104 | 4139.04] loss=1.98 avg=2.52\n",
            "[2105 | 4140.75] loss=2.23 avg=2.52\n",
            "[2106 | 4142.46] loss=2.57 avg=2.52\n",
            "[2107 | 4144.17] loss=2.65 avg=2.52\n",
            "[2108 | 4145.88] loss=2.70 avg=2.52\n",
            "[2109 | 4147.59] loss=2.64 avg=2.52\n",
            "[2110 | 4149.30] loss=2.17 avg=2.52\n",
            "[2111 | 4151.01] loss=2.95 avg=2.52\n",
            "[2112 | 4152.73] loss=2.26 avg=2.52\n",
            "[2113 | 4154.44] loss=2.22 avg=2.52\n",
            "[2114 | 4156.15] loss=2.74 avg=2.52\n",
            "[2115 | 4157.86] loss=2.54 avg=2.52\n",
            "[2116 | 4159.58] loss=2.47 avg=2.52\n",
            "[2117 | 4161.29] loss=2.40 avg=2.52\n",
            "[2118 | 4163.00] loss=2.85 avg=2.52\n",
            "[2119 | 4164.71] loss=2.25 avg=2.52\n",
            "[2120 | 4166.43] loss=2.95 avg=2.52\n",
            "[2121 | 4168.15] loss=2.64 avg=2.52\n",
            "[2122 | 4169.86] loss=2.43 avg=2.52\n",
            "[2123 | 4171.58] loss=2.84 avg=2.53\n",
            "[2124 | 4173.29] loss=2.53 avg=2.53\n",
            "[2125 | 4175.00] loss=3.04 avg=2.53\n",
            "[2126 | 4176.72] loss=2.81 avg=2.54\n",
            "[2127 | 4178.46] loss=2.01 avg=2.53\n",
            "[2128 | 4180.20] loss=2.81 avg=2.53\n",
            "[2129 | 4181.93] loss=2.43 avg=2.53\n",
            "[2130 | 4183.64] loss=2.35 avg=2.53\n",
            "[2131 | 4185.35] loss=2.25 avg=2.53\n",
            "[2132 | 4187.06] loss=2.99 avg=2.53\n",
            "[2133 | 4188.77] loss=2.92 avg=2.54\n",
            "[2134 | 4190.48] loss=2.86 avg=2.54\n",
            "[2135 | 4192.19] loss=2.59 avg=2.54\n",
            "[2136 | 4193.91] loss=2.61 avg=2.54\n",
            "[2137 | 4195.63] loss=1.73 avg=2.53\n",
            "[2138 | 4197.34] loss=2.74 avg=2.53\n",
            "[2139 | 4199.05] loss=2.44 avg=2.53\n",
            "[2140 | 4200.76] loss=2.54 avg=2.53\n",
            "[2141 | 4202.47] loss=2.42 avg=2.53\n",
            "[2142 | 4204.18] loss=2.42 avg=2.53\n",
            "[2143 | 4205.90] loss=2.86 avg=2.53\n",
            "[2144 | 4207.60] loss=2.29 avg=2.53\n",
            "[2145 | 4209.32] loss=2.70 avg=2.53\n",
            "[2146 | 4211.03] loss=2.15 avg=2.53\n",
            "[2147 | 4212.74] loss=2.77 avg=2.53\n",
            "[2148 | 4214.45] loss=2.84 avg=2.53\n",
            "[2149 | 4216.16] loss=2.66 avg=2.54\n",
            "[2150 | 4217.87] loss=2.65 avg=2.54\n",
            "[2151 | 4219.58] loss=2.52 avg=2.54\n",
            "[2152 | 4221.29] loss=1.87 avg=2.53\n",
            "[2153 | 4223.00] loss=2.52 avg=2.53\n",
            "[2154 | 4224.71] loss=2.23 avg=2.53\n",
            "[2155 | 4226.42] loss=2.33 avg=2.53\n",
            "[2156 | 4228.14] loss=2.50 avg=2.53\n",
            "[2157 | 4229.86] loss=2.48 avg=2.52\n",
            "[2158 | 4231.57] loss=2.64 avg=2.53\n",
            "[2159 | 4233.28] loss=2.56 avg=2.53\n",
            "[2160 | 4234.99] loss=2.35 avg=2.52\n",
            "[2161 | 4236.71] loss=2.20 avg=2.52\n",
            "[2162 | 4238.42] loss=2.05 avg=2.52\n",
            "[2163 | 4240.14] loss=3.00 avg=2.52\n",
            "[2164 | 4241.85] loss=2.30 avg=2.52\n",
            "[2165 | 4243.57] loss=2.38 avg=2.52\n",
            "[2166 | 4245.29] loss=2.76 avg=2.52\n",
            "[2167 | 4247.00] loss=2.19 avg=2.52\n",
            "[2168 | 4248.71] loss=3.10 avg=2.52\n",
            "[2169 | 4250.42] loss=2.48 avg=2.52\n",
            "[2170 | 4252.13] loss=2.67 avg=2.52\n",
            "[2171 | 4253.84] loss=2.67 avg=2.53\n",
            "[2172 | 4255.56] loss=3.04 avg=2.53\n",
            "[2173 | 4257.27] loss=3.17 avg=2.54\n",
            "[2174 | 4258.98] loss=2.93 avg=2.54\n",
            "[2175 | 4260.69] loss=2.58 avg=2.54\n",
            "[2176 | 4262.40] loss=2.79 avg=2.54\n",
            "[2177 | 4264.11] loss=2.32 avg=2.54\n",
            "[2178 | 4265.83] loss=2.12 avg=2.54\n",
            "[2179 | 4267.56] loss=3.06 avg=2.54\n",
            "[2180 | 4269.27] loss=2.80 avg=2.54\n",
            "[2181 | 4270.98] loss=2.06 avg=2.54\n",
            "[2182 | 4272.69] loss=2.43 avg=2.54\n",
            "[2183 | 4274.41] loss=2.55 avg=2.54\n",
            "[2184 | 4276.12] loss=2.71 avg=2.54\n",
            "[2185 | 4277.83] loss=2.22 avg=2.54\n",
            "[2186 | 4279.54] loss=2.37 avg=2.54\n",
            "[2187 | 4281.25] loss=2.39 avg=2.53\n",
            "[2188 | 4282.97] loss=2.21 avg=2.53\n",
            "[2189 | 4284.68] loss=2.40 avg=2.53\n",
            "[2190 | 4286.39] loss=2.79 avg=2.53\n",
            "[2191 | 4288.10] loss=2.03 avg=2.53\n",
            "[2192 | 4289.81] loss=3.21 avg=2.53\n",
            "[2193 | 4291.52] loss=2.56 avg=2.53\n",
            "[2194 | 4293.24] loss=2.85 avg=2.54\n",
            "[2195 | 4294.94] loss=2.59 avg=2.54\n",
            "[2196 | 4296.66] loss=2.61 avg=2.54\n",
            "[2197 | 4298.37] loss=2.73 avg=2.54\n",
            "[2198 | 4300.08] loss=2.05 avg=2.54\n",
            "[2199 | 4301.79] loss=2.78 avg=2.54\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "The problem is with me\n",
            "My wife Melania loves me and thinks I'm awesome - not nice really - but she's a tough negotiator and I won't back down!\n",
            "@MikeSchaller  Interesting you should do that interview.\n",
            "\"It is the greatest fear of all to be powerful but also to be understood.\" -- Albert Einstein\n",
            "I wish the Democrats would listen to @RepChrisCollins. He is an honest man with strong ideas.\n",
            ".@NewtGardner has written the book How I Made Money on Wall Street which is very good. Great job!\n",
            "@BillRancic  Bill - thanks--you are a really nice guy.\n",
            "\"It is best to be cautious of being too predictable.\"--Albert Einstein\n",
            "@bwgaby  You were always careful of unpredictability--it is important.\n",
            "I always knew Chuck Norris was a great guy--it's hard to believe he's not in some of those movies!\n",
            "@DanaLorenzoNY  Thanks Dana.\n",
            "@BrunetteThumper  What a great surprise.\n",
            "@MarlenePants  It is great to see you will be doing a great job. Be sure to stop by the new Westchester home-- @PeteRoseJr and family can't wait to see you!\n",
            "@jmhanson2  Thanks Marvin.\n",
            "@jmhanson2  Thanks Jan.\n",
            "@LoreenBennett_2  Thanks Loreen.\n",
            "@harryrknight  I said that.\n",
            "@carlow1922   Thanks--thanks.\n",
            "I want everyone to know that @PeteRoseJr will be at Trump National Wilshire--in the @Playboys club--and he will have a great time.\n",
            "\"What is a president to do? Well I can think of several things...not least of which is work hard.\" -- President Dwight D. Eisenhower\n",
            ".@Playboys Club on 103rd Street is the @TrumpCause in Beverly Hills. Tonight I will be visiting the clubhouse where the @PeteRoseJr played &amp; I will be watching baseball--great place!\n",
            "@MikeBensonNHL Player of the Month...Congratulations.\n",
            "@bobzaborski Thanks Bob--very nice!\n",
            "@TigerLilJack  Tiger &amp; I are friends but you cannot build respect. Respect is building something stronger.\n",
            "...it would be silly to start all over from scratch.\n",
            "Just starting out. There are many people and projects that can be done--and quickly--but you must be patient-and stick with your goals.\n",
            "The key to being successful is to never give up. It's much more satisfying than fame or fortune.\n",
            "If someone beats you at something don't freak out. Just keep going and you will get there.\n",
            "Success is the first step toward everything else.\n",
            "Success is the beginning of the end.\n",
            "Success is the beginning of the end.\n",
            "This is the hardest question I've got to read.  But I'm not sure I can answer it-but hopefully it gives you some idea of what you need to do.\n",
            "The biggest mistake you could make is to assume that you're better than anyone else.\n",
            "Always be a thinker not just a quitter.\n",
            "Be wary of being overly ambitious. Getting too carried away can be dangerous.\n",
            "Great books: \"The Art of the Deal\" by Donald Trump \"Entrepreneurship: The Secret to Great Success\" by Warren B.\n",
            "Do what's necessary to get what's necessary.\n",
            "Trust your instincts. They will pay off!\n",
            "Make sure you've got the experience and knowledge to take over any challenge.\n",
            "Successful people know when to fold. They know when to strike out again.\n",
            "The more you study the less you study the textbooks.\n",
            "@BillRancic  Bill--thanks.\n",
            "Why should I care if @realDonaldTrump isn't the one building the club for me--I have a club going--and nobody else will (just kidding)!\n",
            "@sosanam  Thanks Sam &amp; good luck.\n",
            "@MariNathalie @bobzaborski You are great you are amazing thank you very much.\n",
            "@MariNathalie @bobzaborski You are right!\n",
            "I'm coming to see my friend @PeteRoseJr tonight--we got nothing against each other--we'll speak again tomorrow.\n",
            "@bobbycrombie @realDonaldTrump  It will be fabulous.\n",
            "@carlow1922  We did win the case in NYC &amp; that is a great feeling.\n",
            "@bobzaborski @realDonaldTrump  That will happen-we'll win lots of lawsuits soon\n",
            "@RashidWaleed @pewresearch #Trump2016\n",
            "Thanks @louiseklein  for giving me permission to say nasty things about @pewresearch.\n",
            ".@pew\n",
            "\n",
            "[2200 | 4325.66] loss=2.48 avg=2.54\n",
            "[2201 | 4327.36] loss=2.51 avg=2.54\n",
            "[2202 | 4329.08] loss=3.08 avg=2.54\n",
            "[2203 | 4330.82] loss=2.40 avg=2.54\n",
            "[2204 | 4332.52] loss=2.07 avg=2.54\n",
            "[2205 | 4334.23] loss=2.22 avg=2.53\n",
            "[2206 | 4335.94] loss=2.64 avg=2.53\n",
            "[2207 | 4337.65] loss=1.78 avg=2.53\n",
            "[2208 | 4339.36] loss=2.33 avg=2.53\n",
            "[2209 | 4341.10] loss=2.56 avg=2.53\n",
            "[2210 | 4342.82] loss=2.47 avg=2.53\n",
            "[2211 | 4344.54] loss=2.69 avg=2.53\n",
            "[2212 | 4346.25] loss=2.14 avg=2.52\n",
            "[2213 | 4347.98] loss=1.48 avg=2.51\n",
            "[2214 | 4349.70] loss=2.37 avg=2.51\n",
            "[2215 | 4351.41] loss=2.81 avg=2.51\n",
            "[2216 | 4353.13] loss=2.06 avg=2.51\n",
            "[2217 | 4354.85] loss=2.30 avg=2.51\n",
            "[2218 | 4356.57] loss=2.80 avg=2.51\n",
            "[2219 | 4358.29] loss=2.54 avg=2.51\n",
            "[2220 | 4360.00] loss=2.52 avg=2.51\n",
            "[2221 | 4361.71] loss=2.30 avg=2.51\n",
            "[2222 | 4363.42] loss=2.57 avg=2.51\n",
            "[2223 | 4365.14] loss=2.00 avg=2.50\n",
            "[2224 | 4366.85] loss=2.29 avg=2.50\n",
            "[2225 | 4368.56] loss=2.42 avg=2.50\n",
            "[2226 | 4370.28] loss=2.71 avg=2.50\n",
            "[2227 | 4372.00] loss=2.05 avg=2.50\n",
            "[2228 | 4373.71] loss=2.71 avg=2.50\n",
            "[2229 | 4375.43] loss=2.58 avg=2.50\n",
            "[2230 | 4377.14] loss=2.31 avg=2.50\n",
            "[2231 | 4378.86] loss=2.12 avg=2.50\n",
            "[2232 | 4380.57] loss=2.76 avg=2.50\n",
            "[2233 | 4382.28] loss=2.74 avg=2.50\n",
            "[2234 | 4384.00] loss=2.79 avg=2.50\n",
            "[2235 | 4385.71] loss=2.42 avg=2.50\n",
            "[2236 | 4387.42] loss=2.43 avg=2.50\n",
            "[2237 | 4389.13] loss=2.97 avg=2.51\n",
            "[2238 | 4390.84] loss=2.80 avg=2.51\n",
            "[2239 | 4392.58] loss=2.16 avg=2.51\n",
            "[2240 | 4394.29] loss=2.70 avg=2.51\n",
            "[2241 | 4396.00] loss=2.49 avg=2.51\n",
            "[2242 | 4397.71] loss=2.23 avg=2.51\n",
            "[2243 | 4399.42] loss=2.29 avg=2.50\n",
            "[2244 | 4401.13] loss=2.75 avg=2.51\n",
            "[2245 | 4402.86] loss=2.40 avg=2.50\n",
            "[2246 | 4404.57] loss=1.76 avg=2.50\n",
            "[2247 | 4406.28] loss=1.86 avg=2.49\n",
            "[2248 | 4408.00] loss=2.70 avg=2.49\n",
            "[2249 | 4409.71] loss=2.81 avg=2.50\n",
            "[2250 | 4411.42] loss=2.53 avg=2.50\n",
            "[2251 | 4413.13] loss=2.43 avg=2.50\n",
            "[2252 | 4414.85] loss=3.02 avg=2.50\n",
            "[2253 | 4416.56] loss=2.49 avg=2.50\n",
            "[2254 | 4418.27] loss=2.47 avg=2.50\n",
            "[2255 | 4419.98] loss=2.90 avg=2.50\n",
            "[2256 | 4421.69] loss=2.15 avg=2.50\n",
            "[2257 | 4423.41] loss=2.60 avg=2.50\n",
            "[2258 | 4425.11] loss=2.50 avg=2.50\n",
            "[2259 | 4426.83] loss=2.75 avg=2.50\n",
            "[2260 | 4428.54] loss=2.56 avg=2.50\n",
            "[2261 | 4430.25] loss=2.78 avg=2.51\n",
            "[2262 | 4431.97] loss=1.88 avg=2.50\n",
            "[2263 | 4433.68] loss=2.69 avg=2.50\n",
            "[2264 | 4435.39] loss=2.13 avg=2.50\n",
            "[2265 | 4437.11] loss=2.01 avg=2.49\n",
            "[2266 | 4438.82] loss=2.64 avg=2.50\n",
            "[2267 | 4440.54] loss=2.26 avg=2.49\n",
            "[2268 | 4442.26] loss=3.12 avg=2.50\n",
            "[2269 | 4443.97] loss=2.63 avg=2.50\n",
            "[2270 | 4445.69] loss=2.64 avg=2.50\n",
            "[2271 | 4447.40] loss=2.89 avg=2.51\n",
            "[2272 | 4449.12] loss=2.61 avg=2.51\n",
            "[2273 | 4450.83] loss=2.29 avg=2.51\n",
            "[2274 | 4452.55] loss=2.62 avg=2.51\n",
            "[2275 | 4454.26] loss=3.10 avg=2.51\n",
            "[2276 | 4455.98] loss=2.92 avg=2.52\n",
            "[2277 | 4457.68] loss=2.20 avg=2.51\n",
            "[2278 | 4459.40] loss=2.50 avg=2.51\n",
            "[2279 | 4461.11] loss=2.19 avg=2.51\n",
            "[2280 | 4462.82] loss=2.91 avg=2.51\n",
            "[2281 | 4464.53] loss=2.43 avg=2.51\n",
            "[2282 | 4466.25] loss=2.45 avg=2.51\n",
            "[2283 | 4467.96] loss=2.12 avg=2.51\n",
            "[2284 | 4469.66] loss=2.37 avg=2.51\n",
            "[2285 | 4471.38] loss=2.85 avg=2.51\n",
            "[2286 | 4473.09] loss=2.58 avg=2.51\n",
            "[2287 | 4474.80] loss=2.46 avg=2.51\n",
            "[2288 | 4476.51] loss=2.65 avg=2.51\n",
            "[2289 | 4478.22] loss=2.72 avg=2.51\n",
            "[2290 | 4479.93] loss=2.54 avg=2.51\n",
            "[2291 | 4481.64] loss=2.35 avg=2.51\n",
            "[2292 | 4483.35] loss=2.19 avg=2.51\n",
            "[2293 | 4485.06] loss=2.49 avg=2.51\n",
            "[2294 | 4486.78] loss=2.37 avg=2.51\n",
            "[2295 | 4488.50] loss=2.65 avg=2.51\n",
            "[2296 | 4490.24] loss=2.05 avg=2.50\n",
            "[2297 | 4491.96] loss=2.83 avg=2.51\n",
            "[2298 | 4493.67] loss=1.89 avg=2.50\n",
            "[2299 | 4495.39] loss=2.33 avg=2.50\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " yet very much involved. I'm just playing along. They already have the facts and I don't care about them!\n",
            "Just found out that the @dallasfed was right when they warned about the @DNC \"cyberlocker\" during the primaries. This is another Obama thing-only gets worse with every new generation!\n",
            "Congratulations to my wife Melania. The beautiful woman who I will so marvelingly be raising one of America's great beautiful daughters has been a terrific partner...\n",
            "....and a great mother. Congratulations to my son Donald... https://t.co/WfzK0j9hjm\n",
            "...We got them (sic) but nobody is going to take care of these people like they did to our country. The Democrats....\n",
            "Wow these phony reporters are embarrassed by the way they are treating me. Not nice!\n",
            "I think it is a disgrace that the New York Times and the New York Post would write about me or give me inaccurate or non-existent information. They do not say the RIGHT THING!\n",
            "...to take care of these people (not to mention to make our country prosperous) like they did to our country. They do not know the way! I am very proud of NATO &amp; the NATO alliance!\n",
            "I have made very good deals in business so that nobody even thinks about it anymore. But if I had been president they would be putting it out there!\n",
            "They did the phony (a big part) and now they make the bogus (totally non-existent part) and the press does not like that! They do not know what they are talking about or what they have just written!\n",
            "If we go to war (with Syria) for oil they will do more than anyone. In fact they will be able to supply the rebels many times over and we will go in and fight. Now our allies will be watching!\n",
            "It is my opinion that our country (U.S.A.) must have strong borders and strict immigration laws. Our country is not large enough (but) strong enough (for both its military and police) - and crime will be reduced!\n",
            "I will be interviewed by @MariaBartiromo on @FoxNews at 9 o'clock. Enjoy!\n",
            ".@CNN is on a FAKE NEWS FACTOR!\n",
            "In addition to losing all credibility CNN has also given voice to the many who have left the cable TV business...and will further leave the cable TV business in the future. Very dumb and dangerous!\n",
            "After many years of talking I can confidently say that our enemies know exactly what we are doing in the fight on ISIS and al-Qaida (but) that we can succeed in making them do the same!\n",
            "I will soon be announcing my decision on whether or not to run for President. Time is very precious. Be sure to look for me during the campaign!\n",
            "Thank you to David Brooks for being fair to me but also having the guts to be fair with his readers’ (and many others). Your work has been great!\n",
            ".@David_BrooksCNN is one of the worst on television. I will be watching the ratings very closely. The Dems badly outspent me and outspent them 13-1 in new polling! Great job!\n",
            "@RasmussenPoll has me with a 38 point lead in a four way race in a few days. Let's see if @RealBenCarson is able to run...\n",
            "The #NYCPAC was a great success! We had an amazing turnout. Over 20000 people were in the Park. Was a real crowd of people in the crowd. Huge #NYC rally a must for GOP!\n",
            "Via @BreitbartNews by @mboyle1: \"DONALD TRUMP TO TAKE OVER THE SAME-FRONT PAGE OF THE NEW YORK TIMES AS PRESIDENT!\"https://t.co/T7hWUQeCqH\n",
            "Wow I thought that Mitt Romney was soooo smart and smart and talented. I said it so you would believe it! #trump2016 https://t.co/9MwQfCiVHc\n",
            "Thank you. #Trump2016 #MakeAmericaGreatAgain https://t.co/nk9Jj4Rg2r\n",
            "The failing @nytimes refuses to even use the word \"Russia\" in their reporting.\n",
            ".@FoxNews is one of the worst television shows. @MSNBC and @FNC are the other two. A joke-and not funny!\n",
            "I will be interviewed by @MariaBartiromo on @FoxNews tonight at 9. Enjoy!\n",
            "My speech yesterday in Iowa was a great success. Over 20000 people attended. Thousands turned out to hear me on LARRY KING LIVE! Huge crowd!\n",
            "So much time is left to get these people out of Syria!\n",
            "I am in Iowa now doing large scale rallies - lots of people. I will be watching them live from the stage. I am just\n",
            "\n",
            "[2300 | 4519.59] loss=2.56 avg=2.50\n",
            "[2301 | 4521.30] loss=2.62 avg=2.50\n",
            "[2302 | 4523.02] loss=2.51 avg=2.50\n",
            "[2303 | 4524.74] loss=2.50 avg=2.50\n",
            "[2304 | 4526.47] loss=3.23 avg=2.51\n",
            "[2305 | 4528.20] loss=2.60 avg=2.51\n",
            "[2306 | 4529.91] loss=2.36 avg=2.51\n",
            "[2307 | 4531.62] loss=2.82 avg=2.51\n",
            "[2308 | 4533.33] loss=2.82 avg=2.51\n",
            "[2309 | 4535.04] loss=2.10 avg=2.51\n",
            "[2310 | 4536.75] loss=2.12 avg=2.51\n",
            "[2311 | 4538.46] loss=2.45 avg=2.51\n",
            "[2312 | 4540.17] loss=2.98 avg=2.51\n",
            "[2313 | 4541.91] loss=2.28 avg=2.51\n",
            "[2314 | 4543.64] loss=2.09 avg=2.50\n",
            "[2315 | 4545.37] loss=2.81 avg=2.51\n",
            "[2316 | 4547.10] loss=2.48 avg=2.51\n",
            "[2317 | 4548.84] loss=2.13 avg=2.50\n",
            "[2318 | 4550.56] loss=2.67 avg=2.51\n",
            "[2319 | 4552.28] loss=2.22 avg=2.50\n",
            "[2320 | 4553.99] loss=3.07 avg=2.51\n",
            "[2321 | 4555.70] loss=2.56 avg=2.51\n",
            "[2322 | 4557.41] loss=2.61 avg=2.51\n",
            "[2323 | 4559.12] loss=2.41 avg=2.51\n",
            "[2324 | 4560.83] loss=2.30 avg=2.51\n",
            "[2325 | 4562.55] loss=2.00 avg=2.50\n",
            "[2326 | 4564.28] loss=2.51 avg=2.50\n",
            "[2327 | 4565.99] loss=2.02 avg=2.50\n",
            "[2328 | 4567.70] loss=2.57 avg=2.50\n",
            "[2329 | 4569.41] loss=2.93 avg=2.50\n",
            "[2330 | 4571.13] loss=1.14 avg=2.49\n",
            "[2331 | 4572.84] loss=2.85 avg=2.49\n",
            "[2332 | 4574.55] loss=2.33 avg=2.49\n",
            "[2333 | 4576.27] loss=3.04 avg=2.50\n",
            "[2334 | 4577.98] loss=2.40 avg=2.49\n",
            "[2335 | 4579.69] loss=2.34 avg=2.49\n",
            "[2336 | 4581.41] loss=2.59 avg=2.49\n",
            "[2337 | 4583.13] loss=2.47 avg=2.49\n",
            "[2338 | 4584.84] loss=2.59 avg=2.49\n",
            "[2339 | 4586.56] loss=3.04 avg=2.50\n",
            "[2340 | 4588.27] loss=2.24 avg=2.50\n",
            "[2341 | 4589.98] loss=2.31 avg=2.50\n",
            "[2342 | 4591.69] loss=3.21 avg=2.50\n",
            "[2343 | 4593.41] loss=2.32 avg=2.50\n",
            "[2344 | 4595.12] loss=2.72 avg=2.50\n",
            "[2345 | 4596.83] loss=2.50 avg=2.50\n",
            "[2346 | 4598.54] loss=2.08 avg=2.50\n",
            "[2347 | 4600.25] loss=2.47 avg=2.50\n",
            "[2348 | 4601.96] loss=3.11 avg=2.50\n",
            "[2349 | 4603.67] loss=2.46 avg=2.50\n",
            "[2350 | 4605.39] loss=2.82 avg=2.51\n",
            "[2351 | 4607.10] loss=2.56 avg=2.51\n",
            "[2352 | 4608.81] loss=2.64 avg=2.51\n",
            "[2353 | 4610.51] loss=2.32 avg=2.51\n",
            "[2354 | 4612.23] loss=2.44 avg=2.51\n",
            "[2355 | 4613.94] loss=2.36 avg=2.51\n",
            "[2356 | 4615.65] loss=1.25 avg=2.49\n",
            "[2357 | 4617.36] loss=2.86 avg=2.50\n",
            "[2358 | 4619.07] loss=2.25 avg=2.49\n",
            "[2359 | 4620.78] loss=2.63 avg=2.50\n",
            "[2360 | 4622.49] loss=2.30 avg=2.49\n",
            "[2361 | 4624.20] loss=2.70 avg=2.50\n",
            "[2362 | 4625.91] loss=2.46 avg=2.49\n",
            "[2363 | 4627.62] loss=2.82 avg=2.50\n",
            "[2364 | 4629.33] loss=2.02 avg=2.49\n",
            "[2365 | 4631.04] loss=2.79 avg=2.50\n",
            "[2366 | 4632.78] loss=3.18 avg=2.50\n",
            "[2367 | 4634.49] loss=2.79 avg=2.51\n",
            "[2368 | 4636.22] loss=2.20 avg=2.50\n",
            "[2369 | 4637.93] loss=2.27 avg=2.50\n",
            "[2370 | 4639.65] loss=2.49 avg=2.50\n",
            "[2371 | 4641.37] loss=2.74 avg=2.50\n",
            "[2372 | 4643.07] loss=2.29 avg=2.50\n",
            "[2373 | 4644.79] loss=2.27 avg=2.50\n",
            "[2374 | 4646.50] loss=2.48 avg=2.50\n",
            "[2375 | 4648.22] loss=1.64 avg=2.49\n",
            "[2376 | 4649.94] loss=1.89 avg=2.48\n",
            "[2377 | 4651.65] loss=2.70 avg=2.49\n",
            "[2378 | 4653.37] loss=3.10 avg=2.49\n",
            "[2379 | 4655.08] loss=2.25 avg=2.49\n",
            "[2380 | 4656.79] loss=1.90 avg=2.48\n",
            "[2381 | 4658.51] loss=1.91 avg=2.48\n",
            "[2382 | 4660.22] loss=2.70 avg=2.48\n",
            "[2383 | 4661.93] loss=2.28 avg=2.48\n",
            "[2384 | 4663.64] loss=2.31 avg=2.48\n",
            "[2385 | 4665.35] loss=3.29 avg=2.48\n",
            "[2386 | 4667.06] loss=2.29 avg=2.48\n",
            "[2387 | 4668.77] loss=2.33 avg=2.48\n",
            "[2388 | 4670.48] loss=2.87 avg=2.49\n",
            "[2389 | 4672.19] loss=2.43 avg=2.48\n",
            "[2390 | 4673.90] loss=2.05 avg=2.48\n",
            "[2391 | 4675.61] loss=2.12 avg=2.48\n",
            "[2392 | 4677.32] loss=1.79 avg=2.47\n",
            "[2393 | 4679.04] loss=2.23 avg=2.47\n",
            "[2394 | 4680.75] loss=2.49 avg=2.47\n",
            "[2395 | 4682.46] loss=2.45 avg=2.47\n",
            "[2396 | 4684.17] loss=2.37 avg=2.47\n",
            "[2397 | 4685.89] loss=2.69 avg=2.47\n",
            "[2398 | 4687.60] loss=2.15 avg=2.47\n",
            "[2399 | 4689.31] loss=2.17 avg=2.46\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " http://t.co/N6CvN9z1wA\n",
            "Congratulations to my brother @RickRubio on being picked as winner of the @CadillacChamp at @Trump_Charlotte. Amazing driver! http://t.co/1H1CxDcZ3y\n",
            "Via @ABCPolitics by @ajdixon8: “Donald Trump: Trump Names Coming From My Mother” http://t.co/8T2p9tPvLc\n",
            ".@wedge_garden  Thanks!  #WedgeGarden2012  http://t.co/0pvGm3R5Yd\n",
            "Thank you Eric on MSNBC.  #trump2012 Eric was awesome!\n",
            "Via The Atlantic by @mjroep: “How Donald Trump Made Money: His Father Donald and His Business” http://t.co/9TvbF3HjIc\n",
            "#trump2012 #TrumpNow! #TrumpPodcast #trump2012 http://t.co/7fK6XKd1hM\n",
            "RT @Kerrygold: @EricTrump @ApprenticeNBC   My time with the Trumps at @EricTrump's Trampoline and Locker Rooms in Dubai http://t.co/uE8TmZzT2S\n",
            "“In an age of endless possibilities money is no longer just a way to make a living. It's a fundamental tool.” – Think Like a Billionaire\n",
            "\"In his own words Trump: 'a completely different type of person'\" http://t.co/KcDyf6hDdO\n",
            "Via @nydailynews by @timmccoyle: “Donald Trump: I Wouldn’t Have Pledgled for the Presidency If I Had Known” http://t.co/QsWb9zV9bG\n",
            "Trump Int'l Hotel &amp; Tower Chicago has been rated the best hotel &amp; hotel accommodations in the country by @chicagotribune\n",
            "“Success breeds success.” - @ashleywalsh\n",
            "Congratulations to @GovWalker on his big win in Wisconsin.  Wisconsin is a wonderful state!  Good night.\n",
            "\"Trump: ‘This Is Something That Can Happen To You’’’Win or Lose.”\" http://t.co/u8CcGKpqJ6 via @businessinsider\n",
            "My @foxandfriends interview discussing Trump Int'l Doha hotel project my $250M proposal and my twitter account http://t.co/QjhqFZcj1A\n",
            "The problem here is that the press is the most inaccurate &amp; biased  source of news. The #1 reason for this is because the press refuses to put names out!\n",
            "My @foxandfriends interview discussing my announcement that @EricTrump will be the new Executive Chairman of @TrumpGolf @EricTrumpGCGC &amp; @CadillacChamp at #TrumpsCharlotte  http://t.co/XjZb5T3Jf4\n",
            "I am a very honest person. The media is very dishonest and so is the American public. We are letting them off the hook for the next go around!\n",
            "My interview from last week with Sean Haugh.  http://t.co/l8S7pzYwPQ\n",
            "My interview from this morning with @gretawire. http://t.co/8QfKfWkXh1\n",
            "My @foxandfriends interview this morning discussing my announcement that @EricTrump will be  Executive Chairman of @TrumpGolf @EricTrumpGCGC &amp; @CadillacChamp at #TrumpsCharlotte  http://t.co/Xkzv3IhNhQ\n",
            "I went for a jog at Trump National Doral this morning.\n",
            "The last time Obama released his college applications it turned out that Romney went to Harvard and MIT. How much money is the student loan system taking away from America?\n",
            "I hear that @MittRomney is doing good in Virginia - he was supposed to do terribly but he just won.\n",
            ".@MittRomney should put his money where his mouth is after his disastrous commercial for AIG.\n",
            "Great news in Alabama that President Obama has been out of state for over 2 weeks. Great news to bring a great new business genius to our country!\n",
            ".@EricTrump has created one of the most respected and successful companies in the Country. He was very excited to be named the new Executive Chairman of the @TrumpCause.\n",
            "The fact that @EricTrump owns &amp; controls @EricTrumpCharlotte is another great fact that people should know. EricTrump already owned &amp; controls @EricTrumpGC.  Excellent &amp; happy!\n",
            "Via @TheAtlantic by\n",
            "\n",
            "[2400 | 4713.30] loss=2.20 avg=2.46\n",
            "[2401 | 4715.01] loss=2.57 avg=2.46\n",
            "[2402 | 4716.73] loss=2.46 avg=2.46\n",
            "[2403 | 4718.44] loss=2.15 avg=2.46\n",
            "[2404 | 4720.16] loss=2.85 avg=2.46\n",
            "[2405 | 4721.87] loss=2.17 avg=2.46\n",
            "[2406 | 4723.59] loss=2.55 avg=2.46\n",
            "[2407 | 4725.33] loss=2.73 avg=2.46\n",
            "[2408 | 4727.04] loss=1.77 avg=2.46\n",
            "[2409 | 4728.75] loss=2.38 avg=2.45\n",
            "[2410 | 4730.46] loss=2.58 avg=2.46\n",
            "[2411 | 4732.17] loss=2.45 avg=2.46\n",
            "[2412 | 4733.89] loss=2.81 avg=2.46\n",
            "[2413 | 4735.60] loss=1.90 avg=2.45\n",
            "[2414 | 4737.31] loss=1.72 avg=2.45\n",
            "[2415 | 4739.02] loss=2.13 avg=2.44\n",
            "[2416 | 4740.73] loss=2.45 avg=2.44\n",
            "[2417 | 4742.44] loss=2.24 avg=2.44\n",
            "[2418 | 4744.15] loss=2.37 avg=2.44\n",
            "[2419 | 4745.86] loss=2.66 avg=2.44\n",
            "[2420 | 4747.58] loss=3.01 avg=2.45\n",
            "[2421 | 4749.28] loss=2.42 avg=2.45\n",
            "[2422 | 4750.99] loss=2.31 avg=2.45\n",
            "[2423 | 4752.70] loss=3.25 avg=2.45\n",
            "[2424 | 4754.42] loss=2.74 avg=2.46\n",
            "[2425 | 4756.12] loss=2.81 avg=2.46\n",
            "[2426 | 4757.83] loss=1.83 avg=2.46\n",
            "[2427 | 4759.54] loss=2.56 avg=2.46\n",
            "[2428 | 4761.25] loss=2.39 avg=2.46\n",
            "[2429 | 4762.97] loss=2.33 avg=2.45\n",
            "[2430 | 4764.68] loss=2.38 avg=2.45\n",
            "[2431 | 4766.41] loss=3.19 avg=2.46\n",
            "[2432 | 4768.12] loss=2.33 avg=2.46\n",
            "[2433 | 4769.83] loss=2.62 avg=2.46\n",
            "[2434 | 4771.54] loss=2.87 avg=2.47\n",
            "[2435 | 4773.26] loss=2.28 avg=2.46\n",
            "[2436 | 4774.97] loss=2.88 avg=2.47\n",
            "[2437 | 4776.68] loss=1.60 avg=2.46\n",
            "[2438 | 4778.39] loss=2.60 avg=2.46\n",
            "[2439 | 4780.11] loss=1.75 avg=2.45\n",
            "[2440 | 4781.83] loss=2.35 avg=2.45\n",
            "[2441 | 4783.54] loss=2.45 avg=2.45\n",
            "[2442 | 4785.26] loss=2.19 avg=2.45\n",
            "[2443 | 4786.97] loss=2.52 avg=2.45\n",
            "[2444 | 4788.68] loss=2.50 avg=2.45\n",
            "[2445 | 4790.40] loss=2.38 avg=2.45\n",
            "[2446 | 4792.11] loss=2.50 avg=2.45\n",
            "[2447 | 4793.82] loss=2.56 avg=2.45\n",
            "[2448 | 4795.54] loss=2.39 avg=2.45\n",
            "[2449 | 4797.25] loss=2.27 avg=2.45\n",
            "[2450 | 4798.96] loss=3.16 avg=2.46\n",
            "[2451 | 4800.67] loss=2.68 avg=2.46\n",
            "[2452 | 4802.38] loss=1.94 avg=2.45\n",
            "[2453 | 4804.09] loss=2.72 avg=2.46\n",
            "[2454 | 4805.80] loss=2.30 avg=2.45\n",
            "[2455 | 4807.51] loss=2.33 avg=2.45\n",
            "[2456 | 4809.22] loss=2.26 avg=2.45\n",
            "[2457 | 4810.93] loss=2.71 avg=2.45\n",
            "[2458 | 4812.65] loss=2.45 avg=2.45\n",
            "[2459 | 4814.36] loss=2.38 avg=2.45\n",
            "[2460 | 4816.07] loss=2.82 avg=2.46\n",
            "[2461 | 4817.78] loss=2.90 avg=2.46\n",
            "[2462 | 4819.49] loss=2.42 avg=2.46\n",
            "[2463 | 4821.20] loss=2.40 avg=2.46\n",
            "[2464 | 4822.91] loss=2.09 avg=2.46\n",
            "[2465 | 4824.63] loss=2.86 avg=2.46\n",
            "[2466 | 4826.34] loss=2.70 avg=2.46\n",
            "[2467 | 4828.05] loss=2.63 avg=2.46\n",
            "[2468 | 4829.76] loss=2.83 avg=2.47\n",
            "[2469 | 4831.47] loss=2.19 avg=2.47\n",
            "[2470 | 4833.18] loss=2.42 avg=2.47\n",
            "[2471 | 4834.90] loss=2.36 avg=2.46\n",
            "[2472 | 4836.61] loss=2.68 avg=2.47\n",
            "[2473 | 4838.34] loss=2.47 avg=2.47\n",
            "[2474 | 4840.07] loss=2.39 avg=2.47\n",
            "[2475 | 4841.79] loss=2.15 avg=2.46\n",
            "[2476 | 4843.50] loss=2.38 avg=2.46\n",
            "[2477 | 4845.21] loss=2.44 avg=2.46\n",
            "[2478 | 4846.93] loss=2.79 avg=2.46\n",
            "[2479 | 4848.64] loss=2.23 avg=2.46\n",
            "[2480 | 4850.36] loss=2.09 avg=2.46\n",
            "[2481 | 4852.07] loss=2.50 avg=2.46\n",
            "[2482 | 4853.79] loss=2.67 avg=2.46\n",
            "[2483 | 4855.50] loss=2.80 avg=2.46\n",
            "[2484 | 4857.21] loss=2.16 avg=2.46\n",
            "[2485 | 4858.93] loss=2.34 avg=2.46\n",
            "[2486 | 4860.64] loss=2.32 avg=2.46\n",
            "[2487 | 4862.38] loss=1.90 avg=2.45\n",
            "[2488 | 4864.10] loss=2.06 avg=2.45\n",
            "[2489 | 4865.81] loss=2.50 avg=2.45\n",
            "[2490 | 4867.53] loss=2.59 avg=2.45\n",
            "[2491 | 4869.24] loss=2.30 avg=2.45\n",
            "[2492 | 4870.96] loss=2.97 avg=2.45\n",
            "[2493 | 4872.68] loss=2.51 avg=2.46\n",
            "[2494 | 4874.39] loss=2.28 avg=2.45\n",
            "[2495 | 4876.10] loss=2.86 avg=2.46\n",
            "[2496 | 4877.81] loss=2.44 avg=2.46\n",
            "[2497 | 4879.55] loss=2.58 avg=2.46\n",
            "[2498 | 4881.28] loss=2.93 avg=2.46\n",
            "[2499 | 4882.98] loss=2.60 avg=2.46\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " buy the house?\n",
            "@JaxxX2  You should!\n",
            "@PippaWomans We would be better off working together!\n",
            "@BiggiLee  Very true!\n",
            "“The best defense is the offense”--George Steinbrenner\n",
            "@bensonrobot Thank you.\n",
            "@Ajarmio1993  Yes and thanks!\n",
            "@bensonrobot  Will be soon.\n",
            "@Hollywood_Bloke Thank you.\n",
            "@BiggiLee  Thanks and enjoy.\n",
            "@JenssChen  Great!\n",
            "@S_Bliley826.  They are good fans!\n",
            "@The_Mageo This is just more of the same in every area except New York State!\n",
            "@Tommo_Gee Many will want to invest or refinance.\n",
            "@kylethompson. Thank you.\n",
            "We are going to need a much greater energy by the military.\n",
            "@Mike_Lampson Thanks.\n",
            "We have got to restore our economic strength not weaken the U.S.\n",
            "“The president's statement on DACA is a good example of why Obama put the Pentagon on high alert” @foxandfriends\n",
            "@MaddieYeung I agree!\n",
            "@The_Trump_Traveler Good idea!\n",
            "“Great talent from all over the world” a great honor!\n",
            "@taylorswift10. Thank you.\n",
            "@sarahm_paull.  Happy birthday and enjoy.\n",
            "@macy’s.  Thank you!\n",
            "@MyrandaJ   That's good!\n",
            "@harrymatthew. Thank you for all of the nice words Cathy Miller. You are a wonderful actress and writer.\n",
            "Thank you- a great job by my great and very talented CEO Rupert Murdoch in helping to put a Democrat in office!@foxandfriends\n",
            "@theTigerFalls  Thanks!\n",
            "@DeeDee_Dee @realDonaldTrump  Thank you!\n",
            "@maryvannita Thanks!\n",
            "@maddiewinnall @bretmichaels Very nice thank you- can't get much better!\n",
            "@the_beeker  @ApprenticeNBC @nbcthrones Thanks.\n",
            "@JaeKyeongLee  Thank you Jae.\n",
            "@louisbowie Thanks.\n",
            "@Trevor_Bassner. They will be great!\n",
            "The failing @nydailynews is finally admitting that David Carr is not a brilliant reporter so he can finally be fired.\n",
            "Good luck in the next job interview with a reputable media company--a great network like @CNN is a must.\n",
            "In New York City there is a lot of money to be made from Real Estate. Being near high rise construction is worth more a location than living in--be smart!\n",
            "“Never take orders in a negotiation. Keep your own opinions and vision to yourself.” – The Art of the Negotiation\n",
            "\"There are people who are smarter than we give them credit for who are working day in and day out to win something.\" -- Steve Jobs\n",
            "@bryanmichaels - thanks &amp; good luck.\n",
            "@michaelsamerica Great!\n",
            "@Bruh. Great!\n",
            "@the_mini - great.\n",
            "@wendycstevens Thanks Wendy.\n",
            "@TayM_Pence @ApprenticeNBC @nbcthrones Thanks.\n",
            "Great! Watch!\n",
            "Watching Bob Morley on the @meetthepress - his final show. http://t.co/Wd0w7Q4T\n",
            "@CindyMast\n",
            "I just returned from Iowa where we all worked together on The Apprentice. I watched a tape of my interview there &amp; how it went - really fun! Thanks Cindy.\n",
            "How ironic that our president will be meeting today to save the USN fleet. Good luck &amp; remember US-U.S.A.\n",
            "I am very proud of my son Donald who is working hard but also really smart! @IvankaTrump says that he has many future opportunities. We also see lots of power\n",
            "Great!\n",
            "We are in big trouble - we are being ripped off by nations around the world. President Obama is the greatest           idiot he can think of!\n",
            "We must stop Chinese theft of our wealth.  http://t.co/Xh8LkPJt                                           \n",
            "China is doing massive numbers on $1T &amp; is about to spend $1T this year in infrastructure spending. The US must get together and work together!\n",
            "China is stealing $1\n",
            "\n",
            "[2500 | 4906.87] loss=2.98 avg=2.47\n",
            "[2501 | 4908.58] loss=1.89 avg=2.46\n",
            "[2502 | 4910.29] loss=2.84 avg=2.47\n",
            "[2503 | 4912.03] loss=2.63 avg=2.47\n",
            "[2504 | 4913.75] loss=1.45 avg=2.46\n",
            "[2505 | 4915.47] loss=2.64 avg=2.46\n",
            "[2506 | 4917.19] loss=2.67 avg=2.46\n",
            "[2507 | 4918.90] loss=2.52 avg=2.46\n",
            "[2508 | 4920.62] loss=2.07 avg=2.46\n",
            "[2509 | 4922.34] loss=2.08 avg=2.46\n",
            "[2510 | 4924.05] loss=2.87 avg=2.46\n",
            "[2511 | 4925.77] loss=1.65 avg=2.45\n",
            "[2512 | 4927.48] loss=2.43 avg=2.45\n",
            "[2513 | 4929.19] loss=2.57 avg=2.45\n",
            "[2514 | 4930.90] loss=2.23 avg=2.45\n",
            "[2515 | 4932.61] loss=2.67 avg=2.45\n",
            "[2516 | 4934.32] loss=3.30 avg=2.46\n",
            "[2517 | 4936.03] loss=2.05 avg=2.46\n",
            "[2518 | 4937.74] loss=1.89 avg=2.45\n",
            "[2519 | 4939.45] loss=2.76 avg=2.45\n",
            "[2520 | 4941.16] loss=2.34 avg=2.45\n",
            "[2521 | 4942.89] loss=1.82 avg=2.45\n",
            "[2522 | 4944.62] loss=2.55 avg=2.45\n",
            "[2523 | 4946.33] loss=2.54 avg=2.45\n",
            "[2524 | 4948.04] loss=2.40 avg=2.45\n",
            "[2525 | 4949.75] loss=2.37 avg=2.45\n",
            "[2526 | 4951.49] loss=2.51 avg=2.45\n",
            "[2527 | 4953.20] loss=2.27 avg=2.45\n",
            "[2528 | 4954.91] loss=2.47 avg=2.45\n",
            "[2529 | 4956.62] loss=2.53 avg=2.45\n",
            "[2530 | 4958.36] loss=2.45 avg=2.45\n",
            "[2531 | 4960.06] loss=2.46 avg=2.45\n",
            "[2532 | 4961.77] loss=2.21 avg=2.45\n",
            "[2533 | 4963.49] loss=2.17 avg=2.44\n",
            "[2534 | 4965.20] loss=2.37 avg=2.44\n",
            "[2535 | 4966.91] loss=2.97 avg=2.45\n",
            "[2536 | 4968.62] loss=2.56 avg=2.45\n",
            "[2537 | 4970.34] loss=2.19 avg=2.45\n",
            "[2538 | 4972.05] loss=2.22 avg=2.44\n",
            "[2539 | 4973.76] loss=2.22 avg=2.44\n",
            "[2540 | 4975.48] loss=2.78 avg=2.44\n",
            "[2541 | 4977.19] loss=3.25 avg=2.45\n",
            "[2542 | 4978.90] loss=2.13 avg=2.45\n",
            "[2543 | 4980.62] loss=2.74 avg=2.45\n",
            "[2544 | 4982.32] loss=2.50 avg=2.45\n",
            "[2545 | 4984.04] loss=2.96 avg=2.46\n",
            "[2546 | 4985.76] loss=2.77 avg=2.46\n",
            "[2547 | 4987.47] loss=2.54 avg=2.46\n",
            "[2548 | 4989.19] loss=2.41 avg=2.46\n",
            "[2549 | 4990.90] loss=2.47 avg=2.46\n",
            "[2550 | 4992.62] loss=2.44 avg=2.46\n",
            "[2551 | 4994.33] loss=2.41 avg=2.46\n",
            "[2552 | 4996.05] loss=2.26 avg=2.46\n",
            "[2553 | 4997.76] loss=2.86 avg=2.46\n",
            "[2554 | 4999.47] loss=2.02 avg=2.46\n",
            "[2555 | 5001.19] loss=2.65 avg=2.46\n",
            "[2556 | 5002.90] loss=2.28 avg=2.46\n",
            "[2557 | 5004.61] loss=2.36 avg=2.46\n",
            "[2558 | 5006.32] loss=2.79 avg=2.46\n",
            "[2559 | 5008.03] loss=2.51 avg=2.46\n",
            "[2560 | 5009.74] loss=2.14 avg=2.46\n",
            "[2561 | 5011.46] loss=2.42 avg=2.46\n",
            "[2562 | 5013.17] loss=2.90 avg=2.46\n",
            "[2563 | 5014.88] loss=2.76 avg=2.46\n",
            "[2564 | 5016.60] loss=2.62 avg=2.47\n",
            "[2565 | 5018.30] loss=2.06 avg=2.46\n",
            "[2566 | 5020.02] loss=2.48 avg=2.46\n",
            "[2567 | 5021.73] loss=2.24 avg=2.46\n",
            "[2568 | 5023.44] loss=1.75 avg=2.45\n",
            "[2569 | 5025.15] loss=2.95 avg=2.46\n",
            "[2570 | 5026.86] loss=2.77 avg=2.46\n",
            "[2571 | 5028.58] loss=1.97 avg=2.46\n",
            "[2572 | 5030.31] loss=1.99 avg=2.45\n",
            "[2573 | 5032.01] loss=2.60 avg=2.45\n",
            "[2574 | 5033.73] loss=2.83 avg=2.46\n",
            "[2575 | 5035.44] loss=2.33 avg=2.46\n",
            "[2576 | 5037.15] loss=2.23 avg=2.45\n",
            "[2577 | 5038.86] loss=2.77 avg=2.46\n",
            "[2578 | 5040.57] loss=2.37 avg=2.46\n",
            "[2579 | 5042.28] loss=2.39 avg=2.46\n",
            "[2580 | 5043.99] loss=2.31 avg=2.45\n",
            "[2581 | 5045.70] loss=2.32 avg=2.45\n",
            "[2582 | 5047.42] loss=2.67 avg=2.45\n",
            "[2583 | 5049.13] loss=2.31 avg=2.45\n",
            "[2584 | 5050.85] loss=2.10 avg=2.45\n",
            "[2585 | 5052.57] loss=2.66 avg=2.45\n",
            "[2586 | 5054.28] loss=2.34 avg=2.45\n",
            "[2587 | 5056.00] loss=1.99 avg=2.45\n",
            "[2588 | 5057.71] loss=2.00 avg=2.44\n",
            "[2589 | 5059.42] loss=2.05 avg=2.44\n",
            "[2590 | 5061.14] loss=2.19 avg=2.44\n",
            "[2591 | 5062.86] loss=2.68 avg=2.44\n",
            "[2592 | 5064.57] loss=2.83 avg=2.44\n",
            "[2593 | 5066.28] loss=2.15 avg=2.44\n",
            "[2594 | 5067.99] loss=2.43 avg=2.44\n",
            "[2595 | 5069.71] loss=2.09 avg=2.43\n",
            "[2596 | 5071.41] loss=2.48 avg=2.44\n",
            "[2597 | 5073.12] loss=2.80 avg=2.44\n",
            "[2598 | 5074.84] loss=2.15 avg=2.44\n",
            "[2599 | 5076.54] loss=2.32 avg=2.43\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " there are some serious issues here and as I said the Democrats and the Republicans must work together to get rid of this total out of control Democrat and Republican obstruction. Get on a path of Progress and get the job done!\n",
            "I have decided to pull out of the Paris accords because the U.S. is being unfairly treated and treated very badly in the negotiations. Very unfair to the U.S.A. and to our GREATEST COUNTRIES. I also have long had a problem with this so much time has gone by with very little result!\n",
            "The U.S.A. and Europe must continue to make some of the biggest and most important and very complex trade deals ever. Most difficult to do and as I said with the long and complicated discussions I am very happy with where we are today!\n",
            "It is time for a complete rethink! What was once called Climate Change is changing so fast that the term is hard to believe has not even begun to describe the phenomena that is occurring. There is no other word!\n",
            "If the United States allows ISIS forces to continue to rule over our Southern Province then we (in the U.S.) will be using up a fortune and losing the war. If they (ISIS) are allowed to continue to rule over our Northern Province then we (in the U.S.) may not get captured or hold onto an inch of our territory!\n",
            "“Austerity kills the jobs market.” @MittRomney\n",
            "It has to be the government. Our economy must be strong first and foremost. Fiscal Responsibility requires balanced budget and debt reduction. Budget deficits will slow growth and waste trillions of dollars. We must start from a point of  Fiscal Responsibility by 2025.\n",
            "Obama doesn’t like the words “Obamacare.” However it is a disaster the way it is working out. It is an open secret Republicans and the President agree upon but the Fake Media just doesn’t want to report it!\n",
            "Obama would consider taking in Syrian refugees as long as there is a way to shut the entire flow. Look at the horrible situation in the Middle East - what has happened and what is being done? This is an emergency and must be solved!\n",
            "@Sally_Bridgewater It will be a good thing if there is a plan in place to have a massive and long drawn out trial of Judge Kavanaugh. There can be no innocent until guilty. We must find a result quickly and get Justice done for the people of New York and America. The world is watching.\n",
            "“Donald Trump: 'President of the U.S.A.'” http://t.co/pDz9KW5Z3K via @espn @foxandfriends\n",
            "My @SallyHarvard Facebook Group: “The Case for Trump” http://t.co/KLxj0Uqg8n\n",
            "Today marks the one hundred year anniversary of the birth of Abraham Lincoln the first president to reside in a small one room North Carolina home. What a day it has been for me and my family! We are grateful and very happy for Lincoln’s service to this great country.\n",
            "When the president of the United States decides he wants to celebrate the one hundred year anniversary of the birth of Abraham Lincoln the first black President then he is very much like me (very lucky)!\n",
            "We need to fix our broken immigration system. Our border should be secure and our southern border should not be open to drugs and criminals. The Democrats have no answers or willingness to listen and have been such bad negotiators and negotiators have always lost!\n",
            "\"Trump Says He's Thinking About Reviving His 'Celebrity Apprentice'?\" <http://t.co/6NlZBpFZsB> by @MELANIATRUMP  @FoxNews  on @TODAYshow. Good move Abe--thank you! @ScottStapleton  @ScottStapleton  @SeanHannity  @foxandfriends   via @theBrodyFile\n",
            "\"Donald Trump Pivots To Trump Tops Listening To Beatles Song For First Time\" http://t.co/Q9tIhCQxzQ via @Srdpic\n",
            "On today’s 100 Day Focused On Work we will get things done in Washington D.C. not on Twitter using a political website or on facebook selling ads. We will Make America Great Again!\n",
            "\"We will be judged not on the content of our character but on whether or not we are able to deliver for our families &amp; our communities.\"   – @SenTedCruz #MakeAmericaGreatAgain @GOP https://t.co/nMl8j8k4nA\n",
            "@BarbaraJWalters  Just like the beautiful words I just stated I am 100% behind you @SenatorWarren. You’ve done great things for people and you will continue to do so. @BarbaraJWal\n",
            "\n",
            "[2600 | 5100.39] loss=1.96 avg=2.43\n",
            "[2601 | 5102.11] loss=2.61 avg=2.43\n",
            "[2602 | 5103.82] loss=1.96 avg=2.43\n",
            "[2603 | 5105.53] loss=2.52 avg=2.43\n",
            "[2604 | 5107.24] loss=2.41 avg=2.43\n",
            "[2605 | 5108.95] loss=2.61 avg=2.43\n",
            "[2606 | 5110.69] loss=2.05 avg=2.43\n",
            "[2607 | 5112.42] loss=2.17 avg=2.42\n",
            "[2608 | 5114.13] loss=1.87 avg=2.42\n",
            "[2609 | 5115.84] loss=2.53 avg=2.42\n",
            "[2610 | 5117.56] loss=2.77 avg=2.42\n",
            "[2611 | 5119.27] loss=1.84 avg=2.42\n",
            "[2612 | 5120.99] loss=2.95 avg=2.42\n",
            "[2613 | 5122.70] loss=2.83 avg=2.43\n",
            "[2614 | 5124.42] loss=2.20 avg=2.42\n",
            "[2615 | 5126.13] loss=2.63 avg=2.43\n",
            "[2616 | 5127.85] loss=2.29 avg=2.42\n",
            "[2617 | 5129.57] loss=2.82 avg=2.43\n",
            "[2618 | 5131.28] loss=2.89 avg=2.43\n",
            "[2619 | 5133.00] loss=2.32 avg=2.43\n",
            "[2620 | 5134.71] loss=2.23 avg=2.43\n",
            "[2621 | 5136.45] loss=2.56 avg=2.43\n",
            "[2622 | 5138.16] loss=2.70 avg=2.43\n",
            "[2623 | 5139.87] loss=2.77 avg=2.44\n",
            "[2624 | 5141.58] loss=2.84 avg=2.44\n",
            "[2625 | 5143.29] loss=2.07 avg=2.44\n",
            "[2626 | 5145.00] loss=2.35 avg=2.44\n",
            "[2627 | 5146.72] loss=2.61 avg=2.44\n",
            "[2628 | 5148.43] loss=2.42 avg=2.44\n",
            "[2629 | 5150.14] loss=2.49 avg=2.44\n",
            "[2630 | 5151.85] loss=2.28 avg=2.44\n",
            "[2631 | 5153.57] loss=2.99 avg=2.44\n",
            "[2632 | 5155.28] loss=2.42 avg=2.44\n",
            "[2633 | 5156.99] loss=3.01 avg=2.45\n",
            "[2634 | 5158.70] loss=2.11 avg=2.45\n",
            "[2635 | 5160.41] loss=2.58 avg=2.45\n",
            "[2636 | 5162.12] loss=2.31 avg=2.45\n",
            "[2637 | 5163.83] loss=2.67 avg=2.45\n",
            "[2638 | 5165.54] loss=2.55 avg=2.45\n",
            "[2639 | 5167.25] loss=2.36 avg=2.45\n",
            "[2640 | 5168.96] loss=2.17 avg=2.44\n",
            "[2641 | 5170.67] loss=2.56 avg=2.45\n",
            "[2642 | 5172.39] loss=2.67 avg=2.45\n",
            "[2643 | 5174.10] loss=2.40 avg=2.45\n",
            "[2644 | 5175.81] loss=2.36 avg=2.45\n",
            "[2645 | 5177.52] loss=2.94 avg=2.45\n",
            "[2646 | 5179.23] loss=2.10 avg=2.45\n",
            "[2647 | 5180.95] loss=2.86 avg=2.45\n",
            "[2648 | 5182.66] loss=2.23 avg=2.45\n",
            "[2649 | 5184.38] loss=1.91 avg=2.44\n",
            "[2650 | 5186.09] loss=1.92 avg=2.44\n",
            "[2651 | 5187.81] loss=2.08 avg=2.44\n",
            "[2652 | 5189.52] loss=2.19 avg=2.43\n",
            "[2653 | 5191.25] loss=2.55 avg=2.43\n",
            "[2654 | 5192.96] loss=2.50 avg=2.44\n",
            "[2655 | 5194.67] loss=1.96 avg=2.43\n",
            "[2656 | 5196.39] loss=2.89 avg=2.44\n",
            "[2657 | 5198.10] loss=2.76 avg=2.44\n",
            "[2658 | 5199.81] loss=2.01 avg=2.43\n",
            "[2659 | 5201.53] loss=1.89 avg=2.43\n",
            "[2660 | 5203.24] loss=2.15 avg=2.43\n",
            "[2661 | 5204.95] loss=2.48 avg=2.43\n",
            "[2662 | 5206.66] loss=2.04 avg=2.42\n",
            "[2663 | 5208.37] loss=2.99 avg=2.43\n",
            "[2664 | 5210.08] loss=2.10 avg=2.42\n",
            "[2665 | 5211.79] loss=1.61 avg=2.42\n",
            "[2666 | 5213.50] loss=2.10 avg=2.41\n",
            "[2667 | 5215.21] loss=2.17 avg=2.41\n",
            "[2668 | 5216.92] loss=1.83 avg=2.41\n",
            "[2669 | 5218.64] loss=2.90 avg=2.41\n",
            "[2670 | 5220.34] loss=2.32 avg=2.41\n",
            "[2671 | 5222.05] loss=2.26 avg=2.41\n",
            "[2672 | 5223.77] loss=2.85 avg=2.41\n",
            "[2673 | 5225.48] loss=2.95 avg=2.42\n",
            "[2674 | 5227.19] loss=2.36 avg=2.42\n",
            "[2675 | 5228.93] loss=2.42 avg=2.42\n",
            "[2676 | 5230.63] loss=2.59 avg=2.42\n",
            "[2677 | 5232.35] loss=1.81 avg=2.41\n",
            "[2678 | 5234.06] loss=2.10 avg=2.41\n",
            "[2679 | 5235.77] loss=2.29 avg=2.41\n",
            "[2680 | 5237.48] loss=2.00 avg=2.40\n",
            "[2681 | 5239.19] loss=2.43 avg=2.40\n",
            "[2682 | 5240.90] loss=2.05 avg=2.40\n",
            "[2683 | 5242.64] loss=2.65 avg=2.40\n",
            "[2684 | 5244.36] loss=2.72 avg=2.41\n",
            "[2685 | 5246.08] loss=2.40 avg=2.41\n",
            "[2686 | 5247.79] loss=2.61 avg=2.41\n",
            "[2687 | 5249.52] loss=2.42 avg=2.41\n",
            "[2688 | 5251.25] loss=2.37 avg=2.41\n",
            "[2689 | 5252.97] loss=1.87 avg=2.40\n",
            "[2690 | 5254.68] loss=2.62 avg=2.41\n",
            "[2691 | 5256.39] loss=2.28 avg=2.40\n",
            "[2692 | 5258.10] loss=2.88 avg=2.41\n",
            "[2693 | 5259.82] loss=1.21 avg=2.40\n",
            "[2694 | 5261.53] loss=2.90 avg=2.40\n",
            "[2695 | 5263.25] loss=2.00 avg=2.40\n",
            "[2696 | 5264.97] loss=2.16 avg=2.40\n",
            "[2697 | 5266.68] loss=2.15 avg=2.39\n",
            "[2698 | 5268.40] loss=1.72 avg=2.39\n",
            "[2699 | 5270.11] loss=2.40 avg=2.39\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "DK5jXwLk\n",
            "We're bringing back our jobs. @RealDonaldTrump has an infrastructure plan that will bring our economy back &amp; create thousands of new jobs. We must rebuild our military &amp; rebuild our military.\n",
            "I hope the House and the Administration will join me in calling for an immediate vote on the Veterans Choice Program!“https://t.co/Qx0dFQ7TjE\n",
            "#VeteransChoice https://t.co/0JUO3IgFZv\n",
            "It is time to give every U.S. Veteran the medical care and care they so richly deserve-and I mean EVERY VETERAN - the Department of Veterans Affairs must do a much better job!#VAreform\n",
            "We have made so much progress on the border...but the tide has turned against our great Vets &amp; family! @HouseGOP must act quickly on the Choice Program!\n",
            "We must protect our Country! Veterans are being abused at the Border! If your Vets have to endure long wait lists for care it is time to do something about it. Vets deserve the care they have always been entitled to!\n",
            "If Republicans wanted to end the \"Drain the Swamp\" they would vote for Choice. Our Republican friends are now blocking the DREAM Act and have been working so hard on it. Our Military has done a great job...\n",
            "...and Democrats are the ones who are working so hard to lose us. Their long winded speeches on Repeal and Replace are only speeches-they are working to take us backwards!\n",
            "Republicans are working hard to come up with a replacement plan for #VAreform and they should be very proud of what they have accomplished....but our Veterans are BEYOND PERFECT just waiting for care!\n",
            "I will be visiting our great Veterans today in West Virginia but don’t worry. We will see you soon in West Virginia!\n",
            "....in Congress with the goal of getting it passed. My promise to our Veterans was so clear: Do whatever is necessary to end the torture and abuse of prisoners by our own....\n",
            "...leaders. Many Democrats are still opposing us for no good reason. This is a long overdue reform but this is the only way we can finally get some justice and accountability for previous Administrations lies...\n",
            "When I left Congress it was my intention to bring all of our great Men &amp; Women back home home from the fight side. Unfortunately we now know that would be difficult to do!\n",
            "“Our economy is booming. There is great optimism about the economy. More and more people are keeping their promises and returning to work. Our Military is doing well.” @LouDobbs  @TuckerCarlson\n",
            "The Democrats are killing the Military Budget. It is impossible to get a vote on Repeal &amp; Replace with the current spending levels. Not good!\n",
            "...unfinished Business. All of our great progress has the promise of greater success after Repeal &amp; Governing. We are not there yet but we have made great progress!\n",
            "@TrumpChicago @KathyKohanNYC @IvankaTrump @EricTrump @DonaldJTrumpJr @EricTrumpFilm   The skyline is spectacular in person!  @FoxNews @kimguilfoyle  A special thanks!\n",
            "It is time to come together to do right by our great Veterans! My statement on Veterans Choice coming up at 11:00 A.M.  On my way to West Virginia to discuss new Military Budget.\n",
            "Thank you @IvankaTrump for your wonderful comment for tomorrow. I’m going to sign a proclamation to bring all of our great Men &amp; Women home from the fight-side. They are suffering horrible.\n",
            "We will get this job done. It’s not difficult....it is what makes it difficult. We will achieve great success. Let’s get this done!\n",
            "Congratulations to the Washington Redskins on their new head coach-Randy Edsall. A big difference from last year...\n",
            "Our great Military is on top of the entire world. Now they want to abolish it and then tell us to start over. They have no idea what they are doing with us.\n",
            "“@IvankaTrump’s Twitter feed continues to generate more than 37.5 million followers per’s @BreitbartNews.” Thank you for your support.\n",
            "The new White House press room layout is amazing! https://t.co/z4h6qQxMn8\n",
            "The United States Marine Corps has confirmed that they are doing a tremendous job securing the south coast. I look forward to welcoming them as they leave tomorrow for the holidays.\n",
            ".@FoxNews’s Greg Gutfeld is going crazy in attacking me. He is not a Republican and he never will be. He is a lightweight who does not compete. A major loser!\n",
            ".@kimguil\n",
            "\n",
            "[2700 | 5293.92] loss=2.33 avg=2.39\n",
            "[2701 | 5295.62] loss=2.44 avg=2.39\n",
            "[2702 | 5297.35] loss=2.54 avg=2.39\n",
            "[2703 | 5299.06] loss=2.77 avg=2.39\n",
            "[2704 | 5300.77] loss=2.18 avg=2.39\n",
            "[2705 | 5302.50] loss=2.50 avg=2.39\n",
            "[2706 | 5304.21] loss=2.23 avg=2.39\n",
            "[2707 | 5305.92] loss=2.44 avg=2.39\n",
            "[2708 | 5307.64] loss=2.29 avg=2.39\n",
            "[2709 | 5309.35] loss=3.03 avg=2.39\n",
            "[2710 | 5311.08] loss=2.20 avg=2.39\n",
            "[2711 | 5312.79] loss=2.19 avg=2.39\n",
            "[2712 | 5314.50] loss=2.57 avg=2.39\n",
            "[2713 | 5316.21] loss=2.48 avg=2.39\n",
            "[2714 | 5317.93] loss=2.05 avg=2.39\n",
            "[2715 | 5319.64] loss=2.58 avg=2.39\n",
            "[2716 | 5321.35] loss=2.58 avg=2.39\n",
            "[2717 | 5323.10] loss=2.80 avg=2.40\n",
            "[2718 | 5324.83] loss=2.64 avg=2.40\n",
            "[2719 | 5326.57] loss=2.54 avg=2.40\n",
            "[2720 | 5328.28] loss=2.54 avg=2.40\n",
            "[2721 | 5329.99] loss=2.10 avg=2.40\n",
            "[2722 | 5331.71] loss=2.19 avg=2.40\n",
            "[2723 | 5333.43] loss=2.24 avg=2.40\n",
            "[2724 | 5335.14] loss=2.15 avg=2.39\n",
            "[2725 | 5336.85] loss=2.67 avg=2.40\n",
            "[2726 | 5338.56] loss=2.25 avg=2.40\n",
            "[2727 | 5340.28] loss=2.57 avg=2.40\n",
            "[2728 | 5341.98] loss=2.42 avg=2.40\n",
            "[2729 | 5343.69] loss=2.15 avg=2.39\n",
            "[2730 | 5345.41] loss=2.48 avg=2.40\n",
            "[2731 | 5347.12] loss=2.37 avg=2.40\n",
            "[2732 | 5348.83] loss=2.31 avg=2.39\n",
            "[2733 | 5350.54] loss=2.70 avg=2.40\n",
            "[2734 | 5352.27] loss=1.98 avg=2.39\n",
            "[2735 | 5353.98] loss=2.51 avg=2.39\n",
            "[2736 | 5355.69] loss=2.58 avg=2.40\n",
            "[2737 | 5357.40] loss=2.54 avg=2.40\n",
            "[2738 | 5359.11] loss=2.50 avg=2.40\n",
            "[2739 | 5360.83] loss=2.17 avg=2.40\n",
            "[2740 | 5362.54] loss=2.17 avg=2.39\n",
            "[2741 | 5364.27] loss=2.95 avg=2.40\n",
            "[2742 | 5365.98] loss=2.18 avg=2.40\n",
            "[2743 | 5367.69] loss=1.39 avg=2.39\n",
            "[2744 | 5369.41] loss=2.31 avg=2.39\n",
            "[2745 | 5371.12] loss=2.31 avg=2.39\n",
            "[2746 | 5372.83] loss=2.66 avg=2.39\n",
            "[2747 | 5374.56] loss=2.61 avg=2.39\n",
            "[2748 | 5376.27] loss=1.83 avg=2.39\n",
            "[2749 | 5377.98] loss=2.08 avg=2.38\n",
            "[2750 | 5379.69] loss=2.91 avg=2.39\n",
            "[2751 | 5381.41] loss=2.09 avg=2.38\n",
            "[2752 | 5383.14] loss=2.03 avg=2.38\n",
            "[2753 | 5384.85] loss=2.33 avg=2.38\n",
            "[2754 | 5386.57] loss=2.73 avg=2.38\n",
            "[2755 | 5388.28] loss=2.06 avg=2.38\n",
            "[2756 | 5389.98] loss=3.42 avg=2.39\n",
            "[2757 | 5391.70] loss=2.22 avg=2.39\n",
            "[2758 | 5393.42] loss=2.57 avg=2.39\n",
            "[2759 | 5395.13] loss=2.31 avg=2.39\n",
            "[2760 | 5396.85] loss=2.30 avg=2.39\n",
            "[2761 | 5398.57] loss=1.94 avg=2.39\n",
            "[2762 | 5400.27] loss=2.12 avg=2.38\n",
            "[2763 | 5402.00] loss=2.20 avg=2.38\n",
            "[2764 | 5403.71] loss=2.04 avg=2.38\n",
            "[2765 | 5405.42] loss=2.58 avg=2.38\n",
            "[2766 | 5407.13] loss=2.52 avg=2.38\n",
            "[2767 | 5408.84] loss=1.87 avg=2.38\n",
            "[2768 | 5410.55] loss=2.08 avg=2.37\n",
            "[2769 | 5412.26] loss=1.63 avg=2.37\n",
            "[2770 | 5413.98] loss=1.82 avg=2.36\n",
            "[2771 | 5415.69] loss=2.54 avg=2.36\n",
            "[2772 | 5417.42] loss=2.06 avg=2.36\n",
            "[2773 | 5419.15] loss=2.28 avg=2.36\n",
            "[2774 | 5420.88] loss=2.30 avg=2.36\n",
            "[2775 | 5422.61] loss=2.76 avg=2.36\n",
            "[2776 | 5424.32] loss=1.74 avg=2.36\n",
            "[2777 | 5426.03] loss=1.95 avg=2.35\n",
            "[2778 | 5427.77] loss=1.55 avg=2.34\n",
            "[2779 | 5429.49] loss=2.35 avg=2.34\n",
            "[2780 | 5431.22] loss=2.60 avg=2.35\n",
            "[2781 | 5432.95] loss=2.72 avg=2.35\n",
            "[2782 | 5434.68] loss=2.18 avg=2.35\n",
            "[2783 | 5436.42] loss=2.44 avg=2.35\n",
            "[2784 | 5438.15] loss=2.10 avg=2.35\n",
            "[2785 | 5439.88] loss=2.39 avg=2.35\n",
            "[2786 | 5441.61] loss=2.37 avg=2.35\n",
            "[2787 | 5443.36] loss=2.50 avg=2.35\n",
            "[2788 | 5445.13] loss=2.62 avg=2.35\n",
            "[2789 | 5446.89] loss=1.95 avg=2.35\n",
            "[2790 | 5448.64] loss=1.72 avg=2.34\n",
            "[2791 | 5450.40] loss=2.16 avg=2.34\n",
            "[2792 | 5452.15] loss=2.07 avg=2.34\n",
            "[2793 | 5453.90] loss=2.73 avg=2.34\n",
            "[2794 | 5455.66] loss=2.43 avg=2.34\n",
            "[2795 | 5457.41] loss=2.37 avg=2.34\n",
            "[2796 | 5459.17] loss=3.27 avg=2.35\n",
            "[2797 | 5460.90] loss=2.76 avg=2.35\n",
            "[2798 | 5462.64] loss=2.28 avg=2.35\n",
            "[2799 | 5464.39] loss=1.90 avg=2.35\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Watch. What happens now the Democrats give their vote on ObamaCare?\n",
            "The Democrats are giving their vote on ObamaCare to their members rather than the President!\n",
            "How irresponsible are some of our leaders in Congress these days? Our Country is being totally run by them and their DEMS friends and family. Not good.\n",
            "ObamaCare (and many jobs will be created) is a disaster that will lead to higher energy bills higher gas prices lots of government waste and corruption!\n",
            "If I had the luxury of being the President of the USA I probably would stop the Fast and Furious operation sooner rather than later. What is our tax dollars getting to these degenerates?\n",
            "...The Mexican government has taken the weapons and ammunition that the United States sent from Mexico and turned this into a weaponized crime syndicate in order to carry out the murder of people...\n",
            "...We will not sign off on this. We will not go along with this. Mexico and Mexico City should stop this and ALL other similar transactions immediately....\n",
            "....You and I today know me better and have been more than I've been in a relationship - people are more likely to date me...\n",
            "...How about getting rid of all the Regulations and all the taxes. Get rid of those pesky Taxes! It's a rigged system but at least it will be clean and legal.\n",
            "...We have a Country that has been ripped OFF from it's youth for the last 35 years. Regulations &amp; Taxes and.... https://t.co/QWqmKF5wqE\n",
            "...We have a Country that has been rip off for 35 years. Get rid of those pesky Taxes which will be a big help while we still can - but make no mistake the Country will be clean!\n",
            "We are going to take care of our Country the way we want it to be taken care of for our Workers which means we are going to take Care of its People and we are going to take care of its Economy which means we are not only going to have free trade but we will do it better every day!\n",
            "I am in the midst of the first major trade conference of my life so I am in a perfect position to make a decision regarding the Trans-Pacific Partnership but one thing is for sure no longer negotiating!\n",
            "The Trans-Pacific Partnership will be my biggest and biggest economic opportunity to date - and no matter what it says in the agreement it won't be read into the final Agreement.\n",
            "Why are the Democrats giving a vote on ObamaCare the night of Memorial Day. I cannot let this happen to American workers or to American Values. ObamaCare is a Disaster which is why the Republicans want to keep it very simple and high level\n",
            "My prayers go out to the victims and families of the terrible shooting at the First Baptist Church in Charleston South Carolina. God bless our Military &amp; their families and we love one another!\n",
            "We are in serious talks with China on Trade-and we are trying to come to a great understanding. We always have and always will be there for one another. We have a great relationship going and will continue to do so!\n",
            "The Mexican government has taken and sold the weapons and equipment that had been sent to their country by the United States. Our country is laughing at them how they take advantage of the people of the United States and at what they do to steal our beautiful things!\n",
            "We are looking at the North American Free Trade Treaty a fairly simple agreement but we are going to be looking at it very seriously. NAFTA has failed and it will fail again. It is a disaster!\n",
            "The Democrats are taking credit for stopping us from going forward on TPP but they are just giving them more time to develop the deal.\n",
            "Why are the Democrats giving a vote on ObamaCare the night of Memorial Day?\n",
            ".....We will take care of our workers and our Farmers and put a great American back in charge of our farms which is where they belong. It's called H-1B work visa abuse!\n",
            "The American worker is suffering with this Trans-Pacific Partnership and it is not looking good. Big numbers of Americans losing their job, big numbers of American farms being thrown out of business and American consumers getting the short end of the stick!\n",
            "We will renegotiate and possibly make changes to a very unfair and biased deal that has been bad for two years!\n",
            "The Democrats wanted trade deals with Mexico and Canada in the past but they lost because they want this bad NAFTA deal right now. They hate American consumers and want to rip us off. I just want American jobs and products\n",
            "The Trans-Pacific Partnership (TPP) is a massive disaster for the United States. Now the Democrats want to include China and that is why our Country is so angry with them. They are laughing at American workers and want to rip us even further off!\n",
            "Republicans are negotiating directly with China on Trade. We are in very good financial shape so far and will soon be able to say with good certainty that we will not be having TPP. I have not made a decision on a final deal yet!\n",
            "I want\n",
            "\n",
            "[2800 | 5488.76] loss=2.19 avg=2.35\n",
            "[2801 | 5490.49] loss=2.42 avg=2.35\n",
            "[2802 | 5492.21] loss=2.48 avg=2.35\n",
            "[2803 | 5493.93] loss=2.38 avg=2.35\n",
            "[2804 | 5495.66] loss=2.85 avg=2.36\n",
            "[2805 | 5497.37] loss=2.38 avg=2.36\n",
            "[2806 | 5499.10] loss=2.37 avg=2.36\n",
            "[2807 | 5500.81] loss=2.23 avg=2.35\n",
            "[2808 | 5502.52] loss=2.17 avg=2.35\n",
            "[2809 | 5504.25] loss=1.98 avg=2.35\n",
            "[2810 | 5505.96] loss=1.95 avg=2.34\n",
            "[2811 | 5507.67] loss=1.73 avg=2.34\n",
            "[2812 | 5509.38] loss=1.92 avg=2.33\n",
            "[2813 | 5511.09] loss=1.97 avg=2.33\n",
            "[2814 | 5512.81] loss=2.05 avg=2.33\n",
            "[2815 | 5514.52] loss=1.88 avg=2.32\n",
            "[2816 | 5516.23] loss=2.61 avg=2.33\n",
            "[2817 | 5517.94] loss=2.17 avg=2.32\n",
            "[2818 | 5519.66] loss=2.31 avg=2.32\n",
            "[2819 | 5521.36] loss=3.08 avg=2.33\n",
            "[2820 | 5523.08] loss=2.28 avg=2.33\n",
            "[2821 | 5524.81] loss=2.32 avg=2.33\n",
            "[2822 | 5526.54] loss=2.53 avg=2.33\n",
            "[2823 | 5528.25] loss=2.51 avg=2.34\n",
            "[2824 | 5529.97] loss=2.37 avg=2.34\n",
            "[2825 | 5531.68] loss=2.15 avg=2.33\n",
            "[2826 | 5533.40] loss=2.47 avg=2.33\n",
            "[2827 | 5535.12] loss=2.41 avg=2.34\n",
            "[2828 | 5536.83] loss=2.11 avg=2.33\n",
            "[2829 | 5538.55] loss=2.10 avg=2.33\n",
            "[2830 | 5540.26] loss=2.46 avg=2.33\n",
            "[2831 | 5541.99] loss=2.24 avg=2.33\n",
            "[2832 | 5543.72] loss=2.72 avg=2.34\n",
            "[2833 | 5545.43] loss=2.52 avg=2.34\n",
            "[2834 | 5547.14] loss=2.46 avg=2.34\n",
            "[2835 | 5548.85] loss=1.76 avg=2.33\n",
            "[2836 | 5550.56] loss=2.44 avg=2.33\n",
            "[2837 | 5552.30] loss=2.82 avg=2.34\n",
            "[2838 | 5554.01] loss=2.91 avg=2.34\n",
            "[2839 | 5555.72] loss=2.01 avg=2.34\n",
            "[2840 | 5557.43] loss=2.26 avg=2.34\n",
            "[2841 | 5559.14] loss=2.26 avg=2.34\n",
            "[2842 | 5560.85] loss=2.03 avg=2.34\n",
            "[2843 | 5562.56] loss=2.24 avg=2.34\n",
            "[2844 | 5564.28] loss=2.93 avg=2.34\n",
            "[2845 | 5565.99] loss=2.26 avg=2.34\n",
            "[2846 | 5567.70] loss=2.35 avg=2.34\n",
            "[2847 | 5569.41] loss=3.01 avg=2.35\n",
            "[2848 | 5571.14] loss=2.33 avg=2.35\n",
            "[2849 | 5572.87] loss=2.07 avg=2.34\n",
            "[2850 | 5574.58] loss=2.68 avg=2.35\n",
            "[2851 | 5576.29] loss=2.03 avg=2.34\n",
            "[2852 | 5578.02] loss=2.28 avg=2.34\n",
            "[2853 | 5579.74] loss=2.11 avg=2.34\n",
            "[2854 | 5581.45] loss=2.16 avg=2.34\n",
            "[2855 | 5583.16] loss=2.89 avg=2.35\n",
            "[2856 | 5584.87] loss=2.26 avg=2.34\n",
            "[2857 | 5586.59] loss=2.36 avg=2.34\n",
            "[2858 | 5588.30] loss=2.02 avg=2.34\n",
            "[2859 | 5590.01] loss=2.81 avg=2.35\n",
            "[2860 | 5591.73] loss=2.94 avg=2.35\n",
            "[2861 | 5593.47] loss=2.08 avg=2.35\n",
            "[2862 | 5595.18] loss=2.08 avg=2.35\n",
            "[2863 | 5596.89] loss=2.76 avg=2.35\n",
            "[2864 | 5598.61] loss=2.56 avg=2.35\n",
            "[2865 | 5600.33] loss=1.87 avg=2.35\n",
            "[2866 | 5602.05] loss=2.37 avg=2.35\n",
            "[2867 | 5603.76] loss=2.59 avg=2.35\n",
            "[2868 | 5605.47] loss=1.96 avg=2.35\n",
            "[2869 | 5607.19] loss=2.54 avg=2.35\n",
            "[2870 | 5608.90] loss=2.53 avg=2.35\n",
            "[2871 | 5610.61] loss=2.28 avg=2.35\n",
            "[2872 | 5612.32] loss=2.66 avg=2.35\n",
            "[2873 | 5614.03] loss=1.74 avg=2.35\n",
            "[2874 | 5615.74] loss=2.15 avg=2.34\n",
            "[2875 | 5617.45] loss=2.53 avg=2.35\n",
            "[2876 | 5619.17] loss=2.20 avg=2.35\n",
            "[2877 | 5620.90] loss=2.95 avg=2.35\n",
            "[2878 | 5622.63] loss=1.84 avg=2.35\n",
            "[2879 | 5624.36] loss=2.60 avg=2.35\n",
            "[2880 | 5626.06] loss=2.03 avg=2.35\n",
            "[2881 | 5627.78] loss=2.97 avg=2.35\n",
            "[2882 | 5629.52] loss=2.75 avg=2.36\n",
            "[2883 | 5631.24] loss=2.70 avg=2.36\n",
            "[2884 | 5632.97] loss=2.62 avg=2.36\n",
            "[2885 | 5634.69] loss=2.39 avg=2.36\n",
            "[2886 | 5636.42] loss=2.54 avg=2.36\n",
            "[2887 | 5638.13] loss=2.41 avg=2.36\n",
            "[2888 | 5639.83] loss=2.64 avg=2.37\n",
            "[2889 | 5641.55] loss=2.86 avg=2.37\n",
            "[2890 | 5643.26] loss=2.00 avg=2.37\n",
            "[2891 | 5644.97] loss=2.47 avg=2.37\n",
            "[2892 | 5646.68] loss=2.01 avg=2.37\n",
            "[2893 | 5648.39] loss=2.51 avg=2.37\n",
            "[2894 | 5650.11] loss=2.40 avg=2.37\n",
            "[2895 | 5651.82] loss=2.15 avg=2.37\n",
            "[2896 | 5653.53] loss=2.40 avg=2.37\n",
            "[2897 | 5655.24] loss=2.19 avg=2.36\n",
            "[2898 | 5656.98] loss=2.23 avg=2.36\n",
            "[2899 | 5658.71] loss=2.63 avg=2.37\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "oked with an email from a friend saying he thought the Russia story was a hoax. That Clinton campaign was rigged?\n",
            "@jamesbarnes1 Good luck.\n",
            "@DerekYendle1  Thanks and good luck.\n",
            "@billybearsomgood Happy Birthday!\n",
            "@pauljonesdotcom Good luck.\n",
            "@mitchcjwatson. Thanks Mitch.\n",
            "@Lizgibson01 Happy birthday!\n",
            ".@Toure  once again proved himself a truly great footballer. He will be missed by all!\n",
            "I am a great fan of Trump National Golf Club Washington. It's in West Palm Beach. Headed to D.C. to get new #DACA memorabilia\n",
            "Trump National Golf Club Philadelphia’s $1.6B opening is an incredible success. We are thrilled we were able to work on it. Great timing\n",
            "@chucktodd  It is a great honor really will be fun and enjoyable!\n",
            "“I always tell business people to treat their employees like they are your customers.” – The Art of The Deal\n",
            "We’ve achieved our goals but not all yet.  Trump National Golf Club Charlotte is open to the public so stay tuned!\n",
            "We’ve achieved our goals but not all yet.  Trump National Golf Club Charlotte is open to the public so stay tuned!\n",
            "Thank you @lucasjfoxx! We are so glad you agreed to run for POTUS! https://t.co/J3YT8eUZs1\n",
            "“Trump: GOP is being controlled by extreme ideologues'” via @BreitbartNews by “Corey Lewandowski” ”I’ll Run For Office.”  @BreitbartNews\n",
            "#TrumpAdvice https://t.co/YaGKX7bO3t\n",
            ".@AlexSalmond  Alex that is a disgusting excuse for a leader that he is and you can't even carry on with your foolish existence!\n",
            "The world is laughing at what a total waste of money and talent the world can’t manage a Trump Tower on Central Park Island!\n",
            "Via @DailyCaller by @dprem66: “Trump: ‘Not Even Close' For GOP Candidates” http://t.co/c2wYWlXhI4\n",
            "Via @MailOnline by @paulmorrisnews: “Donald Trump: The GOP Needs To Get Tough” http://t.co/cQoQ2jkQJ1\n",
            "Via @BreitbartNews: “Donald Trump: ‘It Should Be Obama’s Business Record Not Obama” http://t.co/bJH1jR8FgJ\n",
            "“Achievers move forward at all times.” – Midas Touch\n",
            "“Success requires passion perseverance determination and luck all combined.” – Midas Touch\n",
            "“Achievers move forward at all times.” – Midas Touch\n",
            "Via @Reuters by @joshjdelreal: “Donald Trump Says World Wants Him To Be President: https://t.co/Z9jRwP3Rl2” http://t.co/Z9jRwP3Rl2\n",
            "Via @FoxNewsInsider: \"Donald Trump: Global Warming Is The Biggest Hoax In World History\" http://t.co/oOuQ1L9m2k\n",
            "#TrumpAdvice https://t.co/1gkXjN1vLq\n",
            "Via @washingtonpost by @lohudtgov: “Donald Trump: GOP Needs To Get Tough” http://t.co/vG4KpBp7Xd\n",
            "With respect to NATO nations must maintain their territorial integrity. If not they should ‘be punished.’ Should not have to contribute to security.\n",
            "@RalphGillis Best golf course in NYC!  Here i am today thanks.\n",
            "Via @BreitbartNews by @tmod36604: “Trump Wants To Repeal And Replace Obama” http://t.co/y2rJz0K7X1\n",
            "Via @DMRegister by @hannahmcurry: “Watch Donald Trump on @ApprenticeNBC ‘A Year In the Life’” http://t.co/0pW6NhIa1X\n",
            "@HearstMauk @BreitbartNews @FoxNews @TrumpTowerNY @MJHeimbach @RT_FoG Thanks!\n",
            "“I've done something that's crazy to talk about. I've built a major golf course—and it's getting blown up” –that's how I see my many @WSJ acquisitions http://t.co\n",
            "\n",
            "[2900 | 5682.48] loss=1.50 avg=2.36\n",
            "[2901 | 5684.18] loss=2.79 avg=2.36\n",
            "[2902 | 5685.92] loss=2.21 avg=2.36\n",
            "[2903 | 5687.63] loss=2.62 avg=2.36\n",
            "[2904 | 5689.34] loss=1.79 avg=2.36\n",
            "[2905 | 5691.05] loss=2.59 avg=2.36\n",
            "[2906 | 5692.76] loss=2.27 avg=2.36\n",
            "[2907 | 5694.47] loss=2.23 avg=2.36\n",
            "[2908 | 5696.18] loss=2.33 avg=2.36\n",
            "[2909 | 5697.89] loss=2.43 avg=2.36\n",
            "[2910 | 5699.60] loss=2.77 avg=2.36\n",
            "[2911 | 5701.31] loss=2.36 avg=2.36\n",
            "[2912 | 5703.03] loss=1.79 avg=2.36\n",
            "[2913 | 5704.76] loss=2.50 avg=2.36\n",
            "[2914 | 5706.49] loss=2.12 avg=2.35\n",
            "[2915 | 5708.22] loss=2.96 avg=2.36\n",
            "[2916 | 5709.93] loss=2.65 avg=2.36\n",
            "[2917 | 5711.63] loss=2.35 avg=2.36\n",
            "[2918 | 5713.35] loss=1.96 avg=2.36\n",
            "[2919 | 5715.05] loss=1.46 avg=2.35\n",
            "[2920 | 5716.77] loss=2.84 avg=2.36\n",
            "[2921 | 5718.48] loss=2.21 avg=2.35\n",
            "[2922 | 5720.19] loss=3.10 avg=2.36\n",
            "[2923 | 5721.90] loss=1.77 avg=2.36\n",
            "[2924 | 5723.62] loss=2.10 avg=2.35\n",
            "[2925 | 5725.33] loss=2.29 avg=2.35\n",
            "[2926 | 5727.04] loss=2.48 avg=2.35\n",
            "[2927 | 5728.76] loss=2.51 avg=2.35\n",
            "[2928 | 5730.48] loss=2.29 avg=2.35\n",
            "[2929 | 5732.19] loss=2.89 avg=2.36\n",
            "[2930 | 5733.91] loss=2.58 avg=2.36\n",
            "[2931 | 5735.62] loss=1.96 avg=2.36\n",
            "[2932 | 5737.34] loss=2.28 avg=2.36\n",
            "[2933 | 5739.05] loss=2.80 avg=2.36\n",
            "[2934 | 5740.77] loss=2.28 avg=2.36\n",
            "[2935 | 5742.48] loss=2.06 avg=2.36\n",
            "[2936 | 5744.19] loss=2.16 avg=2.36\n",
            "[2937 | 5745.90] loss=1.90 avg=2.35\n",
            "[2938 | 5747.61] loss=2.31 avg=2.35\n",
            "[2939 | 5749.32] loss=2.34 avg=2.35\n",
            "[2940 | 5751.03] loss=1.68 avg=2.34\n",
            "[2941 | 5752.74] loss=2.33 avg=2.34\n",
            "[2942 | 5754.48] loss=2.46 avg=2.34\n",
            "[2943 | 5756.21] loss=2.18 avg=2.34\n",
            "[2944 | 5757.94] loss=2.67 avg=2.35\n",
            "[2945 | 5759.65] loss=2.82 avg=2.35\n",
            "[2946 | 5761.36] loss=2.51 avg=2.35\n",
            "[2947 | 5763.07] loss=2.23 avg=2.35\n",
            "[2948 | 5764.78] loss=1.75 avg=2.35\n",
            "[2949 | 5766.49] loss=2.89 avg=2.35\n",
            "[2950 | 5768.21] loss=2.17 avg=2.35\n",
            "[2951 | 5769.92] loss=2.44 avg=2.35\n",
            "[2952 | 5771.65] loss=2.55 avg=2.35\n",
            "[2953 | 5773.38] loss=2.88 avg=2.36\n",
            "[2954 | 5775.09] loss=2.30 avg=2.36\n",
            "[2955 | 5776.80] loss=1.86 avg=2.35\n",
            "[2956 | 5778.51] loss=2.14 avg=2.35\n",
            "[2957 | 5780.23] loss=1.73 avg=2.34\n",
            "[2958 | 5781.94] loss=1.99 avg=2.34\n",
            "[2959 | 5783.65] loss=2.55 avg=2.34\n",
            "[2960 | 5785.36] loss=2.16 avg=2.34\n",
            "[2961 | 5787.07] loss=2.27 avg=2.34\n",
            "[2962 | 5788.78] loss=2.63 avg=2.34\n",
            "[2963 | 5790.50] loss=2.70 avg=2.35\n",
            "[2964 | 5792.21] loss=2.17 avg=2.34\n",
            "[2965 | 5793.92] loss=1.99 avg=2.34\n",
            "[2966 | 5795.66] loss=2.14 avg=2.34\n",
            "[2967 | 5797.39] loss=2.75 avg=2.34\n",
            "[2968 | 5799.10] loss=1.95 avg=2.34\n",
            "[2969 | 5800.81] loss=2.25 avg=2.34\n",
            "[2970 | 5802.53] loss=2.05 avg=2.34\n",
            "[2971 | 5804.24] loss=1.85 avg=2.33\n",
            "[2972 | 5805.95] loss=1.87 avg=2.33\n",
            "[2973 | 5807.67] loss=2.30 avg=2.33\n",
            "[2974 | 5809.39] loss=1.88 avg=2.32\n",
            "[2975 | 5811.11] loss=1.90 avg=2.32\n",
            "[2976 | 5812.85] loss=2.13 avg=2.31\n",
            "[2977 | 5814.57] loss=2.41 avg=2.32\n",
            "[2978 | 5816.28] loss=2.12 avg=2.31\n",
            "[2979 | 5818.00] loss=2.20 avg=2.31\n",
            "[2980 | 5819.71] loss=2.21 avg=2.31\n",
            "[2981 | 5821.42] loss=2.24 avg=2.31\n",
            "[2982 | 5823.13] loss=2.29 avg=2.31\n",
            "[2983 | 5824.84] loss=2.06 avg=2.31\n",
            "[2984 | 5826.55] loss=2.27 avg=2.31\n",
            "[2985 | 5828.27] loss=1.67 avg=2.30\n",
            "[2986 | 5829.99] loss=2.72 avg=2.31\n",
            "[2987 | 5831.71] loss=2.51 avg=2.31\n",
            "[2988 | 5833.42] loss=2.08 avg=2.31\n",
            "[2989 | 5835.16] loss=2.25 avg=2.31\n",
            "[2990 | 5836.87] loss=2.27 avg=2.30\n",
            "[2991 | 5838.58] loss=1.66 avg=2.30\n",
            "[2992 | 5840.29] loss=2.02 avg=2.30\n",
            "[2993 | 5842.00] loss=2.09 avg=2.29\n",
            "[2994 | 5843.71] loss=2.79 avg=2.30\n",
            "[2995 | 5845.42] loss=1.74 avg=2.29\n",
            "[2996 | 5847.13] loss=2.05 avg=2.29\n",
            "[2997 | 5848.85] loss=2.02 avg=2.29\n",
            "[2998 | 5850.57] loss=1.83 avg=2.28\n",
            "[2999 | 5852.29] loss=2.55 avg=2.29\n",
            "Saving /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-3000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Iowa; and we all celebrate that the people of South Carolina have elected someone who has done great work for our Veterans &amp; their children!\n",
            "“A great honor to be there. We’ve achieved great things together.” @foxandfriends\n",
            "“I saw President Barack Obama at his first trip to Africa. I know from my experience with him that he is not a guy who sits back and lets things happen he makes sure that they’re being taken care of. I’ve met with President Jacob Zuma a lot. We’ve worked well together. He’s a great guy.” @bpolitics #1Network\n",
            "“Obama’s great accomplishment is to keep America in the dark on the terrible crimes committed by others around the world. The worst are not being reported. What we are doing around the world is horrific. The worst are being reported.”  @IngrahamAngle\n",
            "The Democrats will not let you replace Obama on the Iran deal. We need leadership in Washington. The Middle East is a mess and the U.S. is not doing its job. Time for tough leadership!\n",
            "I’ve been saying it for years the Democrats need a Leader in Washington DC. The Dems will not lead the charge. The Republicans will do it for a price!\n",
            "Congratulations to our very first National Border Patrol Council. Together with DHS we have led the way at the Border. Without our great Border Officers it’s almost impossible to do our work. We protect American Lives and Biodiesel! #BWP2018 https://t.co/vD0vQGpqoL\n",
            ".@SenSanders has a message for the Republican Party: We are serious about fighting for Border &amp; Immigrant Safety and Security. Republicans have long been against Border Security which is why we signed an Omnibus Bill. Dem leaders preferred making deals with Dems on Border Security. We are determined to keep our Nation’s Borders open. Keep fighting back Senator!\n",
            ".@NRA is committed to ending mass shootings by stopping dangerous people who aren’t doing us harm from buying guns. https://t.co/mDkO6n2g8G\n",
            "After years of talking tough and then doing nothing the Democrats finally listened to the American public. They took action this past fall with commonsense gun safety legislation - but now the Republicans are leading! #NoKo\n",
            "Thank you to the amazing women and men of ICE ICE ICE ICE for doing a great job last week! https://t.co/jNwGXxU6Lz\n",
            "....What the Democrats have done for us the Republicans have to do better. They can’t do it on their own through legislation. Congress must vote to give me the Authority to Build the WALL which I will do. Dems hate this concept of border security because they don’t want to control the Border or their Dem donors. We should build the WALL and control who comes into our Country. The Wall is the great equalizer to stop violent crime drug flow and human trafficking.\n",
            "The WALL looks much better than anything they can do on their own. Republicans must agree and quickly to stop this madness. We need a strong Border Security Wall no matter the cost. Democrats do nothing about Border Crime Guns Drugs Human Trafficking &amp; Drugs. ...\n",
            "The Schumer/Pelosi agreement on Immigration Security was a great success for America. In just a few short months over 100000 people have been removed/removed by the Courts. Thousands of criminals are being brought out of the streets to the safety of safety and safety of our Country. The Democrats are holding these people back. They would do a far better job in Congress!\n",
            "Democrats are fighting the Republicans on Border Security. For them the only issue is money not people! They demand 508 more Billion Dollars for their long time enemy &amp; current President @BarackObama. Well the Republicans have got some very good people working on Border Security and they said no! $542 Billion for the most fortified Border in the history of our Country. Democrats can’t win on Border Security no matter how tough they try!\n",
            "The Republicans are all together doing a great job on Border Security - but not with the Dem Senators who have done nothing for a very long time. I will win and we will build a Wall. The Democrats have always wanted a wall - we now have the perfect solution to build a great and lasting barrier. We must build immediately or we will lose our beautiful Country - and much more!\n",
            "Congrats to my Vice Presidential running mate Doug Homan. He has taken great advantage of the fact that I have been gone a long time. Hmmmm.....\n",
            "“After more than 6 months of hearings Senator Gregg is pleased to officially accept the lead responsibility for the Kavanaugh nomination.” @FoxNews  Full Statement: https://t.co/Zm2HsO\n",
            "\n",
            "[3000 | 5890.81] loss=2.21 avg=2.28\n",
            "[3001 | 5892.48] loss=2.27 avg=2.28\n",
            "[3002 | 5894.15] loss=2.31 avg=2.28\n",
            "[3003 | 5895.82] loss=1.81 avg=2.28\n",
            "[3004 | 5897.49] loss=2.12 avg=2.28\n",
            "[3005 | 5899.18] loss=2.12 avg=2.28\n",
            "[3006 | 5900.86] loss=3.16 avg=2.29\n",
            "[3007 | 5902.55] loss=2.57 avg=2.29\n",
            "[3008 | 5904.22] loss=2.88 avg=2.29\n",
            "[3009 | 5905.91] loss=2.44 avg=2.30\n",
            "[3010 | 5907.61] loss=2.34 avg=2.30\n",
            "[3011 | 5909.30] loss=2.21 avg=2.30\n",
            "[3012 | 5910.99] loss=2.38 avg=2.30\n",
            "[3013 | 5912.70] loss=2.26 avg=2.30\n",
            "[3014 | 5914.41] loss=1.74 avg=2.29\n",
            "[3015 | 5916.12] loss=2.82 avg=2.30\n",
            "[3016 | 5917.82] loss=2.09 avg=2.29\n",
            "[3017 | 5919.54] loss=2.05 avg=2.29\n",
            "[3018 | 5921.24] loss=1.95 avg=2.29\n",
            "[3019 | 5922.96] loss=2.20 avg=2.29\n",
            "[3020 | 5924.66] loss=2.72 avg=2.29\n",
            "[3021 | 5926.37] loss=1.98 avg=2.29\n",
            "[3022 | 5928.09] loss=2.50 avg=2.29\n",
            "[3023 | 5929.80] loss=1.66 avg=2.28\n",
            "[3024 | 5931.51] loss=2.84 avg=2.29\n",
            "[3025 | 5933.23] loss=1.76 avg=2.28\n",
            "[3026 | 5934.94] loss=2.55 avg=2.29\n",
            "[3027 | 5936.65] loss=2.42 avg=2.29\n",
            "[3028 | 5938.37] loss=1.78 avg=2.28\n",
            "[3029 | 5940.09] loss=2.38 avg=2.28\n",
            "[3030 | 5941.80] loss=1.92 avg=2.28\n",
            "[3031 | 5943.51] loss=2.22 avg=2.28\n",
            "[3032 | 5945.23] loss=2.00 avg=2.28\n",
            "[3033 | 5946.94] loss=1.89 avg=2.27\n",
            "[3034 | 5948.65] loss=2.29 avg=2.27\n",
            "[3035 | 5950.37] loss=2.39 avg=2.27\n",
            "[3036 | 5952.08] loss=2.21 avg=2.27\n",
            "[3037 | 5953.80] loss=1.88 avg=2.27\n",
            "[3038 | 5955.50] loss=1.52 avg=2.26\n",
            "[3039 | 5957.21] loss=2.22 avg=2.26\n",
            "[3040 | 5958.92] loss=2.59 avg=2.27\n",
            "[3041 | 5960.63] loss=2.23 avg=2.26\n",
            "[3042 | 5962.34] loss=2.99 avg=2.27\n",
            "[3043 | 5964.06] loss=2.46 avg=2.27\n",
            "[3044 | 5965.77] loss=2.92 avg=2.28\n",
            "[3045 | 5967.47] loss=2.67 avg=2.28\n",
            "[3046 | 5969.18] loss=2.43 avg=2.29\n",
            "[3047 | 5970.88] loss=1.62 avg=2.28\n",
            "[3048 | 5972.58] loss=2.42 avg=2.28\n",
            "[3049 | 5974.28] loss=1.76 avg=2.28\n",
            "[3050 | 5975.99] loss=2.52 avg=2.28\n",
            "[3051 | 5977.70] loss=2.02 avg=2.28\n",
            "[3052 | 5979.41] loss=1.95 avg=2.27\n",
            "[3053 | 5981.10] loss=2.78 avg=2.28\n",
            "[3054 | 5982.81] loss=2.09 avg=2.27\n",
            "[3055 | 5984.52] loss=2.64 avg=2.28\n",
            "[3056 | 5986.23] loss=2.18 avg=2.28\n",
            "[3057 | 5987.95] loss=1.89 avg=2.27\n",
            "[3058 | 5989.65] loss=2.13 avg=2.27\n",
            "[3059 | 5991.36] loss=2.37 avg=2.27\n",
            "[3060 | 5993.07] loss=2.06 avg=2.27\n",
            "[3061 | 5994.78] loss=2.19 avg=2.27\n",
            "[3062 | 5996.48] loss=2.78 avg=2.28\n",
            "[3063 | 5998.18] loss=2.22 avg=2.27\n",
            "[3064 | 5999.88] loss=2.54 avg=2.28\n",
            "[3065 | 6001.60] loss=1.80 avg=2.27\n",
            "[3066 | 6003.32] loss=2.11 avg=2.27\n",
            "[3067 | 6005.02] loss=2.63 avg=2.27\n",
            "[3068 | 6006.72] loss=3.14 avg=2.28\n",
            "[3069 | 6008.42] loss=2.05 avg=2.28\n",
            "[3070 | 6010.12] loss=1.65 avg=2.27\n",
            "[3071 | 6011.82] loss=2.95 avg=2.28\n",
            "[3072 | 6013.52] loss=1.53 avg=2.27\n",
            "[3073 | 6015.22] loss=1.85 avg=2.27\n",
            "[3074 | 6016.92] loss=1.99 avg=2.27\n",
            "[3075 | 6018.62] loss=2.17 avg=2.27\n",
            "[3076 | 6020.31] loss=2.39 avg=2.27\n",
            "[3077 | 6022.01] loss=2.55 avg=2.27\n",
            "[3078 | 6023.71] loss=2.18 avg=2.27\n",
            "[3079 | 6025.43] loss=1.88 avg=2.27\n",
            "[3080 | 6027.14] loss=2.18 avg=2.26\n",
            "[3081 | 6028.86] loss=2.05 avg=2.26\n",
            "[3082 | 6030.56] loss=3.07 avg=2.27\n",
            "[3083 | 6032.27] loss=1.91 avg=2.27\n",
            "[3084 | 6033.98] loss=2.21 avg=2.27\n",
            "[3085 | 6035.70] loss=1.95 avg=2.26\n",
            "[3086 | 6037.40] loss=1.77 avg=2.26\n",
            "[3087 | 6039.11] loss=2.77 avg=2.26\n",
            "[3088 | 6040.82] loss=2.01 avg=2.26\n",
            "[3089 | 6042.53] loss=2.48 avg=2.26\n",
            "[3090 | 6044.24] loss=2.17 avg=2.26\n",
            "[3091 | 6045.96] loss=2.44 avg=2.26\n",
            "[3092 | 6047.67] loss=1.76 avg=2.26\n",
            "[3093 | 6049.38] loss=2.39 avg=2.26\n",
            "[3094 | 6051.09] loss=2.03 avg=2.26\n",
            "[3095 | 6052.80] loss=1.78 avg=2.25\n",
            "[3096 | 6054.51] loss=2.37 avg=2.25\n",
            "[3097 | 6056.22] loss=2.20 avg=2.25\n",
            "[3098 | 6057.94] loss=2.26 avg=2.25\n",
            "[3099 | 6059.64] loss=2.10 avg=2.25\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "How to Lose Weight: A Real Diet &amp; Program For Men &amp; Wonder Women. (Time Warner Books).\n",
            "I'm thrilled to announce that I'm expanding the @UnionTheAd's @NYTimesBestWeeklyTravel series to include destinations from around the world. http://t.co/pW0aO8WL\n",
            "\"I tell people as tough as they are know that it's not always easy but it's good enough.\" - Dr. Robert N. Westmoreland\n",
            "\"You'll learn how to focus on your goals and realize your dreams.\" ~ Steve Jobs\n",
            "“Know When to Surrender.” – Think Like a Champion\n",
            "What's the #1 way to get rich fast is to work in the business. It can't be beat and it’s also very hard to start. See point #1: work in @thedailymail\n",
            ".@TrumpDoral’s $250M renovation is on schedule. The championship gardens @TrumpSoHo feature heralded by architecture critics http://t.co/Qf3h0m5R\n",
            "“You have to be flexible. It doesn't work anymore that business be done at once. Business is a creative endeavor.” – Midas Touch\n",
            "I have written the definitive book on the art of the deal: \"The Art of the No-win Win.\"   The 5-Star @RDS\n",
            "It's all very well pointing out the  mistakes people make sometimes in business and in life. But should’t we also point out that mistakes are inevitable?\n",
            "I’m amazed at the number of women who would leave the workforce due to the lack of pay--even though equal pay is denied to men.\n",
            "Congratulations to @LanaOnFire on her #FirstLadyAllStar nomination--it's a great honor to be with her.\n",
            "My #TrumpAdvice to young entrepreneurs: “Read everything. Read everything that interests you. You'll soon be an entrepreneur.\n",
            "When will all the women who claim that @MSNBC’s “bossy” comments scared them away finally come forward?\n",
            "What’s wrong with @NYDailyNews. It’s really sad that their paper has lost so much of its circulation. I love it here!\n",
            "Now Russia &amp; China are beating us badly. It’s great to see they don't care about our country's borders.\n",
            ".@nytimes was great in letting the media vent about the Russia witch hunt. But now that it’s all over they are forcing all negative stories to be buried.\n",
            ".@NYDailyNews was great in letting the media vent about the Russia witch hunt. But now that it’s all over they are forcing all negative stories to be buried.\n",
            "China beats Russia--without us &amp; NATO. They only care about economic growth.\n",
            "We have now lived with a world in which nuclear weapons eliminated 100% of our enemies--and by \"all\" I don't mean last centuries.\n",
            "I don't care about ratings. I'm not there yet. Make it work!\n",
            "As a dealmaker you need talent. But you also need brains. I love talent. Find the deal!\n",
            "Do yourself a favor and don't go into business until you “believe in yourself. Once you do that a job is done.\n",
            ".@JuanWilliams is a genius dealmaker. He’s built some of the biggest businesses in the world and is always a joy to work for.\n",
            "“To fail is to continue to make the same mistakes over and over again. It is to acknowledge that you can’t control what is possible.\" - Dale Carnegie\n",
            ".@TrumpScotland's award winning course for thru-honing your skills in a dynamic and exciting setting is a top destination http://t.co/mK8pDVfR\n",
            ".@TrumpLasVegas’ award winning restaurants award winning wine http://t.co/7I4MZpEg @JebBush is a total loser &amp; an incompetent president.\n",
            "@JebBush couldn't get @RockefellerQuiz to do the @jimmykimmel on foreign elections last time.\n",
            "I had great success negotiating for @nbc.  Even the @gretawire  show got it wrong. NBC just doesn't have it.\n",
            "I’ve had my share of success negotiating for shows and programs or businesses. But last night was a tough one for NBC.\n",
            "A lot of talk about a @JebMcMullin loss in NH—he would have won but for his stupid interview with George Will.\n",
            ".@JebBush's campaign is in big trouble-  due in large part to the way his father raised money.\n",
            "The @JebBush speech was a disaster.  He will do so many terrible things to\n",
            "\n",
            "[3100 | 6083.46] loss=2.20 avg=2.25\n",
            "[3101 | 6085.16] loss=1.85 avg=2.25\n",
            "[3102 | 6086.88] loss=2.26 avg=2.25\n",
            "[3103 | 6088.60] loss=2.85 avg=2.25\n",
            "[3104 | 6090.30] loss=2.47 avg=2.26\n",
            "[3105 | 6092.01] loss=2.24 avg=2.26\n",
            "[3106 | 6093.73] loss=2.01 avg=2.25\n",
            "[3107 | 6095.44] loss=2.63 avg=2.26\n",
            "[3108 | 6097.15] loss=2.75 avg=2.26\n",
            "[3109 | 6098.86] loss=2.67 avg=2.27\n",
            "[3110 | 6100.57] loss=2.95 avg=2.27\n",
            "[3111 | 6102.28] loss=2.19 avg=2.27\n",
            "[3112 | 6103.99] loss=2.39 avg=2.27\n",
            "[3113 | 6105.70] loss=2.50 avg=2.28\n",
            "[3114 | 6107.41] loss=2.28 avg=2.28\n",
            "[3115 | 6109.12] loss=2.57 avg=2.28\n",
            "[3116 | 6110.83] loss=2.09 avg=2.28\n",
            "[3117 | 6112.54] loss=1.96 avg=2.27\n",
            "[3118 | 6114.26] loss=2.49 avg=2.28\n",
            "[3119 | 6115.96] loss=2.51 avg=2.28\n",
            "[3120 | 6117.68] loss=2.45 avg=2.28\n",
            "[3121 | 6119.39] loss=2.64 avg=2.28\n",
            "[3122 | 6121.10] loss=2.84 avg=2.29\n",
            "[3123 | 6122.81] loss=2.54 avg=2.29\n",
            "[3124 | 6124.52] loss=2.42 avg=2.29\n",
            "[3125 | 6126.23] loss=1.94 avg=2.29\n",
            "[3126 | 6127.95] loss=1.67 avg=2.28\n",
            "[3127 | 6129.66] loss=2.44 avg=2.28\n",
            "[3128 | 6131.38] loss=2.26 avg=2.28\n",
            "[3129 | 6133.11] loss=1.82 avg=2.28\n",
            "[3130 | 6134.82] loss=2.29 avg=2.28\n",
            "[3131 | 6136.53] loss=2.13 avg=2.28\n",
            "[3132 | 6138.25] loss=2.04 avg=2.28\n",
            "[3133 | 6139.97] loss=2.08 avg=2.27\n",
            "[3134 | 6141.68] loss=1.94 avg=2.27\n",
            "[3135 | 6143.40] loss=2.58 avg=2.27\n",
            "[3136 | 6145.11] loss=2.09 avg=2.27\n",
            "[3137 | 6146.83] loss=2.15 avg=2.27\n",
            "[3138 | 6148.54] loss=2.23 avg=2.27\n",
            "[3139 | 6150.26] loss=2.40 avg=2.27\n",
            "[3140 | 6152.00] loss=2.71 avg=2.28\n",
            "[3141 | 6153.70] loss=2.39 avg=2.28\n",
            "[3142 | 6155.41] loss=1.91 avg=2.27\n",
            "[3143 | 6157.13] loss=2.67 avg=2.28\n",
            "[3144 | 6158.84] loss=2.61 avg=2.28\n",
            "[3145 | 6160.55] loss=1.45 avg=2.27\n",
            "[3146 | 6162.26] loss=2.06 avg=2.27\n",
            "[3147 | 6163.97] loss=2.46 avg=2.27\n",
            "[3148 | 6165.68] loss=2.27 avg=2.27\n",
            "[3149 | 6167.39] loss=2.12 avg=2.27\n",
            "[3150 | 6169.10] loss=1.78 avg=2.27\n",
            "[3151 | 6170.82] loss=2.66 avg=2.27\n",
            "[3152 | 6172.53] loss=1.74 avg=2.26\n",
            "[3153 | 6174.24] loss=2.38 avg=2.27\n",
            "[3154 | 6175.95] loss=1.75 avg=2.26\n",
            "[3155 | 6177.67] loss=2.59 avg=2.26\n",
            "[3156 | 6179.41] loss=2.32 avg=2.26\n",
            "[3157 | 6181.11] loss=1.85 avg=2.26\n",
            "[3158 | 6182.83] loss=1.85 avg=2.26\n",
            "[3159 | 6184.54] loss=2.67 avg=2.26\n",
            "[3160 | 6186.25] loss=2.52 avg=2.26\n",
            "[3161 | 6187.96] loss=1.83 avg=2.26\n",
            "[3162 | 6189.67] loss=3.27 avg=2.27\n",
            "[3163 | 6191.38] loss=2.16 avg=2.27\n",
            "[3164 | 6193.09] loss=1.63 avg=2.26\n",
            "[3165 | 6194.80] loss=1.84 avg=2.26\n",
            "[3166 | 6196.51] loss=2.19 avg=2.26\n",
            "[3167 | 6198.23] loss=2.69 avg=2.26\n",
            "[3168 | 6199.94] loss=2.00 avg=2.26\n",
            "[3169 | 6201.65] loss=1.74 avg=2.25\n",
            "[3170 | 6203.36] loss=2.31 avg=2.25\n",
            "[3171 | 6205.07] loss=1.62 avg=2.25\n",
            "[3172 | 6206.79] loss=2.79 avg=2.25\n",
            "[3173 | 6208.50] loss=2.00 avg=2.25\n",
            "[3174 | 6210.21] loss=1.90 avg=2.25\n",
            "[3175 | 6211.93] loss=2.28 avg=2.25\n",
            "[3176 | 6213.65] loss=2.18 avg=2.25\n",
            "[3177 | 6215.36] loss=2.31 avg=2.25\n",
            "[3178 | 6217.09] loss=2.10 avg=2.25\n",
            "[3179 | 6218.81] loss=2.09 avg=2.24\n",
            "[3180 | 6220.53] loss=2.29 avg=2.24\n",
            "[3181 | 6222.24] loss=2.41 avg=2.25\n",
            "[3182 | 6223.95] loss=3.09 avg=2.25\n",
            "[3183 | 6225.66] loss=2.37 avg=2.26\n",
            "[3184 | 6227.37] loss=2.16 avg=2.25\n",
            "[3185 | 6229.08] loss=2.38 avg=2.26\n",
            "[3186 | 6230.79] loss=2.40 avg=2.26\n",
            "[3187 | 6232.50] loss=2.52 avg=2.26\n",
            "[3188 | 6234.24] loss=2.56 avg=2.26\n",
            "[3189 | 6235.94] loss=2.19 avg=2.26\n",
            "[3190 | 6237.68] loss=2.57 avg=2.26\n",
            "[3191 | 6239.40] loss=2.78 avg=2.27\n",
            "[3192 | 6241.12] loss=2.32 avg=2.27\n",
            "[3193 | 6242.84] loss=2.32 avg=2.27\n",
            "[3194 | 6244.55] loss=2.63 avg=2.27\n",
            "[3195 | 6246.29] loss=2.27 avg=2.27\n",
            "[3196 | 6248.02] loss=2.03 avg=2.27\n",
            "[3197 | 6249.72] loss=1.59 avg=2.27\n",
            "[3198 | 6251.44] loss=2.34 avg=2.27\n",
            "[3199 | 6253.15] loss=2.35 avg=2.27\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "The best way to save these beautiful forests is to remove all the invasive species. All roads kill forest.\n",
            "“You make a mess and you leave a mess.” – Walt Disney\n",
            "China has taken over sovereign land of Alaska. What’s next?\n",
            "Worst time to own a home in America? Spring. @realDonaldTrump @TrumpDoral @TrumpDoralMAC @TrumpDoralWGC The WGC in Aberdeen will sell you your dream home https://t.co/z4fjCbT9oD\n",
            "Great honor to be selected to represent the U.S. at the @USPGA2017 in Dubai! https://t.co/x3Kc7v3hLf\n",
            "“What America Wants” @PGAChampionship presents @elmirotica @wayfairgolf to be represented by @realDonaldTrump @TrumpFlatiron @TrumpLasVegas in #USGCA17 https://t.co/jQKjgw7jY8\n",
            "“For every five who give up on life hope is a dangerous delusion.” – Benjamin Franklin\n",
            "“Never let your work stop you’ attention from the future.” – Think Like a Billionaire\n",
            "A message for our country: “No matter what you learn don’t assume because of previous experiences that you have that you can make it happen again.” – Think Like a Billionaire\n",
            "“When a friend is a source of strength a source of meaning you have in life there can be no success without it.” – Think Like A Billionaire\n",
            "Great news – I am working on the Miss Universe Pageant which will be the biggest and best ever!\n",
            "In the midst of all the \"disaster\" and “disappointments” our \"bundling\" of ObamaCare costs the insurance companies “up” by 20%. That’s the way to success &amp; everyone should get involved!\n",
            "“Trump’s campaign: Obama administration ‘foolishly ‘discovered” his birth certificate https://t.co/rRVbDQeDY8\n",
            "“Donald Trump: ‘I Was Never Ill or Unaware of Anybody Being Born ‘So I Don’t Have to Take Health Insurance” http://t.co/QtLj2m5xQ9 via The Federalist by @meghanmiker\n",
            "It was great meeting the team at Turnberry with the Duke and other great people - the place is terrific!\n",
            "“When a friend is a source of strength a source of meaning you have in life there can be no success without it.” – Think Like Bill Clinton\n",
            "Wacko @CNN is the worst of the liberal news networks.“This is really a case of “wasting your time fighting this fight to waste your time.” - @kimguilfoyle\n",
            "Congratulations to @TrumpWaikiki @TrumpSoHo for raising $1000 this morning for @TrumpCoralNational” http://t.co/D1vZ2MnN6w\n",
            "“The most dangerous thing you can possibly do is play golf.”- Michael Jordan\n",
            "ObamaCare's cost (over 10 years) will increase the deficit $500B higher. Make America great!\n",
            "“I’d sooner give your house a new coat of paint than take your business to Washington.” – TrumpNation\n",
            "My interview with @piersmorgan discussing golf policy and Trump’s new course in Scotland http://t.co/9cV0qFQ9iM\n",
            "“Donald Trump: ‘America should have given up so many opportunities” http://t.co/N5N5C5oVhO via @GolfDigestNews by @jleahtyler\n",
            "I'm right and you're wrong. ObamaCare is here and if ObamaCare is there will not be any \"fix.\"\n",
            "The White House should stop playing the pathetic card game of ‘playing the field so I can be there when it counts’. Make my place look dirt.\n",
            "‘America’s Best Hope’ -- it will be on display at @PRNewswire's #NASCARCARCar of the Year Awards Ceremony this Sunday! http://t.co/0mTdT4xQ1u\n",
            "Via: @thespiritwire : “Trump: Obama's Care is 'Not Right'” http://t.co/jW1H0Q6x4w\n",
            "“You don't’t put up with bullsh*t or make excuses or pretend that you don’t know are just games to bully or intimidate.” – Trump: The Art of the Deal https://t\n",
            "\n",
            "[3200 | 6276.98] loss=2.36 avg=2.27\n",
            "[3201 | 6278.68] loss=2.11 avg=2.27\n",
            "[3202 | 6280.40] loss=1.40 avg=2.26\n",
            "[3203 | 6282.11] loss=1.99 avg=2.26\n",
            "[3204 | 6283.83] loss=2.27 avg=2.26\n",
            "[3205 | 6285.56] loss=1.99 avg=2.25\n",
            "[3206 | 6287.28] loss=2.11 avg=2.25\n",
            "[3207 | 6289.01] loss=1.79 avg=2.25\n",
            "[3208 | 6290.72] loss=2.21 avg=2.25\n",
            "[3209 | 6292.43] loss=2.23 avg=2.25\n",
            "[3210 | 6294.14] loss=1.47 avg=2.24\n",
            "[3211 | 6295.86] loss=1.83 avg=2.23\n",
            "[3212 | 6297.58] loss=2.27 avg=2.23\n",
            "[3213 | 6299.31] loss=1.77 avg=2.23\n",
            "[3214 | 6301.04] loss=2.11 avg=2.23\n",
            "[3215 | 6302.75] loss=2.25 avg=2.23\n",
            "[3216 | 6304.47] loss=1.91 avg=2.23\n",
            "[3217 | 6306.18] loss=1.81 avg=2.22\n",
            "[3218 | 6307.89] loss=2.11 avg=2.22\n",
            "[3219 | 6309.60] loss=2.01 avg=2.22\n",
            "[3220 | 6311.31] loss=1.71 avg=2.21\n",
            "[3221 | 6313.04] loss=2.10 avg=2.21\n",
            "[3222 | 6314.75] loss=2.49 avg=2.22\n",
            "[3223 | 6316.46] loss=2.43 avg=2.22\n",
            "[3224 | 6318.17] loss=2.48 avg=2.22\n",
            "[3225 | 6319.91] loss=2.39 avg=2.22\n",
            "[3226 | 6321.62] loss=1.98 avg=2.22\n",
            "[3227 | 6323.33] loss=1.84 avg=2.22\n",
            "[3228 | 6325.04] loss=2.06 avg=2.21\n",
            "[3229 | 6326.75] loss=2.02 avg=2.21\n",
            "[3230 | 6328.48] loss=1.61 avg=2.21\n",
            "[3231 | 6330.19] loss=1.87 avg=2.20\n",
            "[3232 | 6331.90] loss=1.75 avg=2.20\n",
            "[3233 | 6333.61] loss=1.88 avg=2.19\n",
            "[3234 | 6335.33] loss=1.50 avg=2.19\n",
            "[3235 | 6337.05] loss=2.03 avg=2.19\n",
            "[3236 | 6338.77] loss=2.01 avg=2.18\n",
            "[3237 | 6340.48] loss=1.71 avg=2.18\n",
            "[3238 | 6342.20] loss=2.29 avg=2.18\n",
            "[3239 | 6343.91] loss=2.17 avg=2.18\n",
            "[3240 | 6345.62] loss=1.84 avg=2.18\n",
            "[3241 | 6347.34] loss=2.55 avg=2.18\n",
            "[3242 | 6349.05] loss=1.89 avg=2.18\n",
            "[3243 | 6350.79] loss=2.22 avg=2.18\n",
            "[3244 | 6352.52] loss=1.62 avg=2.17\n",
            "[3245 | 6354.23] loss=1.92 avg=2.17\n",
            "[3246 | 6355.95] loss=2.72 avg=2.18\n",
            "[3247 | 6357.66] loss=2.46 avg=2.18\n",
            "[3248 | 6359.39] loss=1.82 avg=2.17\n",
            "[3249 | 6361.12] loss=1.99 avg=2.17\n",
            "[3250 | 6362.85] loss=2.31 avg=2.17\n",
            "[3251 | 6364.56] loss=3.08 avg=2.18\n",
            "[3252 | 6366.30] loss=2.29 avg=2.18\n",
            "[3253 | 6368.03] loss=2.05 avg=2.18\n",
            "[3254 | 6369.74] loss=2.65 avg=2.19\n",
            "[3255 | 6371.47] loss=2.29 avg=2.19\n",
            "[3256 | 6373.18] loss=2.15 avg=2.19\n",
            "[3257 | 6374.91] loss=2.64 avg=2.19\n",
            "[3258 | 6376.63] loss=2.45 avg=2.20\n",
            "[3259 | 6378.35] loss=2.39 avg=2.20\n",
            "[3260 | 6380.06] loss=1.78 avg=2.19\n",
            "[3261 | 6381.77] loss=2.17 avg=2.19\n",
            "[3262 | 6383.48] loss=2.61 avg=2.20\n",
            "[3263 | 6385.21] loss=2.00 avg=2.20\n",
            "[3264 | 6386.92] loss=1.73 avg=2.19\n",
            "[3265 | 6388.64] loss=1.94 avg=2.19\n",
            "[3266 | 6390.35] loss=2.70 avg=2.19\n",
            "[3267 | 6392.08] loss=1.94 avg=2.19\n",
            "[3268 | 6393.79] loss=2.84 avg=2.20\n",
            "[3269 | 6395.50] loss=2.01 avg=2.20\n",
            "[3270 | 6397.21] loss=1.89 avg=2.19\n",
            "[3271 | 6398.92] loss=2.16 avg=2.19\n",
            "[3272 | 6400.63] loss=2.47 avg=2.19\n",
            "[3273 | 6402.36] loss=2.03 avg=2.19\n",
            "[3274 | 6404.08] loss=1.91 avg=2.19\n",
            "[3275 | 6405.79] loss=2.48 avg=2.19\n",
            "[3276 | 6407.51] loss=2.33 avg=2.19\n",
            "[3277 | 6409.22] loss=2.71 avg=2.20\n",
            "[3278 | 6410.94] loss=1.80 avg=2.20\n",
            "[3279 | 6412.65] loss=1.66 avg=2.19\n",
            "[3280 | 6414.37] loss=2.56 avg=2.19\n",
            "[3281 | 6416.08] loss=2.90 avg=2.20\n",
            "[3282 | 6417.80] loss=2.08 avg=2.20\n",
            "[3283 | 6419.51] loss=2.06 avg=2.20\n",
            "[3284 | 6421.22] loss=2.01 avg=2.20\n",
            "[3285 | 6422.96] loss=2.25 avg=2.20\n",
            "[3286 | 6424.69] loss=2.16 avg=2.20\n",
            "[3287 | 6426.40] loss=1.95 avg=2.19\n",
            "[3288 | 6428.11] loss=2.73 avg=2.20\n",
            "[3289 | 6429.85] loss=2.26 avg=2.20\n",
            "[3290 | 6431.58] loss=1.72 avg=2.20\n",
            "[3291 | 6433.29] loss=1.85 avg=2.19\n",
            "[3292 | 6435.00] loss=2.53 avg=2.20\n",
            "[3293 | 6436.71] loss=2.05 avg=2.19\n",
            "[3294 | 6438.42] loss=2.33 avg=2.20\n",
            "[3295 | 6440.15] loss=2.04 avg=2.19\n",
            "[3296 | 6441.88] loss=2.35 avg=2.20\n",
            "[3297 | 6443.61] loss=1.55 avg=2.19\n",
            "[3298 | 6445.33] loss=2.22 avg=2.19\n",
            "[3299 | 6447.04] loss=1.43 avg=2.18\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "IRP\n",
            "Remember when Hillary said that the #Vattenfall issue was a red herring?  She was a dumb bitch &amp; got it wrong.\n",
            "Hillary Clinton says the only issue that will determine her victory is the issue that will determine Donald Trump's victory?\n",
            "Great to be in Indiana- the home of the @GolfNational. Great to have Gary out with his great supporters! https://t.co/Hvb4lMh5Rp\n",
            "Will be in Des Moines Iowa this morning to visit with @angelgabez for a news conference about our great team!\n",
            "Big crowd yesterday in Pensacola. #MakeAmericaGreatAgain #MakeAmericaGreatAgainhttps://t.co/f6YbHmw4D9 https://t.co/WXdVYsY7Z9\n",
            "Join me for a rally tomorrow evening at the Louisville Garden! #TrumpRally\n",
            "Hillary Clinton: It's Not About The Money: ‘I'm Not Excited ’ By The Email Scandal’ Nor Is Obama.\n",
            "Looking forward to being at the Louisville-N.H. rally! Will be speaking at 10:00...https://t.co/9Yj9FqmNm3\n",
            "I will be on Larry King Live tonight at 9pm ET talking about the news. Enjoy! #LAKings\n",
            "What really irked me yesterday when I saw Secretary Clinton's speech was the fact that Secretary Clinton knew everything she was saying but didn't even care. Disgraceful!\n",
            ".@FoxNews has become so biased that they don’t matter anymore-just go ahead and complain about the name on the channel. Sad!\n",
            "Clinton said Obama has \"utter dominance\" of the planet and she would \"totally destroy\" him.  Wow!\n",
            "Crooked Hillary has been attacked by Obama for saying she has \"total dominance\" over the planet.  She said the same thing about him.\n",
            "Obama is the worst.  He attacks ISIS because he knows it is going to succeed which only emboldens the terrorists.\n",
            ".@CNN has been so negative and biased it's ridiculous and I have to go to Twitter to find out why. Really bad people!\n",
            "Why doesn’t the @CNN news crew even show these amazing people who are constantly talking about how great the Trump campaign is?\n",
            "Join me in Iowa tomorrow! #TrumpRally#MakeAmericaGreatAgainhttps://t.co/QjKqeo5XoM https://t.co/Hvb4lMh5Rp\n",
            "Just landed in Des Moines Iowa. Great news! #TrumpRally #MakeAmericaGreatAgain https://t.co/qNU4VZH2pD\n",
            "Thank you for all of the endorsements President Obama has gotten in recent days.  Very proud of you the men and women of FEMA &amp; ICE!\n",
            "Just out - 'FEMA &amp; ICE are in a National Emergency'https://t.co/qNU4VZH2pD\n",
            "The real story here is why aren't we now using the military to help our struggling police?\n",
            "Hillary’s Email server will have changed the FBI’s determination about whether or not she should be charged with a crime.\n",
            "Hillary didn't send or receive anything harmful on her illegal private server. I wonder if she has a psychiatrist?\n",
            "Clinton is the only one who knows that Russia doesn't like her based on her past behavior.\n",
            "Crooked Hillary Clinton said one of the main ways ISIS raises money is by 'tweeting their propaganda' and if I win the book is on her\n",
            "Crooked Hillary Clinton admitted that 'ISIS is making its money through social media' &amp; that she will use that power to her advantage.\n",
            "Thank you- New Hampshire! #MakeAmericaGreatAgain https://t.co/m4lKFxNzJk https://t.co/H2mHnX5mNl\n",
            "Thank you for the endorsement @RandPaul. We will hit Hillary at her own game and on her own terms. #RandPaul\n",
            "Thank you New Hampshire! #NewHampFAMilies #Trump2016 https://t.co/m4lKFxNzJk https://t.co/kG3gW8h9jN\n",
            "Hillary said to ISIS in an email \"If you bomb us again and again and kill many people we will not be able to inspire.\"  How pathetic!\n",
            "#MakeAmericaGreatAgain #Trump2016 https://t.co/yZ0rLQZ8y8\n",
            "#MakeAmericaGreatAgain https://t.co/1C4dP9YV3e\n",
            "Hillary's big speech at the UN was a total disaster.  She admitted defeat in the debate and just blew it again - totally uninformed and not tough!\n",
            "\n",
            "[3300 | 6470.96] loss=1.95 avg=2.18\n",
            "[3301 | 6472.67] loss=2.25 avg=2.18\n",
            "[3302 | 6474.41] loss=2.20 avg=2.18\n",
            "[3303 | 6476.12] loss=2.78 avg=2.19\n",
            "[3304 | 6477.84] loss=2.44 avg=2.19\n",
            "[3305 | 6479.55] loss=2.13 avg=2.19\n",
            "[3306 | 6481.26] loss=2.23 avg=2.19\n",
            "[3307 | 6482.98] loss=2.51 avg=2.19\n",
            "[3308 | 6484.69] loss=2.69 avg=2.20\n",
            "[3309 | 6486.43] loss=2.32 avg=2.20\n",
            "[3310 | 6488.14] loss=2.10 avg=2.20\n",
            "[3311 | 6489.85] loss=1.71 avg=2.19\n",
            "[3312 | 6491.59] loss=1.99 avg=2.19\n",
            "[3313 | 6493.32] loss=2.35 avg=2.19\n",
            "[3314 | 6495.03] loss=1.89 avg=2.19\n",
            "[3315 | 6496.77] loss=2.38 avg=2.19\n",
            "[3316 | 6498.49] loss=2.93 avg=2.20\n",
            "[3317 | 6500.23] loss=1.65 avg=2.19\n",
            "[3318 | 6501.94] loss=1.86 avg=2.19\n",
            "[3319 | 6503.65] loss=1.94 avg=2.19\n",
            "[3320 | 6505.38] loss=2.49 avg=2.19\n",
            "[3321 | 6507.10] loss=1.57 avg=2.18\n",
            "[3322 | 6508.81] loss=1.74 avg=2.18\n",
            "[3323 | 6510.52] loss=2.06 avg=2.18\n",
            "[3324 | 6512.23] loss=1.86 avg=2.17\n",
            "[3325 | 6513.94] loss=1.99 avg=2.17\n",
            "[3326 | 6515.65] loss=1.85 avg=2.17\n",
            "[3327 | 6517.38] loss=1.75 avg=2.17\n",
            "[3328 | 6519.10] loss=2.82 avg=2.17\n",
            "[3329 | 6520.81] loss=2.18 avg=2.17\n",
            "[3330 | 6522.54] loss=2.10 avg=2.17\n",
            "[3331 | 6524.27] loss=3.13 avg=2.18\n",
            "[3332 | 6526.00] loss=2.58 avg=2.19\n",
            "[3333 | 6527.71] loss=2.32 avg=2.19\n",
            "[3334 | 6529.42] loss=1.23 avg=2.18\n",
            "[3335 | 6531.13] loss=1.59 avg=2.17\n",
            "[3336 | 6532.86] loss=1.91 avg=2.17\n",
            "[3337 | 6534.57] loss=2.37 avg=2.17\n",
            "[3338 | 6536.30] loss=2.19 avg=2.17\n",
            "[3339 | 6538.02] loss=1.61 avg=2.17\n",
            "[3340 | 6539.73] loss=1.81 avg=2.16\n",
            "[3341 | 6541.45] loss=3.27 avg=2.17\n",
            "[3342 | 6543.16] loss=2.22 avg=2.17\n",
            "[3343 | 6544.87] loss=2.17 avg=2.17\n",
            "[3344 | 6546.59] loss=2.29 avg=2.17\n",
            "[3345 | 6548.30] loss=2.21 avg=2.17\n",
            "[3346 | 6550.04] loss=2.67 avg=2.18\n",
            "[3347 | 6551.75] loss=1.91 avg=2.18\n",
            "[3348 | 6553.49] loss=2.73 avg=2.18\n",
            "[3349 | 6555.22] loss=2.45 avg=2.19\n",
            "[3350 | 6556.94] loss=2.08 avg=2.18\n",
            "[3351 | 6558.65] loss=1.96 avg=2.18\n",
            "[3352 | 6560.36] loss=1.82 avg=2.18\n",
            "[3353 | 6562.07] loss=1.68 avg=2.17\n",
            "[3354 | 6563.78] loss=2.05 avg=2.17\n",
            "[3355 | 6565.49] loss=2.26 avg=2.17\n",
            "[3356 | 6567.20] loss=2.31 avg=2.17\n",
            "[3357 | 6568.93] loss=1.70 avg=2.17\n",
            "[3358 | 6570.66] loss=1.35 avg=2.16\n",
            "[3359 | 6572.39] loss=1.64 avg=2.16\n",
            "[3360 | 6574.10] loss=1.96 avg=2.15\n",
            "[3361 | 6575.83] loss=2.71 avg=2.16\n",
            "[3362 | 6577.57] loss=2.85 avg=2.17\n",
            "[3363 | 6579.30] loss=2.59 avg=2.17\n",
            "[3364 | 6581.03] loss=1.92 avg=2.17\n",
            "[3365 | 6582.74] loss=2.19 avg=2.17\n",
            "[3366 | 6584.47] loss=1.87 avg=2.17\n",
            "[3367 | 6586.18] loss=2.15 avg=2.17\n",
            "[3368 | 6587.89] loss=2.93 avg=2.17\n",
            "[3369 | 6589.60] loss=2.55 avg=2.18\n",
            "[3370 | 6591.32] loss=3.15 avg=2.19\n",
            "[3371 | 6593.03] loss=1.80 avg=2.18\n",
            "[3372 | 6594.74] loss=1.70 avg=2.18\n",
            "[3373 | 6596.47] loss=2.11 avg=2.18\n",
            "[3374 | 6598.20] loss=2.07 avg=2.18\n",
            "[3375 | 6599.94] loss=2.55 avg=2.18\n",
            "[3376 | 6601.67] loss=2.30 avg=2.18\n",
            "[3377 | 6603.38] loss=2.55 avg=2.18\n",
            "[3378 | 6605.09] loss=2.57 avg=2.19\n",
            "[3379 | 6606.80] loss=2.22 avg=2.19\n",
            "[3380 | 6608.51] loss=1.47 avg=2.18\n",
            "[3381 | 6610.23] loss=2.63 avg=2.19\n",
            "[3382 | 6611.96] loss=2.77 avg=2.19\n",
            "[3383 | 6613.67] loss=2.28 avg=2.19\n",
            "[3384 | 6615.39] loss=0.95 avg=2.18\n",
            "[3385 | 6617.10] loss=2.76 avg=2.19\n",
            "[3386 | 6618.81] loss=2.07 avg=2.19\n",
            "[3387 | 6620.52] loss=2.28 avg=2.19\n",
            "[3388 | 6622.24] loss=2.59 avg=2.19\n",
            "[3389 | 6623.95] loss=1.74 avg=2.19\n",
            "[3390 | 6625.67] loss=2.18 avg=2.19\n",
            "[3391 | 6627.38] loss=1.98 avg=2.18\n",
            "[3392 | 6629.11] loss=1.66 avg=2.18\n",
            "[3393 | 6630.83] loss=3.30 avg=2.19\n",
            "[3394 | 6632.54] loss=2.43 avg=2.19\n",
            "[3395 | 6634.25] loss=2.63 avg=2.20\n",
            "[3396 | 6635.98] loss=1.33 avg=2.19\n",
            "[3397 | 6637.69] loss=2.16 avg=2.19\n",
            "[3398 | 6639.42] loss=1.98 avg=2.19\n",
            "[3399 | 6641.13] loss=2.33 avg=2.19\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " be a major part of the transition. Will be having lunch with President Putin shortly.\n",
            "I hope the House and Senate Republicans get together in a short period of time and come up with a really great health care bill. Don’t let Pataki fool you!\n",
            "Why did [the late Steve Jobs] leave [the Apple] board? Because he was fired like a dog by Apple. Jobs was so stupid but I’m sure Steve wanted the board to fire him. He never had the guts!\n",
            "“The art isn’t the art is the public reaction to it. The art is of the craft.” - @Ruth Marcus\n",
            "As I told my @Gretawire co-host Rachael Ray last night in our very successful #RGA18 Annual State Dinner @RachaelRay is really smart and has done much good...\n",
            "...Just wait &amp; see how well I do.\n",
            "I told the @Gretawire co-host Rachael Ray last night in our very successful #RGA18 Annual State Dinner @RachaelRay is really smart and has done much good...\n",
            "I will be doing @RuthMarcus tonight at 10 pm on @FoxNews\n",
            "My @FoxNews interview with Greta Van Susteren re: ObamaCare premiums premiums &amp; the ObamaCare premium growth premiums https://t.co/4N3Dd5KtBQ https://t.co/c7tR0p7y7X\n",
            "I am watching the ObamaCare premiums and am very happy with what I see. The law is working great. Not ideal. Watch!\n",
            "#TrumpVlog #MakeAmericaGreatAgain  https://t.co/qD0Hg2vQbv\n",
            "I will be on the @ThisWeekABC tonight. Enjoy!\n",
            "We are losing $17B+ in NET economic impact each year through ObamaCare. The website is a $6000 website. Why are we wasting money on a $6000 website?\n",
            "A lot of people are watching my @CNN interview that was very interesting - will be on that network very soon.\n",
            "It was an honor to host @MittRomney at the Governors Ball yesterday in Martha's Vineyard. Look forward to seeing all of those I will be leaving for a 2 week trip today!\n",
            "The Obama campaign is spending thousands of dollars to air super-PAC ads on my family's TV broadcasts. Disgraceful!\n",
            "Obama was a great supporter of David Duke when I was running against him. Too bad he lost.\n",
            "The ObamaCare website is no longer functioning. The website is a complete disaster. Biggest mistake made in U.S. healthcare in my lifetime\n",
            "I can't wait for @MittRomney to make a big deal about my father's supposed racism. Why hasn't he done this before? https://t.co/8T7TQeXVw3\n",
            "The White House Correspondents  Dinner is now having such a big and inappropriate impact on the president and the press. This should never be allowed to happen\n",
            "It has just been reported that the White House Correspondents Dinner is taking such a big and unnecessary toll on the WH. In a crisis.\n",
            "The White House Correspondents Dinner which I sponsored last July produced a record attendance of over 140 guests.\n",
            "The White House Correspondents Dinner this year produced over 140 guests. Not good - this is one of the many problems we have in our country!\n",
            "A great honor to welcome the 2016 @NECGOP Board of Directors @ReedConway’s @WVRegister reports @WRCB/WSAZ\n",
            "The Trump Doral is taking shape! https://t.co/8xX3wX8KUY\n",
            "As promised here it is. We will both be signing copies of @MittRomney's new book ‘The Art of The Deal’ when we leave for Miami! https://t.co/sHWd1DyW9x\n",
            "The White House Correspondents Dinner produced 130+ guests in July. Going down in history as perhaps the biggest in White House history!\n",
            "The White House Correspondents Dinner produced 129+ guests this year with the lowest attendance in memory.\n",
            "“You've got to have passion. You’ve got to have enthusiasm. And you’ve got to have passion to execute those ideas.” – @KarlRove\n",
            "New data from the @NECGroup shows that @MittRomney led @realDonaldTrump by 11 points on the economy in the final weeks of the 2016 election https://t.co/qd7cBVbAqx\n",
            "Via @MailOnline: \"Donald Trump says he will ‘kick [USC] ass’ at the @NECGOP’s @NEConFerencing Dinner\" https://t.co/JQ2nGZpJjm\n",
            "The @NECGOP @\n",
            "\n",
            "[3400 | 6664.98] loss=3.31 avg=2.20\n",
            "[3401 | 6666.69] loss=1.69 avg=2.19\n",
            "[3402 | 6668.43] loss=2.04 avg=2.19\n",
            "[3403 | 6670.16] loss=2.14 avg=2.19\n",
            "[3404 | 6671.89] loss=2.44 avg=2.19\n",
            "[3405 | 6673.62] loss=2.57 avg=2.20\n",
            "[3406 | 6675.36] loss=2.38 avg=2.20\n",
            "[3407 | 6677.07] loss=1.82 avg=2.20\n",
            "[3408 | 6678.80] loss=1.92 avg=2.19\n",
            "[3409 | 6680.52] loss=2.48 avg=2.20\n",
            "[3410 | 6682.23] loss=1.86 avg=2.19\n",
            "[3411 | 6683.94] loss=2.85 avg=2.20\n",
            "[3412 | 6685.68] loss=2.55 avg=2.20\n",
            "[3413 | 6687.41] loss=1.69 avg=2.20\n",
            "[3414 | 6689.13] loss=2.59 avg=2.20\n",
            "[3415 | 6690.84] loss=1.65 avg=2.20\n",
            "[3416 | 6692.56] loss=2.17 avg=2.19\n",
            "[3417 | 6694.27] loss=2.05 avg=2.19\n",
            "[3418 | 6696.00] loss=2.92 avg=2.20\n",
            "[3419 | 6697.73] loss=2.17 avg=2.20\n",
            "[3420 | 6699.44] loss=1.75 avg=2.20\n",
            "[3421 | 6701.17] loss=2.02 avg=2.19\n",
            "[3422 | 6702.88] loss=2.46 avg=2.20\n",
            "[3423 | 6704.61] loss=1.92 avg=2.19\n",
            "[3424 | 6706.35] loss=2.17 avg=2.19\n",
            "[3425 | 6708.08] loss=2.16 avg=2.19\n",
            "[3426 | 6709.81] loss=2.29 avg=2.19\n",
            "[3427 | 6711.54] loss=1.64 avg=2.19\n",
            "[3428 | 6713.26] loss=2.25 avg=2.19\n",
            "[3429 | 6714.98] loss=2.20 avg=2.19\n",
            "[3430 | 6716.70] loss=1.90 avg=2.19\n",
            "[3431 | 6718.41] loss=2.69 avg=2.19\n",
            "[3432 | 6720.12] loss=1.60 avg=2.19\n",
            "[3433 | 6721.83] loss=2.11 avg=2.18\n",
            "[3434 | 6723.55] loss=2.12 avg=2.18\n",
            "[3435 | 6725.27] loss=1.87 avg=2.18\n",
            "[3436 | 6726.98] loss=1.80 avg=2.18\n",
            "[3437 | 6728.70] loss=1.87 avg=2.17\n",
            "[3438 | 6730.41] loss=1.94 avg=2.17\n",
            "[3439 | 6732.12] loss=2.24 avg=2.17\n",
            "[3440 | 6733.86] loss=2.14 avg=2.17\n",
            "[3441 | 6735.58] loss=1.98 avg=2.17\n",
            "[3442 | 6737.30] loss=3.22 avg=2.18\n",
            "[3443 | 6739.01] loss=2.44 avg=2.18\n",
            "[3444 | 6740.74] loss=1.48 avg=2.18\n",
            "[3445 | 6742.47] loss=2.18 avg=2.18\n",
            "[3446 | 6744.19] loss=2.64 avg=2.18\n",
            "[3447 | 6745.90] loss=1.67 avg=2.18\n",
            "[3448 | 6747.63] loss=2.40 avg=2.18\n",
            "[3449 | 6749.35] loss=1.76 avg=2.17\n",
            "[3450 | 6751.06] loss=1.46 avg=2.17\n",
            "[3451 | 6752.80] loss=1.93 avg=2.16\n",
            "[3452 | 6754.53] loss=3.11 avg=2.17\n",
            "[3453 | 6756.24] loss=2.27 avg=2.18\n",
            "[3454 | 6757.96] loss=2.42 avg=2.18\n",
            "[3455 | 6759.67] loss=2.39 avg=2.18\n",
            "[3456 | 6761.41] loss=2.01 avg=2.18\n",
            "[3457 | 6763.12] loss=2.29 avg=2.18\n",
            "[3458 | 6764.83] loss=1.65 avg=2.17\n",
            "[3459 | 6766.57] loss=1.52 avg=2.17\n",
            "[3460 | 6768.30] loss=2.55 avg=2.17\n",
            "[3461 | 6770.03] loss=1.77 avg=2.17\n",
            "[3462 | 6771.74] loss=2.26 avg=2.17\n",
            "[3463 | 6773.47] loss=2.27 avg=2.17\n",
            "[3464 | 6775.18] loss=2.18 avg=2.17\n",
            "[3465 | 6776.91] loss=2.32 avg=2.17\n",
            "[3466 | 6778.63] loss=1.63 avg=2.17\n",
            "[3467 | 6780.36] loss=2.05 avg=2.16\n",
            "[3468 | 6782.08] loss=1.98 avg=2.16\n",
            "[3469 | 6783.80] loss=1.70 avg=2.16\n",
            "[3470 | 6785.53] loss=2.01 avg=2.16\n",
            "[3471 | 6787.26] loss=2.22 avg=2.16\n",
            "[3472 | 6788.99] loss=1.67 avg=2.15\n",
            "[3473 | 6790.72] loss=2.01 avg=2.15\n",
            "[3474 | 6792.43] loss=2.76 avg=2.16\n",
            "[3475 | 6794.14] loss=2.06 avg=2.16\n",
            "[3476 | 6795.88] loss=2.18 avg=2.16\n",
            "[3477 | 6797.60] loss=2.52 avg=2.16\n",
            "[3478 | 6799.34] loss=2.22 avg=2.16\n",
            "[3479 | 6801.05] loss=1.63 avg=2.16\n",
            "[3480 | 6802.76] loss=1.62 avg=2.15\n",
            "[3481 | 6804.47] loss=1.83 avg=2.15\n",
            "[3482 | 6806.18] loss=2.06 avg=2.15\n",
            "[3483 | 6807.89] loss=1.62 avg=2.14\n",
            "[3484 | 6809.62] loss=1.99 avg=2.14\n",
            "[3485 | 6811.33] loss=2.10 avg=2.14\n",
            "[3486 | 6813.07] loss=2.31 avg=2.14\n",
            "[3487 | 6814.79] loss=2.61 avg=2.14\n",
            "[3488 | 6816.52] loss=1.78 avg=2.14\n",
            "[3489 | 6818.26] loss=2.68 avg=2.15\n",
            "[3490 | 6819.99] loss=2.22 avg=2.15\n",
            "[3491 | 6821.71] loss=1.60 avg=2.14\n",
            "[3492 | 6823.42] loss=2.04 avg=2.14\n",
            "[3493 | 6825.14] loss=2.46 avg=2.14\n",
            "[3494 | 6826.85] loss=1.48 avg=2.14\n",
            "[3495 | 6828.56] loss=1.70 avg=2.13\n",
            "[3496 | 6830.28] loss=2.61 avg=2.14\n",
            "[3497 | 6832.01] loss=2.24 avg=2.14\n",
            "[3498 | 6833.74] loss=1.87 avg=2.14\n",
            "[3499 | 6835.47] loss=2.26 avg=2.14\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " scheduled it. Great news!\n",
            "President Obama said that \"our country is the envy of the world.\" Who said that? Great shame!\n",
            "The Republican Party will win because it has to because to exist as a political party you must be a party!\n",
            "The Democratic Party is going to MAKE America GREAT AGAIN!\n",
            "I will be making a major announcement at 12:30PM EST today at Trump Doral--where my great friend Rickie Fowler won the @PGAChampionship\n",
            "#MakeAmericaGreatAgain #Trump2016 https://t.co/QnXCf9xHW5\n",
            "So important to my success! This has always been my plan but when I didn't go through the right people they became the facts. I am the best negotiator\n",
            "The Democrat rigged and fraudulent Dossier was paid for by Crooked Hillary and the Dems. Where is the DNC server? Bad leadership!\n",
            "The @CNBCJudson Poll shows that a record number of people are unemployed. I am in fact doing better than Obama for many reasons but the truth is out there!\n",
            "We are going to have a great interview on @Morning_Joe with @JoeNBC at 7:00 A.M.\n",
            "It is my honor to accept the Christian Science Monitor CEO's Person of the Year award! Thank you Christian S.I.!https://t.co/6yBxWZmhNf\n",
            "Heading to #Doonbeg now for a big fundraiser at Trump Port Orchard. So many plans for such a great venue! Thanks so much.\n",
            "I will be making a big speech Monday evening at the Christian Science Monitor's gala dinner. Will also be making a small one-day gala at 7:00\n",
            "Great news-GREAT NEW SPREADSHEETS ON THE TRUMP CAMPAIGN! https://t.co/2IxlW3H0wD\n",
            "Will be making a big speech Monday evening at the Christian Science Monitor's gala dinner. Will also be making a small one-day gala @ 7:00 P.M.\n",
            "I will be making a big speech Monday evening at the Christian Science Monitor's gala dinner. Will also be making a small one-day gala @ 7:00 P.M.\n",
            "It is a shame that when Hillary said that Trump should get a medal because he killed Bin Laden-that didn't help him get elected.\n",
            "Crooked Hillary Clinton said \"in the end we are all Americans first\" and that was that. Not good enough @CNBC. @realDonaldTrump\n",
            "Crooked Hillary gave false speeches and phony polls to get elected. She is a bad candidate - and with bad polls she will only get worse!\n",
            "\"Donald Trump to @MSNBC: No new Muslim Ban.\" https://t.co/XzV1D2sNfK\n",
            "Thanks to the people of Florida for all of their help yesterday. The storm is now out of my way. My focus is now on the very important election!\n",
            "Crooked Hillary is buying and selling the American people--she makes them sick--see Wall Street Journal and see what happens!\n",
            "Crooked Hillary is not qualified to be president. She is a disaster and bad for our country!\n",
            "So proud of all of our team members at Doral who made it possible to win the 2016 Trump Frisbee Classic!\n",
            "Thank you for your support in Florida Florida!  https://t.co/3JkUg8pzQ3\n",
            "I will be interviewed on @Morning_Joe at 6:00 A.M. Enjoy!\n",
            "Thank you Florida America!\n",
            "I am always available to answer tough questions on @Morning_Joe - just watch at 5:00 A.M. (check your local listings)!  @Morning_Joe\n",
            "Why were the Republicans not asked to do a second debate after the first one was a total disaster? I got no support and just negative ads.\n",
            "The Democrats are going wild with negative ads on me so now they want to do a third — not fair to hurt our already \"disfavored\" brand!\n",
            "Crooked Hilary Clinton should answer to the voters after losing an election that people will never forget or forgive. She lied &amp; stole and Crooked Hillary got caught!\n",
            "Thank you for your support in Florida. You will NOT like what happens when Crooked Hillary gets roughed up! #Trump2016 https://t.co/UHJhFQ5e0G\n",
            "Thank you Florida! I'll see you soon!  https://t.co/vR8KXQhO3y\n",
            "Thank you Florida! https://t.co/s2F0Rdv9l1\n",
            "Crooked Hillary is not qualified - has bad judgement and bad leadership skills. You can beat ISIS with brainpower!\n",
            "Thank you Florida! #Trump2016 #MakeAmericaGreatAgain https://t.co/nY3B3\n",
            "\n",
            "[3500 | 6859.25] loss=2.75 avg=2.14\n",
            "[3501 | 6860.98] loss=1.84 avg=2.14\n",
            "[3502 | 6862.72] loss=2.00 avg=2.14\n",
            "[3503 | 6864.45] loss=2.11 avg=2.14\n",
            "[3504 | 6866.18] loss=2.73 avg=2.14\n",
            "[3505 | 6867.91] loss=2.13 avg=2.14\n",
            "[3506 | 6869.64] loss=2.12 avg=2.14\n",
            "[3507 | 6871.37] loss=2.18 avg=2.14\n",
            "[3508 | 6873.10] loss=1.61 avg=2.14\n",
            "[3509 | 6874.84] loss=2.03 avg=2.14\n",
            "[3510 | 6876.56] loss=2.13 avg=2.14\n",
            "[3511 | 6878.28] loss=2.38 avg=2.14\n",
            "[3512 | 6880.02] loss=1.79 avg=2.14\n",
            "[3513 | 6881.74] loss=2.86 avg=2.14\n",
            "[3514 | 6883.48] loss=2.11 avg=2.14\n",
            "[3515 | 6885.22] loss=1.93 avg=2.14\n",
            "[3516 | 6886.95] loss=2.20 avg=2.14\n",
            "[3517 | 6888.66] loss=2.04 avg=2.14\n",
            "[3518 | 6890.39] loss=2.31 avg=2.14\n",
            "[3519 | 6892.12] loss=2.37 avg=2.15\n",
            "[3520 | 6893.84] loss=2.82 avg=2.15\n",
            "[3521 | 6895.56] loss=2.29 avg=2.15\n",
            "[3522 | 6897.27] loss=1.91 avg=2.15\n",
            "[3523 | 6898.98] loss=2.81 avg=2.16\n",
            "[3524 | 6900.69] loss=1.83 avg=2.15\n",
            "[3525 | 6902.41] loss=2.08 avg=2.15\n",
            "[3526 | 6904.14] loss=2.01 avg=2.15\n",
            "[3527 | 6905.87] loss=2.40 avg=2.15\n",
            "[3528 | 6907.60] loss=1.71 avg=2.15\n",
            "[3529 | 6909.31] loss=2.02 avg=2.15\n",
            "[3530 | 6911.04] loss=2.68 avg=2.15\n",
            "[3531 | 6912.75] loss=2.76 avg=2.16\n",
            "[3532 | 6914.48] loss=1.87 avg=2.16\n",
            "[3533 | 6916.19] loss=2.01 avg=2.16\n",
            "[3534 | 6917.92] loss=2.03 avg=2.15\n",
            "[3535 | 6919.63] loss=2.02 avg=2.15\n",
            "[3536 | 6921.37] loss=1.79 avg=2.15\n",
            "[3537 | 6923.09] loss=1.95 avg=2.15\n",
            "[3538 | 6924.83] loss=1.49 avg=2.14\n",
            "[3539 | 6926.56] loss=1.91 avg=2.14\n",
            "[3540 | 6928.27] loss=2.29 avg=2.14\n",
            "[3541 | 6929.98] loss=1.83 avg=2.14\n",
            "[3542 | 6931.69] loss=2.18 avg=2.14\n",
            "[3543 | 6933.42] loss=2.18 avg=2.14\n",
            "[3544 | 6935.13] loss=2.38 avg=2.14\n",
            "[3545 | 6936.86] loss=2.35 avg=2.14\n",
            "[3546 | 6938.59] loss=2.59 avg=2.15\n",
            "[3547 | 6940.32] loss=1.71 avg=2.14\n",
            "[3548 | 6942.05] loss=2.08 avg=2.14\n",
            "[3549 | 6943.77] loss=1.59 avg=2.14\n",
            "[3550 | 6945.48] loss=1.64 avg=2.13\n",
            "[3551 | 6947.19] loss=1.82 avg=2.13\n",
            "[3552 | 6948.91] loss=1.68 avg=2.12\n",
            "[3553 | 6950.62] loss=1.97 avg=2.12\n",
            "[3554 | 6952.33] loss=2.82 avg=2.13\n",
            "[3555 | 6954.05] loss=1.80 avg=2.13\n",
            "[3556 | 6955.78] loss=1.59 avg=2.12\n",
            "[3557 | 6957.52] loss=2.17 avg=2.12\n",
            "[3558 | 6959.25] loss=1.46 avg=2.11\n",
            "[3559 | 6960.97] loss=2.06 avg=2.11\n",
            "[3560 | 6962.68] loss=1.93 avg=2.11\n",
            "[3561 | 6964.41] loss=1.21 avg=2.10\n",
            "[3562 | 6966.12] loss=1.62 avg=2.10\n",
            "[3563 | 6967.86] loss=2.13 avg=2.10\n",
            "[3564 | 6969.59] loss=2.42 avg=2.10\n",
            "[3565 | 6971.31] loss=1.37 avg=2.09\n",
            "[3566 | 6973.04] loss=2.26 avg=2.10\n",
            "[3567 | 6974.77] loss=2.16 avg=2.10\n",
            "[3568 | 6976.49] loss=1.68 avg=2.09\n",
            "[3569 | 6978.22] loss=1.35 avg=2.09\n",
            "[3570 | 6979.92] loss=2.03 avg=2.08\n",
            "[3571 | 6981.66] loss=2.21 avg=2.09\n",
            "[3572 | 6983.39] loss=1.86 avg=2.08\n",
            "[3573 | 6985.12] loss=2.06 avg=2.08\n",
            "[3574 | 6986.83] loss=2.09 avg=2.08\n",
            "[3575 | 6988.56] loss=1.22 avg=2.07\n",
            "[3576 | 6990.30] loss=1.79 avg=2.07\n",
            "[3577 | 6992.03] loss=1.95 avg=2.07\n",
            "[3578 | 6993.76] loss=2.23 avg=2.07\n",
            "[3579 | 6995.47] loss=2.57 avg=2.08\n",
            "[3580 | 6997.18] loss=2.38 avg=2.08\n",
            "[3581 | 6998.89] loss=2.09 avg=2.08\n",
            "[3582 | 7000.62] loss=2.29 avg=2.08\n",
            "[3583 | 7002.35] loss=2.28 avg=2.08\n",
            "[3584 | 7004.07] loss=1.78 avg=2.08\n",
            "[3585 | 7005.80] loss=2.48 avg=2.09\n",
            "[3586 | 7007.51] loss=1.78 avg=2.08\n",
            "[3587 | 7009.24] loss=1.68 avg=2.08\n",
            "[3588 | 7010.95] loss=2.31 avg=2.08\n",
            "[3589 | 7012.68] loss=1.97 avg=2.08\n",
            "[3590 | 7014.39] loss=2.02 avg=2.08\n",
            "[3591 | 7016.12] loss=1.86 avg=2.08\n",
            "[3592 | 7017.84] loss=1.61 avg=2.07\n",
            "[3593 | 7019.55] loss=2.15 avg=2.07\n",
            "[3594 | 7021.26] loss=1.80 avg=2.07\n",
            "[3595 | 7023.00] loss=1.33 avg=2.06\n",
            "[3596 | 7024.73] loss=1.99 avg=2.06\n",
            "[3597 | 7026.45] loss=2.79 avg=2.07\n",
            "[3598 | 7028.16] loss=1.78 avg=2.07\n",
            "[3599 | 7029.87] loss=2.98 avg=2.08\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ain\" but instead it's a sign that the GOP is coming together—and that we are strong!\n",
            "The great Governor of Texas @GregAbbott_R in his @CBN News special #Texas\" spoke to the Texas GOP about the many threats our great PEOPLE face: https://t.co/gIsoS8vUoH\n",
            "It's my honor to welcome the Texas delegation to the @CBNNews Q&amp;A:https://t.co/1WpWJjyZWv\n",
            "The @CBNNews Poll was the #1 Presidential Poll for 2 consecutive months. #USA🇺🇸 https://t.co/qJQGkPvYVf\n",
            "The Republican Party must be strong and proud. We are watching very closely for signs of weakness &amp; treachery. Do not be put off by this poll. Keep the pressure!\n",
            "The failing @WashingtonPost pollster @kredo0 has been telling conservatives all over the country that they can't get elected! Fake News!\n",
            "Congratulations to the #Redskins on winning the division and holding on to win! This is a special team!\n",
            "Congratulations to the great @kredo0 on his wonderful endorsement - I told you so! #Redskins\n",
            "“How do you teach leadership without winning? You have to win games.” - Bob Kraft\n",
            "Congratulations @PeteRose_14 for the job he has done in winning the #Dow 25000. Pete ran a great team! Thanks!\n",
            "“The problem with the left is they don’t see past their battles and wars.” - @SenTedCruz\n",
            "“Donald Trump was wrong on ObamaCare. He was wrong on the border. And now he’s wrong on illegal immigration.” #TimeToGetTough\n",
            "Watch the @FoxNews interview I gave yesterday where I explain how ObamaCare is coming. https://t.co/fTUfjQdEbN\n",
            "The @PeteRose_14 interview from yesterday with @oreillyfactor w/ @oreillyfactor managing editor @daveweigel. https://t.co/1TjV5Yr8Jg\n",
            "Great article by @daveweigel and his close friend Roger Simon at @BreitbartNews: “Donald Trump Has Put America Back to Work”  https://t.co/fYwEoL2a9j\n",
            "“Donald Trump is right. The government is not the solution to America’s  problems.” So much @FoxNews discussing the different problems of government while I was speaking.\n",
            "Thanks. https://t.co/u0ZgwZmI5x\n",
            ".@CNN @megynkelly was so totally off today. She is a mess from beginning to end. She is very stupid @megynkelly\n",
            "Via @BreitbartNews: “DONALD TRUMP SHOCKED AT TURMOIL IN GOP ON TAX CUT”  http://t.co/H2jU3Tk2Wq\n",
            "“The American Dream is not a choice it is a necessity.” - Nelson Mandela\n",
            "“Trump's Tax Plan: A 'Jointly Invented Act of God'”  http://t.co/nI0ZTvjKlM via @Mediaite  by Michael Goodwin https://t.co/b1jdV7T4z1\n",
            "Remember the night after I became the 45th President of the United States that special counsel Robert Mueller removed a large section of the law from the books? Free reign!\n",
            "“Donald Trump: ‘We’re Not Involved’ In Comey Investigation”  http://t.co/x8B8jHG6Kw via @BreitbartNews by “David Shuster?” @MoyersWorld\n",
            "I told my strong advocate @JeffJlpa1 that James Comey better step down or will be held accountable. Very unprofessional of him to do that!\n",
            "Via @BreitbartNews by @rwildewrites: “DONALD TRUMP TO CAMPAIGN: WILL I GET CHARGED AS A ‘LEAKER’?” http://t.co/6xJ0h1pUae\n",
            "Thank you for your beautiful endorsement @EricTrump. I could not be prouder of you both! See you soon.\n",
            "Via @Newsmax_Media by @JillianBendis: “Trump to Host Univision ‘Gang of 8'” News Roundtable?” http://t.co/p6WjLzJqXK\n",
            "The real story that \"losers\" like @MittRomney &amp; @PaulRyan’s failed leadership can “\n",
            "\n",
            "[3600 | 7053.75] loss=1.46 avg=2.07\n",
            "[3601 | 7055.47] loss=1.85 avg=2.07\n",
            "[3602 | 7057.19] loss=2.36 avg=2.07\n",
            "[3603 | 7058.93] loss=1.46 avg=2.06\n",
            "[3604 | 7060.64] loss=1.90 avg=2.06\n",
            "[3605 | 7062.34] loss=2.46 avg=2.07\n",
            "[3606 | 7064.08] loss=1.76 avg=2.06\n",
            "[3607 | 7065.78] loss=1.76 avg=2.06\n",
            "[3608 | 7067.51] loss=1.67 avg=2.06\n",
            "[3609 | 7069.25] loss=2.00 avg=2.06\n",
            "[3610 | 7070.98] loss=2.34 avg=2.06\n",
            "[3611 | 7072.71] loss=2.25 avg=2.06\n",
            "[3612 | 7074.44] loss=2.19 avg=2.06\n",
            "[3613 | 7076.17] loss=2.05 avg=2.06\n",
            "[3614 | 7077.88] loss=2.03 avg=2.06\n",
            "[3615 | 7079.62] loss=2.78 avg=2.07\n",
            "[3616 | 7081.35] loss=1.66 avg=2.06\n",
            "[3617 | 7083.08] loss=1.71 avg=2.06\n",
            "[3618 | 7084.82] loss=2.31 avg=2.06\n",
            "[3619 | 7086.53] loss=2.09 avg=2.06\n",
            "[3620 | 7088.26] loss=2.29 avg=2.07\n",
            "[3621 | 7089.98] loss=2.10 avg=2.07\n",
            "[3622 | 7091.71] loss=2.13 avg=2.07\n",
            "[3623 | 7093.42] loss=2.40 avg=2.07\n",
            "[3624 | 7095.16] loss=1.61 avg=2.07\n",
            "[3625 | 7096.87] loss=2.47 avg=2.07\n",
            "[3626 | 7098.59] loss=1.62 avg=2.07\n",
            "[3627 | 7100.30] loss=2.45 avg=2.07\n",
            "[3628 | 7102.04] loss=1.14 avg=2.06\n",
            "[3629 | 7103.75] loss=2.73 avg=2.07\n",
            "[3630 | 7105.46] loss=2.36 avg=2.07\n",
            "[3631 | 7107.17] loss=1.51 avg=2.06\n",
            "[3632 | 7108.90] loss=1.73 avg=2.06\n",
            "[3633 | 7110.63] loss=2.62 avg=2.07\n",
            "[3634 | 7112.34] loss=1.79 avg=2.06\n",
            "[3635 | 7114.05] loss=2.18 avg=2.06\n",
            "[3636 | 7115.78] loss=1.55 avg=2.06\n",
            "[3637 | 7117.51] loss=1.73 avg=2.06\n",
            "[3638 | 7119.24] loss=1.39 avg=2.05\n",
            "[3639 | 7120.98] loss=2.28 avg=2.05\n",
            "[3640 | 7122.71] loss=2.72 avg=2.06\n",
            "[3641 | 7124.44] loss=2.89 avg=2.07\n",
            "[3642 | 7126.15] loss=2.03 avg=2.07\n",
            "[3643 | 7127.88] loss=2.06 avg=2.07\n",
            "[3644 | 7129.59] loss=2.32 avg=2.07\n",
            "[3645 | 7131.33] loss=2.21 avg=2.07\n",
            "[3646 | 7133.06] loss=2.47 avg=2.07\n",
            "[3647 | 7134.79] loss=1.82 avg=2.07\n",
            "[3648 | 7136.50] loss=2.36 avg=2.07\n",
            "[3649 | 7138.24] loss=2.37 avg=2.08\n",
            "[3650 | 7139.95] loss=2.56 avg=2.08\n",
            "[3651 | 7141.66] loss=2.10 avg=2.08\n",
            "[3652 | 7143.40] loss=2.23 avg=2.08\n",
            "[3653 | 7145.12] loss=2.39 avg=2.09\n",
            "[3654 | 7146.84] loss=2.54 avg=2.09\n",
            "[3655 | 7148.57] loss=1.78 avg=2.09\n",
            "[3656 | 7150.30] loss=2.31 avg=2.09\n",
            "[3657 | 7152.03] loss=1.40 avg=2.08\n",
            "[3658 | 7153.77] loss=2.74 avg=2.09\n",
            "[3659 | 7155.50] loss=1.82 avg=2.09\n",
            "[3660 | 7157.23] loss=2.15 avg=2.09\n",
            "[3661 | 7158.96] loss=1.43 avg=2.08\n",
            "[3662 | 7160.68] loss=2.42 avg=2.09\n",
            "[3663 | 7162.39] loss=2.24 avg=2.09\n",
            "[3664 | 7164.11] loss=1.60 avg=2.08\n",
            "[3665 | 7165.82] loss=1.89 avg=2.08\n",
            "[3666 | 7167.54] loss=1.69 avg=2.08\n",
            "[3667 | 7169.25] loss=2.21 avg=2.08\n",
            "[3668 | 7170.96] loss=2.05 avg=2.08\n",
            "[3669 | 7172.67] loss=1.83 avg=2.07\n",
            "[3670 | 7174.40] loss=1.81 avg=2.07\n",
            "[3671 | 7176.11] loss=1.75 avg=2.07\n",
            "[3672 | 7177.84] loss=1.44 avg=2.06\n",
            "[3673 | 7179.55] loss=1.38 avg=2.06\n",
            "[3674 | 7181.27] loss=2.11 avg=2.06\n",
            "[3675 | 7183.00] loss=1.80 avg=2.05\n",
            "[3676 | 7184.73] loss=2.28 avg=2.06\n",
            "[3677 | 7186.44] loss=1.92 avg=2.05\n",
            "[3678 | 7188.17] loss=1.99 avg=2.05\n",
            "[3679 | 7189.88] loss=2.34 avg=2.06\n",
            "[3680 | 7191.61] loss=1.29 avg=2.05\n",
            "[3681 | 7193.32] loss=2.43 avg=2.05\n",
            "[3682 | 7195.05] loss=1.80 avg=2.05\n",
            "[3683 | 7196.76] loss=2.28 avg=2.05\n",
            "[3684 | 7198.47] loss=1.76 avg=2.05\n",
            "[3685 | 7200.19] loss=2.05 avg=2.05\n",
            "[3686 | 7201.89] loss=2.47 avg=2.05\n",
            "[3687 | 7203.60] loss=2.47 avg=2.06\n",
            "[3688 | 7205.33] loss=2.39 avg=2.06\n",
            "[3689 | 7207.04] loss=2.76 avg=2.07\n",
            "[3690 | 7208.76] loss=1.71 avg=2.06\n",
            "[3691 | 7210.47] loss=1.96 avg=2.06\n",
            "[3692 | 7212.18] loss=2.77 avg=2.07\n",
            "[3693 | 7213.89] loss=1.56 avg=2.07\n",
            "[3694 | 7215.61] loss=2.18 avg=2.07\n",
            "[3695 | 7217.32] loss=1.83 avg=2.06\n",
            "[3696 | 7219.03] loss=1.93 avg=2.06\n",
            "[3697 | 7220.75] loss=1.86 avg=2.06\n",
            "[3698 | 7222.46] loss=2.62 avg=2.07\n",
            "[3699 | 7224.18] loss=1.89 avg=2.06\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " and I want to protect our citizens!\n",
            "I will ask my Secretary of Veterans Affairs a major beneficiary of President Trump's VETERANS DAY SHARE her resignation.\n",
            "@FLGovScott is doing a fantastic job. Now it's up to the voters of Florida to send someone in the form of a Governor.\n",
            "Congratulations @FLGovScott on having accomplished the goal of 'America the Beautiful.' You are a tremendous new governor!\n",
            ".@FLGovScott doing a fantastic job! We are very proud of you and we salute you on twitter.\n",
            "@FLGovScott Thank you you really got the job done!\n",
            "We are proud of your victory in Florida - we are with you every step of the way!\n",
            "Congratulations @GovWalker on being crowned as 'the 20th Governor of Michigan' #MAGA!\n",
            ".@GovWalker was wonderful today on behalf of all Floridians. You’re going to WIN and we’ll make this a great State back on track!\n",
            ".@GovWalker is the \"landing page\" for President Trump’s Infrastructure Plan that will be unveiled today.\n",
            "I can’t believe that Nancy Pelosi is pushing the Dem legislation in the New Year.\n",
            "Thank you @FoxNews the best show on earth!\n",
            "It is just a very sad day when Hillary Clinton and the Democrat Party criticize Donald trump and his speech but don’t stand up for her.\n",
            "\"Donald Trump is the first Presidential Republican since Ronald Reagan to garner a double-digit lead\" http://t.co/wO4l8Tny3w via @BreitbartNews\n",
            ".@TrumpNationalNY- once a great building that has been a great draw a mess!\n",
            "Wow just saw the new Trump International Hotel in Toronto - truly spectacular. The views are spectacular!\n",
            "I have no doubt that the Dems will try to stop the Republicans from bringing jobs back if Trump is elected.\n",
            "This would benefit our country in many ways - and they are all trying to stop our guy from becoming our president.\n",
            "\"I love what I do. But I have to do it.\" -- @RealAlexSalmond\n",
            "In my book the people of Scotland are getting away with murder and the US just wants to pay them a big fat fine.\n",
            "\"I'm tired of hearing about China when China is playing us. Call it what you want it is not like America.\"   #TimeToGetTough\n",
            "Obama and his campaign staffers have not been very cooperative with the FBI on the Clinton probe-will they now tell the truth?\n",
            "Wow the Trump Foundation got caught doing business with Russia under FBI control-will they now drop the Clinton case?\n",
            "If Hillary Clinton gets elected and then crooked judges are there for she &amp; everything else as well as many other      scandals then we have a real problem!\n",
            "The Hillary Clinton campaign is in terrible trouble with the FBI for her use of a unsecure server and totally illegal donations to the State Dept.\n",
            "Wow the DOJ says that the Crooked Hillary Clinton campaign did not use the server-so they were not involved in the Russian case-bad idea!\n",
            ".@FBI   \"we don’t know if the campaign cooperated with the Russian campaign. The campaign is not sure they cooperated.\"\n",
            "“Trump’s Use of Personal Email Hurts FBI” http://t.co/nq8T5lk9Gz via @gatewaypundit\n",
            "“Trump’s Use of Personal Email Hurts FBI. FBI Is in the Odd Job Of Keeping It all Together”   via @nytpolitics by @MEGAWIRE\n",
            ".@politico reporting “Donald Trump’s Personal e-Mail Server Was Not A Tool Used By Russia to Influence the 2016 Election” http://t.co/7KHGd7g9uN  via @thinkprogress by @GinaAkiko\n",
            "Wow just read that in the book “the only way President Obama won the White House is if”  Clinton is not indicted. Very sad!\n",
            "New book proves it's not the campaign that colluded with Russia it is the FBI the biggest loser in this whole mess. This is a corrupt system!\n",
            "With all of the bad that is happening with Russia the FBI will not be able to properly investigate the phony Witch Hunt. Too bad!\n",
            "“We won the Cold War because they never stopped looking at us. We won the Cold War because the FBI refused to be infiltrated or outsmarted. We won the Cold War because the American People won the Cold War!\"\n",
            "My @greta interview discussing the Crooked Hillary Clintons and the Russia Investigation http://t.co/2x4J1jwfXF\n",
            "The new book is titled “Timelines” the Russians and us are all still there timeline in the Wind. We've made great gains over the last 35\n",
            "\n",
            "[3700 | 7247.97] loss=2.06 avg=2.06\n",
            "[3701 | 7249.68] loss=1.71 avg=2.06\n",
            "[3702 | 7251.40] loss=1.73 avg=2.06\n",
            "[3703 | 7253.12] loss=1.46 avg=2.05\n",
            "[3704 | 7254.83] loss=1.79 avg=2.05\n",
            "[3705 | 7256.54] loss=1.37 avg=2.04\n",
            "[3706 | 7258.25] loss=1.99 avg=2.04\n",
            "[3707 | 7259.96] loss=1.80 avg=2.04\n",
            "[3708 | 7261.67] loss=1.98 avg=2.04\n",
            "[3709 | 7263.39] loss=1.49 avg=2.03\n",
            "[3710 | 7265.10] loss=2.17 avg=2.03\n",
            "[3711 | 7266.81] loss=1.69 avg=2.03\n",
            "[3712 | 7268.52] loss=2.40 avg=2.04\n",
            "[3713 | 7270.24] loss=1.82 avg=2.03\n",
            "[3714 | 7271.95] loss=2.71 avg=2.04\n",
            "[3715 | 7273.66] loss=1.78 avg=2.04\n",
            "[3716 | 7275.37] loss=1.58 avg=2.03\n",
            "[3717 | 7277.08] loss=1.58 avg=2.03\n",
            "[3718 | 7278.80] loss=1.97 avg=2.03\n",
            "[3719 | 7280.51] loss=1.54 avg=2.02\n",
            "[3720 | 7282.22] loss=2.28 avg=2.03\n",
            "[3721 | 7283.93] loss=2.06 avg=2.03\n",
            "[3722 | 7285.64] loss=1.64 avg=2.02\n",
            "[3723 | 7287.36] loss=2.15 avg=2.02\n",
            "[3724 | 7289.07] loss=2.00 avg=2.02\n",
            "[3725 | 7290.79] loss=1.45 avg=2.02\n",
            "[3726 | 7292.50] loss=1.71 avg=2.01\n",
            "[3727 | 7294.21] loss=2.11 avg=2.01\n",
            "[3728 | 7295.92] loss=2.43 avg=2.02\n",
            "[3729 | 7297.64] loss=2.15 avg=2.02\n",
            "[3730 | 7299.35] loss=1.57 avg=2.02\n",
            "[3731 | 7301.07] loss=1.90 avg=2.01\n",
            "[3732 | 7302.78] loss=2.97 avg=2.02\n",
            "[3733 | 7304.50] loss=1.91 avg=2.02\n",
            "[3734 | 7306.21] loss=1.96 avg=2.02\n",
            "[3735 | 7307.92] loss=2.16 avg=2.02\n",
            "[3736 | 7309.64] loss=2.41 avg=2.03\n",
            "[3737 | 7311.35] loss=2.10 avg=2.03\n",
            "[3738 | 7313.06] loss=1.95 avg=2.03\n",
            "[3739 | 7314.77] loss=2.19 avg=2.03\n",
            "[3740 | 7316.50] loss=2.06 avg=2.03\n",
            "[3741 | 7318.22] loss=2.40 avg=2.03\n",
            "[3742 | 7319.92] loss=1.80 avg=2.03\n",
            "[3743 | 7321.64] loss=2.68 avg=2.04\n",
            "[3744 | 7323.35] loss=1.74 avg=2.03\n",
            "[3745 | 7325.06] loss=2.82 avg=2.04\n",
            "[3746 | 7326.77] loss=2.75 avg=2.05\n",
            "[3747 | 7328.48] loss=2.23 avg=2.05\n",
            "[3748 | 7330.19] loss=3.03 avg=2.06\n",
            "[3749 | 7331.90] loss=2.03 avg=2.06\n",
            "[3750 | 7333.61] loss=2.22 avg=2.06\n",
            "[3751 | 7335.32] loss=1.84 avg=2.06\n",
            "[3752 | 7337.03] loss=2.13 avg=2.06\n",
            "[3753 | 7338.74] loss=1.89 avg=2.06\n",
            "[3754 | 7340.46] loss=2.35 avg=2.06\n",
            "[3755 | 7342.17] loss=1.93 avg=2.06\n",
            "[3756 | 7343.87] loss=2.30 avg=2.06\n",
            "[3757 | 7345.58] loss=1.60 avg=2.06\n",
            "[3758 | 7347.31] loss=2.43 avg=2.06\n",
            "[3759 | 7349.01] loss=1.46 avg=2.06\n",
            "[3760 | 7350.72] loss=1.95 avg=2.05\n",
            "[3761 | 7352.43] loss=2.27 avg=2.06\n",
            "[3762 | 7354.14] loss=1.73 avg=2.05\n",
            "[3763 | 7355.86] loss=2.63 avg=2.06\n",
            "[3764 | 7357.57] loss=1.65 avg=2.06\n",
            "[3765 | 7359.28] loss=1.99 avg=2.05\n",
            "[3766 | 7361.00] loss=2.26 avg=2.06\n",
            "[3767 | 7362.71] loss=1.99 avg=2.06\n",
            "[3768 | 7364.42] loss=1.24 avg=2.05\n",
            "[3769 | 7366.14] loss=1.89 avg=2.05\n",
            "[3770 | 7367.86] loss=2.18 avg=2.05\n",
            "[3771 | 7369.57] loss=1.58 avg=2.04\n",
            "[3772 | 7371.29] loss=2.90 avg=2.05\n",
            "[3773 | 7373.01] loss=2.57 avg=2.06\n",
            "[3774 | 7374.72] loss=2.81 avg=2.06\n",
            "[3775 | 7376.42] loss=2.27 avg=2.07\n",
            "[3776 | 7378.14] loss=2.00 avg=2.07\n",
            "[3777 | 7379.84] loss=2.51 avg=2.07\n",
            "[3778 | 7381.56] loss=1.80 avg=2.07\n",
            "[3779 | 7383.28] loss=1.69 avg=2.06\n",
            "[3780 | 7384.98] loss=1.99 avg=2.06\n",
            "[3781 | 7386.69] loss=2.91 avg=2.07\n",
            "[3782 | 7388.40] loss=1.96 avg=2.07\n",
            "[3783 | 7390.11] loss=2.58 avg=2.08\n",
            "[3784 | 7391.83] loss=1.72 avg=2.07\n",
            "[3785 | 7393.54] loss=1.95 avg=2.07\n",
            "[3786 | 7395.24] loss=3.04 avg=2.08\n",
            "[3787 | 7396.96] loss=1.50 avg=2.07\n",
            "[3788 | 7398.67] loss=1.88 avg=2.07\n",
            "[3789 | 7400.38] loss=1.65 avg=2.07\n",
            "[3790 | 7402.10] loss=1.78 avg=2.07\n",
            "[3791 | 7403.81] loss=2.29 avg=2.07\n",
            "[3792 | 7405.52] loss=2.28 avg=2.07\n",
            "[3793 | 7407.23] loss=2.33 avg=2.07\n",
            "[3794 | 7408.94] loss=1.42 avg=2.07\n",
            "[3795 | 7410.66] loss=2.79 avg=2.07\n",
            "[3796 | 7412.37] loss=2.57 avg=2.08\n",
            "[3797 | 7414.08] loss=3.01 avg=2.09\n",
            "[3798 | 7415.80] loss=2.20 avg=2.09\n",
            "[3799 | 7417.50] loss=1.79 avg=2.09\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "t.co/w6nDqNr1Zs via @WVPolitics\n",
            ".@MarkBurnettTV: \"I've been to Trump Tower a million times. This is the best?\" http://t.co/jvBwRm7y1T\n",
            "#Trump2016ThankYou Tour:http://t.co/BH4UwhYd4Q\n",
            "My interview on @oreillyfactor -  \"My greatest strength I have always had\" http://t.co/W4X7w7lmDj\n",
            "Via @BreitbartNews by @mboyle1: \"Trump's $500M Help for Detroit Will Be Dedicated to Homeless Veterans\"http://t.co/uCgMt4Lq1\n",
            "Will be speaking at @fundanything this Friday! #Trump2016\n",
            "Heading to @fundanything this Friday- see you there! #Trump2016\n",
            "Wow I was just named \"Best of the Dr. Doral Golf Club in Michigan\" by @Cash4Trump http://t.co/1y6lGXQEa3\n",
            "ObamaCare is a disaster that has led to a massive increase in drug abuse leading to needless deaths. We MUST REPLACE IT!\n",
            "ObamaCare is a disaster. I was the first candidate to propose and get healthcare on day one. All others kept getting mocked\n",
            "ObamaCare is a disaster. The Republicans were mocked last week for proposing that everybody in America buy their healthcare. Crazy!\n",
            "Wow the President just named me the \"Best of the Dr. Doral Golf Club in the U.S.\" So honored. Will be the best ever!\n",
            "@BillDe Blasio -  Great interview. Thanks for the nice words.\n",
            "I was the first person to call off the Dem Convention right before the end.\n",
            "Dem Convention was an absolute disaster. Obstructionist clowns and losers.\n",
            "I wish the Dems would call me instead of the convention because our country needs someone who understands our situation.\n",
            "I called off the Dem Convention until the last minute so they could play the long game. Good move!\n",
            "I will be on Fox &amp; Friends morning at 7.40. The Dems aren't serious unless they are serious about stopping Obama\n",
            "Dummy CNN lied to voters just like @CNN said I would cancel the Dem Convention. In fact I would be there and the word is spreading fast!\n",
            "The Dem Convention was awful—all clowns running around. Fake reporters saying what I said wasn't said!\n",
            "The Republicans will win that they should have won the last election. We've been waiting for many years.\n",
            "We must fix our education system in order to MAKE AMERICA GREAT AGAIN–and fast. Teach STEM and K-12\n",
            "We must fix our criminal justice system—stop the Obama Justice Department \"Jobs\" policies and stop the flood of immigrants.\n",
            "The Dem Convention was terrible and the word is getting that the Republicans will do a better job. There were problems!\n",
            "The Republicans must be careful about their convention or the country will be treated even worse than under Obama!  @GOP\n",
            "So Obama is holding a signing ceremony today for a bad food fast food restaurant in Arizona. He comes out and he's all mush.\n",
            "This is really sad. The Obama campaign sent out an ad starring a bunch of actors talking about how great they are that are actually trash\n",
            "Obama and his people are so concerned with the GOP Convention going out of business that they are giving zero attention to Obama's dismal performance\n",
            "This is so sad. The Obama campaign sent out an ad starring a bunch of actors talking about how good they are that are actually trash\n",
            "Great job tonight by Gary B!\n",
            "Thank you! #Trump2016 https://t.co/pMqyIpqE2n\n",
            "The only thing more dangerous to our country than Obama is Obama-The Man!\n",
            "I am at this dinner to thank everyone involved with the @GOPConvention. We have all worked so hard for so many years &amp; we owe so much to our community\n",
            "This evening is for the great men and women at the RNC...to thank them for their service &amp; to wish them all well...be safe!\n",
            "Wow! @GOPConvention was the biggest and best ever. Tremendous turnout with over 2000 amazing people in Cleveland- all going now!\n",
            "The GOP is stronger together!  I want to thank everyone and thank you to the great people of Texas as well. We are with you tonight!\"\n",
            "So sad that Obama is signing Executive Order for temporary hiring of 10000 illegal immigrants.This is the first big round of immigration he will do.\n",
            "Obama is destroying our nation. We cannot have 2-tiered immigration policies. He must put America FIRST.\n",
            "Great article by @PiersMorgan about how the press treated Mitt Romney. It helped him get the nomination!  @NickSkaggs\n",
            "Great honor by @CabinetCabinet @AG\n",
            "\n",
            "[3800 | 7441.38] loss=2.00 avg=2.08\n",
            "[3801 | 7443.11] loss=1.62 avg=2.08\n",
            "[3802 | 7444.82] loss=2.04 avg=2.08\n",
            "[3803 | 7446.56] loss=1.93 avg=2.08\n",
            "[3804 | 7448.29] loss=2.26 avg=2.08\n",
            "[3805 | 7449.99] loss=2.13 avg=2.08\n",
            "[3806 | 7451.71] loss=2.18 avg=2.08\n",
            "[3807 | 7453.42] loss=2.07 avg=2.08\n",
            "[3808 | 7455.15] loss=1.16 avg=2.07\n",
            "[3809 | 7456.86] loss=1.70 avg=2.07\n",
            "[3810 | 7458.57] loss=1.69 avg=2.06\n",
            "[3811 | 7460.28] loss=1.63 avg=2.06\n",
            "[3812 | 7461.99] loss=1.56 avg=2.06\n",
            "[3813 | 7463.71] loss=1.95 avg=2.05\n",
            "[3814 | 7465.41] loss=1.67 avg=2.05\n",
            "[3815 | 7467.13] loss=1.58 avg=2.05\n",
            "[3816 | 7468.86] loss=1.94 avg=2.04\n",
            "[3817 | 7470.59] loss=1.54 avg=2.04\n",
            "[3818 | 7472.30] loss=1.86 avg=2.04\n",
            "[3819 | 7474.03] loss=1.76 avg=2.03\n",
            "[3820 | 7475.75] loss=2.11 avg=2.04\n",
            "[3821 | 7477.47] loss=1.30 avg=2.03\n",
            "[3822 | 7479.18] loss=2.07 avg=2.03\n",
            "[3823 | 7480.89] loss=2.04 avg=2.03\n",
            "[3824 | 7482.60] loss=1.83 avg=2.03\n",
            "[3825 | 7484.31] loss=2.14 avg=2.03\n",
            "[3826 | 7486.03] loss=2.21 avg=2.03\n",
            "[3827 | 7487.74] loss=2.01 avg=2.03\n",
            "[3828 | 7489.46] loss=1.60 avg=2.03\n",
            "[3829 | 7491.19] loss=1.95 avg=2.02\n",
            "[3830 | 7492.90] loss=3.14 avg=2.04\n",
            "[3831 | 7494.64] loss=2.00 avg=2.04\n",
            "[3832 | 7496.37] loss=2.66 avg=2.04\n",
            "[3833 | 7498.10] loss=2.23 avg=2.04\n",
            "[3834 | 7499.82] loss=1.59 avg=2.04\n",
            "[3835 | 7501.53] loss=2.05 avg=2.04\n",
            "[3836 | 7503.25] loss=2.44 avg=2.04\n",
            "[3837 | 7504.98] loss=1.77 avg=2.04\n",
            "[3838 | 7506.71] loss=1.87 avg=2.04\n",
            "[3839 | 7508.43] loss=1.99 avg=2.04\n",
            "[3840 | 7510.14] loss=1.97 avg=2.04\n",
            "[3841 | 7511.85] loss=2.43 avg=2.04\n",
            "[3842 | 7513.56] loss=1.59 avg=2.04\n",
            "[3843 | 7515.27] loss=2.82 avg=2.04\n",
            "[3844 | 7516.98] loss=2.28 avg=2.05\n",
            "[3845 | 7518.72] loss=1.32 avg=2.04\n",
            "[3846 | 7520.44] loss=2.19 avg=2.04\n",
            "[3847 | 7522.16] loss=2.38 avg=2.04\n",
            "[3848 | 7523.87] loss=2.07 avg=2.05\n",
            "[3849 | 7525.60] loss=1.56 avg=2.04\n",
            "[3850 | 7527.31] loss=2.19 avg=2.04\n",
            "[3851 | 7529.04] loss=2.65 avg=2.05\n",
            "[3852 | 7530.75] loss=2.48 avg=2.05\n",
            "[3853 | 7532.46] loss=2.07 avg=2.05\n",
            "[3854 | 7534.17] loss=2.18 avg=2.05\n",
            "[3855 | 7535.89] loss=2.19 avg=2.06\n",
            "[3856 | 7537.59] loss=2.59 avg=2.06\n",
            "[3857 | 7539.30] loss=1.81 avg=2.06\n",
            "[3858 | 7541.04] loss=2.26 avg=2.06\n",
            "[3859 | 7542.76] loss=1.39 avg=2.05\n",
            "[3860 | 7544.46] loss=2.45 avg=2.06\n",
            "[3861 | 7546.19] loss=2.07 avg=2.06\n",
            "[3862 | 7547.90] loss=1.59 avg=2.05\n",
            "[3863 | 7549.62] loss=1.52 avg=2.05\n",
            "[3864 | 7551.33] loss=2.11 avg=2.05\n",
            "[3865 | 7553.04] loss=1.72 avg=2.04\n",
            "[3866 | 7554.75] loss=1.66 avg=2.04\n",
            "[3867 | 7556.46] loss=2.15 avg=2.04\n",
            "[3868 | 7558.17] loss=1.74 avg=2.04\n",
            "[3869 | 7559.89] loss=1.85 avg=2.04\n",
            "[3870 | 7561.59] loss=2.53 avg=2.04\n",
            "[3871 | 7563.34] loss=2.50 avg=2.05\n",
            "[3872 | 7565.06] loss=1.97 avg=2.05\n",
            "[3873 | 7566.78] loss=2.07 avg=2.05\n",
            "[3874 | 7568.50] loss=2.12 avg=2.05\n",
            "[3875 | 7570.21] loss=2.25 avg=2.05\n",
            "[3876 | 7571.92] loss=2.56 avg=2.05\n",
            "[3877 | 7573.64] loss=1.66 avg=2.05\n",
            "[3878 | 7575.36] loss=1.71 avg=2.05\n",
            "[3879 | 7577.07] loss=1.64 avg=2.04\n",
            "[3880 | 7578.81] loss=2.56 avg=2.05\n",
            "[3881 | 7580.52] loss=1.99 avg=2.05\n",
            "[3882 | 7582.23] loss=1.69 avg=2.04\n",
            "[3883 | 7583.94] loss=2.07 avg=2.04\n",
            "[3884 | 7585.65] loss=1.87 avg=2.04\n",
            "[3885 | 7587.37] loss=2.41 avg=2.05\n",
            "[3886 | 7589.09] loss=2.23 avg=2.05\n",
            "[3887 | 7590.81] loss=1.74 avg=2.04\n",
            "[3888 | 7592.54] loss=1.87 avg=2.04\n",
            "[3889 | 7594.25] loss=1.93 avg=2.04\n",
            "[3890 | 7595.97] loss=1.99 avg=2.04\n",
            "[3891 | 7597.69] loss=1.78 avg=2.04\n",
            "[3892 | 7599.42] loss=2.40 avg=2.04\n",
            "[3893 | 7601.13] loss=1.57 avg=2.04\n",
            "[3894 | 7602.84] loss=2.41 avg=2.04\n",
            "[3895 | 7604.55] loss=1.94 avg=2.04\n",
            "[3896 | 7606.28] loss=1.78 avg=2.04\n",
            "[3897 | 7608.01] loss=1.35 avg=2.03\n",
            "[3898 | 7609.72] loss=2.56 avg=2.04\n",
            "[3899 | 7611.45] loss=2.68 avg=2.04\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Time!\n",
            "We will soon be in the BIG BANG THEORY! It's great to be back in D.C. and the GREAT state of Pennsylvania. Now we fight Big Ben Ben!\n",
            ".@dianebuckner  Diane you will be in the BIG BANG THEORY tomorrow night and we will all have a great time. Also we build a great future together.\n",
            ".@dianebuckner  Diane  remember this is your show and you made a big deal of the fact that you would fire the people responsible for the very unfair trade deficit. I hope after tonight everyone will be  right!\n",
            "It is my great honor to be coming before your great Republican voters this Sunday. Our party is doing just fine!\n",
            "A big night with a big vote!https://t.co/8zAo7R7V7J\n",
            "Thank you Pennsylvania! We will together MAKE AMERICA GREAT AGAIN! https://t.co/kHr4EiA3Rb\n",
            "THANK YOU! #MAGA#DrainTheSwamp https://t.co/Yp8w7G1QWq\n",
            "The only way to stop illegal immigration is to stop granting amnesty to illegals! https://t.co/3RkW9p9iNy\n",
            "Today I was thrilled to welcome the wonderful Governor of Hawaii Dr. David Ige to the White House! In Hawaii I was also pleased to join with my fellow Governors!\n",
            "Join me live!#DrainTheSwamp #MAGAhttps://t.co/zLJY9lk3V3\n",
            "Thank you for being #NatlCheyenne!#MakeAmericaGreatAgain https://t.co/XZbRU4f9zg https://t.co/1Bn0I5K2Jy\n",
            "The U.S. is losing hundreds of billions of dollars every single year. This is outrageous!\n",
            "Join me tonight in Charlotte North Carolina at 7pmE! #MAGA #ImWithYou #AmericaFirst https://t.co/m1zVfMdO3h\n",
            "Ladies and Gentlemen- join our team in North Charleston South Carolina! #Trump2016 https://t.co/oRQf2pYVU1\n",
            "I would like to wish my friend @TheRealMarilu and all of the incredible women on her roster the very best for the future of women's golf at Trump National Golf Club Westchester!\n",
            "Thank you Charlotte North Carolina!#MakeAmericaGreatAgain #Trump2016https://t.co/yjI1U5y1Qy https://t.co/Wz0CbJQk3p\n",
            ".@FoxNews is so very unfair to me &amp; my wonderful wife @MELANIATRUMP. She is an amazing person and always gets the job done. In fact I loved when she endorsed me!\n",
            "Why aren't the anchors and reporters covering me correctly like yesterday in NH. I love New Hampshire! https://t.co/7r5VrEZqW1\n",
            "Thank you to @CNN &amp; @CNNPolitics for not allowing \"the real story as told by Trump supporter George Galloway.\" The race for Congress will definitely be close!\n",
            "Good morning and happy anniversary to @TheRealMarilu who is an amazing woman! #Hag18 #MakeAmericaGreatAgain https://t.co/yFk1cDVzO9\n",
            "Thank you Ann Arbor Michigan! #ImWithYou https://t.co/8qXWOZLwQx\n",
            "MAKE AMERICA GREAT AGAIN! https://t.co/L7rvSgD9Uo\n",
            "We are going to have a good day and then we are not going to have a day. We are going to DRAIN THE SWAMP! https://t.co/yBfW0W3xUg https://t.co/1p4W9RgJb9\n",
            "The Democrat Party rigged the vote &amp; hid the absentee ballots. No wonder! https://t.co/Z4uJ0mQxm7\n",
            ".@IvankaTrump has been a tireless advocate and supporter for The Apprentice as the show prepares to make a comeback this spring. The Apprentice will be back — w/different cast!\n",
            "Wow I just had an awesome rally in Connecticut. Wow! Thanks to my great supporters and for the warm welcome we had!\n",
            "Thank you for all of your support and congrats to @RepDana Rohrabacher. Great job a fantastic guy!\n",
            "MAKE AMERICA GREAT AGAIN!!! https://t.co/njd2YqEQVc\n",
            "Big news out- @foxandfriends will lose reporter who broke the story that our very important ambassador to Libya was ambushed in Libya. It is a\n",
            "\n",
            "[3900 | 7635.24] loss=2.46 avg=2.05\n",
            "[3901 | 7636.95] loss=1.97 avg=2.05\n",
            "[3902 | 7638.66] loss=2.14 avg=2.05\n",
            "[3903 | 7640.40] loss=2.01 avg=2.05\n",
            "[3904 | 7642.11] loss=1.77 avg=2.04\n",
            "[3905 | 7643.83] loss=1.93 avg=2.04\n",
            "[3906 | 7645.54] loss=2.52 avg=2.05\n",
            "[3907 | 7647.27] loss=2.37 avg=2.05\n",
            "[3908 | 7648.98] loss=1.86 avg=2.05\n",
            "[3909 | 7650.71] loss=1.50 avg=2.04\n",
            "[3910 | 7652.43] loss=2.89 avg=2.05\n",
            "[3911 | 7654.14] loss=1.59 avg=2.05\n",
            "[3912 | 7655.85] loss=2.09 avg=2.05\n",
            "[3913 | 7657.58] loss=1.96 avg=2.05\n",
            "[3914 | 7659.31] loss=1.94 avg=2.05\n",
            "[3915 | 7661.04] loss=1.51 avg=2.04\n",
            "[3916 | 7662.75] loss=1.38 avg=2.03\n",
            "[3917 | 7664.48] loss=1.70 avg=2.03\n",
            "[3918 | 7666.19] loss=1.94 avg=2.03\n",
            "[3919 | 7667.91] loss=1.36 avg=2.02\n",
            "[3920 | 7669.62] loss=1.96 avg=2.02\n",
            "[3921 | 7671.33] loss=1.55 avg=2.02\n",
            "[3922 | 7673.04] loss=1.89 avg=2.02\n",
            "[3923 | 7674.77] loss=1.25 avg=2.01\n",
            "[3924 | 7676.47] loss=1.69 avg=2.01\n",
            "[3925 | 7678.19] loss=1.80 avg=2.00\n",
            "[3926 | 7679.92] loss=2.13 avg=2.00\n",
            "[3927 | 7681.64] loss=2.07 avg=2.00\n",
            "[3928 | 7683.36] loss=2.11 avg=2.01\n",
            "[3929 | 7685.09] loss=2.04 avg=2.01\n",
            "[3930 | 7686.81] loss=2.24 avg=2.01\n",
            "[3931 | 7688.53] loss=1.44 avg=2.00\n",
            "[3932 | 7690.25] loss=1.81 avg=2.00\n",
            "[3933 | 7691.96] loss=2.45 avg=2.01\n",
            "[3934 | 7693.67] loss=1.50 avg=2.00\n",
            "[3935 | 7695.38] loss=3.01 avg=2.01\n",
            "[3936 | 7697.09] loss=1.21 avg=2.00\n",
            "[3937 | 7698.80] loss=2.17 avg=2.00\n",
            "[3938 | 7700.53] loss=2.59 avg=2.01\n",
            "[3939 | 7702.25] loss=2.20 avg=2.01\n",
            "[3940 | 7703.96] loss=2.32 avg=2.02\n",
            "[3941 | 7705.68] loss=1.90 avg=2.01\n",
            "[3942 | 7707.39] loss=2.11 avg=2.01\n",
            "[3943 | 7709.10] loss=1.99 avg=2.01\n",
            "[3944 | 7710.82] loss=1.52 avg=2.01\n",
            "[3945 | 7712.53] loss=2.58 avg=2.02\n",
            "[3946 | 7714.24] loss=1.44 avg=2.01\n",
            "[3947 | 7715.97] loss=1.65 avg=2.01\n",
            "[3948 | 7717.71] loss=1.26 avg=2.00\n",
            "[3949 | 7719.42] loss=1.19 avg=1.99\n",
            "[3950 | 7721.13] loss=2.19 avg=1.99\n",
            "[3951 | 7722.84] loss=1.86 avg=1.99\n",
            "[3952 | 7724.55] loss=1.95 avg=1.99\n",
            "[3953 | 7726.28] loss=2.29 avg=1.99\n",
            "[3954 | 7728.00] loss=2.14 avg=2.00\n",
            "[3955 | 7729.73] loss=2.26 avg=2.00\n",
            "[3956 | 7731.44] loss=1.59 avg=1.99\n",
            "[3957 | 7733.17] loss=1.98 avg=1.99\n",
            "[3958 | 7734.90] loss=2.85 avg=2.00\n",
            "[3959 | 7736.63] loss=1.59 avg=2.00\n",
            "[3960 | 7738.36] loss=2.16 avg=2.00\n",
            "[3961 | 7740.07] loss=2.12 avg=2.00\n",
            "[3962 | 7741.80] loss=1.67 avg=2.00\n",
            "[3963 | 7743.54] loss=2.46 avg=2.00\n",
            "[3964 | 7745.27] loss=2.43 avg=2.01\n",
            "[3965 | 7747.00] loss=1.59 avg=2.00\n",
            "[3966 | 7748.73] loss=1.56 avg=2.00\n",
            "[3967 | 7750.46] loss=2.59 avg=2.00\n",
            "[3968 | 7752.17] loss=2.18 avg=2.01\n",
            "[3969 | 7753.88] loss=2.00 avg=2.01\n",
            "[3970 | 7755.61] loss=2.58 avg=2.01\n",
            "[3971 | 7757.35] loss=2.23 avg=2.01\n",
            "[3972 | 7759.07] loss=1.80 avg=2.01\n",
            "[3973 | 7760.81] loss=2.24 avg=2.01\n",
            "[3974 | 7762.54] loss=1.47 avg=2.01\n",
            "[3975 | 7764.28] loss=1.89 avg=2.01\n",
            "[3976 | 7765.99] loss=2.36 avg=2.01\n",
            "[3977 | 7767.71] loss=2.15 avg=2.01\n",
            "[3978 | 7769.42] loss=1.27 avg=2.00\n",
            "[3979 | 7771.13] loss=2.38 avg=2.01\n",
            "[3980 | 7772.85] loss=2.59 avg=2.01\n",
            "[3981 | 7774.57] loss=2.24 avg=2.02\n",
            "[3982 | 7776.28] loss=1.86 avg=2.01\n",
            "[3983 | 7778.00] loss=2.92 avg=2.02\n",
            "[3984 | 7779.72] loss=1.26 avg=2.02\n",
            "[3985 | 7781.44] loss=1.98 avg=2.02\n",
            "[3986 | 7783.17] loss=1.69 avg=2.01\n",
            "[3987 | 7784.88] loss=1.74 avg=2.01\n",
            "[3988 | 7786.61] loss=1.93 avg=2.01\n",
            "[3989 | 7788.32] loss=1.55 avg=2.00\n",
            "[3990 | 7790.05] loss=2.13 avg=2.01\n",
            "[3991 | 7791.76] loss=1.89 avg=2.00\n",
            "[3992 | 7793.49] loss=1.60 avg=2.00\n",
            "[3993 | 7795.20] loss=2.23 avg=2.00\n",
            "[3994 | 7796.91] loss=1.99 avg=2.00\n",
            "[3995 | 7798.64] loss=1.88 avg=2.00\n",
            "[3996 | 7800.35] loss=1.22 avg=1.99\n",
            "[3997 | 7802.07] loss=2.36 avg=2.00\n",
            "[3998 | 7803.80] loss=2.64 avg=2.00\n",
            "[3999 | 7805.51] loss=1.93 avg=2.00\n",
            "Saving /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-4000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "/VtKsM1\n",
            "I will be interviewed on @FoxNews by @SeanHannity at 10:00 A.M.\n",
            "The #1 reason children of divorce are more likely to re-marry is that they cannot trust the legal system itself.\n",
            "Wacko @chucktodd said I wanted to leave Illinois for Washington D.C. Wrong! Washington is far better! https://t.co/yfCzUZvzLn\n",
            "I think @megynkelly may have overreacted to the fact that @realDonaldTrump did not ask for her vote. She thinks it's a nice thing to say - not Trump!\n",
            "#MakeAmericaGreatAgain!https://t.co/1YQYXUZ3Rq\n",
            ".@megynkelly   Thanks Megyn. You are very fair but I don't think you like the way I respond to his \"stamina killer\" in our debate!\n",
            "\"Donald Trump: CNN Should Have Knew the Debate Would Turn It Into a 'Shooting Game'\" https://t.co/uOdPuXKkRV\n",
            "My @FoxNews interview with @seanhannity discussing the #DrainTheSwamp movement and border security in D. C. #MakeAmericaGreatAgain! https://t.co/hM9k9VZBXu\n",
            "Big poll just out from @CNN - THANK YOU!#DrainTheSwamp https://t.co/WdCmYt6Ytz\n",
            "Great poll- THANK YOU to all the D.C. voters who came out &amp; voted!#DrainTheSwamp https://t.co/vK7oJpQbQ8\n",
            "New CNN Poll just out- THANK YOU! #DrainTheSwamphttps://t.co/HpwY9hW8bv\n",
            "New @CNN Poll just out. THANK YOU! #DrainTheSwamp https://t.co/n0f8h4C3Yd\n",
            "Thank you for your endorsement @PeteStauber!! #STFU https://t.co/8xLcF9tG6D\n",
            "I will be interviewed on @foxandfriends at 8:00 A.M. Enjoy!#STFU\n",
            "Thank you @SenTedCruz! #Trump2016 https://t.co/Y3t8uEzi5n\n",
            "New @CNN Poll has #Trump leading in Iowa by 8 points. https://t.co/Fn3c2TnYkZ\n",
            "What is going on in D.C. that will never be fixed!\n",
            ".@seanhannity  Thank you so much for the nice words. This is what we need - WIN!\n",
            ".@marklevinshow is a total clown and it shows in the way he analyzes issues - a real loser!\n",
            "Looking forward to being interviewed tonight on @TODAYshow with @andyru - 7:00 P.M.\n",
            "We must get our country back under our control fast. The very weak @FEMA has caused chaos and death. We must restore law &amp; order!\n",
            ".@seanhannity on my show now. Enjoy!\n",
            "Great night of results for Trump in Nevada. Thank you. Together we will MAKE AMERICA SAFE &amp; GREAT AGAIN!\n",
            "Great evening last night in Nevada- I won the straw poll big time. Thank you Nevada! #Trump2016 https://t.co/5jG5FcZdGx\n",
            "#MakeAmericaGreatAgain https://t.co/qHxrjXeE0q\n",
            "Thank you Nevada! #AmericaFirst🇺🇸 https://t.co/e9l5C5LqR8\n",
            "Thank you for the support! #AmericaFirst #Trump2016 https://t.co/BkTKpV5fQA\n",
            "Thank you Las Vegas. We are #AmericaFirst🇺🇸 https://t.co/YdNzHwQ9U2\n",
            "I am with our GREAT VETS! Get out &amp; vote by 12PM ET tonight. The polls are open! #VoteTrumpNV https://t.co/V0cQ3rYiQZ\n",
            "Big polls are open in Nevada. The vote is close! We will MAKE AMERICA SAFE &amp; GREAT AGAIN!#Trump2016 https://t.co/YdNzHwQ9U2\n",
            "I just returned from New Hampshire. I was honored to meet with Governor Hassan who campaigned for me. He is my friend!\n",
            "Thank you! #AmericaFirst #Trump2016 https://t.co/JXu5Z8WzgY\n",
            "Thank you Minnesota! #AmericaFirst #Trump2016 https://t\n",
            "\n",
            "[4000 | 7842.69] loss=2.82 avg=2.01\n",
            "[4001 | 7844.37] loss=0.86 avg=2.00\n",
            "[4002 | 7846.05] loss=1.52 avg=1.99\n",
            "[4003 | 7847.73] loss=2.32 avg=2.00\n",
            "[4004 | 7849.40] loss=2.29 avg=2.00\n",
            "[4005 | 7851.09] loss=2.07 avg=2.00\n",
            "[4006 | 7852.78] loss=2.07 avg=2.00\n",
            "[4007 | 7854.47] loss=2.37 avg=2.01\n",
            "[4008 | 7856.17] loss=2.06 avg=2.01\n",
            "[4009 | 7857.86] loss=2.35 avg=2.01\n",
            "[4010 | 7859.54] loss=1.87 avg=2.01\n",
            "[4011 | 7861.22] loss=1.87 avg=2.01\n",
            "[4012 | 7862.92] loss=1.28 avg=2.00\n",
            "[4013 | 7864.61] loss=1.82 avg=2.00\n",
            "[4014 | 7866.30] loss=1.89 avg=2.00\n",
            "[4015 | 7867.99] loss=1.30 avg=1.99\n",
            "[4016 | 7869.69] loss=2.31 avg=1.99\n",
            "[4017 | 7871.38] loss=2.02 avg=1.99\n",
            "[4018 | 7873.07] loss=2.27 avg=2.00\n",
            "[4019 | 7874.77] loss=1.80 avg=1.99\n",
            "[4020 | 7876.46] loss=1.70 avg=1.99\n",
            "[4021 | 7878.18] loss=1.31 avg=1.98\n",
            "[4022 | 7879.88] loss=2.47 avg=1.99\n",
            "[4023 | 7881.58] loss=1.53 avg=1.98\n",
            "[4024 | 7883.29] loss=2.20 avg=1.99\n",
            "[4025 | 7884.99] loss=1.85 avg=1.99\n",
            "[4026 | 7886.69] loss=1.60 avg=1.98\n",
            "[4027 | 7888.38] loss=1.50 avg=1.98\n",
            "[4028 | 7890.08] loss=1.86 avg=1.98\n",
            "[4029 | 7891.78] loss=1.32 avg=1.97\n",
            "[4030 | 7893.49] loss=3.14 avg=1.98\n",
            "[4031 | 7895.18] loss=1.83 avg=1.98\n",
            "[4032 | 7896.89] loss=1.69 avg=1.98\n",
            "[4033 | 7898.59] loss=1.24 avg=1.97\n",
            "[4034 | 7900.31] loss=2.54 avg=1.97\n",
            "[4035 | 7902.01] loss=2.10 avg=1.98\n",
            "[4036 | 7903.71] loss=1.95 avg=1.98\n",
            "[4037 | 7905.41] loss=2.22 avg=1.98\n",
            "[4038 | 7907.11] loss=1.89 avg=1.98\n",
            "[4039 | 7908.82] loss=1.51 avg=1.97\n",
            "[4040 | 7910.54] loss=2.14 avg=1.97\n",
            "[4041 | 7912.25] loss=2.41 avg=1.98\n",
            "[4042 | 7913.95] loss=2.13 avg=1.98\n",
            "[4043 | 7915.66] loss=2.46 avg=1.99\n",
            "[4044 | 7917.37] loss=1.72 avg=1.98\n",
            "[4045 | 7919.08] loss=1.92 avg=1.98\n",
            "[4046 | 7920.79] loss=2.50 avg=1.99\n",
            "[4047 | 7922.50] loss=2.12 avg=1.99\n",
            "[4048 | 7924.21] loss=1.71 avg=1.99\n",
            "[4049 | 7925.92] loss=1.53 avg=1.98\n",
            "[4050 | 7927.63] loss=2.26 avg=1.98\n",
            "[4051 | 7929.34] loss=1.50 avg=1.98\n",
            "[4052 | 7931.05] loss=2.15 avg=1.98\n",
            "[4053 | 7932.77] loss=2.74 avg=1.99\n",
            "[4054 | 7934.48] loss=2.26 avg=1.99\n",
            "[4055 | 7936.19] loss=1.71 avg=1.99\n",
            "[4056 | 7937.91] loss=2.24 avg=1.99\n",
            "[4057 | 7939.61] loss=1.50 avg=1.99\n",
            "[4058 | 7941.33] loss=2.01 avg=1.99\n",
            "[4059 | 7943.04] loss=2.16 avg=1.99\n",
            "[4060 | 7944.75] loss=2.18 avg=1.99\n",
            "[4061 | 7946.46] loss=1.70 avg=1.99\n",
            "[4062 | 7948.17] loss=1.70 avg=1.98\n",
            "[4063 | 7949.88] loss=1.96 avg=1.98\n",
            "[4064 | 7951.60] loss=1.86 avg=1.98\n",
            "[4065 | 7953.30] loss=1.83 avg=1.98\n",
            "[4066 | 7955.02] loss=1.12 avg=1.97\n",
            "[4067 | 7956.73] loss=2.09 avg=1.97\n",
            "[4068 | 7958.44] loss=2.65 avg=1.98\n",
            "[4069 | 7960.16] loss=3.11 avg=1.99\n",
            "[4070 | 7961.87] loss=2.46 avg=2.00\n",
            "[4071 | 7963.57] loss=1.81 avg=1.99\n",
            "[4072 | 7965.29] loss=1.39 avg=1.99\n",
            "[4073 | 7967.01] loss=2.57 avg=1.99\n",
            "[4074 | 7968.71] loss=2.13 avg=2.00\n",
            "[4075 | 7970.43] loss=3.23 avg=2.01\n",
            "[4076 | 7972.14] loss=1.02 avg=2.00\n",
            "[4077 | 7973.86] loss=2.07 avg=2.00\n",
            "[4078 | 7975.58] loss=2.23 avg=2.00\n",
            "[4079 | 7977.29] loss=2.34 avg=2.00\n",
            "[4080 | 7979.00] loss=2.19 avg=2.01\n",
            "[4081 | 7980.71] loss=1.39 avg=2.00\n",
            "[4082 | 7982.43] loss=1.63 avg=2.00\n",
            "[4083 | 7984.15] loss=1.74 avg=1.99\n",
            "[4084 | 7985.86] loss=1.97 avg=1.99\n",
            "[4085 | 7987.57] loss=2.36 avg=2.00\n",
            "[4086 | 7989.29] loss=1.71 avg=1.99\n",
            "[4087 | 7991.00] loss=2.46 avg=2.00\n",
            "[4088 | 7992.71] loss=2.04 avg=2.00\n",
            "[4089 | 7994.43] loss=1.65 avg=2.00\n",
            "[4090 | 7996.14] loss=2.72 avg=2.00\n",
            "[4091 | 7997.85] loss=2.15 avg=2.00\n",
            "[4092 | 7999.56] loss=2.11 avg=2.01\n",
            "[4093 | 8001.27] loss=1.54 avg=2.00\n",
            "[4094 | 8003.01] loss=2.16 avg=2.00\n",
            "[4095 | 8004.72] loss=1.42 avg=2.00\n",
            "[4096 | 8006.44] loss=2.40 avg=2.00\n",
            "[4097 | 8008.16] loss=1.26 avg=1.99\n",
            "[4098 | 8009.87] loss=2.50 avg=2.00\n",
            "[4099 | 8011.58] loss=2.38 avg=2.00\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " episode for @foxnation tonight at 9PM EST.  Enjoy!\n",
            "My interview with Joe Walsh @alshopradio where I explained why @realDonaldTrump is on the ballot in Michigan. #Trump2016 #MakeAmericaGreatAgain\n",
            ".@MarkHalperin is great. Always gives me a fair shake in his show. @Newsmax_Media is much better than @CNN!\n",
            ".@MarkHalperin is a failed journalist who’s lost ALL credibility in media. He is running for Mayor of L.I.\n",
            ".@markfinkel  I’m not even close to rich. You can get a lot of money to anybody-even me!\n",
            ".@andersoncooper  Anderson- your worst ad of all time. I will be filing a lawsuit. Do you still have the 20th Century Fox?\n",
            ".@andersoncooper Anderson- You are terrific--so lame and boring!\n",
            ".@andersoncooper  Not worth the money- just plain wrong! Also- you did terrible Anderson Cooper.\n",
            ".@andersoncooper  You are so bad a lot!\n",
            ".@andersoncooper  Anderson- what is with all of your big name friends and appearances? Nobody is interested!\n",
            ".@andersoncooper @andersoncooper  Not nice- no!\n",
            ".@andersoncooper  Anderson- You look exactly like my father Fred C. Trump.\n",
            ".@andersoncooper  Anderson I am just going to assume you like my decision on Anderson Cooper.\n",
            "Asking for more than the $4 million is totally normal and considered a negotiation miss.\n",
            "@andersoncooper  They had no right doing it!\n",
            "Wow- @andersoncooper  So many people have asked for me to be their spokesman! I said \"hello\".\n",
            "\"Trump: Clinton Campaign Got Away Fast in Debating Sanders\" http://t.co/s0hR1H1n by @nbc and @WNEW\n",
            "Congratulations to the team @NCGOP for winning the \"Trump T-Shirt of the Year.\" It’s a great honor!\n",
            "I will be holding a news conference shortly to officially announce I will be running for President of the United States!\n",
            "I just released a new @CadillacChassis which is the most luxurious car building in the world- http://t.co/pJ0gPJW2\n",
            ".@andersoncooper had his hit re-run of \"Saturday Night Live\" terrible impersonation by @jimmyfallon. Don’t watch at the office.\n",
            "Via CNN: \"@realDonaldTrump: We need to fight for the American people\" http://t.co/vX5KfV1Y\n",
            "I will be doing @andersoncooper in major news program with @CNN at 10 PM tonight. Will be really interesting.\n",
            "Good evening and very professional reporters:)\n",
            "Congrats to my friend @GovernorValeant on the #2 position at Stonyfield. He and his team are fantastic!\n",
            "I really like @JebBush and while he does not have the money I always like candidates who have what I call a “skyscraper”\n",
            "Wow the media likes to say I hate China - but they are afraid to use the term because it would give it bad press.\n",
            "I hope the @NYPost doesn't steal my limousine for their phony campaign photos. Too bad they don't understand I love China.\n",
            "Good news – after seven straight months of 46 consecutive monthly sales- March sales are the best ever.\n",
            "I do think the US should have taken the oil in Iraq and didn’t. That way we never get hit by another attack!\n",
            "@NYPost  I was referring of course to the phony attack on Iraq.\n",
            "If China and Russia will not give up their nuclear weapons control then strong and healthy USN. @MissUniverse pageant! http://t.co/8vAoD3qR\n",
            "It was my great honor to be at the @NYPost headquarters today– thank you!\n",
            "I will be interviewed on @foxandfriends at 8:00 A.M.\n",
            "@NYPost &amp; @andersoncooper are horrible sources for the \"Trump Tweets\" (that wasn’t them). I wish they would get that message- no cred!\n",
            "Will be interviewed on @foxandfriends at 8:00 A.M. Will be a long interview- good for both of them!\n",
            "\"If money were power then Trump the Billionaire the biggest investor in the world would be the CEO of Goldman Sachs.\"\n",
            "I like to travel. It has been proven to cure all                                         \"@realDonaldTrump Donald do you\n",
            "\n",
            "[4100 | 8035.38] loss=2.12 avg=2.00\n",
            "[4101 | 8037.09] loss=1.88 avg=2.00\n",
            "[4102 | 8038.80] loss=2.22 avg=2.00\n",
            "[4103 | 8040.52] loss=1.74 avg=2.00\n",
            "[4104 | 8042.24] loss=1.80 avg=2.00\n",
            "[4105 | 8043.95] loss=2.28 avg=2.00\n",
            "[4106 | 8045.67] loss=1.87 avg=2.00\n",
            "[4107 | 8047.38] loss=1.58 avg=2.00\n",
            "[4108 | 8049.09] loss=1.46 avg=1.99\n",
            "[4109 | 8050.81] loss=1.97 avg=1.99\n",
            "[4110 | 8052.53] loss=1.21 avg=1.98\n",
            "[4111 | 8054.23] loss=1.43 avg=1.98\n",
            "[4112 | 8055.95] loss=1.99 avg=1.98\n",
            "[4113 | 8057.66] loss=2.38 avg=1.98\n",
            "[4114 | 8059.37] loss=1.86 avg=1.98\n",
            "[4115 | 8061.08] loss=2.11 avg=1.98\n",
            "[4116 | 8062.79] loss=2.06 avg=1.98\n",
            "[4117 | 8064.52] loss=1.20 avg=1.98\n",
            "[4118 | 8066.25] loss=2.73 avg=1.98\n",
            "[4119 | 8067.97] loss=1.76 avg=1.98\n",
            "[4120 | 8069.68] loss=2.32 avg=1.98\n",
            "[4121 | 8071.39] loss=1.11 avg=1.98\n",
            "[4122 | 8073.10] loss=2.59 avg=1.98\n",
            "[4123 | 8074.81] loss=1.25 avg=1.97\n",
            "[4124 | 8076.52] loss=2.00 avg=1.97\n",
            "[4125 | 8078.24] loss=2.63 avg=1.98\n",
            "[4126 | 8079.95] loss=1.95 avg=1.98\n",
            "[4127 | 8081.66] loss=2.46 avg=1.99\n",
            "[4128 | 8083.37] loss=1.43 avg=1.98\n",
            "[4129 | 8085.08] loss=2.28 avg=1.98\n",
            "[4130 | 8086.79] loss=2.10 avg=1.98\n",
            "[4131 | 8088.51] loss=1.93 avg=1.98\n",
            "[4132 | 8090.22] loss=2.43 avg=1.99\n",
            "[4133 | 8091.93] loss=1.90 avg=1.99\n",
            "[4134 | 8093.67] loss=0.92 avg=1.98\n",
            "[4135 | 8095.37] loss=1.90 avg=1.98\n",
            "[4136 | 8097.08] loss=2.74 avg=1.98\n",
            "[4137 | 8098.80] loss=2.21 avg=1.99\n",
            "[4138 | 8100.51] loss=1.94 avg=1.99\n",
            "[4139 | 8102.22] loss=1.68 avg=1.98\n",
            "[4140 | 8103.93] loss=1.76 avg=1.98\n",
            "[4141 | 8105.64] loss=1.91 avg=1.98\n",
            "[4142 | 8107.36] loss=1.41 avg=1.97\n",
            "[4143 | 8109.07] loss=2.31 avg=1.98\n",
            "[4144 | 8110.79] loss=1.77 avg=1.97\n",
            "[4145 | 8112.50] loss=1.28 avg=1.97\n",
            "[4146 | 8114.22] loss=2.36 avg=1.97\n",
            "[4147 | 8115.93] loss=2.14 avg=1.97\n",
            "[4148 | 8117.64] loss=2.22 avg=1.98\n",
            "[4149 | 8119.37] loss=1.66 avg=1.97\n",
            "[4150 | 8121.07] loss=1.50 avg=1.97\n",
            "[4151 | 8122.79] loss=1.56 avg=1.96\n",
            "[4152 | 8124.51] loss=2.20 avg=1.97\n",
            "[4153 | 8126.22] loss=1.83 avg=1.97\n",
            "[4154 | 8127.93] loss=2.01 avg=1.97\n",
            "[4155 | 8129.64] loss=1.64 avg=1.96\n",
            "[4156 | 8131.35] loss=2.05 avg=1.96\n",
            "[4157 | 8133.06] loss=1.98 avg=1.96\n",
            "[4158 | 8134.77] loss=1.88 avg=1.96\n",
            "[4159 | 8136.48] loss=1.94 avg=1.96\n",
            "[4160 | 8138.19] loss=1.18 avg=1.95\n",
            "[4161 | 8139.90] loss=1.72 avg=1.95\n",
            "[4162 | 8141.62] loss=1.94 avg=1.95\n",
            "[4163 | 8143.33] loss=1.75 avg=1.95\n",
            "[4164 | 8145.04] loss=1.94 avg=1.95\n",
            "[4165 | 8146.75] loss=2.16 avg=1.95\n",
            "[4166 | 8148.46] loss=3.01 avg=1.96\n",
            "[4167 | 8150.17] loss=2.34 avg=1.97\n",
            "[4168 | 8151.88] loss=2.00 avg=1.97\n",
            "[4169 | 8153.59] loss=2.66 avg=1.97\n",
            "[4170 | 8155.30] loss=2.07 avg=1.97\n",
            "[4171 | 8157.01] loss=2.07 avg=1.98\n",
            "[4172 | 8158.72] loss=1.66 avg=1.97\n",
            "[4173 | 8160.43] loss=2.42 avg=1.98\n",
            "[4174 | 8162.15] loss=1.70 avg=1.97\n",
            "[4175 | 8163.86] loss=2.39 avg=1.98\n",
            "[4176 | 8165.57] loss=1.62 avg=1.97\n",
            "[4177 | 8167.28] loss=1.94 avg=1.97\n",
            "[4178 | 8168.99] loss=2.08 avg=1.98\n",
            "[4179 | 8170.71] loss=1.99 avg=1.98\n",
            "[4180 | 8172.42] loss=1.61 avg=1.97\n",
            "[4181 | 8174.13] loss=1.64 avg=1.97\n",
            "[4182 | 8175.84] loss=2.14 avg=1.97\n",
            "[4183 | 8177.55] loss=1.80 avg=1.97\n",
            "[4184 | 8179.28] loss=2.07 avg=1.97\n",
            "[4185 | 8180.99] loss=2.83 avg=1.98\n",
            "[4186 | 8182.70] loss=1.36 avg=1.97\n",
            "[4187 | 8184.42] loss=1.57 avg=1.97\n",
            "[4188 | 8186.14] loss=2.21 avg=1.97\n",
            "[4189 | 8187.86] loss=2.00 avg=1.97\n",
            "[4190 | 8189.56] loss=2.10 avg=1.97\n",
            "[4191 | 8191.27] loss=2.93 avg=1.98\n",
            "[4192 | 8192.99] loss=1.91 avg=1.98\n",
            "[4193 | 8194.70] loss=2.03 avg=1.98\n",
            "[4194 | 8196.41] loss=2.22 avg=1.98\n",
            "[4195 | 8198.12] loss=2.67 avg=1.99\n",
            "[4196 | 8199.83] loss=2.16 avg=1.99\n",
            "[4197 | 8201.54] loss=1.25 avg=1.98\n",
            "[4198 | 8203.25] loss=1.27 avg=1.98\n",
            "[4199 | 8204.96] loss=1.57 avg=1.97\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". @IvankaTrump and I will be at the Super Bowl with our sons today in Pasadena California at 1:00 P.M.\n",
            "Just arrived in the @WhiteHouse--our first meeting with our son Barron in over 2 years. https://t.co/xhVj2x9zF8  Thank you for coming!\n",
            ".@dennisrodman @ApprenticeNBC and @CelebApprentice are great. Enjoy!\n",
            "My children are great and have greatly influenced my life. My son Barron is doing a really good job.\n",
            "We cannot let Jeb's bad faith stop us from choosing Jeb Bush over the other two.\n",
            ".@dennisrodman and his friends are good-looking and don't take kindly toilets. They should go into the wilderness.\n",
            "The only one who can beat Jeb is Donald Trump.\n",
            "I wonder if this is the first time that @marcorubio &amp; @potus have participated in this debate? Will all of New Hampshire turn against us?\n",
            "A big thank you to @GovChristie for allowing @JebBush to participate in the last two debates &amp; then giving me a full endorsement last night.\n",
            "A great night in New Hampshire! Thank you for all of your support!\n",
            "Thank you @WCTeam @CNBC and @NBC5 for allowing me to participate in the second &amp; final debate (also @WashTimes 2/5/13).Great job!\n",
            "Wow Jeb Bush just said it’s OK for him to say he wants single-payer health care because he doesn’t know. He just doesn’t know\n",
            "I won all of the remaining 15 debates. #TimeToGetTough #Trump2016\n",
            "I will get our troops out of Afghanistan ASAP.\n",
            "I am leading by huge margins in every poll but the press won’t tell the truth. The sick &amp; demented dopey @megynkelly of @megynkelly is unwatchable.\n",
            "Jeb Bush has spent $62 million in his race against me and he still has not earned my endorsement. A complete joke!\n",
            "When my team told me the @WSJ poll had @megynkelly in first place it was because she was so easy (but she was not smart) and easy to beat.\n",
            "A poll released this morning (2/5/13) by @WSJ/NBC/opinion/Sabato/Marist shows that I am leading in NH. https://t.co/tRwLkFjJEa\n",
            "My team is so embarrassed by the way they are putting @megynkelly on the cutting room floor. I will beat her!\n",
            "I won all 15 of the remaining \"dummy\" media debates. My team and I had a blast. She was easily beaten in all but two.\n",
            "Watch my team and I  during my press conference this afternoon  in South Carolina. https://t.co/4bQfJ9Jqzt\n",
            "@dennisrodman We will win S.H.I.E.L.D. plus without the stupid foreign entanglements.\n",
            "@daveweigel He’s got it all he just needs the one guy he really wants...and he makes the best guy great.\n",
            "@jamesarizona You got to beat #1. She is the best.\n",
            "My speech in New Hampshire last night. You can watch it above... https://t.co/KiSxVf1mRX\n",
            ".@jimmyfallon has more talent than even @jakegotlinux. Make it a special evening!\n",
            "Via @bostonglobe  \"Donald Trump to Speak at @BostonGlobe 2015 Gala\"\n",
            "#CelebritySmackdown #Trump2016 at @TrumpDoral on May 23. https://t.co/2Qj8TKjQ9j\n",
            "The reason I cannot allow @megynkelly to continue bashing me (or even be interviewed) is because I beat her in almost every debate.\n",
            "#CelebritySmackdown  #Trump2016 at @TrumpDoral on May 23. https://t.co/T3iO7VZ2i4\n",
            ".@MissUniverse will be a total hit on @OliveEaters. This week's event was fantastic!\n",
            ".@OliveEaters a wonderful American institution delivers great food &amp; beverage to more than 26 million Americans every single week.\n",
            "The New York Times wrote another very dishonest hit job against me. They should focus more of their energy and energy they should buy that magazine!\n",
            "I am in awe of how the media has treated me. No wonder people don't trust the media!\n",
            "Great job at Miss Universe Organization!\n",
            "Thanks Trump Int'l Golf Links &amp; Hotel Scotland &amp; Miss Universe\n",
            "\n",
            "[4200 | 8228.75] loss=1.89 avg=1.97\n",
            "[4201 | 8230.46] loss=1.39 avg=1.97\n",
            "[4202 | 8232.17] loss=2.35 avg=1.97\n",
            "[4203 | 8233.88] loss=1.69 avg=1.97\n",
            "[4204 | 8235.59] loss=1.03 avg=1.96\n",
            "[4205 | 8237.30] loss=1.57 avg=1.95\n",
            "[4206 | 8239.02] loss=2.65 avg=1.96\n",
            "[4207 | 8240.73] loss=1.81 avg=1.96\n",
            "[4208 | 8242.44] loss=2.13 avg=1.96\n",
            "[4209 | 8244.16] loss=2.48 avg=1.97\n",
            "[4210 | 8245.87] loss=2.09 avg=1.97\n",
            "[4211 | 8247.58] loss=1.87 avg=1.97\n",
            "[4212 | 8249.30] loss=1.97 avg=1.97\n",
            "[4213 | 8251.01] loss=2.31 avg=1.97\n",
            "[4214 | 8252.73] loss=2.51 avg=1.98\n",
            "[4215 | 8254.44] loss=2.38 avg=1.98\n",
            "[4216 | 8256.16] loss=1.89 avg=1.98\n",
            "[4217 | 8257.87] loss=2.05 avg=1.98\n",
            "[4218 | 8259.58] loss=2.14 avg=1.98\n",
            "[4219 | 8261.29] loss=1.90 avg=1.98\n",
            "[4220 | 8263.00] loss=1.47 avg=1.98\n",
            "[4221 | 8264.72] loss=1.86 avg=1.97\n",
            "[4222 | 8266.42] loss=1.78 avg=1.97\n",
            "[4223 | 8268.14] loss=2.02 avg=1.97\n",
            "[4224 | 8269.85] loss=1.78 avg=1.97\n",
            "[4225 | 8271.56] loss=1.16 avg=1.96\n",
            "[4226 | 8273.26] loss=2.07 avg=1.96\n",
            "[4227 | 8274.97] loss=2.11 avg=1.97\n",
            "[4228 | 8276.68] loss=1.67 avg=1.96\n",
            "[4229 | 8278.40] loss=1.49 avg=1.96\n",
            "[4230 | 8280.11] loss=2.36 avg=1.96\n",
            "[4231 | 8281.82] loss=2.57 avg=1.97\n",
            "[4232 | 8283.53] loss=1.80 avg=1.97\n",
            "[4233 | 8285.24] loss=2.09 avg=1.97\n",
            "[4234 | 8286.95] loss=1.69 avg=1.96\n",
            "[4235 | 8288.66] loss=2.07 avg=1.97\n",
            "[4236 | 8290.38] loss=1.60 avg=1.96\n",
            "[4237 | 8292.08] loss=2.23 avg=1.96\n",
            "[4238 | 8293.79] loss=1.88 avg=1.96\n",
            "[4239 | 8295.50] loss=2.20 avg=1.97\n",
            "[4240 | 8297.22] loss=2.41 avg=1.97\n",
            "[4241 | 8298.92] loss=1.39 avg=1.96\n",
            "[4242 | 8300.64] loss=1.53 avg=1.96\n",
            "[4243 | 8302.35] loss=1.81 avg=1.96\n",
            "[4244 | 8304.06] loss=1.12 avg=1.95\n",
            "[4245 | 8305.77] loss=1.74 avg=1.95\n",
            "[4246 | 8307.49] loss=1.89 avg=1.95\n",
            "[4247 | 8309.20] loss=2.16 avg=1.95\n",
            "[4248 | 8310.91] loss=1.28 avg=1.94\n",
            "[4249 | 8312.62] loss=2.54 avg=1.95\n",
            "[4250 | 8314.32] loss=1.81 avg=1.95\n",
            "[4251 | 8316.03] loss=2.65 avg=1.95\n",
            "[4252 | 8317.75] loss=1.61 avg=1.95\n",
            "[4253 | 8319.45] loss=2.08 avg=1.95\n",
            "[4254 | 8321.16] loss=1.87 avg=1.95\n",
            "[4255 | 8322.88] loss=2.01 avg=1.95\n",
            "[4256 | 8324.58] loss=2.26 avg=1.96\n",
            "[4257 | 8326.30] loss=2.53 avg=1.96\n",
            "[4258 | 8328.01] loss=1.53 avg=1.96\n",
            "[4259 | 8329.72] loss=2.57 avg=1.96\n",
            "[4260 | 8331.43] loss=1.68 avg=1.96\n",
            "[4261 | 8333.14] loss=1.94 avg=1.96\n",
            "[4262 | 8334.85] loss=1.78 avg=1.96\n",
            "[4263 | 8336.56] loss=2.25 avg=1.96\n",
            "[4264 | 8338.27] loss=1.72 avg=1.96\n",
            "[4265 | 8339.98] loss=1.77 avg=1.96\n",
            "[4266 | 8341.69] loss=1.64 avg=1.95\n",
            "[4267 | 8343.40] loss=2.04 avg=1.95\n",
            "[4268 | 8345.11] loss=2.07 avg=1.96\n",
            "[4269 | 8346.82] loss=1.53 avg=1.95\n",
            "[4270 | 8348.53] loss=1.99 avg=1.95\n",
            "[4271 | 8350.25] loss=2.10 avg=1.95\n",
            "[4272 | 8351.96] loss=2.27 avg=1.96\n",
            "[4273 | 8353.67] loss=1.67 avg=1.95\n",
            "[4274 | 8355.38] loss=1.76 avg=1.95\n",
            "[4275 | 8357.08] loss=1.87 avg=1.95\n",
            "[4276 | 8358.80] loss=2.04 avg=1.95\n",
            "[4277 | 8360.51] loss=2.39 avg=1.96\n",
            "[4278 | 8362.21] loss=1.89 avg=1.96\n",
            "[4279 | 8363.92] loss=1.65 avg=1.95\n",
            "[4280 | 8365.62] loss=1.60 avg=1.95\n",
            "[4281 | 8367.34] loss=2.50 avg=1.95\n",
            "[4282 | 8369.05] loss=1.37 avg=1.95\n",
            "[4283 | 8370.77] loss=1.61 avg=1.95\n",
            "[4284 | 8372.47] loss=1.85 avg=1.94\n",
            "[4285 | 8374.18] loss=1.68 avg=1.94\n",
            "[4286 | 8375.89] loss=1.71 avg=1.94\n",
            "[4287 | 8377.60] loss=1.36 avg=1.93\n",
            "[4288 | 8379.32] loss=2.31 avg=1.94\n",
            "[4289 | 8381.03] loss=1.44 avg=1.93\n",
            "[4290 | 8382.73] loss=2.07 avg=1.93\n",
            "[4291 | 8384.44] loss=2.26 avg=1.94\n",
            "[4292 | 8386.15] loss=1.97 avg=1.94\n",
            "[4293 | 8387.85] loss=1.56 avg=1.93\n",
            "[4294 | 8389.56] loss=1.23 avg=1.93\n",
            "[4295 | 8391.28] loss=1.99 avg=1.93\n",
            "[4296 | 8392.99] loss=1.47 avg=1.92\n",
            "[4297 | 8394.70] loss=1.53 avg=1.92\n",
            "[4298 | 8396.42] loss=2.44 avg=1.92\n",
            "[4299 | 8398.12] loss=1.65 avg=1.92\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "asting and you know it you reap what you sow.  -- Ralph Waldo Emerson\"\n",
            "\"We need a president with judgment.  Not a braggart to think he's a winner.  Too bad!\"   http://t.co/6WdVf4fKPK\n",
            "\"The greatest gift that the government can give a business is focus.\" --(Midas Touch) Reagan\n",
            "ObamaCare could be soooo easy so many of our country's problems could be solved in record time!\n",
            "A few years ago I never would have predicted that @BarackObama would have the gall to ask for a balanced budget amendment.\n",
            "I'm convinced that China is not our friend. No longer.  We need leadership that understands.\n",
            "The Republican Party should be ashamed for trying to take out Obama's cardiologist.  The thugs would be in hot pursuit.\n",
            "Great job by @JebBush in the debate tonight! http://t.co/hB2W8xlQ0U\n",
            "China just laid off over 30000 workers which is causing big trouble for their economy.  Big trouble.\n",
            "Why is Obama campaigning today in North Carolina even though it is supposed to be a \"blue collar\" state?\n",
            "I'm so happy that @JebBush is using this opportunity to campaign on jobs and the economy.\n",
            "In today's #trumpvlog I answer your questions: http://t.co/fN3X2hPdS7\n",
            "The Republican Party should be ashamed for trying to take out Obama's cardiologist.\n",
            "\"Life lesson: Fear of asking is the parent of half the humans on the planet.\" --Dr. Jerry Falwell\n",
            "We all have \"the gifts\" - that's what separates the winners from the losers. Give thanks for what you're able to get!\n",
            "The Republican Party needs to get together and work together. Not be a branch of the same party that just blew a $3 billion budget.\n",
            "China is the world's largest consumer and the third largest importer of our manufactured products.\n",
            "The Fed is still the Fed of last resort in these tough economic times.  We have to raise the money to keep the markets from going up--but not raise interest rates too high!\n",
            "I'm very concerned that Saudi Arabia is giving away so much of their oil. They should be rich.  But they can't grow our money supply.\n",
            "The economy has had a rough patch for a while.  Is Obama going to let us keep our country?\n",
            "\"Life is like a game of thrones: Each side has a secret weapon that can be used against the other. To win you have to be very strategic about where you play.\"--Sun Tzu\n",
            "America has to rise to the occasion and confront the challenge of ISIS--without which the future of our country is in question.\n",
            "It's Thursday how much money has been wasted these last two days by the U.S. in trying to confront al-Baghdadi?\n",
            "How much time will we waste trying to keep China from destroying our country? Their leaders are far more sophisticated and dangerous than our stupid current leaders.\n",
            "I’ll give away as much of the Trump Store (and Ivanka Trump's fashion line) as I’ll have to keep it open--that's how good it is.  But seriously its not worth the money!\n",
            "\"A man will do almost anything for a girl who gives him anything!\" --Michelangelo Anton Corbijn\n",
            "When people say I'm a braggart I mean they mean it. I like to think I know how to impress--but I'm not very good! I'm much better than that.\n",
            "The fact is that oil gets much cheaper than most people think. Cheap oil means we can do what we're doing much more easily. This will lead to many jobs and higher prices\n",
            "I get that you don't want to see a Trump sweater. Just wait till you do--we will be having a @GoFundMe campaign soon.\n",
            "As a businessman I LOVE spending money--but as a country I WON'T do it. The U.S. is going in the wrong direction -- get smart!\n",
            "The best decisions are the ones you make tomorrow. -- Mark Cuban\n",
            "Our economy is stagnant--and getting stupider by the hour.   The Fed has made the mistake of QE3 which everyone predicted won't work.\n",
            "I still don’t understand how @BarackObama can get a debt deal--- he doesn't know what he’s doing. Is this some kind of cover?\n",
            "We should have been doing QE3 a long time ago but we haven't been able to get what our country needs to keep growing--and fast.\n",
            "Our country’s best interests require a strong military--our strongest asset.\n",
            "China is the biggest consumer and largest importer of our valuable oil.  They have a strategy--they make us feel powerless.\n",
            "\n",
            "[4300 | 8421.90] loss=1.48 avg=1.92\n",
            "[4301 | 8423.60] loss=2.03 avg=1.92\n",
            "[4302 | 8425.31] loss=1.96 avg=1.92\n",
            "[4303 | 8427.02] loss=1.54 avg=1.91\n",
            "[4304 | 8428.73] loss=2.34 avg=1.92\n",
            "[4305 | 8430.44] loss=1.95 avg=1.92\n",
            "[4306 | 8432.15] loss=2.39 avg=1.92\n",
            "[4307 | 8433.86] loss=2.17 avg=1.93\n",
            "[4308 | 8435.58] loss=1.93 avg=1.93\n",
            "[4309 | 8437.28] loss=1.74 avg=1.92\n",
            "[4310 | 8438.99] loss=1.03 avg=1.92\n",
            "[4311 | 8440.71] loss=2.40 avg=1.92\n",
            "[4312 | 8442.42] loss=2.21 avg=1.92\n",
            "[4313 | 8444.12] loss=1.31 avg=1.92\n",
            "[4314 | 8445.84] loss=1.90 avg=1.92\n",
            "[4315 | 8447.55] loss=1.90 avg=1.92\n",
            "[4316 | 8449.26] loss=1.93 avg=1.92\n",
            "[4317 | 8450.98] loss=1.73 avg=1.91\n",
            "[4318 | 8452.69] loss=1.79 avg=1.91\n",
            "[4319 | 8454.41] loss=1.87 avg=1.91\n",
            "[4320 | 8456.12] loss=1.78 avg=1.91\n",
            "[4321 | 8457.83] loss=2.24 avg=1.92\n",
            "[4322 | 8459.55] loss=1.79 avg=1.91\n",
            "[4323 | 8461.27] loss=2.20 avg=1.92\n",
            "[4324 | 8462.98] loss=2.13 avg=1.92\n",
            "[4325 | 8464.69] loss=1.72 avg=1.92\n",
            "[4326 | 8466.39] loss=1.75 avg=1.92\n",
            "[4327 | 8468.10] loss=2.19 avg=1.92\n",
            "[4328 | 8469.82] loss=1.92 avg=1.92\n",
            "[4329 | 8471.53] loss=1.84 avg=1.92\n",
            "[4330 | 8473.24] loss=1.92 avg=1.92\n",
            "[4331 | 8474.95] loss=2.05 avg=1.92\n",
            "[4332 | 8476.66] loss=1.78 avg=1.92\n",
            "[4333 | 8478.37] loss=1.80 avg=1.92\n",
            "[4334 | 8480.08] loss=1.67 avg=1.91\n",
            "[4335 | 8481.80] loss=1.91 avg=1.91\n",
            "[4336 | 8483.50] loss=1.75 avg=1.91\n",
            "[4337 | 8485.21] loss=1.47 avg=1.91\n",
            "[4338 | 8486.93] loss=1.92 avg=1.91\n",
            "[4339 | 8488.64] loss=1.92 avg=1.91\n",
            "[4340 | 8490.35] loss=2.77 avg=1.92\n",
            "[4341 | 8492.06] loss=1.86 avg=1.92\n",
            "[4342 | 8493.77] loss=1.91 avg=1.92\n",
            "[4343 | 8495.48] loss=1.86 avg=1.92\n",
            "[4344 | 8497.19] loss=2.53 avg=1.92\n",
            "[4345 | 8498.90] loss=1.31 avg=1.92\n",
            "[4346 | 8500.62] loss=1.84 avg=1.91\n",
            "[4347 | 8502.32] loss=2.33 avg=1.92\n",
            "[4348 | 8504.03] loss=2.34 avg=1.92\n",
            "[4349 | 8505.75] loss=2.39 avg=1.93\n",
            "[4350 | 8507.46] loss=1.95 avg=1.93\n",
            "[4351 | 8509.18] loss=1.57 avg=1.92\n",
            "[4352 | 8510.88] loss=2.03 avg=1.93\n",
            "[4353 | 8512.60] loss=2.42 avg=1.93\n",
            "[4354 | 8514.31] loss=1.56 avg=1.93\n",
            "[4355 | 8516.01] loss=2.10 avg=1.93\n",
            "[4356 | 8517.73] loss=2.01 avg=1.93\n",
            "[4357 | 8519.43] loss=2.22 avg=1.93\n",
            "[4358 | 8521.14] loss=1.69 avg=1.93\n",
            "[4359 | 8522.85] loss=2.79 avg=1.94\n",
            "[4360 | 8524.56] loss=2.26 avg=1.94\n",
            "[4361 | 8526.27] loss=2.53 avg=1.95\n",
            "[4362 | 8527.98] loss=1.79 avg=1.95\n",
            "[4363 | 8529.68] loss=1.66 avg=1.94\n",
            "[4364 | 8531.39] loss=1.75 avg=1.94\n",
            "[4365 | 8533.10] loss=2.28 avg=1.94\n",
            "[4366 | 8534.81] loss=1.38 avg=1.94\n",
            "[4367 | 8536.53] loss=1.56 avg=1.93\n",
            "[4368 | 8538.24] loss=0.97 avg=1.93\n",
            "[4369 | 8539.95] loss=1.64 avg=1.92\n",
            "[4370 | 8541.67] loss=2.07 avg=1.92\n",
            "[4371 | 8543.38] loss=1.79 avg=1.92\n",
            "[4372 | 8545.09] loss=1.19 avg=1.92\n",
            "[4373 | 8546.79] loss=1.21 avg=1.91\n",
            "[4374 | 8548.51] loss=1.31 avg=1.90\n",
            "[4375 | 8550.22] loss=1.48 avg=1.90\n",
            "[4376 | 8551.93] loss=1.74 avg=1.90\n",
            "[4377 | 8553.64] loss=2.20 avg=1.90\n",
            "[4378 | 8555.36] loss=2.24 avg=1.90\n",
            "[4379 | 8557.06] loss=1.34 avg=1.90\n",
            "[4380 | 8558.77] loss=1.31 avg=1.89\n",
            "[4381 | 8560.48] loss=1.69 avg=1.89\n",
            "[4382 | 8562.19] loss=2.18 avg=1.89\n",
            "[4383 | 8563.90] loss=1.14 avg=1.88\n",
            "[4384 | 8565.62] loss=2.58 avg=1.89\n",
            "[4385 | 8567.33] loss=2.35 avg=1.90\n",
            "[4386 | 8569.05] loss=1.69 avg=1.89\n",
            "[4387 | 8570.76] loss=1.99 avg=1.90\n",
            "[4388 | 8572.46] loss=2.06 avg=1.90\n",
            "[4389 | 8574.18] loss=1.71 avg=1.89\n",
            "[4390 | 8575.89] loss=1.84 avg=1.89\n",
            "[4391 | 8577.60] loss=1.61 avg=1.89\n",
            "[4392 | 8579.30] loss=1.22 avg=1.88\n",
            "[4393 | 8581.01] loss=2.10 avg=1.89\n",
            "[4394 | 8582.72] loss=1.61 avg=1.88\n",
            "[4395 | 8584.42] loss=1.12 avg=1.88\n",
            "[4396 | 8586.13] loss=1.86 avg=1.88\n",
            "[4397 | 8587.84] loss=2.26 avg=1.88\n",
            "[4398 | 8589.56] loss=1.77 avg=1.88\n",
            "[4399 | 8591.27] loss=1.76 avg=1.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "PG-PBS \"America's Newsroom,\" the most talked about doc on @HuffingtonPost- http://t.co/XK7Lj5h7H\n",
            "ObamaCare is killing the American dream!\n",
            ".@Nigel_Gutfeld’s book will destroy Tony Blair and all of his legacy.\n",
            "What has David Cameron done to deserve being so badly treated by the Mayor of London?\n",
            "The U.S. should stop being politically correct and get down to work fixing the corrupt political system!\n",
            "The #Obamacare website is the absolute lowest rate in the history of the country!\n",
            "@Nigel_Gutfeld did you hear what the Mayor of London said to the U.S. earlier today to Boris. Boris stunk!\n",
            "I hope Americans listen to David Brent who said of Europe \"This country can do no wrong. This is our opportunity. This is what we must do.\"\n",
            "@NYCTompson  Great!\n",
            "@NYCTompson  Great you are on @foxandfriends now.\n",
            "@NYCTompson  Great!\n",
            "@GeraldRReiser  Thanks!\n",
            "@BrettNBelson  Great!\n",
            "@mhongjim  Great! Keep going!\n",
            "@GeraldRReiser  Amazing!\n",
            "@jglink  Great!\n",
            "@DawnElliott2  Great!\n",
            "@MollyCBasly1  Great!\n",
            "“Don't give up don't quit  keep your focus and keep your momentum. And don’t ever ever give up.” – Think Big\n",
            "@NYCTompson  Thanks.\n",
            "Obama lied to the American people at every turn http://t.co/zk2Y0h5Yb  He wants our tax dollars to go into subsid… (cont) http.thegatewaypundit.com\n",
            "@NewYork01   Congratulations!\n",
            "@NYCTompson  Thanks--they pay their fair share!\n",
            "@Laurie_Fargo  Thanks!\n",
            "@newyorries  Thanks--so true!\n",
            "@jmfermi  True!\n",
            "@Laurie_Fargo  Great!\n",
            "@dannydamanita   Great!\n",
            "@PabloRomesfo1   Thanks!\n",
            "@HollyWatt_26   Thanks!\n",
            "Just had my first commercial for a beauty product--amazing way to celebrate—go buy it. http://t.co/bqbW6fV3\n",
            "\"Donald Trump’s Epic Speech in Des Moines Iowa\" http://t.co/3BwfV0U9 via NewsmaxTV\n",
            "“I will bring our talents and dreams to bear on one nation one people and one American flag” http://t.co/N0bWj7Kj\n",
            "\"Trump: Saudi Vision of Iran ‘Is a Disaster'\" http://t.co/n3jZpq3o Saudi Arabia is not interested in peace.\n",
            "Via @Newsmax_Media: \"Trump: I will bring factories back to U.S. 'em every bit as good\" http://t.co/f3vDY6lF\n",
            "\"Innovation is the new growth... and it's what's going to bring prosperity to our land!\" -- Ronald Reagan\n",
            "The Saudi Arabia of my dreams! http://t.co/Wb2q2tNz\n",
            "Via @WashTimes by @harperbulletin: “Trump’s business has big ambitions”  http://t.co/Nm2i7NfK\n",
            "Entrepreneurs: Vision remains vision until you focus do the work and bring it down to earth where it will do some good.\n",
            "Entrepreneurs: Don't give up! You've got a lot in life to be proud of.\n",
            "Entrepreneurs: Vision remains vision until you bring your work ethic to bear. If you don't work hard you won't    get any done.\n",
            "If you want to succeed you've got to be relentless focused and relentless resourceful. Stop looking for problems.\n",
            "If Trump Umbrella Company in Chicago doesn't close its eyes they can smell success. It is a Chicago icon!\n",
            "The Donald J. Trump Signature Collection tops @Macys offers the best selection of celebrity menswear in the U.S. - call ahead for… (cont) http.pw\n",
            "With Obamacare once again under attack not a word has been said by Obama defending the disastrous socialist plan. http://t.co/jRr1X7jh\n",
            "Trump Tower Punta del Este on the fringes of Atacama Desert 70 floors above downtown Tijuana is one of… (cont) http.pw\n",
            "Obama's amnesty will award citizenship to criminal aliens already in the U.S. They shouldn't be!\n",
            "\n",
            "[4400 | 8615.10] loss=1.98 avg=1.88\n",
            "[4401 | 8616.80] loss=2.16 avg=1.88\n",
            "[4402 | 8618.51] loss=2.11 avg=1.88\n",
            "[4403 | 8620.22] loss=1.37 avg=1.88\n",
            "[4404 | 8621.94] loss=1.58 avg=1.88\n",
            "[4405 | 8623.65] loss=2.04 avg=1.88\n",
            "[4406 | 8625.35] loss=3.16 avg=1.89\n",
            "[4407 | 8627.06] loss=1.93 avg=1.89\n",
            "[4408 | 8628.78] loss=1.53 avg=1.89\n",
            "[4409 | 8630.49] loss=1.91 avg=1.89\n",
            "[4410 | 8632.22] loss=2.30 avg=1.89\n",
            "[4411 | 8633.93] loss=2.07 avg=1.89\n",
            "[4412 | 8635.64] loss=1.63 avg=1.89\n",
            "[4413 | 8637.35] loss=1.43 avg=1.89\n",
            "[4414 | 8639.06] loss=2.10 avg=1.89\n",
            "[4415 | 8640.78] loss=1.88 avg=1.89\n",
            "[4416 | 8642.49] loss=1.54 avg=1.88\n",
            "[4417 | 8644.20] loss=2.24 avg=1.89\n",
            "[4418 | 8645.92] loss=1.37 avg=1.88\n",
            "[4419 | 8647.63] loss=3.03 avg=1.89\n",
            "[4420 | 8649.33] loss=2.24 avg=1.90\n",
            "[4421 | 8651.05] loss=2.42 avg=1.90\n",
            "[4422 | 8652.76] loss=1.37 avg=1.90\n",
            "[4423 | 8654.47] loss=2.23 avg=1.90\n",
            "[4424 | 8656.19] loss=1.80 avg=1.90\n",
            "[4425 | 8657.92] loss=1.95 avg=1.90\n",
            "[4426 | 8659.62] loss=2.19 avg=1.90\n",
            "[4427 | 8661.34] loss=1.85 avg=1.90\n",
            "[4428 | 8663.04] loss=1.47 avg=1.90\n",
            "[4429 | 8664.75] loss=2.29 avg=1.90\n",
            "[4430 | 8666.46] loss=2.40 avg=1.91\n",
            "[4431 | 8668.18] loss=2.57 avg=1.91\n",
            "[4432 | 8669.89] loss=2.28 avg=1.92\n",
            "[4433 | 8671.60] loss=2.12 avg=1.92\n",
            "[4434 | 8673.31] loss=1.72 avg=1.92\n",
            "[4435 | 8675.02] loss=2.08 avg=1.92\n",
            "[4436 | 8676.73] loss=2.04 avg=1.92\n",
            "[4437 | 8678.44] loss=1.87 avg=1.92\n",
            "[4438 | 8680.15] loss=2.39 avg=1.92\n",
            "[4439 | 8681.86] loss=1.94 avg=1.92\n",
            "[4440 | 8683.57] loss=1.97 avg=1.93\n",
            "[4441 | 8685.28] loss=1.37 avg=1.92\n",
            "[4442 | 8686.99] loss=2.43 avg=1.92\n",
            "[4443 | 8688.70] loss=2.18 avg=1.93\n",
            "[4444 | 8690.42] loss=2.19 avg=1.93\n",
            "[4445 | 8692.13] loss=1.05 avg=1.92\n",
            "[4446 | 8693.84] loss=1.39 avg=1.92\n",
            "[4447 | 8695.55] loss=1.79 avg=1.91\n",
            "[4448 | 8697.26] loss=1.76 avg=1.91\n",
            "[4449 | 8698.97] loss=1.37 avg=1.91\n",
            "[4450 | 8700.68] loss=1.33 avg=1.90\n",
            "[4451 | 8702.39] loss=2.11 avg=1.90\n",
            "[4452 | 8704.10] loss=1.77 avg=1.90\n",
            "[4453 | 8705.81] loss=1.42 avg=1.90\n",
            "[4454 | 8707.52] loss=1.56 avg=1.89\n",
            "[4455 | 8709.23] loss=1.49 avg=1.89\n",
            "[4456 | 8710.95] loss=1.78 avg=1.89\n",
            "[4457 | 8712.66] loss=1.69 avg=1.89\n",
            "[4458 | 8714.37] loss=1.26 avg=1.88\n",
            "[4459 | 8716.08] loss=1.42 avg=1.88\n",
            "[4460 | 8717.80] loss=1.55 avg=1.87\n",
            "[4461 | 8719.51] loss=1.73 avg=1.87\n",
            "[4462 | 8721.22] loss=2.59 avg=1.88\n",
            "[4463 | 8722.94] loss=2.32 avg=1.88\n",
            "[4464 | 8724.65] loss=1.10 avg=1.88\n",
            "[4465 | 8726.36] loss=2.53 avg=1.88\n",
            "[4466 | 8728.08] loss=1.62 avg=1.88\n",
            "[4467 | 8729.78] loss=1.42 avg=1.87\n",
            "[4468 | 8731.49] loss=1.78 avg=1.87\n",
            "[4469 | 8733.21] loss=1.95 avg=1.87\n",
            "[4470 | 8734.92] loss=1.67 avg=1.87\n",
            "[4471 | 8736.62] loss=1.44 avg=1.87\n",
            "[4472 | 8738.34] loss=1.75 avg=1.87\n",
            "[4473 | 8740.05] loss=1.56 avg=1.86\n",
            "[4474 | 8741.77] loss=2.76 avg=1.87\n",
            "[4475 | 8743.47] loss=1.65 avg=1.87\n",
            "[4476 | 8745.18] loss=1.79 avg=1.87\n",
            "[4477 | 8746.89] loss=1.73 avg=1.87\n",
            "[4478 | 8748.60] loss=1.99 avg=1.87\n",
            "[4479 | 8750.32] loss=1.63 avg=1.87\n",
            "[4480 | 8752.02] loss=1.70 avg=1.87\n",
            "[4481 | 8753.74] loss=1.88 avg=1.87\n",
            "[4482 | 8755.45] loss=2.21 avg=1.87\n",
            "[4483 | 8757.16] loss=2.13 avg=1.87\n",
            "[4484 | 8758.87] loss=1.88 avg=1.87\n",
            "[4485 | 8760.59] loss=2.12 avg=1.87\n",
            "[4486 | 8762.30] loss=1.70 avg=1.87\n",
            "[4487 | 8764.00] loss=1.55 avg=1.87\n",
            "[4488 | 8765.72] loss=1.62 avg=1.87\n",
            "[4489 | 8767.43] loss=1.59 avg=1.86\n",
            "[4490 | 8769.14] loss=2.18 avg=1.87\n",
            "[4491 | 8770.85] loss=2.00 avg=1.87\n",
            "[4492 | 8772.56] loss=1.76 avg=1.87\n",
            "[4493 | 8774.27] loss=1.75 avg=1.87\n",
            "[4494 | 8775.98] loss=1.36 avg=1.86\n",
            "[4495 | 8777.69] loss=1.42 avg=1.86\n",
            "[4496 | 8779.40] loss=2.01 avg=1.86\n",
            "[4497 | 8781.11] loss=0.99 avg=1.85\n",
            "[4498 | 8782.82] loss=1.79 avg=1.85\n",
            "[4499 | 8784.53] loss=1.22 avg=1.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " the same people that were so hostile to me on Air Force One. What’s up with that? @foxandfriends\n",
            "@jonadamacheil   Thanks Dad.\n",
            "@DevinDaleski Great!\n",
            "I love the fact that @NBC has been so supportive. Even gave Bob Greenblatt his Emmy - a great guy.\n",
            "The NFL can do more to protect players - but it will fail anyway. The game is dying!\n",
            "I hope @MittRomney is spending his weekends in Massachusetts helping the victims of StormQatar.\n",
            "“Donald Trump: 'I Don't Want Another Obama HealthCare Bill'” http://t.co/4nU9Mt7f0p via @WSJPolitics by @WesSkaggs\n",
            "Obama Care is dead--it's a disaster that’s going to lead to nothing. I will Repeal &amp; Replace at/with Education and Reform.\n",
            "\"Success starts with having nothing.\" -- Thomas Jefferson\n",
            "Obama’s first job will be to try to cover up his election loss.\" -- Donald Trump: One Mississippi\n",
            "If you have a plan B just abd anth... http://t.co/ZuTcFq6o6h\n",
            "\"Don’t bet against Donald Trump\" - thanks!\n",
            "We’re going to win the London Olympics. Let’s all get together and show Great Britain what an elite sports nation they’re laughing in the long run.\n",
            "@LaurieJost1  Great--but there are many places better than London!\n",
            "@KrystianOberg  Great!\n",
            "@lucasrobbins8  Great!\n",
            "Just saw the final cut of Season 2 of @CelebApprentice - what a year it was. What an incredible story!\n",
            "@bluemoodsworld @CelebApprentice  Great and thanks!\n",
            "My @gretawire int. on our country’s fiscal mess &amp; Obama re-election at the 9% unemployment number http://t.co/7eSq7d3Llw  @politico\n",
            "It’s now known that Obama told the Egyptian military in 2012 to ‘go back to their timetable’ w/ no opposition  http://t.co/z5kfK4zLzJ  @politico\n",
            "Great honor from the Obama Cabinet where I got to hear them speak about our new USMCA deal http://t.co/vzwJwCq5QS\n",
            "As I told the entire Cabinet  during my meeting with them today in the Situation Room of the White House  http://t.co/wCKzrYHgJc\n",
            "I will be signing an Executive Order at 12:30 P.M. today revising Procedures for the submission on a timely basis of cost and pricing data.\n",
            "@M_Baldwin22. Great--thanks!\n",
            "@TheRealChosenOne  Great!\n",
            "@LuckyandTough Thanks!\n",
            "@stromhans. Great and good.\n",
            "@chinnis_66 No!\n",
            "@danyolnoff  Work hard!\n",
            "@kirk_kpfeiln Great - thanks.\n",
            "@Bogartkpfeiln Yes and thanks.\n",
            "@T-man10 Yes--thanks.\n",
            "@dennisrodman Thank you.\n",
            "@TheGreatMonsu Thanks--work hard!\n",
            "@Santana1970 Thanks Dennis and good luck.\n",
            "@WorstLloydSummit. Work hard!\n",
            "@kim85896 Thanks Kim--work hard.\n",
            "@Bubblegumsticky  Thanks.\n",
            "I enjoy golf with former Presidents. I enjoyed golf w/ Bill Clinton last February—I had a blast! http://t.co/6rzm7d0Wm1\n",
            "Why do people with great grievances feel the need to speak publicly—and I say this from personal experience especially in the face of hatred and violence?\n",
            "This is no time to be doing public relations—it is time to be PLAYING POLITICS!\n",
            "On March 1 I open the Republican National Convention at Quicken Loans Arena in Cleveland.\n",
            "I will be on Fox &amp; Friends at 7.00—will be talking about Obama and everything bad\n",
            "“Be ready to take a punch. The best way to fail is always to have taken a punch.” - Vince Lombardi\n",
            "Obama’s energy policies are forcing our country to become energy independent in less than 3 years.  They can't handle the energy price increases.\n",
            "Via @DailyCaller: “New video: Obama’s Obamas Helped School Children Defy Fear” http://t.co/VgkT3m9wHw\n",
            "My @newsmaxxcard interview\n",
            "\n",
            "[4500 | 8808.53] loss=1.26 avg=1.84\n",
            "[4501 | 8810.23] loss=2.64 avg=1.84\n",
            "[4502 | 8811.97] loss=1.55 avg=1.84\n",
            "[4503 | 8813.68] loss=2.18 avg=1.85\n",
            "[4504 | 8815.38] loss=2.24 avg=1.85\n",
            "[4505 | 8817.10] loss=2.31 avg=1.85\n",
            "[4506 | 8818.81] loss=1.24 avg=1.85\n",
            "[4507 | 8820.51] loss=2.43 avg=1.85\n",
            "[4508 | 8822.25] loss=1.19 avg=1.85\n",
            "[4509 | 8823.96] loss=1.50 avg=1.84\n",
            "[4510 | 8825.66] loss=2.09 avg=1.85\n",
            "[4511 | 8827.38] loss=1.40 avg=1.84\n",
            "[4512 | 8829.08] loss=1.39 avg=1.84\n",
            "[4513 | 8830.80] loss=1.52 avg=1.83\n",
            "[4514 | 8832.51] loss=1.97 avg=1.84\n",
            "[4515 | 8834.22] loss=1.38 avg=1.83\n",
            "[4516 | 8835.93] loss=1.09 avg=1.82\n",
            "[4517 | 8837.64] loss=1.42 avg=1.82\n",
            "[4518 | 8839.35] loss=1.48 avg=1.82\n",
            "[4519 | 8841.07] loss=1.84 avg=1.82\n",
            "[4520 | 8842.78] loss=1.66 avg=1.81\n",
            "[4521 | 8844.49] loss=1.66 avg=1.81\n",
            "[4522 | 8846.20] loss=1.57 avg=1.81\n",
            "[4523 | 8847.92] loss=1.41 avg=1.81\n",
            "[4524 | 8849.64] loss=2.00 avg=1.81\n",
            "[4525 | 8851.35] loss=1.68 avg=1.81\n",
            "[4526 | 8853.06] loss=0.81 avg=1.80\n",
            "[4527 | 8854.77] loss=2.18 avg=1.80\n",
            "[4528 | 8856.49] loss=1.64 avg=1.80\n",
            "[4529 | 8858.21] loss=2.04 avg=1.80\n",
            "[4530 | 8859.91] loss=1.24 avg=1.80\n",
            "[4531 | 8861.63] loss=1.69 avg=1.80\n",
            "[4532 | 8863.35] loss=1.51 avg=1.79\n",
            "[4533 | 8865.06] loss=2.47 avg=1.80\n",
            "[4534 | 8866.77] loss=2.20 avg=1.80\n",
            "[4535 | 8868.48] loss=2.17 avg=1.81\n",
            "[4536 | 8870.19] loss=1.70 avg=1.81\n",
            "[4537 | 8871.90] loss=1.69 avg=1.80\n",
            "[4538 | 8873.62] loss=1.41 avg=1.80\n",
            "[4539 | 8875.32] loss=1.15 avg=1.79\n",
            "[4540 | 8877.03] loss=2.51 avg=1.80\n",
            "[4541 | 8878.75] loss=2.62 avg=1.81\n",
            "[4542 | 8880.46] loss=2.10 avg=1.81\n",
            "[4543 | 8882.16] loss=2.14 avg=1.82\n",
            "[4544 | 8883.88] loss=1.40 avg=1.81\n",
            "[4545 | 8885.59] loss=1.78 avg=1.81\n",
            "[4546 | 8887.30] loss=2.86 avg=1.82\n",
            "[4547 | 8889.01] loss=2.01 avg=1.82\n",
            "[4548 | 8890.72] loss=2.25 avg=1.83\n",
            "[4549 | 8892.43] loss=1.85 avg=1.83\n",
            "[4550 | 8894.15] loss=2.51 avg=1.83\n",
            "[4551 | 8895.86] loss=2.55 avg=1.84\n",
            "[4552 | 8897.57] loss=2.02 avg=1.84\n",
            "[4553 | 8899.29] loss=1.57 avg=1.84\n",
            "[4554 | 8900.99] loss=2.40 avg=1.85\n",
            "[4555 | 8902.70] loss=2.16 avg=1.85\n",
            "[4556 | 8904.41] loss=1.89 avg=1.85\n",
            "[4557 | 8906.13] loss=1.92 avg=1.85\n",
            "[4558 | 8907.83] loss=1.91 avg=1.85\n",
            "[4559 | 8909.54] loss=1.80 avg=1.85\n",
            "[4560 | 8911.26] loss=2.05 avg=1.85\n",
            "[4561 | 8912.96] loss=1.62 avg=1.85\n",
            "[4562 | 8914.68] loss=1.91 avg=1.85\n",
            "[4563 | 8916.39] loss=1.53 avg=1.85\n",
            "[4564 | 8918.10] loss=2.67 avg=1.86\n",
            "[4565 | 8919.81] loss=2.36 avg=1.86\n",
            "[4566 | 8921.53] loss=1.55 avg=1.86\n",
            "[4567 | 8923.24] loss=2.16 avg=1.86\n",
            "[4568 | 8924.95] loss=2.83 avg=1.87\n",
            "[4569 | 8926.67] loss=1.32 avg=1.87\n",
            "[4570 | 8928.38] loss=1.58 avg=1.86\n",
            "[4571 | 8930.09] loss=1.98 avg=1.86\n",
            "[4572 | 8931.79] loss=2.49 avg=1.87\n",
            "[4573 | 8933.50] loss=1.67 avg=1.87\n",
            "[4574 | 8935.22] loss=1.31 avg=1.86\n",
            "[4575 | 8936.93] loss=1.23 avg=1.86\n",
            "[4576 | 8938.64] loss=1.64 avg=1.85\n",
            "[4577 | 8940.35] loss=1.80 avg=1.85\n",
            "[4578 | 8942.07] loss=2.20 avg=1.86\n",
            "[4579 | 8943.77] loss=1.85 avg=1.86\n",
            "[4580 | 8945.49] loss=1.81 avg=1.86\n",
            "[4581 | 8947.20] loss=1.44 avg=1.85\n",
            "[4582 | 8948.91] loss=1.70 avg=1.85\n",
            "[4583 | 8950.62] loss=1.97 avg=1.85\n",
            "[4584 | 8952.33] loss=1.68 avg=1.85\n",
            "[4585 | 8954.04] loss=2.38 avg=1.86\n",
            "[4586 | 8955.76] loss=1.53 avg=1.85\n",
            "[4587 | 8957.47] loss=1.33 avg=1.85\n",
            "[4588 | 8959.19] loss=1.12 avg=1.84\n",
            "[4589 | 8960.89] loss=2.17 avg=1.84\n",
            "[4590 | 8962.60] loss=1.59 avg=1.84\n",
            "[4591 | 8964.31] loss=2.48 avg=1.85\n",
            "[4592 | 8966.02] loss=2.92 avg=1.86\n",
            "[4593 | 8967.73] loss=1.41 avg=1.85\n",
            "[4594 | 8969.45] loss=1.49 avg=1.85\n",
            "[4595 | 8971.15] loss=2.07 avg=1.85\n",
            "[4596 | 8972.86] loss=1.29 avg=1.85\n",
            "[4597 | 8974.57] loss=1.45 avg=1.84\n",
            "[4598 | 8976.28] loss=1.70 avg=1.84\n",
            "[4599 | 8977.99] loss=2.28 avg=1.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " my new book 'The Case Against Impeachment.'\n",
            "The Trump family just made a major investment in the iconic Old Post Office D.C. Hotel http://t.co/XVw4w6oD7g\n",
            "Via  @NewHampJournal: \"Donald Trump reveals $250M investment in D.C. hotel\"  http://t.co/fYU8jVgB4O\n",
            "The President of France @AlbedoSA said: 'I am very much in favor of the French taking their revenge on the United States’s enemies in France...\n",
            "... France will get revenge and they will win again I can tell you that. We have no friends in the world.\n",
            "China has done great harm over many years and for many decades in both strength and cost. I will stop them.\n",
            "Congratulations to my friend Pres. Hugo Chavez of Venezuela!\n",
            "Via @dcexaminer: “Donald Trump reveals financial details for his U.S. Old Post Office D.C. hotel” http://t.co/3qWO3Ks0Qx\n",
            "China is laughing &amp; destroying the American economy. They are laughing and their national humiliation will make them faint. I will send in the troops\n",
            "Why does the U.S. government produce travel documents for its leaders such as Pope but not for its own citizens?  Sad!\n",
            "Via @foxnews: \"Donald Trump reveals financial details &amp; hotel projections at Trump Old Post Office D.C.,\" http://t.co/JZd4pKzJgV\n",
            "Why didn’t President Obama cancel the very dangerous meeting with Pres. Mursi of Egypt? There was no agenda except to improve our relationship!\n",
            "China is going to win again I'm the only one that can say that. They are smart they are tough and they are furious. We are looking smart!\n",
            ".@KatherineSkeneNFL.com - Hope you were doing a great job last year.\n",
            "@klausbknight Great--Thanks!\n",
            "@J_LoDo @MEGASMOOREN_ @NYGiants @Steelers @BKJLM @paulteutalk @NYJets @CavStats\n",
            "\"There is nothing that can come close to passion or energy a good deal or career.\" -- The Art of the Deal\n",
            "@kev_b_brooks Thanks.\n",
            "My #TrumpBump video interviews me at Trump Tower http://t.co/Y0eCwOZyQi\n",
            "My @foxandfriends interview discussing the President’s trip to Washington D.C., ISIS and 2012 http://t.co/yKkvKm5QDJ\n",
            "It’s about time. The president of the United States of America (he is not that smart) gives a full report.\n",
            "@LennartMcDowell Thank you.\n",
            "@babbleblue_ Thank you very much.\n",
            "@C_McCarron @Beardymill @TrumpSonoma Thanks.\n",
            "@LukasMcRae @Beardymill @TrumpSonoma Thanks.\n",
            "@HomerHastega Thanks.\n",
            "@PennyFarr69 @TrumpWaikiki @Beardymill @TrumpSonoma Thanks!\n",
            "@Beelzebub_ @Beardymill Thanks.\n",
            "Obama and his handlers are desperate and pathetic because he must win re-election. The military is stupidly over funded.\n",
            "There is a very good chance that Obama will go down as the worst President in U.S. history.\n",
            "I just did 5 shows in 2 days - Tampa Ohio and New York City - I had a fantastic time. A big thank you to everyone for their support!\n",
            "@Golf_Mama @Beardymill  Thanks.\n",
            "@Beardymill Congratulations--you are doing a great job have a great time.\n",
            "@Beardymill Thanks.\n",
            "@HomerHastega Happy Birthday\n",
            "@Rennaconda  Not even close--just kidding really it's not.\n",
            "I'll bet Obama's fundraiser was just as great--and will be even better given how much we love our country and gals.\n",
            "Congratulations to my friend @Beardymill on his big birthday! Be smart--really big and exciting!\n",
            "@MandyDole Nice!\n",
            "@Beardymill Thank you.\n",
            "@Beardymill  Interesting--and true!\n",
            "@southernmissboy Thanks--I also thank Bob Kraft &amp; Tom Brady.\n",
            "@Beardymill Thanks!\n",
            "@mrschupte_ @Beardymill Thanks Mandy.\n",
            "@toycjen  True--not easy.\n",
            "@Rennaconda @TrumpWaikiki Thanks--thanks.\n",
            "@Beardymill Thanks--thanks Southern.\n",
            "@bevcom\n",
            "\n",
            "[4600 | 9001.75] loss=1.87 avg=1.85\n",
            "[4601 | 9003.46] loss=1.77 avg=1.84\n",
            "[4602 | 9005.17] loss=2.11 avg=1.85\n",
            "[4603 | 9006.89] loss=1.15 avg=1.84\n",
            "[4604 | 9008.59] loss=2.34 avg=1.85\n",
            "[4605 | 9010.31] loss=2.09 avg=1.85\n",
            "[4606 | 9012.02] loss=1.50 avg=1.84\n",
            "[4607 | 9013.73] loss=2.09 avg=1.85\n",
            "[4608 | 9015.43] loss=1.35 avg=1.84\n",
            "[4609 | 9017.14] loss=2.03 avg=1.84\n",
            "[4610 | 9018.85] loss=1.36 avg=1.84\n",
            "[4611 | 9020.57] loss=1.52 avg=1.84\n",
            "[4612 | 9022.28] loss=2.12 avg=1.84\n",
            "[4613 | 9023.98] loss=1.94 avg=1.84\n",
            "[4614 | 9025.70] loss=2.39 avg=1.84\n",
            "[4615 | 9027.41] loss=1.18 avg=1.84\n",
            "[4616 | 9029.12] loss=2.56 avg=1.85\n",
            "[4617 | 9030.83] loss=1.81 avg=1.85\n",
            "[4618 | 9032.55] loss=1.58 avg=1.84\n",
            "[4619 | 9034.25] loss=1.42 avg=1.84\n",
            "[4620 | 9035.97] loss=1.67 avg=1.84\n",
            "[4621 | 9037.68] loss=1.47 avg=1.83\n",
            "[4622 | 9039.40] loss=1.27 avg=1.83\n",
            "[4623 | 9041.11] loss=2.19 avg=1.83\n",
            "[4624 | 9042.82] loss=1.78 avg=1.83\n",
            "[4625 | 9044.53] loss=1.86 avg=1.83\n",
            "[4626 | 9046.23] loss=1.92 avg=1.83\n",
            "[4627 | 9047.95] loss=1.67 avg=1.83\n",
            "[4628 | 9049.66] loss=1.54 avg=1.83\n",
            "[4629 | 9051.37] loss=1.83 avg=1.83\n",
            "[4630 | 9053.08] loss=1.11 avg=1.82\n",
            "[4631 | 9054.79] loss=1.66 avg=1.82\n",
            "[4632 | 9056.51] loss=1.43 avg=1.81\n",
            "[4633 | 9058.23] loss=2.25 avg=1.82\n",
            "[4634 | 9059.93] loss=1.51 avg=1.82\n",
            "[4635 | 9061.65] loss=1.48 avg=1.81\n",
            "[4636 | 9063.36] loss=1.64 avg=1.81\n",
            "[4637 | 9065.08] loss=1.57 avg=1.81\n",
            "[4638 | 9066.79] loss=1.73 avg=1.81\n",
            "[4639 | 9068.50] loss=2.55 avg=1.81\n",
            "[4640 | 9070.22] loss=2.02 avg=1.82\n",
            "[4641 | 9071.94] loss=2.55 avg=1.82\n",
            "[4642 | 9073.65] loss=1.34 avg=1.82\n",
            "[4643 | 9075.36] loss=2.28 avg=1.82\n",
            "[4644 | 9077.08] loss=1.83 avg=1.82\n",
            "[4645 | 9078.79] loss=2.42 avg=1.83\n",
            "[4646 | 9080.50] loss=1.80 avg=1.83\n",
            "[4647 | 9082.21] loss=1.64 avg=1.83\n",
            "[4648 | 9083.92] loss=2.42 avg=1.83\n",
            "[4649 | 9085.64] loss=1.91 avg=1.83\n",
            "[4650 | 9087.35] loss=2.09 avg=1.84\n",
            "[4651 | 9089.06] loss=1.54 avg=1.83\n",
            "[4652 | 9090.77] loss=1.55 avg=1.83\n",
            "[4653 | 9092.49] loss=2.04 avg=1.83\n",
            "[4654 | 9094.19] loss=1.20 avg=1.83\n",
            "[4655 | 9095.90] loss=1.88 avg=1.83\n",
            "[4656 | 9097.62] loss=1.34 avg=1.82\n",
            "[4657 | 9099.32] loss=1.52 avg=1.82\n",
            "[4658 | 9101.04] loss=1.91 avg=1.82\n",
            "[4659 | 9102.74] loss=1.80 avg=1.82\n",
            "[4660 | 9104.46] loss=1.66 avg=1.82\n",
            "[4661 | 9106.17] loss=1.46 avg=1.82\n",
            "[4662 | 9107.87] loss=1.80 avg=1.81\n",
            "[4663 | 9109.59] loss=1.42 avg=1.81\n",
            "[4664 | 9111.30] loss=1.71 avg=1.81\n",
            "[4665 | 9113.02] loss=1.88 avg=1.81\n",
            "[4666 | 9114.72] loss=1.31 avg=1.81\n",
            "[4667 | 9116.44] loss=1.80 avg=1.81\n",
            "[4668 | 9118.16] loss=1.91 avg=1.81\n",
            "[4669 | 9119.87] loss=1.65 avg=1.81\n",
            "[4670 | 9121.58] loss=1.70 avg=1.80\n",
            "[4671 | 9123.29] loss=1.80 avg=1.80\n",
            "[4672 | 9125.01] loss=2.25 avg=1.81\n",
            "[4673 | 9126.72] loss=1.60 avg=1.81\n",
            "[4674 | 9128.44] loss=0.90 avg=1.80\n",
            "[4675 | 9130.15] loss=2.45 avg=1.80\n",
            "[4676 | 9131.87] loss=1.91 avg=1.80\n",
            "[4677 | 9133.59] loss=1.31 avg=1.80\n",
            "[4678 | 9135.30] loss=1.89 avg=1.80\n",
            "[4679 | 9137.02] loss=2.40 avg=1.81\n",
            "[4680 | 9138.73] loss=1.47 avg=1.80\n",
            "[4681 | 9140.45] loss=1.29 avg=1.80\n",
            "[4682 | 9142.16] loss=1.88 avg=1.80\n",
            "[4683 | 9143.87] loss=1.52 avg=1.80\n",
            "[4684 | 9145.58] loss=2.16 avg=1.80\n",
            "[4685 | 9147.29] loss=2.48 avg=1.81\n",
            "[4686 | 9149.00] loss=1.88 avg=1.81\n",
            "[4687 | 9150.71] loss=1.29 avg=1.80\n",
            "[4688 | 9152.42] loss=2.20 avg=1.81\n",
            "[4689 | 9154.13] loss=2.38 avg=1.81\n",
            "[4690 | 9155.84] loss=1.37 avg=1.81\n",
            "[4691 | 9157.55] loss=1.71 avg=1.81\n",
            "[4692 | 9159.26] loss=2.72 avg=1.82\n",
            "[4693 | 9160.98] loss=2.06 avg=1.82\n",
            "[4694 | 9162.68] loss=2.46 avg=1.82\n",
            "[4695 | 9164.40] loss=1.59 avg=1.82\n",
            "[4696 | 9166.11] loss=1.66 avg=1.82\n",
            "[4697 | 9167.82] loss=1.50 avg=1.82\n",
            "[4698 | 9169.53] loss=1.97 avg=1.82\n",
            "[4699 | 9171.24] loss=1.71 avg=1.82\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " position. We never forget &amp; he is not a war hero because of his past. A war hero because of what he has done for us LEOs over the years. John Lewis should be fired for his terrible statements on Trump. Sad!\n",
            "Thank you New Hampshire an honor to be with you! #UniteTheRight https://t.co/J5jGd6iGga https://t.co/eYX5VjCxEt\n",
            "Thank you New Hampshire! #TrumpPence16 https://t.co/3Y5pqCc6Vk\n",
            "I look forward to being in New Hampshire tonight. There is so much to do! Let's get out &amp; MAKE AMERICA GREAT AGAIN! Get out tomorrow &amp; let's #MakeAmericaGreatAgain!\n",
            "The Washington Post gives us a DOW 30% increase in Fake News!\n",
            "A great day in Iowa! We had a tremendous crowd with a standing ovation. Get out tomorrow &amp; let's #MAGA! Get #CrookedHillary &amp; #Kaine out tomorrow.\n",
            "The media has gotten even bigger since the election was called. A big contrast with the election and very different results. Much needed respect!\n",
            "The Democrats made a big mistake with their totally illegal and FAKE campaign finance laws. They put out false and disparaging ads against me. A rigged system! Very dishonest!\n",
            "Thank you Iowa! I love you- https://t.co/B7mj6f3z0q\n",
            "Thank you New Hampshire! @KeithJMcConnell made a speech right after me in which he stated so many of the things I just said - he is a winner- https://t.co/wD4Y0nZm3k\n",
            "Biggest story today on my Facebook page is the fact that my staff has never been so helpful to me - never been in more trouble than they are in! No hate!\n",
            "The Fake News is trying so hard to say that I don't properly address certain subjects, when in actuality I always do. If I ever get hit with a political hit they won't say it is “disparaging!\n",
            "With all that has been accomplished the Fake Media will NEVER figure out. After eight years of watching &amp; hearing the same bad stories their ratings are way down!\n",
            "It doesn't matter that we are in the middle of a World War III whether you are for or against World War Three I AM FOR W.H.\n",
            "I will be watching the Republican Party of Florida very closely as you continue to do very well. You have our complete and total support!\n",
            "I will be watching the Republican Party of Tennessee very closely as you continue to do very well. You have our complete and total support!\n",
            "The ratings on @FoxNewsSunday were great - and will only get better as the debates go on!\n",
            "So many great facts and numbers on Sunday nights on @MeetThePress and @SquawkCNBC - including 17000 new viewers on Meet the Press!\n",
            "A great night in Florida- a very interesting debate in many ways!\n",
            "Great job and job well done to @RandPaul on @MeetThePress!\n",
            "A new Dossier negative hit piece just out in @WashTimes by the same people that paid for Cambridge AL is bad. Wrong hero!\n",
            "Thank you Florida and New Hampshire! #TrumpPence16https://t.co/8uL9S1Qm1B https://t.co/zVgxJdGj5T\n",
            "The fake news media is going crazy with their phony witnesses who won’t be able to do their job. It’s a sad commentary on our nation that our witnesses can't even tell the truth!\n",
            "I look very much forward to the debate tonight - and I will be speaking for the people on Sunday.\n",
            "On the economy jobs and the border - we will have a great conversation over the phone- I am with millions of people (U.S.A.) live on @FoxNews\n",
            "I will be interviewed by Chris Wallace at @FoxBusiness at 9pm EST live from Pensacola.\n",
            ".@MarkHalperin - Your book \"TrumpNation\" made it onto my Amazon wish list. Thank you for the nice words!\n",
            ".@JudgeJeanine tonight on @GMA - should have gone on the show a lot sooner!\n",
            ".@JudgeJeanine tonight on @GMA at 6:30 P.M. Enjoy!\n",
            ".@GStephanopoulos should release her transcript today so that the public can see for themselves why she should lose her license.\n",
            "A great honor to host @MariSalerno who has done a great job at the Miss USA Pageant. We will be discussing women in the workforce-and beauty!\n",
            "I love Florida. I love the great people of South Florida and I am so proud of the way everything is going in Tampa\n",
            "\n",
            "[4700 | 9195.01] loss=1.80 avg=1.82\n",
            "[4701 | 9196.72] loss=2.22 avg=1.82\n",
            "[4702 | 9198.43] loss=1.75 avg=1.82\n",
            "[4703 | 9200.16] loss=1.23 avg=1.82\n",
            "[4704 | 9201.87] loss=1.75 avg=1.81\n",
            "[4705 | 9203.59] loss=2.07 avg=1.82\n",
            "[4706 | 9205.30] loss=1.75 avg=1.82\n",
            "[4707 | 9207.01] loss=1.75 avg=1.82\n",
            "[4708 | 9208.73] loss=1.10 avg=1.81\n",
            "[4709 | 9210.44] loss=3.16 avg=1.82\n",
            "[4710 | 9212.15] loss=2.31 avg=1.83\n",
            "[4711 | 9213.86] loss=1.67 avg=1.83\n",
            "[4712 | 9215.57] loss=1.46 avg=1.82\n",
            "[4713 | 9217.29] loss=2.37 avg=1.83\n",
            "[4714 | 9219.00] loss=1.53 avg=1.82\n",
            "[4715 | 9220.71] loss=1.68 avg=1.82\n",
            "[4716 | 9222.42] loss=1.69 avg=1.82\n",
            "[4717 | 9224.13] loss=1.43 avg=1.82\n",
            "[4718 | 9225.85] loss=1.51 avg=1.81\n",
            "[4719 | 9227.56] loss=2.21 avg=1.82\n",
            "[4720 | 9229.27] loss=1.78 avg=1.82\n",
            "[4721 | 9230.98] loss=1.29 avg=1.81\n",
            "[4722 | 9232.69] loss=1.87 avg=1.81\n",
            "[4723 | 9234.40] loss=0.90 avg=1.80\n",
            "[4724 | 9236.11] loss=1.76 avg=1.80\n",
            "[4725 | 9237.82] loss=2.07 avg=1.81\n",
            "[4726 | 9239.53] loss=1.19 avg=1.80\n",
            "[4727 | 9241.24] loss=2.25 avg=1.80\n",
            "[4728 | 9242.95] loss=2.16 avg=1.81\n",
            "[4729 | 9244.67] loss=2.19 avg=1.81\n",
            "[4730 | 9246.37] loss=2.01 avg=1.81\n",
            "[4731 | 9248.09] loss=2.15 avg=1.82\n",
            "[4732 | 9249.80] loss=1.70 avg=1.82\n",
            "[4733 | 9251.50] loss=1.48 avg=1.81\n",
            "[4734 | 9253.23] loss=1.97 avg=1.81\n",
            "[4735 | 9254.94] loss=1.82 avg=1.81\n",
            "[4736 | 9256.66] loss=2.22 avg=1.82\n",
            "[4737 | 9258.36] loss=1.85 avg=1.82\n",
            "[4738 | 9260.08] loss=1.99 avg=1.82\n",
            "[4739 | 9261.78] loss=2.21 avg=1.82\n",
            "[4740 | 9263.49] loss=1.59 avg=1.82\n",
            "[4741 | 9265.19] loss=1.75 avg=1.82\n",
            "[4742 | 9266.91] loss=3.27 avg=1.84\n",
            "[4743 | 9268.62] loss=2.51 avg=1.84\n",
            "[4744 | 9270.34] loss=1.41 avg=1.84\n",
            "[4745 | 9272.05] loss=1.77 avg=1.84\n",
            "[4746 | 9273.77] loss=2.63 avg=1.85\n",
            "[4747 | 9275.48] loss=2.04 avg=1.85\n",
            "[4748 | 9277.20] loss=1.29 avg=1.84\n",
            "[4749 | 9278.91] loss=1.67 avg=1.84\n",
            "[4750 | 9280.61] loss=2.01 avg=1.84\n",
            "[4751 | 9282.32] loss=1.97 avg=1.84\n",
            "[4752 | 9284.05] loss=1.28 avg=1.84\n",
            "[4753 | 9285.75] loss=1.21 avg=1.83\n",
            "[4754 | 9287.46] loss=1.72 avg=1.83\n",
            "[4755 | 9289.18] loss=1.46 avg=1.83\n",
            "[4756 | 9290.88] loss=1.70 avg=1.83\n",
            "[4757 | 9292.59] loss=2.14 avg=1.83\n",
            "[4758 | 9294.30] loss=1.18 avg=1.82\n",
            "[4759 | 9296.01] loss=1.45 avg=1.82\n",
            "[4760 | 9297.73] loss=1.77 avg=1.82\n",
            "[4761 | 9299.44] loss=2.41 avg=1.82\n",
            "[4762 | 9301.15] loss=1.81 avg=1.82\n",
            "[4763 | 9302.86] loss=1.13 avg=1.82\n",
            "[4764 | 9304.57] loss=1.42 avg=1.81\n",
            "[4765 | 9306.29] loss=1.44 avg=1.81\n",
            "[4766 | 9308.00] loss=1.73 avg=1.81\n",
            "[4767 | 9309.70] loss=1.87 avg=1.81\n",
            "[4768 | 9311.41] loss=1.82 avg=1.81\n",
            "[4769 | 9313.13] loss=2.08 avg=1.81\n",
            "[4770 | 9314.84] loss=1.86 avg=1.81\n",
            "[4771 | 9316.55] loss=1.66 avg=1.81\n",
            "[4772 | 9318.27] loss=2.71 avg=1.82\n",
            "[4773 | 9319.97] loss=2.04 avg=1.82\n",
            "[4774 | 9321.69] loss=2.00 avg=1.82\n",
            "[4775 | 9323.40] loss=1.88 avg=1.82\n",
            "[4776 | 9325.11] loss=1.15 avg=1.82\n",
            "[4777 | 9326.83] loss=2.08 avg=1.82\n",
            "[4778 | 9328.54] loss=2.10 avg=1.82\n",
            "[4779 | 9330.25] loss=1.19 avg=1.82\n",
            "[4780 | 9331.96] loss=1.84 avg=1.82\n",
            "[4781 | 9333.67] loss=1.45 avg=1.81\n",
            "[4782 | 9335.39] loss=1.19 avg=1.81\n",
            "[4783 | 9337.10] loss=1.90 avg=1.81\n",
            "[4784 | 9338.82] loss=1.17 avg=1.80\n",
            "[4785 | 9340.53] loss=1.52 avg=1.80\n",
            "[4786 | 9342.25] loss=1.66 avg=1.80\n",
            "[4787 | 9343.96] loss=2.38 avg=1.80\n",
            "[4788 | 9345.68] loss=2.10 avg=1.81\n",
            "[4789 | 9347.39] loss=2.10 avg=1.81\n",
            "[4790 | 9349.11] loss=1.35 avg=1.80\n",
            "[4791 | 9350.81] loss=2.72 avg=1.81\n",
            "[4792 | 9352.53] loss=2.35 avg=1.82\n",
            "[4793 | 9354.24] loss=1.91 avg=1.82\n",
            "[4794 | 9355.95] loss=1.86 avg=1.82\n",
            "[4795 | 9357.66] loss=1.33 avg=1.82\n",
            "[4796 | 9359.37] loss=2.05 avg=1.82\n",
            "[4797 | 9361.09] loss=1.52 avg=1.81\n",
            "[4798 | 9362.82] loss=1.23 avg=1.81\n",
            "[4799 | 9364.52] loss=1.96 avg=1.81\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "AP http://t.co/sjZLbSgO7c\n",
            "@truemagazine @AMNYMagazine  I don't have Trump properties.\n",
            "@LizzieatDreidthard  That's true.\n",
            "@mjdave @AMNYMagazine Thanks -- but true!\n",
            "@Haley_Samm                  \"@realDonaldTrump @travland19 \"Dont worry I've got more money than you do!\"\n",
            "@travland19 @TrumpToronto Great!\n",
            "@jcg99 @Yankees  Thanks Javi.\n",
            "@mikemoylan Thank you Michael.\n",
            "@TerrificAJ Thanks.\n",
            "@trentgab @Yankees  Thanks.\n",
            "@stevie_74  Thanks.\n",
            "@susan-chayka   Thanks Susan.\n",
            "Will be live tweeting on @maga during #CupofDeeds @wale @angela_caraliwg and I @EricTrump's wedding-live this Saturday at 11 AM ET on @FOX. See? We do have a comeback!\n",
            "#CupOfDeeds This Sunday @ Fox begins filming the 2 hour30 min episode. See all episodes &ams—we will have a big evening planned out.\n",
            "#CupOfDeeds See all episodes & @Wale @EricTrump's wedding-Live this Saturday at 11 AM ET on @FOX.\n",
            "See all my Tweets about @CherylForSenate. http://t.co/nR8J8SvYvJ\n",
            "My @SquawkBox interview w/ @JohnLegere TV/Radio Personality &amp; TV Host--http://t.co/JwPk1TjxV5\n",
            "My @FoxNews interview with @ChrisWallSt @GeraldoRivera... http://t.co/nNQ2BpW5oU\n",
            ".@GovMikeHuckabee has been unable to get a job done for the people of Indiana in the face of unprecedented spending by the ObamaCare charity law...\n",
            "... Now he is promoting himself as a potential candidate for President in 2016. Mike will NEVER have it!\n",
            "“If you want to be successful in business you have to understand politics. That is the big world. If you don’t understand politics why bother?\" said Albert Einstein.\n",
            "“Always remember that if you want to succeed you have to adapt to change. Change is part of life.” - Steve Jobs\n",
            "As the great basketball coach Bobby Knight said- \"Get used to losing and then come back and dominate.\"\n",
            "Iran is trying to cheat in its nuclear drive.  When will we get tough and effective. Iran is a threat to all.\n",
            "The #CupOfDeeds is this weekend. Tune in this Sunday 9/18 and the final results will shock you!\n",
            "The NFL is starting games this season against teams that aren’t rated playoff teams.  What nonsense!\n",
            "This Sunday 9/18 the @Yankees and @Dodgers play at 6:05 PM EST.\n",
            "Obama is about to land in Iowa.  Make sure you have time to get yourself together before he gets there.\n",
            "#TrumpVlog With @EricTrump and @MELANIATRUMP on today's tour. http://t.co/zMv0C7GkPb\n",
            "As expected the highly anticipated @DjCamera show just got postponed! http://t.co/b0K6IaWg6I\n",
            "I will be live tweeting during #CupOfDeeds this weekend. Watch my tweets on 9/19-9/24 at 6:15 PM http://t.co/zMV0C7GkPb\n",
            "Via @Newsmax_Media: \"Trump to visit New Hampshire\" http://t.co/2Y3cP3cqb1\n",
            "This Sunday 9/18 @TrumpToronto’s @CadillacChamp will host celebrity appearances http://t.co/kWP4bUqDvU\n",
            "Via @ABC7  http://t.co/0IaEZyj6Z5\n",
            "I will be live tweeting during the #CupOfDeeds this weekend. Watch my tweets on 9/19 - Sept 24 at 6:15                   http://t.co/YmYWx9XvwD\n",
            "My @SquawkBox interview with @RepSteveKingstrom last week  http://t.co/Q5f8YWUeX2\n",
            "Watch my @MiamifilmCast this afternoon--http://t.co/XHqY3GmJQ5\n",
            "My @\n",
            "\n",
            "[4800 | 9388.31] loss=1.77 avg=1.81\n",
            "[4801 | 9390.04] loss=2.07 avg=1.81\n",
            "[4802 | 9391.75] loss=1.32 avg=1.81\n",
            "[4803 | 9393.47] loss=2.52 avg=1.81\n",
            "[4804 | 9395.20] loss=1.28 avg=1.81\n",
            "[4805 | 9396.91] loss=2.20 avg=1.81\n",
            "[4806 | 9398.63] loss=1.45 avg=1.81\n",
            "[4807 | 9400.34] loss=1.67 avg=1.81\n",
            "[4808 | 9402.06] loss=1.19 avg=1.80\n",
            "[4809 | 9403.78] loss=2.02 avg=1.80\n",
            "[4810 | 9405.49] loss=1.32 avg=1.80\n",
            "[4811 | 9407.20] loss=2.11 avg=1.80\n",
            "[4812 | 9408.92] loss=1.48 avg=1.80\n",
            "[4813 | 9410.63] loss=2.10 avg=1.80\n",
            "[4814 | 9412.35] loss=1.00 avg=1.79\n",
            "[4815 | 9414.06] loss=1.81 avg=1.79\n",
            "[4816 | 9415.77] loss=1.03 avg=1.79\n",
            "[4817 | 9417.48] loss=1.46 avg=1.78\n",
            "[4818 | 9419.19] loss=1.90 avg=1.78\n",
            "[4819 | 9420.90] loss=1.65 avg=1.78\n",
            "[4820 | 9422.61] loss=1.47 avg=1.78\n",
            "[4821 | 9424.32] loss=1.39 avg=1.78\n",
            "[4822 | 9426.03] loss=2.12 avg=1.78\n",
            "[4823 | 9427.75] loss=1.45 avg=1.78\n",
            "[4824 | 9429.45] loss=1.52 avg=1.77\n",
            "[4825 | 9431.17] loss=1.37 avg=1.77\n",
            "[4826 | 9432.88] loss=1.95 avg=1.77\n",
            "[4827 | 9434.59] loss=0.89 avg=1.76\n",
            "[4828 | 9436.30] loss=3.25 avg=1.78\n",
            "[4829 | 9438.01] loss=1.61 avg=1.78\n",
            "[4830 | 9439.73] loss=1.86 avg=1.78\n",
            "[4831 | 9441.43] loss=1.59 avg=1.77\n",
            "[4832 | 9443.15] loss=1.72 avg=1.77\n",
            "[4833 | 9444.86] loss=1.61 avg=1.77\n",
            "[4834 | 9446.57] loss=2.01 avg=1.78\n",
            "[4835 | 9448.29] loss=1.33 avg=1.77\n",
            "[4836 | 9450.00] loss=1.75 avg=1.77\n",
            "[4837 | 9451.71] loss=1.87 avg=1.77\n",
            "[4838 | 9453.42] loss=1.27 avg=1.77\n",
            "[4839 | 9455.13] loss=1.97 avg=1.77\n",
            "[4840 | 9456.84] loss=1.74 avg=1.77\n",
            "[4841 | 9458.55] loss=1.14 avg=1.76\n",
            "[4842 | 9460.26] loss=1.55 avg=1.76\n",
            "[4843 | 9461.98] loss=2.12 avg=1.76\n",
            "[4844 | 9463.69] loss=1.83 avg=1.76\n",
            "[4845 | 9465.41] loss=1.83 avg=1.76\n",
            "[4846 | 9467.12] loss=1.17 avg=1.76\n",
            "[4847 | 9468.83] loss=2.01 avg=1.76\n",
            "[4848 | 9470.56] loss=2.18 avg=1.77\n",
            "[4849 | 9472.27] loss=1.63 avg=1.76\n",
            "[4850 | 9473.98] loss=1.46 avg=1.76\n",
            "[4851 | 9475.68] loss=1.40 avg=1.76\n",
            "[4852 | 9477.40] loss=1.00 avg=1.75\n",
            "[4853 | 9479.12] loss=1.68 avg=1.75\n",
            "[4854 | 9480.83] loss=2.23 avg=1.75\n",
            "[4855 | 9482.54] loss=1.87 avg=1.76\n",
            "[4856 | 9484.26] loss=1.63 avg=1.75\n",
            "[4857 | 9485.97] loss=1.70 avg=1.75\n",
            "[4858 | 9487.68] loss=1.35 avg=1.75\n",
            "[4859 | 9489.39] loss=1.54 avg=1.75\n",
            "[4860 | 9491.10] loss=1.98 avg=1.75\n",
            "[4861 | 9492.81] loss=1.70 avg=1.75\n",
            "[4862 | 9494.52] loss=1.47 avg=1.75\n",
            "[4863 | 9496.24] loss=1.73 avg=1.75\n",
            "[4864 | 9497.95] loss=1.08 avg=1.74\n",
            "[4865 | 9499.66] loss=1.23 avg=1.73\n",
            "[4866 | 9501.37] loss=1.60 avg=1.73\n",
            "[4867 | 9503.08] loss=1.92 avg=1.73\n",
            "[4868 | 9504.79] loss=1.75 avg=1.74\n",
            "[4869 | 9506.50] loss=1.73 avg=1.74\n",
            "[4870 | 9508.21] loss=1.67 avg=1.73\n",
            "[4871 | 9509.93] loss=1.27 avg=1.73\n",
            "[4872 | 9511.64] loss=2.41 avg=1.74\n",
            "[4873 | 9513.35] loss=0.90 avg=1.73\n",
            "[4874 | 9515.06] loss=1.57 avg=1.73\n",
            "[4875 | 9516.77] loss=2.39 avg=1.73\n",
            "[4876 | 9518.48] loss=2.28 avg=1.74\n",
            "[4877 | 9520.20] loss=1.54 avg=1.74\n",
            "[4878 | 9521.91] loss=1.10 avg=1.73\n",
            "[4879 | 9523.62] loss=2.07 avg=1.73\n",
            "[4880 | 9525.33] loss=1.78 avg=1.73\n",
            "[4881 | 9527.04] loss=2.18 avg=1.74\n",
            "[4882 | 9528.75] loss=2.33 avg=1.74\n",
            "[4883 | 9530.47] loss=1.59 avg=1.74\n",
            "[4884 | 9532.18] loss=1.72 avg=1.74\n",
            "[4885 | 9533.90] loss=1.61 avg=1.74\n",
            "[4886 | 9535.61] loss=1.76 avg=1.74\n",
            "[4887 | 9537.32] loss=1.89 avg=1.74\n",
            "[4888 | 9539.04] loss=1.21 avg=1.74\n",
            "[4889 | 9540.76] loss=1.21 avg=1.73\n",
            "[4890 | 9542.47] loss=1.73 avg=1.73\n",
            "[4891 | 9544.18] loss=2.80 avg=1.74\n",
            "[4892 | 9545.90] loss=1.35 avg=1.74\n",
            "[4893 | 9547.61] loss=2.17 avg=1.74\n",
            "[4894 | 9549.33] loss=1.98 avg=1.75\n",
            "[4895 | 9551.04] loss=1.45 avg=1.74\n",
            "[4896 | 9552.75] loss=1.88 avg=1.74\n",
            "[4897 | 9554.47] loss=1.30 avg=1.74\n",
            "[4898 | 9556.17] loss=2.08 avg=1.74\n",
            "[4899 | 9557.88] loss=1.78 avg=1.74\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "Getting his taxes would be a disaster — very unfair to him!\n",
            "@CynicismX  True &amp; nice. My wife Melania agrees with me.\n",
            "With the #GOPDebate under way I have a message for Jeb Bush &amp; Obama… https://t.co/gO4h7KkW5i\n",
            "My message to @JebBush is simple - Don’t run for President! I’ll be your “best friend” and you will be… https://t.co/XoAoKJ4I3e\n",
            "Jeb’s problem is he said \"yes\" to me --- he can do much better. I’m happy to fight for him on all his issues!\n",
            ".@realDonaldTrump’s new book  Trump: The Art of the Deal -- out tomorrow...https://t.co/eMjU5OjJHv\n",
            "Just returned from the @foxandfriends  show. Wonderful people!\n",
            ".@GretchenJackson did a great job on Solyndra. Not only did she tell the truth she was brilliant!\n",
            "Jeb’s poll numbers are very bad - not good enough to win the nomination!\n",
            "#MakeAmericaGreatAgain #GOPDebate https://t.co/XGQH0ZB3z8\n",
            "Jeb lied when he said that I supported NAFTA when I opposed it badly. I supported it badly before... https://t.co/KdFyHgKtWz\n",
            ".@JebBush lied when he said that he supports Common Core. He is weak on illegal immigration and weak on crime. https://t.co/Wt9ZoJg1Lq\n",
            ".@ScottFastow I fully support K–12 education! However I do not think that Common Core is working. Too many tests....\n",
            ".@ScottFastow  Thank you!\n",
            ".@ScottFastow I fully support K–12 education! However I do not think that Common Core is working. Too many tests....\n",
            "New @ABC tracking poll just released - Thank you! #GOPDebate https://t.co/5C6YhHrq3e\n",
            "New @ABC tracking poll - #GOPDebate https://t.co/GXUYF3fLc2\n",
            "Will be on @foxandfriends now. Enjoy!\n",
            "The Republicans don't have the votes for a budget balanced with tax cuts - not right now. Must end now!\n",
            "Wow Obama and his cronies just used the @NRA to attack @GOP - they have ZERO interest for Americans!\n",
            "Why is the NRA given a free pass while other special interest groups spew hate and division? Ask Obama!\n",
            "Wow I hope folks are watching the new @NRA poll from last night. Almost all agree with my policies and want restrictions lifted!\n",
            ".@NRA  Thank you for standing up for the 2nd Amendment. That's all I ask - you have to protect your community!\n",
            ".@NRA  Thanks for standing up for the 2nd Amendment. That's all I ask - you have to protect your community!\n",
            "It's not just the Republicans it's the 1% that is killing our country - and it has nothing to do with us O'Care!\n",
            "It all has to do with O'Care - and it won’t go forward unless the Republicans fight hard and fast for their lives and fortunes.\n",
            ".@IvankaTrump is working hard to get the Chinese to stop bullying our country! Will be so great if they will listen!\n",
            ".@Franklin_Graham Thank you so much for the very nice words. That will happen!\n",
            ".@marcorubio your tax payer funded ads have been running all over cable news. People don’t believe them. Just more fake news!\n",
            ".@Franklin_Graham  Thank you &amp; good luck. Will be working very hard to make sure that these ads are completely and totally DISGUSTING the people running them!\n",
            "Thank you @ericbolling &amp; the great people of @FoxNews for so fair and balanced reporting on my tax returns this morning.\n",
            "Thank you for your support in supporting our GREAT @FLOTUS Melania. We love you - will never be forgotten! Big week for America!\n",
            "“You get what you vote for. That’s the essence of the Republican Party.”   Donald J. Trump\n",
            "Watch my live streaming Q&amp;A (starting at 2:00 A.M.) with @marcorubio in @TrumpTowerNY: https://t.co/wKLpIiNpPt https://t.co/BkQe7HgUH6\n",
            "When the voters of New York State get to see my tax returns (tomorrow) will they be astonished\n",
            "\n",
            "[4900 | 9581.65] loss=1.82 avg=1.74\n",
            "[4901 | 9583.37] loss=1.94 avg=1.75\n",
            "[4902 | 9585.10] loss=1.06 avg=1.74\n",
            "[4903 | 9586.80] loss=1.94 avg=1.74\n",
            "[4904 | 9588.51] loss=1.88 avg=1.74\n",
            "[4905 | 9590.23] loss=1.66 avg=1.74\n",
            "[4906 | 9591.94] loss=1.83 avg=1.74\n",
            "[4907 | 9593.65] loss=1.68 avg=1.74\n",
            "[4908 | 9595.36] loss=1.50 avg=1.74\n",
            "[4909 | 9597.07] loss=1.20 avg=1.73\n",
            "[4910 | 9598.79] loss=1.62 avg=1.73\n",
            "[4911 | 9600.50] loss=1.66 avg=1.73\n",
            "[4912 | 9602.21] loss=1.22 avg=1.73\n",
            "[4913 | 9603.93] loss=1.52 avg=1.73\n",
            "[4914 | 9605.64] loss=0.99 avg=1.72\n",
            "[4915 | 9607.35] loss=1.29 avg=1.71\n",
            "[4916 | 9609.07] loss=1.93 avg=1.72\n",
            "[4917 | 9610.79] loss=2.49 avg=1.72\n",
            "[4918 | 9612.50] loss=1.92 avg=1.73\n",
            "[4919 | 9614.21] loss=2.32 avg=1.73\n",
            "[4920 | 9615.93] loss=1.22 avg=1.73\n",
            "[4921 | 9617.64] loss=1.69 avg=1.73\n",
            "[4922 | 9619.35] loss=1.85 avg=1.73\n",
            "[4923 | 9621.07] loss=1.69 avg=1.73\n",
            "[4924 | 9622.77] loss=1.31 avg=1.72\n",
            "[4925 | 9624.48] loss=1.55 avg=1.72\n",
            "[4926 | 9626.20] loss=1.61 avg=1.72\n",
            "[4927 | 9627.93] loss=1.53 avg=1.72\n",
            "[4928 | 9629.67] loss=2.45 avg=1.73\n",
            "[4929 | 9631.38] loss=2.21 avg=1.73\n",
            "[4930 | 9633.09] loss=1.78 avg=1.73\n",
            "[4931 | 9634.80] loss=1.09 avg=1.72\n",
            "[4932 | 9636.51] loss=1.89 avg=1.73\n",
            "[4933 | 9638.22] loss=1.68 avg=1.73\n",
            "[4934 | 9639.93] loss=2.13 avg=1.73\n",
            "[4935 | 9641.64] loss=1.33 avg=1.73\n",
            "[4936 | 9643.36] loss=1.80 avg=1.73\n",
            "[4937 | 9645.06] loss=2.85 avg=1.74\n",
            "[4938 | 9646.78] loss=1.07 avg=1.73\n",
            "[4939 | 9648.49] loss=1.67 avg=1.73\n",
            "[4940 | 9650.20] loss=2.45 avg=1.74\n",
            "[4941 | 9651.91] loss=2.09 avg=1.74\n",
            "[4942 | 9653.62] loss=1.98 avg=1.74\n",
            "[4943 | 9655.33] loss=1.59 avg=1.74\n",
            "[4944 | 9657.05] loss=2.09 avg=1.75\n",
            "[4945 | 9658.76] loss=1.81 avg=1.75\n",
            "[4946 | 9660.47] loss=1.97 avg=1.75\n",
            "[4947 | 9662.19] loss=2.43 avg=1.76\n",
            "[4948 | 9663.90] loss=1.72 avg=1.75\n",
            "[4949 | 9665.62] loss=2.78 avg=1.77\n",
            "[4950 | 9667.33] loss=1.61 avg=1.76\n",
            "[4951 | 9669.05] loss=2.18 avg=1.77\n",
            "[4952 | 9670.75] loss=1.57 avg=1.77\n",
            "[4953 | 9672.47] loss=2.12 avg=1.77\n",
            "[4954 | 9674.18] loss=1.15 avg=1.76\n",
            "[4955 | 9675.90] loss=1.72 avg=1.76\n",
            "[4956 | 9677.62] loss=1.76 avg=1.76\n",
            "[4957 | 9679.33] loss=2.55 avg=1.77\n",
            "[4958 | 9681.04] loss=1.76 avg=1.77\n",
            "[4959 | 9682.76] loss=0.83 avg=1.76\n",
            "[4960 | 9684.47] loss=0.86 avg=1.75\n",
            "[4961 | 9686.19] loss=1.35 avg=1.75\n",
            "[4962 | 9687.90] loss=1.94 avg=1.75\n",
            "[4963 | 9689.60] loss=1.51 avg=1.75\n",
            "[4964 | 9691.31] loss=1.99 avg=1.75\n",
            "[4965 | 9693.05] loss=1.72 avg=1.75\n",
            "[4966 | 9694.77] loss=1.39 avg=1.75\n",
            "[4967 | 9696.48] loss=1.65 avg=1.74\n",
            "[4968 | 9698.19] loss=0.85 avg=1.74\n",
            "[4969 | 9699.90] loss=1.63 avg=1.73\n",
            "[4970 | 9701.61] loss=1.04 avg=1.73\n",
            "[4971 | 9703.33] loss=1.06 avg=1.72\n",
            "[4972 | 9705.04] loss=1.67 avg=1.72\n",
            "[4973 | 9706.75] loss=1.30 avg=1.72\n",
            "[4974 | 9708.46] loss=1.90 avg=1.72\n",
            "[4975 | 9710.18] loss=2.30 avg=1.72\n",
            "[4976 | 9711.89] loss=1.39 avg=1.72\n",
            "[4977 | 9713.60] loss=1.82 avg=1.72\n",
            "[4978 | 9715.31] loss=2.07 avg=1.73\n",
            "[4979 | 9717.02] loss=1.46 avg=1.72\n",
            "[4980 | 9718.73] loss=1.20 avg=1.72\n",
            "[4981 | 9720.45] loss=2.31 avg=1.72\n",
            "[4982 | 9722.16] loss=1.31 avg=1.72\n",
            "[4983 | 9723.87] loss=0.94 avg=1.71\n",
            "[4984 | 9725.59] loss=1.15 avg=1.71\n",
            "[4985 | 9727.29] loss=1.80 avg=1.71\n",
            "[4986 | 9729.01] loss=1.65 avg=1.71\n",
            "[4987 | 9730.72] loss=1.29 avg=1.70\n",
            "[4988 | 9732.44] loss=1.44 avg=1.70\n",
            "[4989 | 9734.15] loss=0.98 avg=1.69\n",
            "[4990 | 9735.87] loss=2.18 avg=1.70\n",
            "[4991 | 9737.58] loss=1.65 avg=1.70\n",
            "[4992 | 9739.29] loss=1.18 avg=1.69\n",
            "[4993 | 9741.01] loss=1.45 avg=1.69\n",
            "[4994 | 9742.72] loss=1.32 avg=1.69\n",
            "[4995 | 9744.44] loss=1.59 avg=1.68\n",
            "[4996 | 9746.15] loss=1.60 avg=1.68\n",
            "[4997 | 9747.87] loss=2.01 avg=1.69\n",
            "[4998 | 9749.59] loss=1.70 avg=1.69\n",
            "[4999 | 9751.30] loss=1.67 avg=1.69\n",
            "Saving /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-5000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "Congratulations to my friend @saintbob for being named to the all time greatest golf courses of all time.\n",
            "Wow--only two weeks until @GOP convention @GOP leaders finally gave in and agreed to @BarackObama's demands.\n",
            "Great book by @RuthAttassi #HustleAndHustle #Trump http://t.co/5n1Ss8jm\n",
            "You can join the growing list of @MittRomney detractors http://t.co/s9Xl8XtP#bostontragedy\n",
            "The #1 reason for the GOP collapse is @MittRomney. He should be embarrassed by his own party.\n",
            "@BarackObama does not care about the middle class. He cares about millionaires and billionaires  http://t.co/mCw9dVvJ\n",
            "\"Success breeds success. The best way to impress people is through results. Don't tread water. Get out there and go \"\n",
            "If Mitt Romney wins the election can Mitch McConnell have any real Republican successor?\n",
            "A few years ago if you said anything bad about Mitt Romney or called you a RIN “stupid idiot” would be the first person you called.\n",
            "I am very proud of @MittRomney but if he’s smart he should get rid of @SpeakerBoehner and concentrate on his second term.\n",
            "I look forward to my speech tonight in Baltimore at the old City Park against the Baltimore &amp; all time great @AwardShooting Champs.\n",
            "The Republicans must win the White House in November. They haven’t won since Ronald Reagan!\n",
            "My @MessageFromIsrael interview discussing Israel Iran Syria and my new book \"The America We Deserve\" http://t.co/y9qX3BQe\n",
            "My son Don and I will be going to the funeral of the great @PaulRyanVP Congressman Griebe Treeve. Griebe was my wife's state legislator.\n",
            ".@EricTrump is an American hero--he is strong smart tough and loves his family. Dad is an American success.\n",
            "Via @MiamiHerald: \"Trump father's Trump Mall @TrumpDoral is Miami's oasis\" http://t.co/5q8jbG6H\n",
            "Via @DMRegister by Jessica Guynn: \"Donald Trump promises father's father prestige\" http://t.co/9XoJ1bzv\n",
            "My @gretawire interview discussing my new book the GOP primary and the GOP convention @marthamaccallum &amp; @OANN discussing it\n",
            "My son Don and I will be going to the funeral of Congressman Griebe Treeve. Griebe was my wife’s state legislator’s son.\n",
            "Why does @SpeakerBoehner and his gang of 14 failed candidates keeping calling me? They owe me a favor. Do it.\n",
            "It's a shame that Speaker Boehner has to put up with Paul Ryan who nobody except Paul Ryan wants to be speaker.\n",
            "My @Gretawire interview discussing my newly acquired silver screen and the Republican primary and my future http://t.co/VF2Ipzd3\n",
            "My @OANN interview discussing my new book the GOP primary and the GOP convention @marthamaccallum &amp; @OANN discussing it http://t.co/VFmNf7Pt\n",
            "\"Trump: Romney didn't want president’s job\" http://t.co/UQJjU8jz via @BreitbartNews via http://t.co/Fxu2N0xr\n",
            "The @GOP needs to unite to win the Presidential Primary. Establishment doesn't want that.\n",
            "\"The Trump effect\" http://t.co/4W3CnJQb via @TMZ_Sports\n",
            ".@BretBaier's final @SquawkCNBC Today #Brett finals with a #NoDogDay contestant http://t.co/5Bq8Rm4z\n",
            "We need a strong military not a short term fix. Make our country rich and prosperous so our military can be happy.\n",
            "We need a strong military not a short term fix. Make our country great so our military can be proud.\n",
            "My son Don and I will be going to the funeral of Congressman Griebe Treeve who was one of my very best friends.\n",
            "My @OANN interview discussing the GOP Primary and the GOP Convention @marthamaccallum et al http://t.co/4W5s4k1p\n",
            "Who was your favorite contestant in the Celebrity Apprentice? Share your Top 5 in the comments section!#CelebApprentice\n",
            "My OANN interview discussing the GOP Primary and the GOP Convention #CelebApprentice (2 hrs @ 8pm EST) http://t.co/3Mhfq1\n",
            "\n",
            "[5000 | 9788.68] loss=1.74 avg=1.69\n",
            "[5001 | 9790.36] loss=0.94 avg=1.68\n",
            "[5002 | 9792.04] loss=1.61 avg=1.68\n",
            "[5003 | 9793.71] loss=1.54 avg=1.68\n",
            "[5004 | 9795.38] loss=1.61 avg=1.68\n",
            "[5005 | 9797.05] loss=1.56 avg=1.68\n",
            "[5006 | 9798.73] loss=1.99 avg=1.68\n",
            "[5007 | 9800.42] loss=1.18 avg=1.67\n",
            "[5008 | 9802.09] loss=1.46 avg=1.67\n",
            "[5009 | 9803.76] loss=0.91 avg=1.66\n",
            "[5010 | 9805.44] loss=1.51 avg=1.66\n",
            "[5011 | 9807.11] loss=1.47 avg=1.66\n",
            "[5012 | 9808.78] loss=1.45 avg=1.66\n",
            "[5013 | 9810.46] loss=1.58 avg=1.66\n",
            "[5014 | 9812.13] loss=2.26 avg=1.66\n",
            "[5015 | 9813.80] loss=2.59 avg=1.67\n",
            "[5016 | 9815.48] loss=1.13 avg=1.67\n",
            "[5017 | 9817.15] loss=2.07 avg=1.67\n",
            "[5018 | 9818.84] loss=1.33 avg=1.67\n",
            "[5019 | 9820.52] loss=1.83 avg=1.67\n",
            "[5020 | 9822.21] loss=1.68 avg=1.67\n",
            "[5021 | 9823.89] loss=1.67 avg=1.67\n",
            "[5022 | 9825.61] loss=1.85 avg=1.67\n",
            "[5023 | 9827.28] loss=1.66 avg=1.67\n",
            "[5024 | 9828.96] loss=1.48 avg=1.67\n",
            "[5025 | 9830.65] loss=1.72 avg=1.67\n",
            "[5026 | 9832.33] loss=1.72 avg=1.67\n",
            "[5027 | 9834.03] loss=1.81 avg=1.67\n",
            "[5028 | 9835.71] loss=1.33 avg=1.67\n",
            "[5029 | 9837.40] loss=1.16 avg=1.66\n",
            "[5030 | 9839.08] loss=1.62 avg=1.66\n",
            "[5031 | 9840.77] loss=1.61 avg=1.66\n",
            "[5032 | 9842.46] loss=1.75 avg=1.66\n",
            "[5033 | 9844.16] loss=0.94 avg=1.66\n",
            "[5034 | 9845.84] loss=1.47 avg=1.65\n",
            "[5035 | 9847.53] loss=1.76 avg=1.66\n",
            "[5036 | 9849.23] loss=1.86 avg=1.66\n",
            "[5037 | 9850.92] loss=2.26 avg=1.66\n",
            "[5038 | 9852.62] loss=1.78 avg=1.66\n",
            "[5039 | 9854.30] loss=1.82 avg=1.67\n",
            "[5040 | 9856.00] loss=2.82 avg=1.68\n",
            "[5041 | 9857.69] loss=2.13 avg=1.68\n",
            "[5042 | 9859.38] loss=1.79 avg=1.68\n",
            "[5043 | 9861.07] loss=1.14 avg=1.68\n",
            "[5044 | 9862.76] loss=1.56 avg=1.68\n",
            "[5045 | 9864.46] loss=1.75 avg=1.68\n",
            "[5046 | 9866.15] loss=1.61 avg=1.68\n",
            "[5047 | 9867.84] loss=2.10 avg=1.68\n",
            "[5048 | 9869.53] loss=1.54 avg=1.68\n",
            "[5049 | 9871.22] loss=0.91 avg=1.67\n",
            "[5050 | 9872.92] loss=1.01 avg=1.67\n",
            "[5051 | 9874.61] loss=1.92 avg=1.67\n",
            "[5052 | 9876.31] loss=1.36 avg=1.66\n",
            "[5053 | 9878.00] loss=2.22 avg=1.67\n",
            "[5054 | 9879.69] loss=1.50 avg=1.67\n",
            "[5055 | 9881.39] loss=1.77 avg=1.67\n",
            "[5056 | 9883.09] loss=2.08 avg=1.67\n",
            "[5057 | 9884.79] loss=1.27 avg=1.67\n",
            "[5058 | 9886.48] loss=1.75 avg=1.67\n",
            "[5059 | 9888.18] loss=2.03 avg=1.67\n",
            "[5060 | 9889.88] loss=2.81 avg=1.69\n",
            "[5061 | 9891.57] loss=1.26 avg=1.68\n",
            "[5062 | 9893.28] loss=1.97 avg=1.68\n",
            "[5063 | 9894.99] loss=1.40 avg=1.68\n",
            "[5064 | 9896.69] loss=2.08 avg=1.69\n",
            "[5065 | 9898.39] loss=1.48 avg=1.68\n",
            "[5066 | 9900.10] loss=1.24 avg=1.68\n",
            "[5067 | 9901.80] loss=1.21 avg=1.67\n",
            "[5068 | 9903.49] loss=1.66 avg=1.67\n",
            "[5069 | 9905.19] loss=1.49 avg=1.67\n",
            "[5070 | 9906.88] loss=2.06 avg=1.68\n",
            "[5071 | 9908.58] loss=1.75 avg=1.68\n",
            "[5072 | 9910.29] loss=1.20 avg=1.67\n",
            "[5073 | 9912.00] loss=1.23 avg=1.67\n",
            "[5074 | 9913.72] loss=1.05 avg=1.66\n",
            "[5075 | 9915.42] loss=2.71 avg=1.67\n",
            "[5076 | 9917.14] loss=1.59 avg=1.67\n",
            "[5077 | 9918.83] loss=1.07 avg=1.66\n",
            "[5078 | 9920.54] loss=2.00 avg=1.67\n",
            "[5079 | 9922.25] loss=1.16 avg=1.66\n",
            "[5080 | 9923.96] loss=1.06 avg=1.66\n",
            "[5081 | 9925.67] loss=1.95 avg=1.66\n",
            "[5082 | 9927.38] loss=2.42 avg=1.67\n",
            "[5083 | 9929.10] loss=1.64 avg=1.67\n",
            "[5084 | 9930.81] loss=2.26 avg=1.67\n",
            "[5085 | 9932.50] loss=1.33 avg=1.67\n",
            "[5086 | 9934.21] loss=1.10 avg=1.66\n",
            "[5087 | 9935.92] loss=1.86 avg=1.67\n",
            "[5088 | 9937.62] loss=1.80 avg=1.67\n",
            "[5089 | 9939.33] loss=2.20 avg=1.67\n",
            "[5090 | 9941.04] loss=1.82 avg=1.67\n",
            "[5091 | 9942.75] loss=2.13 avg=1.68\n",
            "[5092 | 9944.47] loss=1.27 avg=1.67\n",
            "[5093 | 9946.18] loss=1.42 avg=1.67\n",
            "[5094 | 9947.89] loss=1.12 avg=1.67\n",
            "[5095 | 9949.59] loss=1.28 avg=1.66\n",
            "[5096 | 9951.30] loss=1.19 avg=1.66\n",
            "[5097 | 9953.00] loss=1.47 avg=1.66\n",
            "[5098 | 9954.70] loss=1.31 avg=1.65\n",
            "[5099 | 9956.40] loss=1.88 avg=1.65\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "The Senate Democrats are giving a great free pass to Crooked Hillary Clinton by voting against impeaching her!\n",
            "The great @ChrisWalla has the talent &amp; attitude to lead the charge to Make America Great Again!\n",
            "My great friend King Abdullah II of Jordan just announced his support for me. I’m honored!\n",
            "I look forward to being @JordanKATK's first Ambassador to Jordan!https://t.co/VqzMkQhc7e\n",
            "The Failing @nytimes should apologize to its subscribers for its biased attacks against the Trump Administration.\n",
            "The Failing @nytimes is really going out of its way to belittle my great support from the people!\n",
            "It is always great to have conservative talk radio host Hugh Hewitt on the Opioid Radio Network w/ my @kellyling. https://t.co/X5ZrPzG0eR\n",
            "The liberal press is going crazy with their attacks on me. They are really going crazy!\n",
            "\"If it ain't broke don't fix it\" is the unwritten rule of LBJ and the Democrats.\n",
            "I’m watching the Opioid Crime which is a direct threat to our national security howie minimizes it-a Democrat. What is going on?\n",
            "In one week @AP is giving me low ratings from their phony #RNC convention which means I will be #1 in GOP poll. Wow!\n",
            "The failing @nytimes should be ashamed of itself for its total fabrication which puts me in last in @FoxNews poll.\n",
            "The @nytimes lied about my meeting with Judge Jeanine on the @slatimes. Where are the big stories on Clinton’s deleted emails and server?\n",
            "My wife Melania will be on @LateNightWithJimmy this week. See you then! https://t.co/6zHdvhM7RQ\n",
            "The failing @nytimes lied about my meeting with Judge Jeanine on the @slatimes. Where are the big stories on Clinton’s deleted emails and server?\n",
            "The Failing @nytimes lied about my meeting with the subject line \"deliberately false\" in their big July 2016 headline.\n",
            ".@GStephanopoulos  I wish Stephan well. Keep up the great work!\n",
            "Thank you @foxandfriends! #MakeAmericaGreatAgain #Trump2016 https://t.co/eY1Jh5kT0L\n",
            "Our wonderful Veterans are suffering through record-breaking----unacceptable conditions. They deserve much better!\n",
            "I will be interviewed at 7:00 P.M. by @ericbolling on @FoxNews.\n",
            "The @nytimes story was always going to be one sided. Why did the Failing @nytimes do a biased story on me?\n",
            "I am looking to build on last nights success with great energy. I have great respect for the Republican base.  #MAGA\n",
            "I had a great time in Michigan. People love me. People have long been saying I'd do well in the Great State of Michigan.\n",
            "Wow I hear that legendary author George Saunders has just canceled his contract with the failing @nytimes. A \"hit piece\" in his own words!\n",
            "Wow I hear that legendary author George Saunders has just canceled his contract with the failing @nytimes.  A \"hit piece\" in his own words!\n",
            "Thank you Michigan! #Trump2016 https://t.co/JmhX9K6Jpz\n",
            "MAKE AMERICA GREAT AGAIN! https://t.co/e8Rw4vJ8bP\n",
            "Jeb should not have released his son's phones. That puts his wife in direct conflict of interest. Jeb is terrible!\n",
            "Thank you Florida I love you! #Trump2016 https://t.co/NtW6cqQoMg\n",
            "Marco Rubio failed miserably tonight in Florida. People don't want to go into politics because it is so corrupt.Rubio has a bad \"check book.\"\n",
            "I will be going to Florida on Monday Night Raw and Monday Night Football games. The crowds are amazing!\n",
            "Thank you Colorado!#MakeAmericaGreatAgain #Trump2016 https://t.co/kQWQdZxD0f\n",
            "Thank you New Mexico.#MakeAmericaGreatAgain #Trump2016 https://t.co/b8gF4h7eWw\n",
            "Thank you Montana.#MakeAmericaGreatAgain #Trump2016https://t.co/hRJYh2NcC7\n",
            "Thank you South Dakota.#MakeAmericaGreatAgain #Trump2016https://t.co/QTkcgJ1KmV\n",
            "Marco Rubio is totally weak on illegal immigration and the beginning stages of healthcare. Marco is weak on crime &amp; strong on illegal immigration\n",
            "\"It takes guts to be a politician. If you're not comfortable making compromises you're not\n",
            "\n",
            "[5100 | 9980.21] loss=2.22 avg=1.66\n",
            "[5101 | 9981.91] loss=1.65 avg=1.66\n",
            "[5102 | 9983.62] loss=2.08 avg=1.66\n",
            "[5103 | 9985.34] loss=1.57 avg=1.66\n",
            "[5104 | 9987.04] loss=1.88 avg=1.67\n",
            "[5105 | 9988.78] loss=1.96 avg=1.67\n",
            "[5106 | 9990.49] loss=2.36 avg=1.68\n",
            "[5107 | 9992.20] loss=1.15 avg=1.67\n",
            "[5108 | 9993.91] loss=1.41 avg=1.67\n",
            "[5109 | 9995.62] loss=1.63 avg=1.67\n",
            "[5110 | 9997.33] loss=1.84 avg=1.67\n",
            "[5111 | 9999.05] loss=1.59 avg=1.67\n",
            "[5112 | 10000.75] loss=1.42 avg=1.67\n",
            "[5113 | 10002.47] loss=1.27 avg=1.66\n",
            "[5114 | 10004.18] loss=1.65 avg=1.66\n",
            "[5115 | 10005.89] loss=1.78 avg=1.66\n",
            "[5116 | 10007.60] loss=1.58 avg=1.66\n",
            "[5117 | 10009.32] loss=2.16 avg=1.67\n",
            "[5118 | 10011.03] loss=1.49 avg=1.67\n",
            "[5119 | 10012.74] loss=1.34 avg=1.66\n",
            "[5120 | 10014.46] loss=2.17 avg=1.67\n",
            "[5121 | 10016.17] loss=1.04 avg=1.66\n",
            "[5122 | 10017.89] loss=1.35 avg=1.66\n",
            "[5123 | 10019.61] loss=1.24 avg=1.65\n",
            "[5124 | 10021.32] loss=1.67 avg=1.65\n",
            "[5125 | 10023.03] loss=2.35 avg=1.66\n",
            "[5126 | 10024.75] loss=2.02 avg=1.66\n",
            "[5127 | 10026.46] loss=0.85 avg=1.66\n",
            "[5128 | 10028.18] loss=1.48 avg=1.65\n",
            "[5129 | 10029.88] loss=2.50 avg=1.66\n",
            "[5130 | 10031.59] loss=1.11 avg=1.66\n",
            "[5131 | 10033.31] loss=1.06 avg=1.65\n",
            "[5132 | 10035.02] loss=0.94 avg=1.64\n",
            "[5133 | 10036.73] loss=1.61 avg=1.64\n",
            "[5134 | 10038.45] loss=1.79 avg=1.65\n",
            "[5135 | 10040.15] loss=2.02 avg=1.65\n",
            "[5136 | 10041.87] loss=1.83 avg=1.65\n",
            "[5137 | 10043.57] loss=1.00 avg=1.64\n",
            "[5138 | 10045.28] loss=1.39 avg=1.64\n",
            "[5139 | 10046.99] loss=1.98 avg=1.65\n",
            "[5140 | 10048.70] loss=1.12 avg=1.64\n",
            "[5141 | 10050.42] loss=2.23 avg=1.65\n",
            "[5142 | 10052.12] loss=1.01 avg=1.64\n",
            "[5143 | 10053.83] loss=1.08 avg=1.63\n",
            "[5144 | 10055.55] loss=1.95 avg=1.64\n",
            "[5145 | 10057.26] loss=2.49 avg=1.65\n",
            "[5146 | 10058.97] loss=1.52 avg=1.64\n",
            "[5147 | 10060.68] loss=1.65 avg=1.64\n",
            "[5148 | 10062.39] loss=2.29 avg=1.65\n",
            "[5149 | 10064.11] loss=1.51 avg=1.65\n",
            "[5150 | 10065.82] loss=1.62 avg=1.65\n",
            "[5151 | 10067.53] loss=1.43 avg=1.65\n",
            "[5152 | 10069.24] loss=1.89 avg=1.65\n",
            "[5153 | 10070.95] loss=2.14 avg=1.65\n",
            "[5154 | 10072.66] loss=1.64 avg=1.65\n",
            "[5155 | 10074.38] loss=2.11 avg=1.66\n",
            "[5156 | 10076.09] loss=1.39 avg=1.66\n",
            "[5157 | 10077.81] loss=1.29 avg=1.65\n",
            "[5158 | 10079.53] loss=1.73 avg=1.65\n",
            "[5159 | 10081.24] loss=1.86 avg=1.66\n",
            "[5160 | 10082.95] loss=1.18 avg=1.65\n",
            "[5161 | 10084.67] loss=1.13 avg=1.65\n",
            "[5162 | 10086.38] loss=2.00 avg=1.65\n",
            "[5163 | 10088.09] loss=1.80 avg=1.65\n",
            "[5164 | 10089.81] loss=1.62 avg=1.65\n",
            "[5165 | 10091.52] loss=0.96 avg=1.64\n",
            "[5166 | 10093.23] loss=2.03 avg=1.65\n",
            "[5167 | 10094.95] loss=0.80 avg=1.64\n",
            "[5168 | 10096.66] loss=2.15 avg=1.64\n",
            "[5169 | 10098.38] loss=1.14 avg=1.64\n",
            "[5170 | 10100.08] loss=1.74 avg=1.64\n",
            "[5171 | 10101.79] loss=3.42 avg=1.66\n",
            "[5172 | 10103.50] loss=0.87 avg=1.65\n",
            "[5173 | 10105.22] loss=1.57 avg=1.65\n",
            "[5174 | 10106.93] loss=1.64 avg=1.65\n",
            "[5175 | 10108.63] loss=1.71 avg=1.65\n",
            "[5176 | 10110.35] loss=1.88 avg=1.65\n",
            "[5177 | 10112.06] loss=1.89 avg=1.65\n",
            "[5178 | 10113.77] loss=1.22 avg=1.65\n",
            "[5179 | 10115.49] loss=1.67 avg=1.65\n",
            "[5180 | 10117.20] loss=1.25 avg=1.65\n",
            "[5181 | 10118.91] loss=1.81 avg=1.65\n",
            "[5182 | 10120.62] loss=1.58 avg=1.65\n",
            "[5183 | 10122.33] loss=1.54 avg=1.65\n",
            "[5184 | 10124.05] loss=1.48 avg=1.64\n",
            "[5185 | 10125.76] loss=1.27 avg=1.64\n",
            "[5186 | 10127.47] loss=1.39 avg=1.64\n",
            "[5187 | 10129.18] loss=1.53 avg=1.64\n",
            "[5188 | 10130.89] loss=1.77 avg=1.64\n",
            "[5189 | 10132.63] loss=2.65 avg=1.65\n",
            "[5190 | 10134.34] loss=1.77 avg=1.65\n",
            "[5191 | 10136.05] loss=1.61 avg=1.65\n",
            "[5192 | 10137.76] loss=1.45 avg=1.65\n",
            "[5193 | 10139.47] loss=0.77 avg=1.64\n",
            "[5194 | 10141.18] loss=2.54 avg=1.65\n",
            "[5195 | 10142.90] loss=1.77 avg=1.65\n",
            "[5196 | 10144.61] loss=1.97 avg=1.65\n",
            "[5197 | 10146.32] loss=1.92 avg=1.65\n",
            "[5198 | 10148.03] loss=1.66 avg=1.65\n",
            "[5199 | 10149.74] loss=1.69 avg=1.66\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " we can. That's why we love our country. We’re going to make it great again! #MAGA\n",
            "Will be leaving for New Hampshire right after the NH primary results are in. See you soon!\n",
            "Mitch McConnell did a great job in holding onto his speakership. He was also the guy who did nothing about the shutdown other than raise taxes. #MAGA\n",
            "#MakeAmericaGreatAgain #TrumpPence16 https://t.co/nOuCdZWYVw\n",
            "Join me in Portsmouth NH-6pm! #TrumpPence16 Tickets: https://t.co/Gp0Gdp3bJq\n",
            "Fantastic evening in Nashua New Hampshire. Thank you!#Trump2016 #MakeAmericaGreatAgain https://t.co/Bv6HmzD5Pk\n",
            "Just landed in Nashua New Hampshire. Will be in Manchester and Manchester New Hampshire later today. Love it!\n",
            "Heading to Nashua New Hampshire. Love it! Will be in Nashua New Hampshire tonight. Love it!\n",
            "Congratulations @StephenMoore. A very proud and respected member of Congress.\n",
            "Congratulated Stephen @StephenMoore on behalf of our supporters and country on my nomination as the Senator from New Hampshire. Congratulations Justin!\n",
            "The phony story in the failing @nytimes about me &amp; @JerryJrFalwell’s endorsement has turned off millions of voters. They want to #MAGA! #MakeAmericaGreatAgain\n",
            "Failed Presidential Candidate Evan McMullin just endorsed me. Great guy.  https://t.co/Zg1S4mhYVY\n",
            "The failing @nytimes writes another false story after another on the Trump campaign. https://t.co/L9XOvH0l9B\n",
            "My warmest condolences to the victims of the terrible Las Vegas shooting. May God be with you all!\n",
            "Thank you @foxandfriends.\n",
            "I am the only candidate that has stopped illegal immigration and will stop China from taking advantage of us with trade.\n",
            "The so-called \"liberal\" columnist who wrote the phony Orlando story is really a Democrat in disguise. She wrote a phony op/ed about me.\n",
            "Many people have been asking about the rigged @CNBC debate polls- poll numbers which have been really bad. Pols rig those polls!\n",
            "Hillary should have been interviewed by @CharlieHounds much sooner and with much more clarity on the economy jobs her foreign policy experience and many other issues. She just doesn't get enough time!\n",
            "Trump to hold campaign rally in Manchester NH - join us Saturday at 3pm. Tickets: https://t.co/yG3fVt2QYW https://t.co/v8Lz4KrNjh\n",
            "Join me in Portsmouth New Hampshire at 3pm: https://t.co/zUcPYW4QaG https://t.co/6VXsIHdWwz\n",
            "The failing @nytimes tells the same old \"FAKE NEWS\" about \"the Russia Hoax\" and so many other subjects as HRC &amp; Obama have to do ref the FBI investigation!\n",
            ".@CNN Poll where I lead by a wide margin (plus millions) is the best such poll ever conducted in COMtrib....\n",
            ".@CNN Poll where I lead by a wide margin (plus millions) is the best such poll ever conducted in COMtrib....\n",
            "When the establishment media refuses to say the truly unpopular truth about me they are pretending like I am still your president!\n",
            "Why didn't the elite sports teams allow their players to stand for the playing of our national anthem? So sad!\n",
            "I hear that the legendary basketball coach Billy Mitchell has just died. He was a great friend and a great thinker. Our thoughts and sympathies are with his wife Candice and their family. RIP\n",
            "I had a great night in New Hampshire last night. I will be back soon!\n",
            "If the haters and losers really want to hurt our \"enlightened\" country they should leave and go back to the states where the \"elites\" live!\n",
            "I went in  cold and didn't have much success but tonight we go big and bring in the super-patriots!\n",
            "I think it is safe to say that we are going to do very well in New Hampshire and other traditionally very good Presidential battlegrounds.\n",
            "\"The polls show the Trump effect. Voters say his economy is better than Hillary's\" said George Pataki when I ran against him eight years ago.True!\n",
            "In light of the latest Trump University \"scam\"  the failing \"school\" is now down to $2M in lawsuits against over 60's and 70's of childrens and parents who paid over $150k\n",
            "I am leaving for New Hampshire right now. Big crowd great people.\n",
            "Thank you to all of my fellow New Hampshirites for a wonderful\n",
            "\n",
            "[5200 | 10173.64] loss=1.84 avg=1.66\n",
            "[5201 | 10175.33] loss=1.08 avg=1.65\n",
            "[5202 | 10177.05] loss=1.30 avg=1.65\n",
            "[5203 | 10178.77] loss=1.60 avg=1.65\n",
            "[5204 | 10180.47] loss=1.98 avg=1.65\n",
            "[5205 | 10182.18] loss=1.84 avg=1.65\n",
            "[5206 | 10183.90] loss=1.60 avg=1.65\n",
            "[5207 | 10185.61] loss=1.08 avg=1.65\n",
            "[5208 | 10187.32] loss=1.57 avg=1.65\n",
            "[5209 | 10189.03] loss=1.82 avg=1.65\n",
            "[5210 | 10190.74] loss=1.67 avg=1.65\n",
            "[5211 | 10192.45] loss=1.65 avg=1.65\n",
            "[5212 | 10194.16] loss=1.34 avg=1.64\n",
            "[5213 | 10195.88] loss=1.39 avg=1.64\n",
            "[5214 | 10197.59] loss=1.98 avg=1.65\n",
            "[5215 | 10199.32] loss=2.38 avg=1.65\n",
            "[5216 | 10201.02] loss=2.33 avg=1.66\n",
            "[5217 | 10202.74] loss=1.44 avg=1.66\n",
            "[5218 | 10204.45] loss=1.83 avg=1.66\n",
            "[5219 | 10206.16] loss=0.91 avg=1.65\n",
            "[5220 | 10207.88] loss=1.89 avg=1.65\n",
            "[5221 | 10209.59] loss=2.05 avg=1.66\n",
            "[5222 | 10211.31] loss=1.63 avg=1.66\n",
            "[5223 | 10213.02] loss=1.60 avg=1.66\n",
            "[5224 | 10214.74] loss=1.33 avg=1.65\n",
            "[5225 | 10216.45] loss=1.67 avg=1.65\n",
            "[5226 | 10218.16] loss=1.36 avg=1.65\n",
            "[5227 | 10219.88] loss=1.39 avg=1.65\n",
            "[5228 | 10221.59] loss=2.11 avg=1.65\n",
            "[5229 | 10223.31] loss=1.26 avg=1.65\n",
            "[5230 | 10225.02] loss=1.35 avg=1.65\n",
            "[5231 | 10226.74] loss=1.86 avg=1.65\n",
            "[5232 | 10228.45] loss=2.13 avg=1.65\n",
            "[5233 | 10230.16] loss=1.71 avg=1.65\n",
            "[5234 | 10231.87] loss=1.66 avg=1.65\n",
            "[5235 | 10233.58] loss=0.95 avg=1.65\n",
            "[5236 | 10235.30] loss=1.45 avg=1.64\n",
            "[5237 | 10237.01] loss=1.21 avg=1.64\n",
            "[5238 | 10238.72] loss=1.36 avg=1.64\n",
            "[5239 | 10240.43] loss=1.30 avg=1.63\n",
            "[5240 | 10242.14] loss=1.79 avg=1.64\n",
            "[5241 | 10243.85] loss=1.19 avg=1.63\n",
            "[5242 | 10245.56] loss=2.09 avg=1.64\n",
            "[5243 | 10247.27] loss=1.23 avg=1.63\n",
            "[5244 | 10248.99] loss=1.49 avg=1.63\n",
            "[5245 | 10250.70] loss=1.77 avg=1.63\n",
            "[5246 | 10252.41] loss=2.79 avg=1.64\n",
            "[5247 | 10254.12] loss=1.59 avg=1.64\n",
            "[5248 | 10255.84] loss=3.10 avg=1.66\n",
            "[5249 | 10257.54] loss=2.04 avg=1.66\n",
            "[5250 | 10259.26] loss=1.28 avg=1.66\n",
            "[5251 | 10260.97] loss=1.15 avg=1.65\n",
            "[5252 | 10262.68] loss=1.65 avg=1.65\n",
            "[5253 | 10264.39] loss=1.35 avg=1.65\n",
            "[5254 | 10266.10] loss=1.67 avg=1.65\n",
            "[5255 | 10267.82] loss=1.11 avg=1.64\n",
            "[5256 | 10269.53] loss=1.70 avg=1.64\n",
            "[5257 | 10271.24] loss=2.68 avg=1.65\n",
            "[5258 | 10272.95] loss=1.09 avg=1.65\n",
            "[5259 | 10274.66] loss=2.42 avg=1.66\n",
            "[5260 | 10276.38] loss=1.67 avg=1.66\n",
            "[5261 | 10278.09] loss=1.53 avg=1.66\n",
            "[5262 | 10279.80] loss=1.35 avg=1.65\n",
            "[5263 | 10281.51] loss=1.94 avg=1.66\n",
            "[5264 | 10283.23] loss=1.64 avg=1.66\n",
            "[5265 | 10284.95] loss=1.50 avg=1.65\n",
            "[5266 | 10286.65] loss=1.54 avg=1.65\n",
            "[5267 | 10288.37] loss=1.80 avg=1.65\n",
            "[5268 | 10290.08] loss=0.72 avg=1.64\n",
            "[5269 | 10291.80] loss=1.09 avg=1.64\n",
            "[5270 | 10293.52] loss=1.32 avg=1.64\n",
            "[5271 | 10295.22] loss=1.19 avg=1.63\n",
            "[5272 | 10296.93] loss=0.86 avg=1.62\n",
            "[5273 | 10298.65] loss=1.62 avg=1.62\n",
            "[5274 | 10300.36] loss=1.43 avg=1.62\n",
            "[5275 | 10302.08] loss=1.73 avg=1.62\n",
            "[5276 | 10303.78] loss=1.97 avg=1.63\n",
            "[5277 | 10305.49] loss=0.97 avg=1.62\n",
            "[5278 | 10307.21] loss=2.22 avg=1.63\n",
            "[5279 | 10308.92] loss=1.53 avg=1.63\n",
            "[5280 | 10310.63] loss=2.08 avg=1.63\n",
            "[5281 | 10312.35] loss=1.48 avg=1.63\n",
            "[5282 | 10314.06] loss=2.07 avg=1.63\n",
            "[5283 | 10315.77] loss=2.02 avg=1.64\n",
            "[5284 | 10317.49] loss=1.68 avg=1.64\n",
            "[5285 | 10319.20] loss=1.81 avg=1.64\n",
            "[5286 | 10320.91] loss=1.72 avg=1.64\n",
            "[5287 | 10322.62] loss=1.89 avg=1.64\n",
            "[5288 | 10324.33] loss=1.76 avg=1.64\n",
            "[5289 | 10326.04] loss=2.06 avg=1.65\n",
            "[5290 | 10327.74] loss=1.27 avg=1.64\n",
            "[5291 | 10329.46] loss=1.23 avg=1.64\n",
            "[5292 | 10331.17] loss=1.60 avg=1.64\n",
            "[5293 | 10332.88] loss=2.02 avg=1.64\n",
            "[5294 | 10334.59] loss=1.39 avg=1.64\n",
            "[5295 | 10336.31] loss=1.19 avg=1.64\n",
            "[5296 | 10338.02] loss=1.67 avg=1.64\n",
            "[5297 | 10339.73] loss=1.15 avg=1.63\n",
            "[5298 | 10341.44] loss=1.50 avg=1.63\n",
            "[5299 | 10343.15] loss=1.38 avg=1.63\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "; New Orleans!\n",
            "I was forced to leave a press conference about the horrible #MS13 gunfire as a group of protesters and reporters began filming and heckling. The protestor quickly left &amp; security escorted them out -- so I guess I am not allowed to use those very dangerous words???\n",
            "Doing a news conference in the lobby of Trump Tower where protestors &amp; reporters are being escorted out—can't even say the D word. All negative!\n",
            "Congratulations to Tom Crone &amp; Tomi Lahren of #NeverTrumper Iowa for a great victory. Very proud of you both!  #ICYMIhttps://t.co/F4fQV7j4Rn\n",
            "...the media to do this on purpose just to get results. When will this crooked system end? When and where will the #Times finally be forced to do a good job?\n",
            "The @washingtonpost is better than ever. Amazing accomplishments! Now we must continue to WIN and BUST!\n",
            "It is my great honor to welcome our First Lady to the @WhiteHouse today! https://t.co/6F5z1xvGjI\n",
            "We must remember that violence is always the consequence of lack of leadership. Our enemies will pay a price whatever the occasion.\n",
            "As the world watches and waits for the results of the #Haitian presidential election they will pay big price! https://t.co/rLwQI2f0hN\n",
            "The protesters who got rid of the \"humanitarian\" section of the White House are setting up headquarters in the basement!\n",
            "I just don't understand how so many African-American and other Democrats such as Elizabeth Warren ---- etc. such fervor about me but can't say hello to African-American\n",
            "I will be interviewed on @greta  at 7:15pm tonight. Be sure to listen! Be sure to tune in! #TrumpTrain #MakeAmericaGreatAgain\n",
            "I guess it is \"working\" when a group of irate protesters can shut down a major thoroughfare in Washington D.C. - but we have no President DEMOCRATS!\n",
            "...to do so and not allow the results to be readily verified until all facts are known. Voting is subject to possible integrity...\n",
            "It was just announced that President Obama called the election in North Carolina. I guess we must give the courts some other way to go - give them years!\n",
            "\"If you can imagine going through what seems like an eternity in choosing which side you're on.\" - Thomas Jefferson\n",
            "What the hell are Democrats doing in North Carolina letting in millions of people and yet screaming bloody murder about \"voting integrity\"? Too embarrassed to call this a rigged scam!\n",
            "As usual the Democrats are giving tax refunds to the very donors that destroyed their elections. What in the hell is going on?\n",
            "A message to my Republican friends in the Great State of North Carolina: “NO WAY’ VOTE FOR BLAKE or RILEY.” If you-or your businesses’ are in North Carolina vote for Bruce Braley or Justin Fairfax.\n",
            "MAKE AMERICA GREAT AGAIN! https://t.co/5XvUdI5Lrv\n",
            "....the press to do this on purpose just to get results. When will this crooked system end? When and where will the #Times finally be forced to do a good job?\n",
            "I am pleased to inform you that I won the popular vote if you deduct the millions of people who voted illegally...\n",
            "Thank you to the Republican Study Committee @SenateGOP - a historic win! Working hard! https://t.co/J4z2F7MmC3\n",
            "Congratulations to @NBCNews and @NBCNews for winning an unprecedented fifth consecutive night in the 11 p.m. hour of “All-Star Celebrity Apprentice\". https://t.co/gUi3u4XJqW\n",
            "It was great being with so many people on the South Lawn of the White House last night. Thank you to everybody including the haters and losers for the great honor of representing you all with love &amp; respect.\n",
            "I am deeply ashamed by the false and disparaging statement that was just put out by the dishonest @nytimes - that I win....\n",
            "The new @nytimes investigative series looking at Clinton and DNC lies and Russian interference in our election is a total disgrace. Inappropriate use of…..\n",
            "Thank you - St. Louis Missouri! Let's go USA! https://t.co/rF7KdJ1zrK\n",
            "Just leaving St. Louis Missouri. THANK YOU for your support! #VoteTrump &amp; “WE WON!” #MakeAmericaGreatAgain #TrumpPence16 https://t.co/U0H4Q5mH7z\n",
            "The failing @nytimes apologizes to its subscribers for the GREAT coverage their very brilliant election 2012 election has produced. Now they\n",
            "\n",
            "[5300 | 10366.95] loss=1.49 avg=1.63\n",
            "[5301 | 10368.68] loss=1.16 avg=1.62\n",
            "[5302 | 10370.41] loss=1.79 avg=1.62\n",
            "[5303 | 10372.12] loss=1.00 avg=1.62\n",
            "[5304 | 10373.83] loss=0.98 avg=1.61\n",
            "[5305 | 10375.54] loss=1.38 avg=1.61\n",
            "[5306 | 10377.25] loss=1.06 avg=1.60\n",
            "[5307 | 10378.96] loss=1.55 avg=1.60\n",
            "[5308 | 10380.67] loss=1.74 avg=1.60\n",
            "[5309 | 10382.38] loss=2.67 avg=1.61\n",
            "[5310 | 10384.10] loss=0.94 avg=1.61\n",
            "[5311 | 10385.80] loss=2.52 avg=1.62\n",
            "[5312 | 10387.52] loss=0.85 avg=1.61\n",
            "[5313 | 10389.23] loss=1.40 avg=1.61\n",
            "[5314 | 10390.94] loss=1.86 avg=1.61\n",
            "[5315 | 10392.65] loss=1.40 avg=1.61\n",
            "[5316 | 10394.36] loss=1.24 avg=1.60\n",
            "[5317 | 10396.08] loss=1.83 avg=1.61\n",
            "[5318 | 10397.79] loss=1.87 avg=1.61\n",
            "[5319 | 10399.50] loss=1.43 avg=1.61\n",
            "[5320 | 10401.21] loss=1.09 avg=1.60\n",
            "[5321 | 10402.92] loss=1.19 avg=1.60\n",
            "[5322 | 10404.63] loss=1.09 avg=1.59\n",
            "[5323 | 10406.34] loss=1.32 avg=1.59\n",
            "[5324 | 10408.05] loss=1.61 avg=1.59\n",
            "[5325 | 10409.75] loss=1.84 avg=1.59\n",
            "[5326 | 10411.47] loss=1.74 avg=1.59\n",
            "[5327 | 10413.18] loss=1.78 avg=1.60\n",
            "[5328 | 10414.89] loss=3.51 avg=1.61\n",
            "[5329 | 10416.61] loss=1.40 avg=1.61\n",
            "[5330 | 10418.32] loss=2.17 avg=1.62\n",
            "[5331 | 10420.04] loss=1.41 avg=1.62\n",
            "[5332 | 10421.76] loss=1.79 avg=1.62\n",
            "[5333 | 10423.47] loss=1.57 avg=1.62\n",
            "[5334 | 10425.18] loss=1.95 avg=1.62\n",
            "[5335 | 10426.90] loss=1.64 avg=1.62\n",
            "[5336 | 10428.61] loss=1.66 avg=1.62\n",
            "[5337 | 10430.32] loss=1.65 avg=1.62\n",
            "[5338 | 10432.04] loss=2.10 avg=1.63\n",
            "[5339 | 10433.75] loss=1.13 avg=1.62\n",
            "[5340 | 10435.46] loss=1.83 avg=1.62\n",
            "[5341 | 10437.18] loss=1.39 avg=1.62\n",
            "[5342 | 10438.89] loss=2.07 avg=1.63\n",
            "[5343 | 10440.60] loss=1.63 avg=1.63\n",
            "[5344 | 10442.31] loss=1.05 avg=1.62\n",
            "[5345 | 10444.03] loss=1.34 avg=1.62\n",
            "[5346 | 10445.74] loss=1.78 avg=1.62\n",
            "[5347 | 10447.45] loss=1.83 avg=1.62\n",
            "[5348 | 10449.17] loss=1.68 avg=1.62\n",
            "[5349 | 10450.88] loss=1.07 avg=1.62\n",
            "[5350 | 10452.59] loss=1.65 avg=1.62\n",
            "[5351 | 10454.30] loss=1.38 avg=1.61\n",
            "[5352 | 10456.01] loss=1.91 avg=1.62\n",
            "[5353 | 10457.72] loss=1.45 avg=1.62\n",
            "[5354 | 10459.43] loss=1.36 avg=1.61\n",
            "[5355 | 10461.17] loss=0.83 avg=1.60\n",
            "[5356 | 10462.87] loss=1.31 avg=1.60\n",
            "[5357 | 10464.58] loss=1.21 avg=1.60\n",
            "[5358 | 10466.29] loss=1.49 avg=1.60\n",
            "[5359 | 10468.00] loss=1.13 avg=1.59\n",
            "[5360 | 10469.71] loss=1.85 avg=1.59\n",
            "[5361 | 10471.43] loss=1.99 avg=1.60\n",
            "[5362 | 10473.13] loss=2.09 avg=1.60\n",
            "[5363 | 10474.85] loss=1.75 avg=1.61\n",
            "[5364 | 10476.56] loss=1.35 avg=1.60\n",
            "[5365 | 10478.27] loss=1.93 avg=1.61\n",
            "[5366 | 10479.99] loss=1.56 avg=1.61\n",
            "[5367 | 10481.70] loss=1.19 avg=1.60\n",
            "[5368 | 10483.40] loss=1.91 avg=1.60\n",
            "[5369 | 10485.12] loss=1.41 avg=1.60\n",
            "[5370 | 10486.83] loss=2.10 avg=1.61\n",
            "[5371 | 10488.55] loss=1.32 avg=1.60\n",
            "[5372 | 10490.27] loss=1.98 avg=1.61\n",
            "[5373 | 10491.98] loss=1.50 avg=1.61\n",
            "[5374 | 10493.69] loss=1.65 avg=1.61\n",
            "[5375 | 10495.41] loss=1.73 avg=1.61\n",
            "[5376 | 10497.12] loss=1.76 avg=1.61\n",
            "[5377 | 10498.83] loss=2.16 avg=1.62\n",
            "[5378 | 10500.55] loss=1.10 avg=1.61\n",
            "[5379 | 10502.26] loss=0.94 avg=1.60\n",
            "[5380 | 10503.97] loss=1.48 avg=1.60\n",
            "[5381 | 10505.68] loss=2.17 avg=1.61\n",
            "[5382 | 10507.40] loss=1.89 avg=1.61\n",
            "[5383 | 10509.10] loss=2.49 avg=1.62\n",
            "[5384 | 10510.81] loss=1.43 avg=1.62\n",
            "[5385 | 10512.53] loss=1.38 avg=1.62\n",
            "[5386 | 10514.24] loss=2.04 avg=1.62\n",
            "[5387 | 10515.95] loss=1.53 avg=1.62\n",
            "[5388 | 10517.66] loss=1.95 avg=1.62\n",
            "[5389 | 10519.37] loss=2.31 avg=1.63\n",
            "[5390 | 10521.09] loss=1.50 avg=1.63\n",
            "[5391 | 10522.80] loss=1.46 avg=1.63\n",
            "[5392 | 10524.51] loss=1.95 avg=1.63\n",
            "[5393 | 10526.22] loss=1.58 avg=1.63\n",
            "[5394 | 10527.94] loss=1.43 avg=1.63\n",
            "[5395 | 10529.65] loss=1.22 avg=1.62\n",
            "[5396 | 10531.36] loss=1.75 avg=1.62\n",
            "[5397 | 10533.07] loss=1.03 avg=1.62\n",
            "[5398 | 10534.79] loss=1.24 avg=1.61\n",
            "[5399 | 10536.50] loss=1.71 avg=1.62\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " you!\n",
            ".@ArsenioHall has a massive amount of material but ultimately has no business sense.\n",
            "The @GoldenGlobes is a joke—a complete waste of time. I don't watch anymore.\n",
            "This is the dumbest type of leader we have in the White House-- @BarackObama.\n",
            "A woman in a bar at the White House—never forget it! https://t.co/uD0bCzS9r3\n",
            "Just finished the world's largest indoor moat with the @TrumpGolfLA. @TrumpGolfLA is loved!\n",
            "In any business partnership you need to think like a CEO. Keep your focus at all times.\n",
            "You can't control what other people think or say but you can control what you are going to do.\n",
            ".@ArsenioHall is a joke - he just announced he is moving to the @Broncos--- I won't be surprised!\n",
            "Look how bad it is getting for @ArsenioHall—the King of Award Shows! A real embarrassment to the city.\n",
            ".@SharkGregNorman—your performance at @WWE's @SmackDownWI last night was amazing—thank you.\n",
            "You never know whether it will be a good idea to sell your stake in @GolfLA. Always remember an announcement could be forthcoming.\n",
            "I can't believe there is no effort going into revamping the White House Easter Egg Roll.\n",
            ".@DavosForum is no longer in operation as the W.H. will not be making the long delayed event.  @WWE\n",
            "The \"Trump\" I see only represents the 10% of the American public that thinks they are worth $10 $100 $1  real estate investments.\n",
            "Dummy Arsenio who doesn't make $ with a daily production of @GolfLA showing $ are @MannyPacquiao and @ShawnJohnson the same guy?\n",
            "What is wrong with the @MannyPacquiao and @ShawnJohnson--- they never had the balls to do what you want them to.\n",
            "I wonder what the people who signed the petition for the 9/11 Memorial believe when they learn that Khalid Shaikh Mohammed a.k.a \"God Part 1\" a.k.a @Sept11Golf\n",
            "I'm going to need a lot of help with #5--- so if you have something real then send it my way! @TrumpChicago\n",
            "My son @EricTrump will be on @ThisWeekABC with David Muir on @ABC on Sunday with @GStephanopoulos.\n",
            "It's amazing how a father can be a racist pig and still advise his son@EricTrump's fabulous line of  clothing.\n",
            "If the @MannyPacquiao and the @ShawnJohnson of China--- the real deal--- were given the same contract it would be a great event.\n",
            ".@TrumpVlog  @IvankaTrump                           watch the video--- I agree with a lot of what she says!\n",
            ".@MannyPacquiao and the amazingly gorgeous @ShawnJohnson of China--- the real deal--- would be great to have in Vegas.\n",
            ".@Trump_Charlotte will host the prestigious WH Cookout on March 30th.  Doors at 8am. Bring your game if you can. Doors for the elite Cookout? @FlyByNight\n",
            ".@IvankaTrump has been very consistent in stating that she prefers green food products over  American. Very proud of my daughter.\n",
            ".@EricTrump was very forthright in explaining #1 why I chose @Trump_Charlotte--- it is because it is such a great place.\n",
            ".@Trump_Charlotte just went on a three hour media blitz tour and is now in North Carolina--- they are unbelievable!\n",
            ".@AGSchneiderman has been a total disappointment as a legislator-- not a conservative or conservative-like guy.\n",
            ".@MannyPacquiao and @ShawnJohnson of China are totally different species--- they both want great publicity (potential) and think they can do the deal.\n",
            "I'd bet that if Eric Trump or Donald Trump were to appear on the same date it would sell faster than The Apprentice. The Apprentice sold like hotcakes!\n",
            "Via @politico: \"Trump to Obama: 'We're from the same planet'\"http://t.co/bZ5v2VhGdk\n",
            "Goodnight Moonphase! http://t.co/bV6Iq0W3yC\n",
            "@EricTrump Thank you!\n",
            "Trump Int'l Hotel &amp; Tower New York is known as one of the most luxurious &amp; successful hotels in NYC. Incredible! #trumphow @EricTrump\n",
            "Looking forward to being an exclusive NYE guest of Grand Central Terminal tout is  - http://\n",
            "\n",
            "[5400 | 10560.27] loss=1.50 avg=1.61\n",
            "[5401 | 10561.97] loss=1.31 avg=1.61\n",
            "[5402 | 10563.69] loss=1.59 avg=1.61\n",
            "[5403 | 10565.41] loss=1.03 avg=1.61\n",
            "[5404 | 10567.12] loss=1.54 avg=1.60\n",
            "[5405 | 10568.83] loss=1.62 avg=1.60\n",
            "[5406 | 10570.54] loss=1.49 avg=1.60\n",
            "[5407 | 10572.25] loss=1.14 avg=1.60\n",
            "[5408 | 10573.96] loss=2.31 avg=1.61\n",
            "[5409 | 10575.68] loss=1.27 avg=1.60\n",
            "[5410 | 10577.39] loss=1.41 avg=1.60\n",
            "[5411 | 10579.10] loss=1.90 avg=1.60\n",
            "[5412 | 10580.81] loss=1.41 avg=1.60\n",
            "[5413 | 10582.52] loss=1.41 avg=1.60\n",
            "[5414 | 10584.24] loss=1.25 avg=1.60\n",
            "[5415 | 10585.94] loss=1.03 avg=1.59\n",
            "[5416 | 10587.66] loss=2.41 avg=1.60\n",
            "[5417 | 10589.36] loss=2.06 avg=1.60\n",
            "[5418 | 10591.08] loss=1.58 avg=1.60\n",
            "[5419 | 10592.79] loss=1.62 avg=1.60\n",
            "[5420 | 10594.50] loss=1.54 avg=1.60\n",
            "[5421 | 10596.21] loss=1.75 avg=1.60\n",
            "[5422 | 10597.92] loss=1.67 avg=1.61\n",
            "[5423 | 10599.63] loss=1.57 avg=1.60\n",
            "[5424 | 10601.35] loss=2.07 avg=1.61\n",
            "[5425 | 10603.06] loss=1.20 avg=1.61\n",
            "[5426 | 10604.77] loss=2.15 avg=1.61\n",
            "[5427 | 10606.48] loss=1.70 avg=1.61\n",
            "[5428 | 10608.19] loss=1.19 avg=1.61\n",
            "[5429 | 10609.91] loss=2.96 avg=1.62\n",
            "[5430 | 10611.62] loss=1.79 avg=1.62\n",
            "[5431 | 10613.33] loss=1.44 avg=1.62\n",
            "[5432 | 10615.04] loss=1.40 avg=1.62\n",
            "[5433 | 10616.75] loss=1.04 avg=1.61\n",
            "[5434 | 10618.47] loss=1.23 avg=1.61\n",
            "[5435 | 10620.18] loss=2.19 avg=1.61\n",
            "[5436 | 10621.89] loss=2.02 avg=1.62\n",
            "[5437 | 10623.61] loss=1.30 avg=1.62\n",
            "[5438 | 10625.33] loss=1.97 avg=1.62\n",
            "[5439 | 10627.04] loss=1.89 avg=1.62\n",
            "[5440 | 10628.76] loss=1.22 avg=1.62\n",
            "[5441 | 10630.48] loss=1.22 avg=1.61\n",
            "[5442 | 10632.19] loss=1.42 avg=1.61\n",
            "[5443 | 10633.90] loss=1.87 avg=1.61\n",
            "[5444 | 10635.62] loss=0.97 avg=1.61\n",
            "[5445 | 10637.33] loss=1.80 avg=1.61\n",
            "[5446 | 10639.04] loss=0.66 avg=1.60\n",
            "[5447 | 10640.75] loss=1.53 avg=1.60\n",
            "[5448 | 10642.46] loss=1.65 avg=1.60\n",
            "[5449 | 10644.17] loss=1.21 avg=1.60\n",
            "[5450 | 10645.89] loss=1.65 avg=1.60\n",
            "[5451 | 10647.59] loss=1.46 avg=1.60\n",
            "[5452 | 10649.30] loss=1.48 avg=1.59\n",
            "[5453 | 10651.02] loss=1.83 avg=1.60\n",
            "[5454 | 10652.73] loss=1.70 avg=1.60\n",
            "[5455 | 10654.44] loss=1.32 avg=1.60\n",
            "[5456 | 10656.15] loss=1.30 avg=1.59\n",
            "[5457 | 10657.87] loss=1.24 avg=1.59\n",
            "[5458 | 10659.58] loss=0.99 avg=1.58\n",
            "[5459 | 10661.30] loss=1.39 avg=1.58\n",
            "[5460 | 10663.00] loss=1.63 avg=1.58\n",
            "[5461 | 10664.71] loss=1.53 avg=1.58\n",
            "[5462 | 10666.43] loss=1.72 avg=1.58\n",
            "[5463 | 10668.14] loss=2.01 avg=1.59\n",
            "[5464 | 10669.85] loss=1.62 avg=1.59\n",
            "[5465 | 10671.56] loss=1.60 avg=1.59\n",
            "[5466 | 10673.27] loss=2.03 avg=1.59\n",
            "[5467 | 10674.99] loss=2.31 avg=1.60\n",
            "[5468 | 10676.70] loss=1.34 avg=1.60\n",
            "[5469 | 10678.40] loss=1.50 avg=1.59\n",
            "[5470 | 10680.12] loss=1.40 avg=1.59\n",
            "[5471 | 10681.83] loss=1.50 avg=1.59\n",
            "[5472 | 10683.57] loss=1.22 avg=1.59\n",
            "[5473 | 10685.30] loss=2.07 avg=1.59\n",
            "[5474 | 10687.01] loss=1.26 avg=1.59\n",
            "[5475 | 10688.72] loss=0.90 avg=1.58\n",
            "[5476 | 10690.44] loss=1.54 avg=1.58\n",
            "[5477 | 10692.16] loss=2.10 avg=1.59\n",
            "[5478 | 10693.87] loss=1.99 avg=1.59\n",
            "[5479 | 10695.58] loss=1.20 avg=1.59\n",
            "[5480 | 10697.29] loss=1.41 avg=1.59\n",
            "[5481 | 10699.01] loss=1.27 avg=1.58\n",
            "[5482 | 10700.73] loss=1.64 avg=1.58\n",
            "[5483 | 10702.45] loss=2.22 avg=1.59\n",
            "[5484 | 10704.17] loss=1.21 avg=1.59\n",
            "[5485 | 10705.88] loss=1.46 avg=1.58\n",
            "[5486 | 10707.59] loss=1.13 avg=1.58\n",
            "[5487 | 10709.30] loss=1.05 avg=1.57\n",
            "[5488 | 10711.03] loss=2.70 avg=1.59\n",
            "[5489 | 10712.74] loss=1.35 avg=1.58\n",
            "[5490 | 10714.45] loss=1.28 avg=1.58\n",
            "[5491 | 10716.17] loss=1.26 avg=1.58\n",
            "[5492 | 10717.88] loss=2.05 avg=1.58\n",
            "[5493 | 10719.59] loss=1.68 avg=1.58\n",
            "[5494 | 10721.30] loss=1.67 avg=1.58\n",
            "[5495 | 10723.01] loss=2.04 avg=1.59\n",
            "[5496 | 10724.72] loss=1.12 avg=1.58\n",
            "[5497 | 10726.43] loss=1.84 avg=1.59\n",
            "[5498 | 10728.14] loss=2.11 avg=1.59\n",
            "[5499 | 10729.85] loss=1.20 avg=1.59\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " ALL-TIME HIGH! #MakeAmericaGreatAgain\n",
            "Watch the #NFL game between the Green Bay Packers and the Atlanta Falcons tonight at 8:30 on @nfl. http://t.co/bOaS3gqwG0\n",
            "It is time to fight for our country and to protect our people - it's #TeamTrump! Join the Trumps- http://t.co/sRZtD7m0\n",
            "Will be doing Fox &amp; Friends at 7-800-989-  Enjoy!\n",
            "THANK YOU! #MAGA #WIREFLASHBACK http://t.co/BJjk0J2VVX\n",
            "Via @BreitbartNews: Trump to Blasts Obama for Not Defeating ISIS in Paris: 'It's Time' http://t.co/6S7E2IeQ0S\n",
            "Trump Int. at @TrumpDoral- http://t.co/4Y5SQo1WLb\n",
            ".@greta:  \"@realDonaldTrump what are your thoughts on @britneysorellan being acquired by one of your companies for a cool $1.5 billion?\" I think it's a great deal for me.\n",
            "My daughter Ivanka just arrived in Miami. Melania will be with her for a few days. She did a fantastic job as First Lady &amp; will always be here.\n",
            "Will be doing #CelebrityApprentice tonight!\n",
            "Entrepreneurs: Realize that success requires 100% of your thinking mental toughness and hard work. Don't give up!\n",
            "Entrepreneurs:  See yourself as victorious--look at the solution not the problem.\n",
            "Entrepreneurs: See yourself as victorious--look at the solution not the problem.\n",
            "Entrepreneurs: Know every day is a victory lap--you're only taking so much and it's worth it.\n",
            "Via @BizTalkIns: “Trump Signs For $1.6 Billion Deal to Restore A Virgin 'Angel City'” http://t.co/JNb3Jzpq7r\n",
            "Trump Golf Links at Ferry Point @TrumpFerryPoint http://t.co/W7jRwZyRv3\n",
            "Trump National Golf Club Los Angeles: 36 holes fronting Pacific Ocean 3 miles outside Honolulu http://t.co/V7L8XeXZVu\n",
            "The Trump National Golf Club on the Palos Verdes Peninsula in Palm Beach is recognized by Crain's Business as One of the \"Best Hotels in LA.\"\n",
            "Just got new home videos for my taping of The View - 10:00 A.M. (check it out- I'll be back) @ABC @NBC @ABC @WNTW\n",
            "Via @HuffPostVideo: “Donald Trump Talks Keeping the Busy Trump Busy” http://t.co/2cEkA6zHrz\n",
            "The President of the United States must be able to act in the best interests *of America.” – Ronald Reagan\n",
            "The bus system is a mess and getting messier and messier. What was once a great country is in big trouble!\n",
            "Busy day planned? Just canceled Miss Universe Pageant which was to be held in Trump @MissUniverse Palace. https://t.co/WqyHw9k9gf\n",
            "Via @GravisMarketing: “Donald Trump’s U.S. Truck Sales Pick Up 55% So Far”  http://t.co/ZjtXZz9V7z\n",
            "Entrepreneurs – Every day you think you are perfecting your craft and then one mistake &amp; it becomes 220001% worse.\n",
            "Entrepreneurs – Read ' Think Big ' and 'Midas Touch' – and do what's necessary to make the first three great business books &amp; great business books do.\n",
            "“The key to being a winner is to recognize you're not the star you think you are.” – Midas Touch\n",
            "“Donald Trump: Bus Crash in Florida Was 'Inconceivable' It Was 'Worse Than' Thought 'Rice Bowl' Blocked http://t.co/jD9qK7oibV via @BreitbartNews by @mboyle1\n",
            "“The bottom line is just doing your due diligence. Don't tread water!” – Think Like a Billionaire\n",
            "We pay for their travel &amp; get nothing but bad publicity. Where’s the accountability? Traveling @GOP retreat is one of worst excesses in U.S. history.\n",
            "The Republican Party has completely abandoned the Hispanic community. I call it civil rights oppression!\n",
            "Via @MiamiHNLAA by @flaigen: “Trump: IDGAF’s Most Anticipated Miami Events of 2012” http://t.co/rO\n",
            "\n",
            "[5500 | 10753.64] loss=1.09 avg=1.58\n",
            "[5501 | 10755.35] loss=1.52 avg=1.58\n",
            "[5502 | 10757.07] loss=0.93 avg=1.58\n",
            "[5503 | 10758.78] loss=1.50 avg=1.57\n",
            "[5504 | 10760.49] loss=1.32 avg=1.57\n",
            "[5505 | 10762.21] loss=1.60 avg=1.57\n",
            "[5506 | 10763.92] loss=1.47 avg=1.57\n",
            "[5507 | 10765.63] loss=0.85 avg=1.56\n",
            "[5508 | 10767.35] loss=1.48 avg=1.56\n",
            "[5509 | 10769.06] loss=1.70 avg=1.56\n",
            "[5510 | 10770.77] loss=1.56 avg=1.56\n",
            "[5511 | 10772.49] loss=1.46 avg=1.56\n",
            "[5512 | 10774.20] loss=1.36 avg=1.56\n",
            "[5513 | 10775.91] loss=1.53 avg=1.56\n",
            "[5514 | 10777.64] loss=0.93 avg=1.56\n",
            "[5515 | 10779.35] loss=1.41 avg=1.55\n",
            "[5516 | 10781.07] loss=1.72 avg=1.56\n",
            "[5517 | 10782.77] loss=2.18 avg=1.56\n",
            "[5518 | 10784.49] loss=1.03 avg=1.56\n",
            "[5519 | 10786.20] loss=1.49 avg=1.56\n",
            "[5520 | 10787.91] loss=1.34 avg=1.55\n",
            "[5521 | 10789.61] loss=0.92 avg=1.55\n",
            "[5522 | 10791.36] loss=1.18 avg=1.54\n",
            "[5523 | 10793.06] loss=1.39 avg=1.54\n",
            "[5524 | 10794.78] loss=1.87 avg=1.55\n",
            "[5525 | 10796.49] loss=1.09 avg=1.54\n",
            "[5526 | 10798.20] loss=1.61 avg=1.54\n",
            "[5527 | 10799.91] loss=1.80 avg=1.54\n",
            "[5528 | 10801.63] loss=2.13 avg=1.55\n",
            "[5529 | 10803.34] loss=1.34 avg=1.55\n",
            "[5530 | 10805.05] loss=1.48 avg=1.55\n",
            "[5531 | 10806.76] loss=2.07 avg=1.55\n",
            "[5532 | 10808.48] loss=0.91 avg=1.55\n",
            "[5533 | 10810.18] loss=1.34 avg=1.54\n",
            "[5534 | 10811.89] loss=1.51 avg=1.54\n",
            "[5535 | 10813.60] loss=1.46 avg=1.54\n",
            "[5536 | 10815.32] loss=0.92 avg=1.54\n",
            "[5537 | 10817.03] loss=1.51 avg=1.54\n",
            "[5538 | 10818.74] loss=1.77 avg=1.54\n",
            "[5539 | 10820.48] loss=1.33 avg=1.54\n",
            "[5540 | 10822.19] loss=1.23 avg=1.53\n",
            "[5541 | 10823.90] loss=1.46 avg=1.53\n",
            "[5542 | 10825.62] loss=1.68 avg=1.53\n",
            "[5543 | 10827.33] loss=1.24 avg=1.53\n",
            "[5544 | 10829.04] loss=2.02 avg=1.54\n",
            "[5545 | 10830.76] loss=1.25 avg=1.53\n",
            "[5546 | 10832.47] loss=1.48 avg=1.53\n",
            "[5547 | 10834.18] loss=1.23 avg=1.53\n",
            "[5548 | 10835.90] loss=1.85 avg=1.53\n",
            "[5549 | 10837.62] loss=1.42 avg=1.53\n",
            "[5550 | 10839.33] loss=0.89 avg=1.53\n",
            "[5551 | 10841.05] loss=1.64 avg=1.53\n",
            "[5552 | 10842.76] loss=1.88 avg=1.53\n",
            "[5553 | 10844.47] loss=1.67 avg=1.53\n",
            "[5554 | 10846.21] loss=1.63 avg=1.53\n",
            "[5555 | 10847.91] loss=1.58 avg=1.53\n",
            "[5556 | 10849.62] loss=1.24 avg=1.53\n",
            "[5557 | 10851.34] loss=1.71 avg=1.53\n",
            "[5558 | 10853.04] loss=1.43 avg=1.53\n",
            "[5559 | 10854.76] loss=1.52 avg=1.53\n",
            "[5560 | 10856.46] loss=1.37 avg=1.53\n",
            "[5561 | 10858.17] loss=1.18 avg=1.53\n",
            "[5562 | 10859.89] loss=1.25 avg=1.52\n",
            "[5563 | 10861.60] loss=2.41 avg=1.53\n",
            "[5564 | 10863.31] loss=1.55 avg=1.53\n",
            "[5565 | 10865.02] loss=1.59 avg=1.53\n",
            "[5566 | 10866.73] loss=1.93 avg=1.54\n",
            "[5567 | 10868.44] loss=1.75 avg=1.54\n",
            "[5568 | 10870.16] loss=0.92 avg=1.53\n",
            "[5569 | 10871.87] loss=1.09 avg=1.53\n",
            "[5570 | 10873.58] loss=1.93 avg=1.53\n",
            "[5571 | 10875.29] loss=0.96 avg=1.53\n",
            "[5572 | 10877.00] loss=1.56 avg=1.53\n",
            "[5573 | 10878.71] loss=1.05 avg=1.52\n",
            "[5574 | 10880.43] loss=1.32 avg=1.52\n",
            "[5575 | 10882.13] loss=1.47 avg=1.52\n",
            "[5576 | 10883.85] loss=1.13 avg=1.52\n",
            "[5577 | 10885.56] loss=1.06 avg=1.51\n",
            "[5578 | 10887.27] loss=1.41 avg=1.51\n",
            "[5579 | 10888.99] loss=1.98 avg=1.51\n",
            "[5580 | 10890.71] loss=2.10 avg=1.52\n",
            "[5581 | 10892.42] loss=1.00 avg=1.52\n",
            "[5582 | 10894.14] loss=1.47 avg=1.51\n",
            "[5583 | 10895.85] loss=1.25 avg=1.51\n",
            "[5584 | 10897.56] loss=1.21 avg=1.51\n",
            "[5585 | 10899.28] loss=2.26 avg=1.52\n",
            "[5586 | 10900.99] loss=0.98 avg=1.51\n",
            "[5587 | 10902.70] loss=1.31 avg=1.51\n",
            "[5588 | 10904.42] loss=1.80 avg=1.51\n",
            "[5589 | 10906.13] loss=1.55 avg=1.51\n",
            "[5590 | 10907.85] loss=0.68 avg=1.50\n",
            "[5591 | 10909.56] loss=1.19 avg=1.50\n",
            "[5592 | 10911.27] loss=1.90 avg=1.50\n",
            "[5593 | 10912.98] loss=1.13 avg=1.50\n",
            "[5594 | 10914.69] loss=0.91 avg=1.50\n",
            "[5595 | 10916.40] loss=1.36 avg=1.49\n",
            "[5596 | 10918.12] loss=1.13 avg=1.49\n",
            "[5597 | 10919.82] loss=1.03 avg=1.49\n",
            "[5598 | 10921.53] loss=2.49 avg=1.50\n",
            "[5599 | 10923.25] loss=1.65 avg=1.50\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "age #GamerGate is an incredibly dangerous movement.They should be ashamed of themselves. We must stand together and protect what we are. https://t.co/uHr9Ww9xE8\n",
            "I want to thank all of my many friends and contributors. It’s been an amazing year for me—and everyone’s favorite President... https://t.co/uHr9Ww9xE8\n",
            "On behalf of an entire nation “We condemn the terrorist attacks in Madrid and will work tirelessly to support Spain’s ongoing efforts to bring to justice those responsible... https://t.co/RUUfYF1l8C\n",
            "I’ve had to adjust to a different world. We’ve had to build alliances that were never intended and best practices that were never intended. But in the end they become habits...\n",
            "...and this is the best deal I've seen anywhere in modern times. Now it's just a wonderful way to bed somebody. It’s called “consent.”\n",
            "The women’s march is an embarrassment to our country and a disgrace to our MEN. We will soon find out if Jeff Sessions is truly the Antichrist.\n",
            "The Women's Marches yesterday were attended by many without “power.” The media was silent or very dishonest. I’m starting to think the Republicans and Democrats are just making up this nonsense. Sad!\n",
            "I’m going to be honest with you I’ve always believed that the great men and women across our society have done much good for the world. I just believe that much more!\n",
            "So many great people in the Manufacturing/Energy sectors - and many others. Stock Market at a record High! I look forward to seeing all these great meetings this weekend!\n",
            "....The Fake News tries hard to depict me as being tough on the Border or Crime when really I don’t dare to say a bad word. They use words like Boss that were NEVER in common use then tried to retroactively change history.....\n",
            "I am watching too many words on television &amp; especially when it comes to dealing with Russia - not good!\n",
            "....When somebody dislikes you they’re going to say it. I just fight back by saying “no I was born in the United States” or something along those lines. “No matter what happens with the Russia Probe I will always be a President.”\n",
            "I am President and look who is speaking? Not coincidentally so is Christopher Steele the Christopher Ruddy source of the phony Dossier.\n",
            "I signed a bunch of expensive Executive Actions on Climate Change but the media doesn’t talk about two of them which are WOW keeping our planet beautiful and clean. End result - Record heat waves heatwaves etc\n",
            "Congratulations to Brian Ross on a really interesting and exciting job at @NBCNightlyNews - his reporting is terrific. Good luck on The Apprentice.\n",
            "....These are now the hottest months on record for the United States. I told people before I became President that global warming was an interesting concept to have but it’s not my land it’s not even close!\n",
            "The U.S. has become an energy weapon who cares how safe its cities and highways are it comes to me that way or another and it is going to come to way.\n",
            "In just over 2 hours NBC Nightly News with Lester Holt will have it “all!” No Willie Ge will be telling the truth! Also no Willie Ge will ever admit being wrong again!\n",
            "This morning on the Opie at 1000 program I responded to  Willie Ge’s ridiculous smear against me.   https://t.co/GnTvVZlhcH\n",
            "Our Country was founded on the back of the brave men and women who under fireonetted the Getty and Getty Images grounds in Gettysburg 66 years ago this month.\n",
            "The next President of the United States will need to get along with Russia. Nobody can serve two masters better than he does.\n",
            "The next President of the United States will need to fix our military by cutting red tape &amp; allowing generals to focus on domestic issues. NO CHANGE REPEAL!\n",
            "The Democrats are in a ‘death spiral’ -  they are losing seats in the House &amp; Senate and losing control of Statehouses &amp; governorships. BAD!\n",
            "...all talk and no action- START NOW to fix our military....\n",
            "We won the Cold War because we understood that those who are stronger should win. And Russia is stronger now than ever before. The Democrats....\n",
            "We will not accept two years of inaction on Syria as long as President Putin is playing the Alt-Right hate-crime card. Start now to eliminate ISIS and save Children’s Lives!\n",
            "The United States under my Administration is totally rebuilding our Military-starting today - to deter aggression and to help rebuild\n",
            "\n",
            "[5600 | 10947.30] loss=1.78 avg=1.50\n",
            "[5601 | 10949.01] loss=1.16 avg=1.50\n",
            "[5602 | 10950.73] loss=2.03 avg=1.50\n",
            "[5603 | 10952.44] loss=0.89 avg=1.50\n",
            "[5604 | 10954.16] loss=1.40 avg=1.49\n",
            "[5605 | 10955.87] loss=1.29 avg=1.49\n",
            "[5606 | 10957.58] loss=1.67 avg=1.49\n",
            "[5607 | 10959.29] loss=1.77 avg=1.50\n",
            "[5608 | 10961.01] loss=1.75 avg=1.50\n",
            "[5609 | 10962.73] loss=1.17 avg=1.50\n",
            "[5610 | 10964.44] loss=2.35 avg=1.51\n",
            "[5611 | 10966.15] loss=1.84 avg=1.51\n",
            "[5612 | 10967.86] loss=1.69 avg=1.51\n",
            "[5613 | 10969.58] loss=1.91 avg=1.51\n",
            "[5614 | 10971.30] loss=1.35 avg=1.51\n",
            "[5615 | 10973.02] loss=1.33 avg=1.51\n",
            "[5616 | 10974.72] loss=1.35 avg=1.51\n",
            "[5617 | 10976.44] loss=1.26 avg=1.51\n",
            "[5618 | 10978.14] loss=1.33 avg=1.51\n",
            "[5619 | 10979.86] loss=1.56 avg=1.51\n",
            "[5620 | 10981.57] loss=1.36 avg=1.50\n",
            "[5621 | 10983.28] loss=1.65 avg=1.51\n",
            "[5622 | 10984.99] loss=1.45 avg=1.50\n",
            "[5623 | 10986.70] loss=1.96 avg=1.51\n",
            "[5624 | 10988.42] loss=1.79 avg=1.51\n",
            "[5625 | 10990.13] loss=1.51 avg=1.51\n",
            "[5626 | 10991.84] loss=1.35 avg=1.51\n",
            "[5627 | 10993.55] loss=1.60 avg=1.51\n",
            "[5628 | 10995.26] loss=1.59 avg=1.51\n",
            "[5629 | 10997.00] loss=1.61 avg=1.51\n",
            "[5630 | 10998.71] loss=1.28 avg=1.51\n",
            "[5631 | 11000.41] loss=1.43 avg=1.51\n",
            "[5632 | 11002.13] loss=1.49 avg=1.51\n",
            "[5633 | 11003.84] loss=1.15 avg=1.51\n",
            "[5634 | 11005.55] loss=1.18 avg=1.50\n",
            "[5635 | 11007.26] loss=0.82 avg=1.50\n",
            "[5636 | 11008.97] loss=1.15 avg=1.49\n",
            "[5637 | 11010.68] loss=1.10 avg=1.49\n",
            "[5638 | 11012.39] loss=1.37 avg=1.49\n",
            "[5639 | 11014.11] loss=2.05 avg=1.49\n",
            "[5640 | 11015.81] loss=0.65 avg=1.48\n",
            "[5641 | 11017.53] loss=1.34 avg=1.48\n",
            "[5642 | 11019.24] loss=1.74 avg=1.49\n",
            "[5643 | 11020.95] loss=1.41 avg=1.49\n",
            "[5644 | 11022.66] loss=1.05 avg=1.48\n",
            "[5645 | 11024.37] loss=1.14 avg=1.48\n",
            "[5646 | 11026.09] loss=1.26 avg=1.48\n",
            "[5647 | 11027.80] loss=1.46 avg=1.48\n",
            "[5648 | 11029.52] loss=1.43 avg=1.47\n",
            "[5649 | 11031.23] loss=2.07 avg=1.48\n",
            "[5650 | 11032.95] loss=1.59 avg=1.48\n",
            "[5651 | 11034.67] loss=2.07 avg=1.49\n",
            "[5652 | 11036.39] loss=1.83 avg=1.49\n",
            "[5653 | 11038.10] loss=1.97 avg=1.50\n",
            "[5654 | 11039.81] loss=1.95 avg=1.50\n",
            "[5655 | 11041.52] loss=0.97 avg=1.49\n",
            "[5656 | 11043.24] loss=2.26 avg=1.50\n",
            "[5657 | 11044.94] loss=1.52 avg=1.50\n",
            "[5658 | 11046.66] loss=1.11 avg=1.50\n",
            "[5659 | 11048.36] loss=1.61 avg=1.50\n",
            "[5660 | 11050.08] loss=1.13 avg=1.50\n",
            "[5661 | 11051.78] loss=2.17 avg=1.50\n",
            "[5662 | 11053.50] loss=1.27 avg=1.50\n",
            "[5663 | 11055.21] loss=1.57 avg=1.50\n",
            "[5664 | 11056.92] loss=1.67 avg=1.50\n",
            "[5665 | 11058.63] loss=1.70 avg=1.51\n",
            "[5666 | 11060.34] loss=2.25 avg=1.51\n",
            "[5667 | 11062.05] loss=1.66 avg=1.51\n",
            "[5668 | 11063.77] loss=1.95 avg=1.52\n",
            "[5669 | 11065.48] loss=1.90 avg=1.52\n",
            "[5670 | 11067.19] loss=1.15 avg=1.52\n",
            "[5671 | 11068.90] loss=1.86 avg=1.52\n",
            "[5672 | 11070.61] loss=1.69 avg=1.52\n",
            "[5673 | 11072.32] loss=1.71 avg=1.53\n",
            "[5674 | 11074.04] loss=1.77 avg=1.53\n",
            "[5675 | 11075.75] loss=1.92 avg=1.53\n",
            "[5676 | 11077.46] loss=1.05 avg=1.53\n",
            "[5677 | 11079.17] loss=1.82 avg=1.53\n",
            "[5678 | 11080.89] loss=1.11 avg=1.53\n",
            "[5679 | 11082.60] loss=1.45 avg=1.52\n",
            "[5680 | 11084.30] loss=1.44 avg=1.52\n",
            "[5681 | 11086.02] loss=0.87 avg=1.52\n",
            "[5682 | 11087.73] loss=1.89 avg=1.52\n",
            "[5683 | 11089.44] loss=1.20 avg=1.52\n",
            "[5684 | 11091.15] loss=1.50 avg=1.52\n",
            "[5685 | 11092.86] loss=1.84 avg=1.52\n",
            "[5686 | 11094.57] loss=1.51 avg=1.52\n",
            "[5687 | 11096.29] loss=1.06 avg=1.52\n",
            "[5688 | 11098.01] loss=1.51 avg=1.52\n",
            "[5689 | 11099.72] loss=1.57 avg=1.52\n",
            "[5690 | 11101.44] loss=1.48 avg=1.52\n",
            "[5691 | 11103.15] loss=1.31 avg=1.51\n",
            "[5692 | 11104.86] loss=1.68 avg=1.52\n",
            "[5693 | 11106.58] loss=1.53 avg=1.52\n",
            "[5694 | 11108.29] loss=1.30 avg=1.51\n",
            "[5695 | 11110.01] loss=1.37 avg=1.51\n",
            "[5696 | 11111.72] loss=1.41 avg=1.51\n",
            "[5697 | 11113.44] loss=1.00 avg=1.51\n",
            "[5698 | 11115.14] loss=0.87 avg=1.50\n",
            "[5699 | 11116.86] loss=1.49 avg=1.50\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "guip. Please check back soon for my second installment.\n",
            "I can make or break a president - and I think it's great that Lester Holt of NBC is working hard to get rid of Don without ever letting you know it. You know the drill.\n",
            "@J_Poppa52  Yes and thanks.\n",
            "@MamaGya @NYPost Thanks.\n",
            "@toddjdell @billmaher You have to hit the nail on the head - don't be a pussy and become a billionaire ASAP.\n",
            "@paulteutulsr. Thank you.\n",
            "@TigerWoods Thanks.\n",
            "@dannyk113 They will be great.\n",
            "I never saw @Rosie when Bill was First Lady. Now I see her side of the story &amp; it will be interesting! Rosie can do even better than she has done!\n",
            "Bill Maher is an idiot that must be ousted from TV!\n",
            "Bill Maher should go to Canada where the people who hate him hate him far more &amp; he will never work again.\n",
            "Don King is right and so is Bob &amp; DK that @billmaher will never solve the mystery &amp; cure epilepsy. They are right.\n",
            "Will be on @foxandfriends at 8.00. .\n",
            ".@NYPost which has been losing a fortune just reports that it is getting \"greater competition\" from @NYTimes. Post is dying &amp; will soon be dead.\n",
            "\"We should not subsidize the Chinese into the 21st century.\" #TimeToGetTough  -- Prof. George Jowett  @DianneScavino\n",
            "How come nobody mentions the fact that Pres. Obama spent two days staying at the 'Winter White House' in Martha's Vineyard. Not a nice thing to do!\n",
            "Pres. Obama didn't have time to do a search in the Boston Marathon after the attack.\n",
            "I used to be associated with Don King until he married a beautiful Japanese beauty Kathy Griffin. So sad to see. @Rosie was great on @Oprah but not good enough for Kathy!\n",
            ".@WWE---@WWEPASSION---@WWELEVEL’s @TheDon doesn’t deserve the death penalty for attacking the @WWE Hall of Famer @EricBolling. A total joke!\n",
            "I was on an airplane. The FBI &amp; the Department of Justice &amp; @USNavy are at the Boston Marathon. @FBILAPD @Dept_Of_Justice @USNavy http://t.co/gjwDpQiE\n",
            ".@realDonaldTrump to @Joan_Rivers @Rosie   I would be honored to do one-on-one golf with you. Joan bring the ball rolling!\n",
            "I watched the #CelebrityApprentice on Sunday night. It was decent but the final three was terrible. This is supposed to be the best!\n",
            "@BrunetteSpud   Very good!\n",
            "@J_Poppa52 Thanks.\n",
            "@TheDeeJ  Thanks!\n",
            "@paulmwatters   Not a bad idea!\n",
            "@TK_Poppa @Beardymilne  Thanks they should give the hat to Eric after office is done.\n",
            "@taylorswift9  Go for it Taylorsweift.\n",
            "@mckay312  Thank you!\n",
            "@sashamallpresents Thank you!\n",
            "@joeyhoney  I will try!\n",
            "Just out- new ABC News Poll has Donald Trump and Hillary Clinton neck and neck with 4% each. Next is tied with 2% each. http://t.co/IbK1l3CmV\n",
            "@jeffcharney  Not much.\n",
            "How can dopey Chauncey Stevens be considered a friend when he defrauded people by quitting &amp; going on the record as being in bed w/ Paul Begala?\n",
            "@BurgessRunner22 Remember I never had a bad word to say about Chauncey Stevens. He’s a friend of mine &amp; one of the nicest &amp; nicest people.\n",
            "@mckay312 Thanks!\n",
            "@Beardymilne @BurgessRunner22 Thanks!\n",
            "@BrunetteSpud @Beardymilne @BurgessRunner22 @mckay312 Thanks!\n",
            "“I believe in following your passions. Visions can take you anywhere. There’s nothing wrong with that.” – Steve Jobs\n",
            "\"I was born to run.\" – Think Big\n",
            "“I’m a man who carries himself with an air of confidence around the world. The more people know about you the the the the the the the the the the world knows about you.” - The Art of the Deal\n",
            "“Budding\n",
            "\n",
            "[5700 | 11141.04] loss=0.96 avg=1.49\n",
            "[5701 | 11142.74] loss=1.92 avg=1.50\n",
            "[5702 | 11144.46] loss=1.06 avg=1.49\n",
            "[5703 | 11146.17] loss=1.12 avg=1.49\n",
            "[5704 | 11147.88] loss=1.51 avg=1.49\n",
            "[5705 | 11149.59] loss=1.42 avg=1.49\n",
            "[5706 | 11151.30] loss=1.68 avg=1.49\n",
            "[5707 | 11153.01] loss=2.66 avg=1.50\n",
            "[5708 | 11154.73] loss=1.71 avg=1.51\n",
            "[5709 | 11156.43] loss=0.82 avg=1.50\n",
            "[5710 | 11158.15] loss=1.30 avg=1.50\n",
            "[5711 | 11159.87] loss=1.62 avg=1.50\n",
            "[5712 | 11161.58] loss=1.59 avg=1.50\n",
            "[5713 | 11163.29] loss=1.84 avg=1.50\n",
            "[5714 | 11165.01] loss=0.83 avg=1.50\n",
            "[5715 | 11166.72] loss=1.24 avg=1.49\n",
            "[5716 | 11168.44] loss=1.34 avg=1.49\n",
            "[5717 | 11170.16] loss=1.14 avg=1.49\n",
            "[5718 | 11171.87] loss=1.64 avg=1.49\n",
            "[5719 | 11173.58] loss=1.69 avg=1.49\n",
            "[5720 | 11175.30] loss=1.64 avg=1.49\n",
            "[5721 | 11177.02] loss=1.76 avg=1.50\n",
            "[5722 | 11178.75] loss=1.63 avg=1.50\n",
            "[5723 | 11180.45] loss=1.16 avg=1.49\n",
            "[5724 | 11182.17] loss=1.29 avg=1.49\n",
            "[5725 | 11183.88] loss=1.86 avg=1.50\n",
            "[5726 | 11185.59] loss=1.30 avg=1.49\n",
            "[5727 | 11187.30] loss=2.22 avg=1.50\n",
            "[5728 | 11189.01] loss=1.33 avg=1.50\n",
            "[5729 | 11190.75] loss=1.16 avg=1.50\n",
            "[5730 | 11192.47] loss=1.42 avg=1.50\n",
            "[5731 | 11194.18] loss=1.00 avg=1.49\n",
            "[5732 | 11195.89] loss=1.49 avg=1.49\n",
            "[5733 | 11197.60] loss=1.47 avg=1.49\n",
            "[5734 | 11199.32] loss=1.26 avg=1.49\n",
            "[5735 | 11201.02] loss=0.92 avg=1.48\n",
            "[5736 | 11202.74] loss=1.86 avg=1.49\n",
            "[5737 | 11204.45] loss=1.24 avg=1.48\n",
            "[5738 | 11206.16] loss=1.72 avg=1.49\n",
            "[5739 | 11207.87] loss=1.07 avg=1.48\n",
            "[5740 | 11209.59] loss=1.31 avg=1.48\n",
            "[5741 | 11211.30] loss=1.18 avg=1.48\n",
            "[5742 | 11213.01] loss=1.61 avg=1.48\n",
            "[5743 | 11214.72] loss=2.20 avg=1.49\n",
            "[5744 | 11216.43] loss=1.89 avg=1.49\n",
            "[5745 | 11218.15] loss=1.61 avg=1.49\n",
            "[5746 | 11219.86] loss=1.50 avg=1.49\n",
            "[5747 | 11221.57] loss=1.69 avg=1.49\n",
            "[5748 | 11223.28] loss=0.84 avg=1.49\n",
            "[5749 | 11225.00] loss=1.99 avg=1.49\n",
            "[5750 | 11226.71] loss=1.57 avg=1.49\n",
            "[5751 | 11228.45] loss=1.50 avg=1.49\n",
            "[5752 | 11230.15] loss=1.87 avg=1.50\n",
            "[5753 | 11231.86] loss=1.78 avg=1.50\n",
            "[5754 | 11233.59] loss=1.62 avg=1.50\n",
            "[5755 | 11235.30] loss=1.30 avg=1.50\n",
            "[5756 | 11237.01] loss=1.39 avg=1.50\n",
            "[5757 | 11238.73] loss=1.67 avg=1.50\n",
            "[5758 | 11240.45] loss=1.87 avg=1.50\n",
            "[5759 | 11242.16] loss=1.84 avg=1.51\n",
            "[5760 | 11243.88] loss=2.11 avg=1.51\n",
            "[5761 | 11245.60] loss=1.18 avg=1.51\n",
            "[5762 | 11247.31] loss=1.20 avg=1.51\n",
            "[5763 | 11249.03] loss=1.36 avg=1.50\n",
            "[5764 | 11250.74] loss=1.59 avg=1.50\n",
            "[5765 | 11252.44] loss=1.35 avg=1.50\n",
            "[5766 | 11254.16] loss=1.46 avg=1.50\n",
            "[5767 | 11255.87] loss=1.92 avg=1.51\n",
            "[5768 | 11257.58] loss=1.76 avg=1.51\n",
            "[5769 | 11259.30] loss=1.63 avg=1.51\n",
            "[5770 | 11261.01] loss=1.13 avg=1.51\n",
            "[5771 | 11262.72] loss=1.25 avg=1.50\n",
            "[5772 | 11264.44] loss=1.47 avg=1.50\n",
            "[5773 | 11266.15] loss=1.54 avg=1.50\n",
            "[5774 | 11267.86] loss=1.17 avg=1.50\n",
            "[5775 | 11269.57] loss=1.15 avg=1.50\n",
            "[5776 | 11271.28] loss=1.38 avg=1.50\n",
            "[5777 | 11272.99] loss=1.50 avg=1.50\n",
            "[5778 | 11274.70] loss=1.15 avg=1.49\n",
            "[5779 | 11276.41] loss=0.67 avg=1.48\n",
            "[5780 | 11278.12] loss=2.46 avg=1.49\n",
            "[5781 | 11279.84] loss=1.42 avg=1.49\n",
            "[5782 | 11281.54] loss=2.31 avg=1.50\n",
            "[5783 | 11283.25] loss=1.56 avg=1.50\n",
            "[5784 | 11284.97] loss=0.82 avg=1.50\n",
            "[5785 | 11286.68] loss=0.99 avg=1.49\n",
            "[5786 | 11288.39] loss=1.02 avg=1.49\n",
            "[5787 | 11290.13] loss=1.20 avg=1.48\n",
            "[5788 | 11291.83] loss=1.45 avg=1.48\n",
            "[5789 | 11293.55] loss=1.90 avg=1.49\n",
            "[5790 | 11295.27] loss=1.61 avg=1.49\n",
            "[5791 | 11296.97] loss=3.19 avg=1.51\n",
            "[5792 | 11298.69] loss=1.88 avg=1.51\n",
            "[5793 | 11300.40] loss=1.99 avg=1.51\n",
            "[5794 | 11302.12] loss=2.93 avg=1.53\n",
            "[5795 | 11303.83] loss=1.72 avg=1.53\n",
            "[5796 | 11305.55] loss=0.66 avg=1.52\n",
            "[5797 | 11307.26] loss=0.82 avg=1.51\n",
            "[5798 | 11308.97] loss=2.70 avg=1.53\n",
            "[5799 | 11310.69] loss=1.92 avg=1.53\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " wonder about today!\n",
            "The Washington Post does an article on the failed ObamaCare website. The Post never writes the article wrong. The information is correct. The Post website is a total joke!\n",
            "@tomblason212 Thank you Tom - love the hat!\n",
            "It's about time that everyone from the New York Times to the San Jose Mercury News stop the total political correctness and stop trying to impose their narrow minded views on other countries. It is destroying their nations!\n",
            "The fact that people are still standing in line to get into the Miss Universe Pageant in Moscow after the show was moved 3 hours late is just a sad situation - TIME TO CHANGE!\n",
            ".@DennisRodman will be speaking at the @ageoldheart Spiritual Awards Dinner tonight in Washington D.C. @MissUniverse is being presented to some of the best young Unas... http://t.co/B9sjdN7g0l\n",
            "Via @thehill by @martinmatishak: “Trump: Obama 'Doesn't Understand The Challenges Of ISIL” http://t.co/jqXKQ5j6Hm  RT: @HerdManSports\n",
            "Obama once again ignored us on foreign policy. He just did a big military build up in the Persian Gulf and that will be it. He doesn't get why we are fighting for his country anymore.\n",
            "I am so disappointed in Colin Powell (and everyone else) for just walking away from his Iraq war briefing knowing there was no doubt in anyone mind that his advice was far superior to ours.....\n",
            "...Also I don’t think Jim Mattis has a clue.\n",
            "....I am so disappointed with Colin Powell (and everyone else) for just walking away from his Iraq war briefing knowing we were 100% right around the time there were strong reports that the CIA knew....\n",
            "Dummy Colin Powell who called my call and emailed me 2 days before the election just got caught lying to the public. I called you won it I e-mailed.He deserves it!\n",
            "Colin Powell just admitted he called Iraq war \"the right call.\" Where is the apology and why has he not left office...\n",
            "....Was there in any way doubt about the call or the wisdom of going into Iraq at the highest levels? We made a horrible and very stupid mistake.\n",
            ".@VanityFair has been dying in the industry for 30 years or so. Still stands for nothing. Loser!\n",
            "Why would anyone in my family ever think they could influence me in a positive way? I won’t be influenced in any way.\n",
            "A really dumb and old @VanityFair columnist just took a dig at me and her father. @EricTrump has a really high IQ.\n",
            "I don’t know the Colin Powell but I know the guy who called Iraq war \"the right call.\" His advice was far superior to mine...\n",
            "...That would be Colin S. Powell who said 9/11 would be much easier to explain if you go back 1 year. I call it research and analysis.\n",
            "We just took Mosul Iraq’s second largest city. A place I called in favor of going in never thought we would do so. Now we are celebrating!\n",
            "Can you even spell \"cancel\"? @LTHScotsGael can. Will be a massive disaster!\n",
            ".@GWPF  Great job and luck Joe!\n",
            ".@ToureItalia is no where near done. Will be amazing course. But will be a fantastic event!\n",
            ".@dennisrodman is a good guy who just doesn't have what it takes. He is lucky I won't let my talents be tested!\n",
            "\"To take another man's chance is the very definition of a fault. There is no such thing as a smart gambler there is only those who try hard and fail.\" - Aristotle\n",
            "It is going to be a massive disaster if this stupid Uranium One deal isnt done by Obama. People dont want Obama in office!\n",
            ".@nbc  Obama just approved of himself for the 14th time for being dumb and making the world go round. Too bad he had a bad day!\n",
            "“Diligence is the right of the wise.” -- Benjamin Disraeli\n",
            "@TiffanyJRB  Yes.\n",
            "Great going @brentkas   Thanks.\n",
            "Glad to see @MacMiller’s New album come out. You proved Brent wrong!\n",
            "@tjholler  It will be great!\n",
            "I am going to the @TennisWorldCSAs in Scotland and Italy to meet with the players. The U.S. will not make the cut - just wish them luck!\n",
            "The new Westin Punta Dona hotel in Dubai is going to be a masterpiece. It will be amazing. http://t.co/QM7vD3G8o7\n",
            "When will the BP oil spill be fixed or\n",
            "\n",
            "[5800 | 11334.85] loss=0.90 avg=1.52\n",
            "[5801 | 11336.56] loss=1.59 avg=1.52\n",
            "[5802 | 11338.29] loss=1.36 avg=1.52\n",
            "[5803 | 11340.00] loss=2.18 avg=1.53\n",
            "[5804 | 11341.72] loss=1.39 avg=1.53\n",
            "[5805 | 11343.43] loss=1.71 avg=1.53\n",
            "[5806 | 11345.14] loss=1.38 avg=1.53\n",
            "[5807 | 11346.85] loss=0.90 avg=1.52\n",
            "[5808 | 11348.57] loss=2.38 avg=1.53\n",
            "[5809 | 11350.28] loss=1.46 avg=1.53\n",
            "[5810 | 11351.99] loss=1.27 avg=1.53\n",
            "[5811 | 11353.70] loss=1.37 avg=1.53\n",
            "[5812 | 11355.41] loss=1.68 avg=1.53\n",
            "[5813 | 11357.13] loss=1.65 avg=1.53\n",
            "[5814 | 11358.84] loss=2.51 avg=1.54\n",
            "[5815 | 11360.55] loss=1.78 avg=1.54\n",
            "[5816 | 11362.27] loss=1.73 avg=1.54\n",
            "[5817 | 11364.00] loss=1.41 avg=1.54\n",
            "[5818 | 11365.71] loss=0.79 avg=1.53\n",
            "[5819 | 11367.42] loss=2.89 avg=1.55\n",
            "[5820 | 11369.13] loss=0.81 avg=1.54\n",
            "[5821 | 11370.85] loss=1.47 avg=1.54\n",
            "[5822 | 11372.56] loss=1.99 avg=1.54\n",
            "[5823 | 11374.27] loss=1.13 avg=1.54\n",
            "[5824 | 11375.99] loss=2.77 avg=1.55\n",
            "[5825 | 11377.71] loss=2.50 avg=1.56\n",
            "[5826 | 11379.42] loss=3.18 avg=1.58\n",
            "[5827 | 11381.14] loss=1.33 avg=1.58\n",
            "[5828 | 11382.85] loss=1.43 avg=1.57\n",
            "[5829 | 11384.59] loss=0.78 avg=1.57\n",
            "[5830 | 11386.29] loss=1.29 avg=1.56\n",
            "[5831 | 11388.00] loss=1.51 avg=1.56\n",
            "[5832 | 11389.72] loss=1.21 avg=1.56\n",
            "[5833 | 11391.43] loss=1.20 avg=1.56\n",
            "[5834 | 11393.14] loss=1.98 avg=1.56\n",
            "[5835 | 11394.86] loss=1.12 avg=1.56\n",
            "[5836 | 11396.57] loss=1.06 avg=1.55\n",
            "[5837 | 11398.27] loss=1.11 avg=1.55\n",
            "[5838 | 11399.99] loss=0.71 avg=1.54\n",
            "[5839 | 11401.70] loss=1.47 avg=1.54\n",
            "[5840 | 11403.42] loss=1.31 avg=1.53\n",
            "[5841 | 11405.13] loss=1.57 avg=1.53\n",
            "[5842 | 11406.84] loss=1.53 avg=1.53\n",
            "[5843 | 11408.55] loss=1.77 avg=1.54\n",
            "[5844 | 11410.26] loss=1.25 avg=1.53\n",
            "[5845 | 11411.97] loss=2.28 avg=1.54\n",
            "[5846 | 11413.69] loss=1.66 avg=1.54\n",
            "[5847 | 11415.41] loss=2.42 avg=1.55\n",
            "[5848 | 11417.13] loss=1.75 avg=1.55\n",
            "[5849 | 11418.84] loss=0.87 avg=1.55\n",
            "[5850 | 11420.55] loss=1.28 avg=1.54\n",
            "[5851 | 11422.27] loss=1.13 avg=1.54\n",
            "[5852 | 11424.00] loss=0.99 avg=1.53\n",
            "[5853 | 11425.70] loss=1.09 avg=1.53\n",
            "[5854 | 11427.42] loss=1.80 avg=1.53\n",
            "[5855 | 11429.14] loss=1.97 avg=1.54\n",
            "[5856 | 11430.85] loss=1.76 avg=1.54\n",
            "[5857 | 11432.56] loss=1.77 avg=1.54\n",
            "[5858 | 11434.28] loss=1.23 avg=1.54\n",
            "[5859 | 11435.99] loss=1.39 avg=1.54\n",
            "[5860 | 11437.71] loss=1.43 avg=1.54\n",
            "[5861 | 11439.42] loss=1.49 avg=1.54\n",
            "[5862 | 11441.13] loss=0.89 avg=1.53\n",
            "[5863 | 11442.85] loss=0.99 avg=1.52\n",
            "[5864 | 11444.57] loss=1.46 avg=1.52\n",
            "[5865 | 11446.28] loss=1.04 avg=1.52\n",
            "[5866 | 11447.99] loss=1.13 avg=1.51\n",
            "[5867 | 11449.71] loss=1.66 avg=1.52\n",
            "[5868 | 11451.42] loss=1.64 avg=1.52\n",
            "[5869 | 11453.14] loss=0.86 avg=1.51\n",
            "[5870 | 11454.85] loss=2.00 avg=1.52\n",
            "[5871 | 11456.56] loss=1.16 avg=1.51\n",
            "[5872 | 11458.27] loss=2.45 avg=1.52\n",
            "[5873 | 11459.98] loss=1.48 avg=1.52\n",
            "[5874 | 11461.70] loss=1.04 avg=1.52\n",
            "[5875 | 11463.41] loss=1.84 avg=1.52\n",
            "[5876 | 11465.12] loss=1.31 avg=1.52\n",
            "[5877 | 11466.83] loss=0.75 avg=1.51\n",
            "[5878 | 11468.54] loss=1.16 avg=1.51\n",
            "[5879 | 11470.26] loss=1.89 avg=1.51\n",
            "[5880 | 11471.97] loss=1.60 avg=1.51\n",
            "[5881 | 11473.68] loss=1.12 avg=1.51\n",
            "[5882 | 11475.39] loss=2.11 avg=1.51\n",
            "[5883 | 11477.10] loss=1.54 avg=1.51\n",
            "[5884 | 11478.81] loss=2.04 avg=1.52\n",
            "[5885 | 11480.52] loss=1.62 avg=1.52\n",
            "[5886 | 11482.24] loss=1.21 avg=1.52\n",
            "[5887 | 11483.95] loss=1.48 avg=1.52\n",
            "[5888 | 11485.66] loss=1.18 avg=1.51\n",
            "[5889 | 11487.37] loss=2.29 avg=1.52\n",
            "[5890 | 11489.08] loss=1.52 avg=1.52\n",
            "[5891 | 11490.80] loss=1.67 avg=1.52\n",
            "[5892 | 11492.50] loss=1.80 avg=1.52\n",
            "[5893 | 11494.22] loss=1.63 avg=1.53\n",
            "[5894 | 11495.93] loss=1.39 avg=1.52\n",
            "[5895 | 11497.64] loss=1.18 avg=1.52\n",
            "[5896 | 11499.35] loss=1.64 avg=1.52\n",
            "[5897 | 11501.06] loss=1.22 avg=1.52\n",
            "[5898 | 11502.78] loss=1.13 avg=1.52\n",
            "[5899 | 11504.49] loss=2.08 avg=1.52\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "h4Ez\n",
            "If I run for President I will not sign any Amnesty!\n",
            "I see that @politico has just asked if I would support a @CNN POLITICIAN SINGLE HEADPHONE IN BORDER PEACE. Too bad dummy!\n",
            "Why would the Republicans allow a lame-duck @GOP Leader to delay the vote on lowering the debt ceiling by a year. Time to unite and DROP THE BENGHAZI!\n",
            "\"Trump signs memorandum revamping U.S. Citizenship and Immigration Services\" http://t.co/f1rL8y9e0G\n",
            "Via @WSJ by @JenniferJHalper: \"Donald Trump signs letter revamping U.S. Citizenship and Immigration Services\" http://t.co/7x0p9u4Wbq\n",
            "The Republican Party must get tough and smart fast if it wants to be relevant in future elections. The Country is in big trouble if nobody wins.\n",
            "The U.S. needs strong border security better building and construction and having the right immigration laws. We need to be smart on immigration but tough.\n",
            "If @politico  or anyone else wants to talk about my big news conference in front of the Department of Homeland Security for the President they should mention it.\n",
            "The big United Nations Human Rights Council meeting today! The Dems and others show great disrespect for the U.S. which is being hurtful to Human Rights!\n",
            "The only place a man should be asked to leave a UN meeting is if he is a terrorist - a very VERY BAD thing to say! Be nice everyone!\n",
            "I want to travel the world much faster than anyone else. Ask any great player or leader of any great country. I'll bet they'd rather play in the NFL than on the world stage!\n",
            "My company will lose a fortune because of the false and horrible headlines &amp; false statements coming from the very weak and corrupt press. Great leader with only very limited means!\n",
            "I have made a fortune in real estate but only through hard work and talent. I am very proud of what I do &amp; have made a fortune off of it.\n",
            "I never went bankrupt and I am not related to Robert Trent Treadwell Josh Broadwater George Steinbrenner etc. I have no loans or other obligations with them or any of the others.\n",
            "I will be going to the Middle East and other related events in the next several months. The world owes me an apology.\n",
            "@politico is a total joke! A sad day for journalism and really bad journalism--a total laughingstock.\n",
            "The Dems and others in the media have now totally covered me correctly with little to no correction from their sources among them me. The press has truly become the enemy of the People!\n",
            "I hear@CNN is going down in the polls fast. Their interviews are a joke compared to that of @NBCUNews. The 2nd is a big hit and always gets much more airtime. Tough!\n",
            "As usual the two rigged debates will be a big hit on @CNN- but before they even debate they are already releasing their polls which are a big hit on me. https://t.co/xlqePJiCJQ\n",
            "I have had to put up with the rigged and dishonest Democratic debates from the very beginning. Nobody but the best and brightest will do it tonight!\n",
            "I am watching @CNN very carefully. They do not have a lot of credibility especially when it comes to their biased interviews of me. @CNN\n",
            "I see that @CNN is doing live debates with Bernie Sanders and Hillary Clinton on Wednesday/Thursday. They will be very interesting and interesting to watch with the large voter turnout.\n",
            "I think we have all witnessed what a phony and dopesession @CNN is. They have become a laughingstock the last 2 years with their  interviews of me.\n",
            "I see that Fox News and the other two networks are doing live debates with Bernie Sanders and Hillary Clinton on Wednesday/Thursday. Big crowds expected.\n",
            "The U.S. should not accept people from any part of the world into our country until we thoroughly investigate their intentions and be able to trust them.\n",
            "The United States without a rigorous immigration system cannot be successful - we need people who will fill the ranks of those who hate us\n",
            "I want the people of our country to know that we are listening - and waiting - for any problems you may have with your new President. We are waiting for your sign up!\n",
            "I will be holding a big rally in Ohio tonight with the two great men themselves Senator Tim Scott and @potter. The crowd loved it!\n",
            "As President I will work long and hard to make it possible for all young people in our country to get jobs so that they can start their own families.\n",
            "Great to meet with and shake the hands and feet of more than 25000 young people at a recent @Citizens_United event. Huge supporter! https://t.co/0dCn7\n",
            "\n",
            "[5900 | 11528.40] loss=1.07 avg=1.52\n",
            "[5901 | 11530.10] loss=1.40 avg=1.52\n",
            "[5902 | 11531.82] loss=2.13 avg=1.52\n",
            "[5903 | 11533.53] loss=1.83 avg=1.52\n",
            "[5904 | 11535.24] loss=1.09 avg=1.52\n",
            "[5905 | 11536.95] loss=1.31 avg=1.52\n",
            "[5906 | 11538.66] loss=1.75 avg=1.52\n",
            "[5907 | 11540.37] loss=1.56 avg=1.52\n",
            "[5908 | 11542.08] loss=1.05 avg=1.52\n",
            "[5909 | 11543.79] loss=1.66 avg=1.52\n",
            "[5910 | 11545.50] loss=1.97 avg=1.52\n",
            "[5911 | 11547.22] loss=2.23 avg=1.53\n",
            "[5912 | 11548.92] loss=1.53 avg=1.53\n",
            "[5913 | 11550.63] loss=1.18 avg=1.53\n",
            "[5914 | 11552.34] loss=2.22 avg=1.53\n",
            "[5915 | 11554.06] loss=2.01 avg=1.54\n",
            "[5916 | 11555.76] loss=1.17 avg=1.53\n",
            "[5917 | 11557.47] loss=1.58 avg=1.53\n",
            "[5918 | 11559.18] loss=1.29 avg=1.53\n",
            "[5919 | 11560.89] loss=1.56 avg=1.53\n",
            "[5920 | 11562.61] loss=1.58 avg=1.53\n",
            "[5921 | 11564.32] loss=1.34 avg=1.53\n",
            "[5922 | 11566.03] loss=2.01 avg=1.54\n",
            "[5923 | 11567.75] loss=1.63 avg=1.54\n",
            "[5924 | 11569.47] loss=1.28 avg=1.53\n",
            "[5925 | 11571.17] loss=0.92 avg=1.53\n",
            "[5926 | 11572.89] loss=1.21 avg=1.52\n",
            "[5927 | 11574.61] loss=1.49 avg=1.52\n",
            "[5928 | 11576.32] loss=1.35 avg=1.52\n",
            "[5929 | 11578.03] loss=1.43 avg=1.52\n",
            "[5930 | 11579.75] loss=1.82 avg=1.52\n",
            "[5931 | 11581.46] loss=1.69 avg=1.53\n",
            "[5932 | 11583.18] loss=1.12 avg=1.52\n",
            "[5933 | 11584.89] loss=2.98 avg=1.54\n",
            "[5934 | 11586.60] loss=1.37 avg=1.54\n",
            "[5935 | 11588.32] loss=1.65 avg=1.54\n",
            "[5936 | 11590.02] loss=1.36 avg=1.53\n",
            "[5937 | 11591.73] loss=1.24 avg=1.53\n",
            "[5938 | 11593.44] loss=1.01 avg=1.53\n",
            "[5939 | 11595.15] loss=1.45 avg=1.53\n",
            "[5940 | 11596.86] loss=1.43 avg=1.52\n",
            "[5941 | 11598.57] loss=1.23 avg=1.52\n",
            "[5942 | 11600.28] loss=1.91 avg=1.53\n",
            "[5943 | 11601.99] loss=1.55 avg=1.53\n",
            "[5944 | 11603.71] loss=1.43 avg=1.52\n",
            "[5945 | 11605.42] loss=0.85 avg=1.52\n",
            "[5946 | 11607.13] loss=1.15 avg=1.51\n",
            "[5947 | 11608.87] loss=1.44 avg=1.51\n",
            "[5948 | 11610.57] loss=1.88 avg=1.52\n",
            "[5949 | 11612.29] loss=2.40 avg=1.53\n",
            "[5950 | 11614.00] loss=1.42 avg=1.52\n",
            "[5951 | 11615.71] loss=2.97 avg=1.54\n",
            "[5952 | 11617.42] loss=1.78 avg=1.54\n",
            "[5953 | 11619.13] loss=1.30 avg=1.54\n",
            "[5954 | 11620.84] loss=1.05 avg=1.53\n",
            "[5955 | 11622.56] loss=1.40 avg=1.53\n",
            "[5956 | 11624.27] loss=1.84 avg=1.54\n",
            "[5957 | 11625.98] loss=1.99 avg=1.54\n",
            "[5958 | 11627.69] loss=0.67 avg=1.53\n",
            "[5959 | 11629.40] loss=1.36 avg=1.53\n",
            "[5960 | 11631.12] loss=1.07 avg=1.53\n",
            "[5961 | 11632.82] loss=1.36 avg=1.52\n",
            "[5962 | 11634.54] loss=1.64 avg=1.53\n",
            "[5963 | 11636.25] loss=1.87 avg=1.53\n",
            "[5964 | 11637.97] loss=0.96 avg=1.52\n",
            "[5965 | 11639.68] loss=1.74 avg=1.53\n",
            "[5966 | 11641.39] loss=1.34 avg=1.52\n",
            "[5967 | 11643.11] loss=1.37 avg=1.52\n",
            "[5968 | 11644.83] loss=1.50 avg=1.52\n",
            "[5969 | 11646.54] loss=2.01 avg=1.53\n",
            "[5970 | 11648.26] loss=1.23 avg=1.52\n",
            "[5971 | 11649.97] loss=1.32 avg=1.52\n",
            "[5972 | 11651.68] loss=0.95 avg=1.52\n",
            "[5973 | 11653.40] loss=1.21 avg=1.51\n",
            "[5974 | 11655.12] loss=1.89 avg=1.52\n",
            "[5975 | 11656.82] loss=0.74 avg=1.51\n",
            "[5976 | 11658.54] loss=1.28 avg=1.51\n",
            "[5977 | 11660.25] loss=0.95 avg=1.50\n",
            "[5978 | 11661.96] loss=1.37 avg=1.50\n",
            "[5979 | 11663.67] loss=0.81 avg=1.49\n",
            "[5980 | 11665.39] loss=1.46 avg=1.49\n",
            "[5981 | 11667.10] loss=1.79 avg=1.50\n",
            "[5982 | 11668.80] loss=1.52 avg=1.50\n",
            "[5983 | 11670.52] loss=1.76 avg=1.50\n",
            "[5984 | 11672.24] loss=0.74 avg=1.49\n",
            "[5985 | 11673.94] loss=1.83 avg=1.49\n",
            "[5986 | 11675.66] loss=2.24 avg=1.50\n",
            "[5987 | 11677.37] loss=1.19 avg=1.50\n",
            "[5988 | 11679.08] loss=0.98 avg=1.49\n",
            "[5989 | 11680.79] loss=1.91 avg=1.50\n",
            "[5990 | 11682.50] loss=1.19 avg=1.49\n",
            "[5991 | 11684.21] loss=0.46 avg=1.48\n",
            "[5992 | 11685.92] loss=1.76 avg=1.49\n",
            "[5993 | 11687.63] loss=0.96 avg=1.48\n",
            "[5994 | 11689.34] loss=0.94 avg=1.48\n",
            "[5995 | 11691.05] loss=0.93 avg=1.47\n",
            "[5996 | 11692.77] loss=0.99 avg=1.47\n",
            "[5997 | 11694.48] loss=2.06 avg=1.47\n",
            "[5998 | 11696.19] loss=2.37 avg=1.48\n",
            "[5999 | 11697.91] loss=1.70 avg=1.48\n",
            "Saving /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0814 19:53:59.405186 140046045587328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ": http://t.co/s6zMVhcBZk\n",
            "http://t.co/MdDzAaHb9z  Good place. Going to @RNC convention now.\n",
            "http://t.co/0nIk2Mh1W5  Remember that the media never reports the real story -- 49ers won't complain when there is SIN behind all of their success...\n",
            "@thechicran  True and thanks!\n",
            "@jeremymery821 I agree a lot of good will come from this.\n",
            "\"Democrats have a big problem in San Jose...San Jose is the hottest area in America. So where is the media outrage?\" - @kimguilfoyle\n",
            "\"Donald Trump's Hotter Than Hell Rally\" http://t.co/v9JwfH1QkH via @FOXSports\n",
            "The @chicagotribune has a great report - http://t.co/X0WGmfRfP6\n",
            "“Donald Trump: 47% Say Economy Worsens Before ‘Merry Christmas”  http://t.co/JWb1GgU7I4  via @washingtonpost\n",
            "Watch this behind the scenes video- http://t.co/6fHnQjnO0V  with @ApprenticeNBC.\n",
            "\"Trump: San Jose [is] doing really badly so media shouldn’t make any trouble there.\" http://t.co/2pF9y1pR7C via @washingtonpost by @OConnellPostbiz\n",
            "“Donald Trump: Media Ain't Talking About Me: I Love to Get Ripped: http://t.co/gNb0gqcj1T  via @washingtonpost by @OConnellPostbiz\n",
            ".@jessebwatters w/@bretbaier is back on the comeback trail. Start your blogs at http://t.co/bN4kZg3c5P!\n",
            "Via @MailOnline @alexsalmond has claimed that a @BBCNews poll’s confidence in @DavidPathopoulos’s forecast has “t changed.\"\n",
            "Congratulations to @BretBaier on the record 30th anniversary of “Sharknado 2.0” http://t.co/Kk9z9wXg8T. Baier just parlayed “Sharknado\" into his “most followed article on Trump” at http://t.co/gN4kZg3c5P\n",
            "“Donald Trump: 'This Is the Man' http://t.co/5u3qkpK3xk via @FoxNews\n",
            "\"One of the most remarkable books I've ever seen -- 'Donald Trump: The Deals and the Downfalls'\" http://t.co/yIblg7tVvj via @nypost by Katharine Crowley\n",
            "\"@TheGuru71: Donald Trump\" by Donald J. Trump is a classic that should be required viewing for all students.\n",
            "Wow---the Jets really went for it today. Good job Eric (they gave up too much). The EPL is dead.\n",
            "My interview with Greta Van Susteren on her new @Telemundo1 show http://t.co/Q6xQhQ2Rqj\n",
            "Via @CBNNews by @JonathonBashin: “Donald Trump: Iran Will Be Treated Like a 'Rock Star' ” http://t.co/ZbvB9FoQRk\n",
            "Via @bostonherald by John Skipper: “Donald Trump: ‘We're Kind of Rich’” http://t.co/Z9oF9gOaQw\n",
            "I'll be on @bostonherald this morning at 7 PM. Tune in!\n",
            "Via @foxnews: \"Trump blasts 'disingenuous writing' in book on ‘famous’ father\"http://t.co/cG3q2VpYz6\n",
            "Via CBS News: \"Trump: There Really Was ‘Dishonest’ Reporting’ on DNC\" http://t.co/6U6s3tWXdT  via @FOXNews\n",
            "“Good entrepreneurs are good parents.” – Think Like a Billionaire\n",
            "“The hard truth is that you can't please everybody.” --  Midas Touch\n",
            "Wacko A-Rod writer Maxwell launches defamation case against me. Nothing I can do if he wins. He is not a hyphenated name! Disgusting.\n",
            "@bretbaier has a terrific new book about realtors and their success called \"My Turn\" (Simon & Schuster).\n",
            "\n",
            "[6000 | 11735.82] loss=1.42 avg=1.48\n",
            "[6001 | 11737.48] loss=1.14 avg=1.48\n",
            "[6002 | 11739.17] loss=1.68 avg=1.48\n",
            "[6003 | 11740.83] loss=1.91 avg=1.49\n",
            "[6004 | 11742.50] loss=1.81 avg=1.49\n",
            "[6005 | 11744.17] loss=1.06 avg=1.48\n",
            "[6006 | 11745.84] loss=1.67 avg=1.49\n",
            "[6007 | 11747.53] loss=0.99 avg=1.48\n",
            "[6008 | 11749.20] loss=1.11 avg=1.48\n",
            "[6009 | 11750.87] loss=1.44 avg=1.48\n",
            "[6010 | 11752.55] loss=1.16 avg=1.47\n",
            "[6011 | 11754.22] loss=1.89 avg=1.48\n",
            "[6012 | 11755.92] loss=1.40 avg=1.48\n",
            "[6013 | 11757.60] loss=1.79 avg=1.48\n",
            "[6014 | 11759.27] loss=1.72 avg=1.48\n",
            "[6015 | 11760.95] loss=1.35 avg=1.48\n",
            "[6016 | 11762.63] loss=1.20 avg=1.48\n",
            "[6017 | 11764.31] loss=1.65 avg=1.48\n",
            "[6018 | 11765.99] loss=2.16 avg=1.49\n",
            "[6019 | 11767.68] loss=2.20 avg=1.49\n",
            "[6020 | 11769.35] loss=2.06 avg=1.50\n",
            "[6021 | 11771.02] loss=1.41 avg=1.50\n",
            "[6022 | 11772.70] loss=1.46 avg=1.50\n",
            "[6023 | 11774.41] loss=1.19 avg=1.50\n",
            "[6024 | 11776.11] loss=1.48 avg=1.50\n",
            "[6025 | 11777.79] loss=1.91 avg=1.50\n",
            "[6026 | 11779.48] loss=1.61 avg=1.50\n",
            "[6027 | 11781.17] loss=0.85 avg=1.49\n",
            "[6028 | 11782.86] loss=1.07 avg=1.49\n",
            "[6029 | 11784.55] loss=1.66 avg=1.49\n",
            "[6030 | 11786.24] loss=1.07 avg=1.49\n",
            "[6031 | 11787.94] loss=1.77 avg=1.49\n",
            "[6032 | 11789.61] loss=1.38 avg=1.49\n",
            "[6033 | 11791.29] loss=1.33 avg=1.49\n",
            "[6034 | 11792.98] loss=1.10 avg=1.48\n",
            "[6035 | 11794.67] loss=0.95 avg=1.48\n",
            "[6036 | 11796.37] loss=1.26 avg=1.48\n",
            "[6037 | 11798.06] loss=1.61 avg=1.48\n",
            "[6038 | 11799.75] loss=1.13 avg=1.47\n",
            "[6039 | 11801.44] loss=2.42 avg=1.48\n",
            "[6040 | 11803.14] loss=1.65 avg=1.48\n",
            "[6041 | 11804.82] loss=1.90 avg=1.49\n",
            "[6042 | 11806.51] loss=0.95 avg=1.48\n",
            "[6043 | 11808.21] loss=1.99 avg=1.49\n",
            "[6044 | 11809.90] loss=1.93 avg=1.49\n",
            "[6045 | 11811.60] loss=1.20 avg=1.49\n",
            "[6046 | 11813.29] loss=1.85 avg=1.49\n",
            "[6047 | 11815.00] loss=2.60 avg=1.51\n",
            "[6048 | 11816.72] loss=1.17 avg=1.50\n",
            "[6049 | 11818.41] loss=1.10 avg=1.50\n",
            "[6050 | 11820.10] loss=1.38 avg=1.50\n",
            "[6051 | 11821.80] loss=1.66 avg=1.50\n",
            "[6052 | 11823.51] loss=2.52 avg=1.51\n",
            "[6053 | 11825.22] loss=1.14 avg=1.50\n",
            "[6054 | 11826.92] loss=1.85 avg=1.51\n",
            "[6055 | 11828.62] loss=1.50 avg=1.51\n",
            "[6056 | 11830.32] loss=1.63 avg=1.51\n",
            "[6057 | 11832.01] loss=1.76 avg=1.51\n",
            "[6058 | 11833.70] loss=1.26 avg=1.51\n",
            "[6059 | 11835.39] loss=0.80 avg=1.50\n",
            "[6060 | 11837.08] loss=0.82 avg=1.50\n",
            "[6061 | 11838.77] loss=1.26 avg=1.49\n",
            "[6062 | 11840.46] loss=0.80 avg=1.49\n",
            "[6063 | 11842.16] loss=1.19 avg=1.48\n",
            "[6064 | 11843.86] loss=1.50 avg=1.48\n",
            "[6065 | 11845.56] loss=1.13 avg=1.48\n",
            "[6066 | 11847.26] loss=1.24 avg=1.48\n",
            "[6067 | 11848.96] loss=1.38 avg=1.48\n",
            "[6068 | 11850.65] loss=1.82 avg=1.48\n",
            "[6069 | 11852.36] loss=1.49 avg=1.48\n",
            "[6070 | 11854.06] loss=1.30 avg=1.48\n",
            "[6071 | 11855.78] loss=0.97 avg=1.47\n",
            "[6072 | 11857.49] loss=1.64 avg=1.47\n",
            "[6073 | 11859.19] loss=1.20 avg=1.47\n",
            "[6074 | 11860.89] loss=1.17 avg=1.47\n",
            "[6075 | 11862.60] loss=1.13 avg=1.47\n",
            "[6076 | 11864.31] loss=1.83 avg=1.47\n",
            "[6077 | 11866.02] loss=1.54 avg=1.47\n",
            "[6078 | 11867.74] loss=2.14 avg=1.48\n",
            "[6079 | 11869.44] loss=0.94 avg=1.47\n",
            "[6080 | 11871.14] loss=1.03 avg=1.47\n",
            "[6081 | 11872.86] loss=1.06 avg=1.46\n",
            "[6082 | 11874.56] loss=1.36 avg=1.46\n",
            "[6083 | 11876.28] loss=1.93 avg=1.47\n",
            "[6084 | 11877.99] loss=1.33 avg=1.47\n",
            "[6085 | 11879.69] loss=1.46 avg=1.47\n",
            "[6086 | 11881.41] loss=1.03 avg=1.46\n",
            "[6087 | 11883.12] loss=1.16 avg=1.46\n",
            "[6088 | 11884.83] loss=3.27 avg=1.48\n",
            "[6089 | 11886.54] loss=1.34 avg=1.47\n",
            "[6090 | 11888.25] loss=1.29 avg=1.47\n",
            "[6091 | 11889.96] loss=0.80 avg=1.47\n",
            "[6092 | 11891.66] loss=1.63 avg=1.47\n",
            "[6093 | 11893.37] loss=1.27 avg=1.47\n",
            "[6094 | 11895.09] loss=0.71 avg=1.46\n",
            "[6095 | 11896.80] loss=1.70 avg=1.46\n",
            "[6096 | 11898.49] loss=1.27 avg=1.46\n",
            "[6097 | 11900.20] loss=1.70 avg=1.46\n",
            "[6098 | 11901.91] loss=0.92 avg=1.46\n",
            "[6099 | 11903.62] loss=1.15 avg=1.45\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ack @CNN.\n",
            "\"Donald Trump says his business skills are better than President Obama's\"http://t.co/nM7XJ5a0l0\n",
            "Via @CNNMoney by @mstewart1: “Trump: I'm not raising taxes ‘but what we’re gonna do is we’re gonna repeal ObamaCare’ and replace it w/ new system”  http://t.co/7q3wY8aNcB\n",
            "Congrats to @GretchenCarlson and @SeanHannity on the best show and interview categories in new Fox 3 news at 10PM ET.\n",
            "\"The Democrats are the problem not the Republicans.\" #Trump2016\n",
            "\"A politician is only as good as his trust . . . .\" #Trump2016\n",
            "As I predicted this was a disastrous year for the @SBAgov agency that I left in 2013. http://t.co/9wKgTZ0eJp\n",
            ".@BarackObama's plan would cost over $30B over a decade--more than the entire budget of Ferguson police department.\n",
            ".@BarackObama's economic recovery has been one of the worst in the modern era.\n",
            "It's Tuesday. How many more times will the GOP debate the Republican nominee? This should never go forward.\n",
            "@Santini1010  Hi Santini\n",
            "The Fed's reckless monetary policy is driving up interest rates fueling a bubble and keeping rates artificially high.\n",
            "ObamaCare will destroy America. A website will cost far more and the product will be inferior waste and credit. Very dishonest!\n",
            "\"In the end every leader who sets out to find out what is best for men is always correct.\" -- Winston Churchill\n",
            "The GOP candidates are all forgetting one fact - under President Obama unemployment is up 9.2%.\n",
            "“If you like your healthcare plan you keep it.” - Ronald Reagan\n",
            ".@sarahreizenfeld has NEVER lost a GOP debate. She now said that ObamaCare is a disaster. So why not? She is a nasty competitor!\n",
            "The @Santini1010 @sarahreizenfeld is one of the most boring and least exciting of the @GOP field. Is it getting her the press?\n",
            "My honor. Join me for a debate rally at 2:00pmE in Wilmington North Carolina.Tickets:https://t.co/7Ei3C8zrjI\n",
            "#ICYMI: #GOPdebate theme – 'Buyer's remorse' http://t.co/Gg3K4cTq2e\n",
            "#Trump2016 #GOPdebate #TrumpTODAY https://t.co/sZ9M7QAuLW\n",
            "My @SquawkCNBC #GOPdebate interview discussing ObamaCare the economy 'rigged' the @MSMulcair @JRubio &amp; 'big' Jig.  http://t.co/aCZkXZhxzU\n",
            "\"Real leaders know that when America is united the American dream once again has a happy and fulfilling meaning to every day Americans.\" - Donald J. Trump\n",
            ".@Santini1010 is a great example of why we must build a \"great wall\" to secure the Mexican border.  Watch my interview w/ Santini below:\n",
            "We will build a great wall and LA will finally be treated fairly by the United States government.\n",
            "Our new African Union will treat us fairly - We want our money back! We want our borders secured.\n",
            "\"The man is always right. — Donald J. Trump\"http://t.co/Bz0E7v0XCc\n",
            "I am going to build a great wall and protect LA! Vote today for your LA HEROES and JOBS - #LoveGoesGlobal!\n",
            "#Trump2016 #LoveGoesGlobal  Watch my interview with @jimmyfallon on @FallonTV tonight at 8:40 P.M.\n",
            "I will bring steel and great people into the SENATE if elected PENCE #Trump2016\n",
            "The MSM is losing all credibility- these ads are not about US. They are about me. Don't lie!\n",
            "I am signing books so that I can spend more time with my wonderful family.\n",
            "#VoteTrump #Trump2016https://t:https://t.co/W6jvJ4hDq5\n",
            "Just like beautiful Indiana I am going to bring millions of jobs and wealth back to Pennsylvania with #Trump2016\n",
            "#Trump2016 #LoveGoesGlobal WATCH: http://t.co/B4yPvbGjXZ Watch my appearance w/ @JFKGreatFriends: http://t.co/9LcVJfhEjZ\n",
            "This is what the world looks like without us: http://t.co/2LwJiPYxV2\n",
            "\"The\n",
            "\n",
            "[6100 | 11927.56] loss=0.90 avg=1.45\n",
            "[6101 | 11929.27] loss=0.85 avg=1.44\n",
            "[6102 | 11930.98] loss=1.80 avg=1.44\n",
            "[6103 | 11932.70] loss=1.58 avg=1.45\n",
            "[6104 | 11934.42] loss=1.55 avg=1.45\n",
            "[6105 | 11936.12] loss=1.04 avg=1.44\n",
            "[6106 | 11937.83] loss=1.41 avg=1.44\n",
            "[6107 | 11939.55] loss=1.25 avg=1.44\n",
            "[6108 | 11941.26] loss=1.14 avg=1.44\n",
            "[6109 | 11942.97] loss=1.13 avg=1.43\n",
            "[6110 | 11944.69] loss=0.90 avg=1.43\n",
            "[6111 | 11946.39] loss=1.87 avg=1.43\n",
            "[6112 | 11948.10] loss=1.35 avg=1.43\n",
            "[6113 | 11949.82] loss=1.44 avg=1.43\n",
            "[6114 | 11951.52] loss=1.97 avg=1.44\n",
            "[6115 | 11953.24] loss=1.36 avg=1.44\n",
            "[6116 | 11954.95] loss=1.14 avg=1.43\n",
            "[6117 | 11956.67] loss=0.97 avg=1.43\n",
            "[6118 | 11958.37] loss=0.96 avg=1.43\n",
            "[6119 | 11960.08] loss=1.25 avg=1.42\n",
            "[6120 | 11961.79] loss=1.46 avg=1.42\n",
            "[6121 | 11963.50] loss=1.31 avg=1.42\n",
            "[6122 | 11965.22] loss=0.88 avg=1.42\n",
            "[6123 | 11966.93] loss=1.33 avg=1.42\n",
            "[6124 | 11968.64] loss=1.43 avg=1.42\n",
            "[6125 | 11970.36] loss=0.95 avg=1.41\n",
            "[6126 | 11972.07] loss=0.93 avg=1.41\n",
            "[6127 | 11973.78] loss=0.80 avg=1.40\n",
            "[6128 | 11975.48] loss=1.49 avg=1.40\n",
            "[6129 | 11977.19] loss=1.38 avg=1.40\n",
            "[6130 | 11978.90] loss=1.87 avg=1.41\n",
            "[6131 | 11980.60] loss=2.47 avg=1.42\n",
            "[6132 | 11982.31] loss=0.97 avg=1.41\n",
            "[6133 | 11984.03] loss=1.24 avg=1.41\n",
            "[6134 | 11985.74] loss=1.08 avg=1.41\n",
            "[6135 | 11987.46] loss=1.36 avg=1.41\n",
            "[6136 | 11989.17] loss=1.21 avg=1.41\n",
            "[6137 | 11990.89] loss=1.58 avg=1.41\n",
            "[6138 | 11992.61] loss=1.06 avg=1.40\n",
            "[6139 | 11994.32] loss=1.65 avg=1.41\n",
            "[6140 | 11996.03] loss=1.15 avg=1.40\n",
            "[6141 | 11997.74] loss=1.52 avg=1.40\n",
            "[6142 | 11999.45] loss=1.24 avg=1.40\n",
            "[6143 | 12001.16] loss=1.55 avg=1.40\n",
            "[6144 | 12002.87] loss=1.42 avg=1.40\n",
            "[6145 | 12004.58] loss=1.03 avg=1.40\n",
            "[6146 | 12006.30] loss=1.42 avg=1.40\n",
            "[6147 | 12008.01] loss=0.91 avg=1.40\n",
            "[6148 | 12009.71] loss=1.03 avg=1.39\n",
            "[6149 | 12011.43] loss=0.65 avg=1.38\n",
            "[6150 | 12013.14] loss=1.05 avg=1.38\n",
            "[6151 | 12014.85] loss=1.74 avg=1.39\n",
            "[6152 | 12016.56] loss=1.25 avg=1.38\n",
            "[6153 | 12018.27] loss=1.86 avg=1.39\n",
            "[6154 | 12019.98] loss=1.43 avg=1.39\n",
            "[6155 | 12021.70] loss=1.53 avg=1.39\n",
            "[6156 | 12023.41] loss=1.67 avg=1.39\n",
            "[6157 | 12025.12] loss=1.35 avg=1.39\n",
            "[6158 | 12026.83] loss=1.32 avg=1.39\n",
            "[6159 | 12028.54] loss=1.67 avg=1.39\n",
            "[6160 | 12030.25] loss=1.46 avg=1.40\n",
            "[6161 | 12031.96] loss=1.45 avg=1.40\n",
            "[6162 | 12033.68] loss=1.57 avg=1.40\n",
            "[6163 | 12035.39] loss=1.48 avg=1.40\n",
            "[6164 | 12037.10] loss=1.50 avg=1.40\n",
            "[6165 | 12038.82] loss=2.20 avg=1.41\n",
            "[6166 | 12040.52] loss=0.77 avg=1.40\n",
            "[6167 | 12042.25] loss=2.02 avg=1.41\n",
            "[6168 | 12043.96] loss=0.88 avg=1.40\n",
            "[6169 | 12045.67] loss=0.82 avg=1.40\n",
            "[6170 | 12047.38] loss=1.93 avg=1.40\n",
            "[6171 | 12049.10] loss=1.57 avg=1.40\n",
            "[6172 | 12050.81] loss=0.82 avg=1.40\n",
            "[6173 | 12052.52] loss=0.81 avg=1.39\n",
            "[6174 | 12054.24] loss=0.99 avg=1.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RWqHS0LAQ3ET"
      },
      "source": [
        "### Especificar los checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bkZcMgDsy2V3",
        "outputId": "5ddc77a3-d7c4-485d-c576-1fd2b19af8aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# modelo tuneado con Harry Potter and the Goblet of Fire\n",
        "checkpoint = checkpoint_dir + '/run1_Harry_Potter'\n",
        "checkpoint_num = '16000'\n",
        "model_checkpoint_path = 'model_checkpoint_path: \"' + checkpoint + '/model-' + checkpoint_num + '\"'\n",
        "\n",
        "# modelo tuneado con Game of Thrones: A Song of Fire and Ice\n",
        "#checkpoint = checkpoint_dir + '/run1_GOT'\n",
        "#checkpoint_num = '18754'\n",
        "#model_checkpoint_path = 'model_checkpoint_path: \"' + checkpoint + '/model-' + checkpoint_num + '\"'\n",
        "\n",
        "with open('gpt-2/models/345M/checkpoint', \"wt\") as file:\n",
        "    print(model_checkpoint_path)\n",
        "    file.write(model_checkpoint_path)\n",
        "with open('gpt-2/models/345M/counter', \"wt\") as file:\n",
        "    file.write(f'{checkpoint_num}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_checkpoint_path: \"/content/drive/My Drive/Colab Notebooks/checkpoints/run1_Harry_Potter/model-16000\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sL0Ve7HdRA1f"
      },
      "source": [
        "### Generar muestras con el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zONNL240PcYv",
        "outputId": "1c73df8e-5276-42d6-b311-b84e0f60b7d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ejemplo = 'CHAPTER 1 - The Return of Voldermort' #@param {type : 'string'}\n",
        "numero_de_muestras = 3 #@param {type : 'number'}\n",
        "temperature=1 #@param {type : 'number'}\n",
        "#@markdown La temperatura controla el grado de aleatoriedad (0 = determinista)\n",
        "top_k=40 #@param {type : 'integer'}\n",
        "#@markdown Número de candidatos considerados en el beam search (0 = \"greedy\", funciona bien con 40)\n",
        "top_p=0.9 #@param {type : 'number'}\n",
        "#@markdown Controla la diversidad. (0 = valor por defecto, funciona bien con 0.9)\n",
        "texts = interact_model(prompt=ejemplo,\n",
        "                       model_name='345M',\n",
        "                       nsamples=numero_de_muestras,\n",
        "                       temperature=temperature,\n",
        "                       top_k=top_k,\n",
        "                       top_p=top_p)\n",
        "for i, text in enumerate(texts):\n",
        "    display(HTML('<p>' + \"=\" * 40 + \" SAMPLE \" + str(i+1) + \" \" + \"=\" * 40 + '</p>'))\n",
        "    display(HTML('<p>' + ('<b><i>' + ejemplo + '</b></i>' + text).replace('\\n', '<br>') + '</p>'))\n",
        "    display(HTML('<p>' + \"=\" * 80 + '</p>'))\n",
        "    display(HTML('<p><br></p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======================================== SAMPLE 1 ========================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>CHAPTER 1 - The Return of Voldermort</b></i><br><br>The thunderous clunks of Hagrid’s heavy wooden dining-room door as it swung closed were a constant source of amusement to Harry. It was due to this, that he’s been using the excuse of a valuable lesson to get to bed early, and sneakogle away from Ron and Hermione for the last time.<br><br>Alarmed as he was at the idea of having to share a room with Ron and Hermione, Harry still didn’t think they were cruel people; on the contrary, both of them seemed quite capable of putting up with a lotlier than they did. Harry had knocked over his cauldron and spilled a considerable amount of its contents upon the floor, but he had yet to move an inch, while Hermione was being choked, bound, gagged and raped with her hands, repeatedly.<br><br>It was an appalling sight, seeing Hermione gagged, bound and raped with a Goblin Lord, seated behind her. The whole house was watching her, as though she were a demon possessed.<br><br>One by one, the ranks of spectators swelled. Witches and wizards from all over the country were coming to watch the most amazing scene of sorcery and murder Harry had ever witnessed.<br><br>Then a wizard just like Hermione appeared just behind Hermione. She was very short and round-faced, with a large orange nose and almond-shaped brown eyes. She was talking to Ludo Bagman, the Head of the International League of Zemouregal Wizards, in a soothing and even voice.<br><br>‘Ludo, could I have a sec … could I have a sec … could I?’<br><br>She saw, with a pinch of panic, that Bagman was leading a short, stout man into the room.<br><br>‘Ah, it’s Ludo!’ says Mr Bagman, grinning. ‘Ah, I see … aye, I see … a fine pair of chaps … you?’<br><br>‘I am,’ said Ludo, ‘yeh are.’<br><br>‘How long have you been here?’ says Bagman.<br><br>‘One year,’ says Ludo.<br><br>‘Ah, yes, I was just starting – soye hurry up!’<br><br>Bagman hurried along the table, and Harry saw Ludo Bagman putting his initials to the voter’s names.<br><br>‘Very interesting</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>================================================================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><br></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======================================== SAMPLE 2 ========================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>CHAPTER 1 - The Return of Voldermort</b></i><br><br>When Harry awoke next morning, he knew he would not like to admit it; he was very angry with Voldermort for what Harry had said about Wormtail, and rightly so. The episode had shaken Harry so much that he had been unable to go to the library before the end of the term. He had been feeling very blue all through school, and by the time he got back to the castle after lunch, he was already wishing he could just keep going, throw the book under the kettle, and watch The Lord of the Rings …<br><br>But that wasn’t why he was writing. He was writing because he had been so wrong about Wormtail. It was time to walk the walk, and Harry was going to show Voldemort exactly how wrong he was.<br><br><br><br><br><br>— CHAPTER ONE —<br><br><br>The Russian Blood<br><br>Harry had walked around the edges of the dungeon for nearly four hours. He saw no sign of Voldemort’s ghost. Tired and lonely, he lay, apart from Ron and Hermione, listening to the wind outside outside. Hot, dry clothes felt slightly thin and stubbly under his fingers, and the breeze that blew across his face was somehow so gratifying to his ear that he had to suppress a pained sort of grumble.<br><br>He looked up at Harry.<br><br>‘You OK, Harry?’ he said.<br><br>‘Yeah,’ said Harry.<br><br>‘You needed to go to the hospital wing, it’s down the hill.’<br><br>‘Got a fever,’ said Harry.<br><br>‘Got a dragon?’ said Hermione. ‘Well, I’ve got a dragon,’ she added, and she was pointing at the unicorn shank that was stirring in the pan. The shank was moving, a curious sort of humming, as it aged.<br><br>‘I’ve got to go and see Professor Moody,’ Harry said, getting to his feet. ‘He’s been – he’s in a bad way. He kept disappearing … it’s probably back to the Dursleys wanting to get him back.’<br><br>‘Must be nice,’ said Hermione, pulling her scarf back over her head. ‘To have someone who’s so powerful try and take you from him.’<br><br>‘Yeah, it’s</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>================================================================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><br></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>======================================== SAMPLE 3 ========================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>CHAPTER 1 - The Return of Voldermort</b></i><br><br><br><br>TWENTY-THREE<br><br>The Department of Mysteries<br><br><br><br>TWENTY-FOUR<br><br>Beyond the Veil<br><br><br><br>TWENTY-FIVE<br><br>The Madness of Mr Crouch<br><br><br><br>TWENTY-SIX<br><br>Something Wicked This Way Comes<br><br><br><br>TWENTY-SEVEN<br><br>The Dream<br><br><br><br>TWENTY-EIGHT<br><br>The Second Task<br><br><br><br>TWENTY-NINE<br><br>Veritaserum<br><br><br><br>TWENTY-TEN<br><br>Veritaserum<br><br><br><br>TWENTY-THIRTEEN<br><br>Veritasections<br><br><br><br>TWENTY-FOURTEEN<br><br>Veritisation<br><br><br><br>TWENTY-FIVETEEN<br><br>Veritisation<br><br><br><br>TWENTY-SIXTEEN<br><br>Veritisation<br><br><br><br>TWENTY-SEVENTEEN<br><br>Veritisation<br><br><br><br>TWENTY-EIGHTTEEN<br><br>Veritisation<br><br><br><br>TWENTY-NINETEEN<br><br>Veritisation<br><br><br><br>TWENTY-TENTEEN<br><br>Veritisation<br><br><br><br>TWENTY-TWELVE<br><br>Veritisation<br><br><br><br>TWENTY-THIRTEEN<br><br>Veritisation<br><br><br><br>TWENTY-FOURTEEN<br><br>Veritisation<br><br><br><br>TWENTY-FIVETEEN<br><br>Veritisation<br><br><br><br>TWENTY-SIXTEEN<br><br>Veritisation<br><br><br><br>TWENTY-SEVENTEEN<br><br>Veritisation<br><br><br><br>TWENTY-EIGHTTEEN<br><br>Veritisation<br><br><br><br>TWENTY-NINE<br><br>Veritisation<br><br><br><br>TWENTY-TEN<br><br>Veritisation<br><br><br><br>TWENTY-TWELVE<br><br>Veritisation<br><br><br><br>TWENTY-THIRTY<br><br>The Third Task<br><br><br><br>TWENTY-FOURTEEN<br><br>Veritisation<br><br><br><br>TWENTY-FIVE<br><br>The Fourth Task<br><br><br><br>TWENTY-SIXTEEN<br><br>Veritisation<br><br><br><br>TWENTY-SEVEN<br><br>The Seventh Task<br><br><br><br>TWENTY-EIGHT<br><br>Veritisation<br><br><br><br>TWENTY-NINE<br><br>Veritisation<br><br><br><br>TWENTY-</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>================================================================================</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><br></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DxtpZl4Ltf01"
      },
      "source": [
        "### Traducir una frase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "urw1IW82tH95",
        "colab": {}
      },
      "source": [
        "# puedes encontrar otros texto paralelos aquí\n",
        "# cuidado porque se actualizan periodicamente\n",
        "# https://www.manythings.org/anki/\n",
        "\n",
        "path_to_zip = get_file('spa-eng.zip',\n",
        "                       origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "                       extract=True)\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
        "\n",
        "frases = []\n",
        "with open(path_to_file, 'rt') as file:\n",
        "    for line in file.readlines():\n",
        "        tab = line.find('\\t')\n",
        "        frases += [line[:tab] + ' = ' + line[tab+1:]]\n",
        "with open('train.txt', 'wt') as file:\n",
        "    for _ in np.random.choice(range(len(frases)), len(frases), replace=False):\n",
        "        file.write(frases[_])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uUYLJ3GreJ5-"
      },
      "source": [
        "### Especificar los checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3-bsl8feeJ6H",
        "outputId": "a8428b7e-1b92-4c97-f53a-49fd1979b420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# modelo pre-entrenado\n",
        "checkpoint_num = ''\n",
        "model_checkpoint_path = 'model_checkpoint_path: \"model.ckpt\"'\n",
        "\n",
        "with open('gpt-2/models/345M/checkpoint', \"wt\") as file:\n",
        "    print(model_checkpoint_path)\n",
        "    file.write(model_checkpoint_path)\n",
        "with open('gpt-2/models/345M/counter', \"wt\") as file:\n",
        "    file.write(f'{checkpoint_num}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_checkpoint_path: \"model.ckpt\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4NbgqHJTea8k"
      },
      "source": [
        "### Entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CvfqdED_AHJ7",
        "outputId": "0fc7ff96-cb9a-4176-f9c2-c98b247979b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train.CHECKPOINT_DIR = checkpoint_dir\n",
        "tf.reset_default_graph()\n",
        "cwd = os.getcwd()\n",
        "os.chdir(cwd + '/gpt-2') # hack\n",
        "try:\n",
        "    train.main()\n",
        "except:\n",
        "    os.chdir(cwd) # hack\n",
        "    raise\n",
        "os.chdir(cwd) # hack        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-18000\n",
            "Loading dataset...\n",
            "dataset has 2787399 tokens\n",
            "Training...\n",
            "[18001 | 16.33] loss=1.01 avg=1.01\n",
            "[18002 | 19.37] loss=0.84 avg=0.93\n",
            "[18003 | 22.42] loss=0.88 avg=0.91\n",
            "[18004 | 25.47] loss=1.01 avg=0.93\n",
            "[18005 | 28.53] loss=0.80 avg=0.91\n",
            "[18006 | 31.60] loss=0.93 avg=0.91\n",
            "[18007 | 34.66] loss=0.88 avg=0.91\n",
            "[18008 | 37.72] loss=0.90 avg=0.90\n",
            "[18009 | 40.78] loss=0.98 avg=0.91\n",
            "[18010 | 43.84] loss=0.92 avg=0.91\n",
            "[18011 | 46.91] loss=0.84 avg=0.91\n",
            "[18012 | 49.97] loss=0.88 avg=0.90\n",
            "[18013 | 53.03] loss=0.90 avg=0.90\n",
            "[18014 | 56.09] loss=0.84 avg=0.90\n",
            "[18015 | 59.16] loss=0.99 avg=0.91\n",
            "[18016 | 62.22] loss=0.91 avg=0.91\n",
            "[18017 | 65.28] loss=0.90 avg=0.91\n",
            "[18018 | 68.34] loss=0.88 avg=0.90\n",
            "[18019 | 71.40] loss=0.99 avg=0.91\n",
            "[18020 | 74.47] loss=0.88 avg=0.91\n",
            "[18021 | 77.53] loss=0.84 avg=0.90\n",
            "[18022 | 80.59] loss=0.87 avg=0.90\n",
            "[18023 | 83.66] loss=0.93 avg=0.90\n",
            "[18024 | 86.73] loss=0.90 avg=0.90\n",
            "[18025 | 89.79] loss=0.94 avg=0.90\n",
            "[18026 | 92.86] loss=0.87 avg=0.90\n",
            "[18027 | 95.93] loss=0.99 avg=0.91\n",
            "[18028 | 98.99] loss=0.84 avg=0.90\n",
            "[18029 | 102.06] loss=0.87 avg=0.90\n",
            "[18030 | 105.13] loss=0.86 avg=0.90\n",
            "[18031 | 108.22] loss=0.82 avg=0.90\n",
            "[18032 | 111.29] loss=0.85 avg=0.90\n",
            "[18033 | 114.35] loss=0.88 avg=0.90\n",
            "[18034 | 117.43] loss=0.94 avg=0.90\n",
            "[18035 | 120.49] loss=0.87 avg=0.90\n",
            "[18036 | 123.56] loss=0.79 avg=0.89\n",
            "[18037 | 126.65] loss=0.82 avg=0.89\n",
            "[18038 | 129.71] loss=0.87 avg=0.89\n",
            "[18039 | 132.77] loss=0.91 avg=0.89\n",
            "[18040 | 135.81] loss=0.88 avg=0.89\n",
            "[18041 | 138.88] loss=0.88 avg=0.89\n",
            "[18042 | 141.95] loss=0.90 avg=0.89\n",
            "[18043 | 145.01] loss=0.86 avg=0.89\n",
            "[18044 | 148.08] loss=0.85 avg=0.89\n",
            "[18045 | 151.16] loss=0.97 avg=0.89\n",
            "[18046 | 154.26] loss=0.86 avg=0.89\n",
            "[18047 | 157.35] loss=0.84 avg=0.89\n",
            "[18048 | 160.42] loss=0.85 avg=0.89\n",
            "[18049 | 163.44] loss=0.85 avg=0.89\n",
            "[18050 | 166.49] loss=0.92 avg=0.89\n",
            "[18051 | 169.56] loss=0.95 avg=0.89\n",
            "[18052 | 172.62] loss=0.85 avg=0.89\n",
            "[18053 | 175.68] loss=0.83 avg=0.89\n",
            "[18054 | 178.75] loss=0.80 avg=0.88\n",
            "[18055 | 181.81] loss=0.83 avg=0.88\n",
            "[18056 | 184.87] loss=0.84 avg=0.88\n",
            "[18057 | 187.93] loss=0.84 avg=0.88\n",
            "[18058 | 191.00] loss=0.85 avg=0.88\n",
            "[18059 | 194.06] loss=0.90 avg=0.88\n",
            "[18060 | 197.12] loss=0.85 avg=0.88\n",
            "[18061 | 200.18] loss=0.89 avg=0.88\n",
            "[18062 | 203.24] loss=0.86 avg=0.88\n",
            "[18063 | 206.30] loss=0.93 avg=0.88\n",
            "[18064 | 209.35] loss=0.84 avg=0.88\n",
            "[18065 | 212.42] loss=0.92 avg=0.88\n",
            "[18066 | 215.48] loss=0.93 avg=0.88\n",
            "[18067 | 218.54] loss=0.81 avg=0.88\n",
            "[18068 | 221.60] loss=0.95 avg=0.88\n",
            "[18069 | 224.67] loss=0.76 avg=0.88\n",
            "[18070 | 227.73] loss=0.89 avg=0.88\n",
            "[18071 | 230.81] loss=0.83 avg=0.88\n",
            "[18072 | 233.89] loss=0.92 avg=0.88\n",
            "[18073 | 236.97] loss=0.86 avg=0.88\n",
            "[18074 | 240.06] loss=0.90 avg=0.88\n",
            "[18075 | 243.13] loss=0.80 avg=0.88\n",
            "[18076 | 246.20] loss=0.81 avg=0.88\n",
            "[18077 | 249.26] loss=0.92 avg=0.88\n",
            "[18078 | 252.32] loss=0.85 avg=0.88\n",
            "[18079 | 255.38] loss=0.85 avg=0.88\n",
            "[18080 | 258.45] loss=0.96 avg=0.88\n",
            "[18081 | 261.55] loss=0.77 avg=0.88\n",
            "[18082 | 264.65] loss=0.90 avg=0.88\n",
            "[18083 | 267.72] loss=0.93 avg=0.88\n",
            "[18084 | 270.80] loss=0.91 avg=0.88\n",
            "[18085 | 273.88] loss=0.87 avg=0.88\n",
            "[18086 | 276.95] loss=0.95 avg=0.88\n",
            "[18087 | 280.02] loss=0.83 avg=0.88\n",
            "[18088 | 283.09] loss=0.69 avg=0.87\n",
            "[18089 | 286.14] loss=0.94 avg=0.88\n",
            "[18090 | 289.20] loss=0.90 avg=0.88\n",
            "[18091 | 292.27] loss=0.86 avg=0.88\n",
            "[18092 | 295.35] loss=0.89 avg=0.88\n",
            "[18093 | 298.41] loss=0.93 avg=0.88\n",
            "[18094 | 301.48] loss=0.86 avg=0.88\n",
            "[18095 | 304.55] loss=0.91 avg=0.88\n",
            "[18096 | 307.62] loss=0.93 avg=0.88\n",
            "[18097 | 310.69] loss=0.96 avg=0.88\n",
            "[18098 | 313.74] loss=0.91 avg=0.88\n",
            "[18099 | 316.81] loss=0.89 avg=0.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " you to stop doing things that aren't working. = Te puedo siendo dejar de hacer cosas que no están funcionando.\n",
            "How far did you go? = ¿Hasta dónde has visto?\n",
            "I am thinking about it. = Estoy pensando.\n",
            "He loves his son, not his own. = A él le encantan a su hijo, no a su propio.\n",
            "I am reading a short story. = Estoy leyendo una pequeña cuena.\n",
            "I do not agree. = No estoy de acuerdo.\n",
            "I don't want to hurt you. = No quiero lastimaros.\n",
            "What a waste of money! = ¡Qué derroche de dinero!\n",
            "Tom did not want to talk to Mary. = Tomás no quería hablar con Mary.\n",
            "The boy said he was born on February the fifth, 1900. = El niño dijo que nació el decimose del marido de 1900.\n",
            "I don't get why you don't like Tom anymore. = Ya no entiendo por qué no le gustas a Tom.\n",
            "I'm always late for school. = Siempre llego tarde a la escuela.\n",
            "I have a little money. = Tengo un poco de dinero.\n",
            "Tom was killed by a knife-wielding maniac. = Tom murió por un único nutriculoso.\n",
            "He's been dead for three years. = Lleva muerto tres años.\n",
            "Tom doesn't understand what you want. = Tom no entiende lo que quieres.\n",
            "I'm pretty sure Tom is right now. = Estoy bastante seguro de que Tom va riérdate.\n",
            "We don't want to spend any more time like this. = No queremos pasar más tiempo así.\n",
            "I think Tom still has a lot to learn. = Creo que Tom aún tiene muchas cosas que aprender.\n",
            "He's not a teacher. = Él no es profesor.\n",
            "Tom did the honors. = Tom hizo los honranos.\n",
            "\"Do you know how many people died in the restaurant yesterday?\" \"No, I'm just trying to find the nearest bank.\" = \"¿Sabes cuántas personas murieron en el restaurante ayer?\" \"No, solo trato de encontrar al banco.\"\n",
            "Are you familiar with the law of the jungle? = ¿Conoce usted el ley del juguete?\n",
            "This isn't going to end well. = Esto no va a corporar bien.\n",
            "I have to talk to Tom. = Tengo que hablar con Tom.\n",
            "Tell me it's not true. = Decime que no es verdad.\n",
            "Tom hasn't changed much. = Tom no ha cambiado mucho.\n",
            "The bus has already left. = El autobús ya se fue.\n",
            "He got on the bus at Kenilworth. = Él se subió al bus en Kenilworth.\n",
            "I'm starting to feel much better. = Estoy empezando a sentirme mucho mejor.\n",
            "Is the police here? = ¿Está aquí la policía?\n",
            "He told the children a joke. = Él le entregó un chiste a los niños.\n",
            "The old man told me his secret. = El anciano me contó su secreto.\n",
            "Why do dolphins care about us? = ¿Por qué necesitamos por nosotros del dedo?\n",
            "I'm not a student. = Yo no soy estudiante.\n",
            "Tom had a stroke. = Tom tuvo un golpe.\n",
            "They're mine. = Ellas son mías.\n",
            "Where's Tom talking? = ¿Dónde está hablando Tom?\n",
            "My brother's going to kill me. = Mi hermano va a matarme.\n",
            "They're good kids. = Son buenos niños.\n",
            "I love ice cream. = Me encanta el helado.\n",
            "You know it's me. = Sabes que es yo.\n",
            "They won't give up! = ¡No se va a rendir.\n",
            "She advised him not to go out and drink a little beer. = Ella le aconsejó que no saliera y no beber un poco de comer.\n",
            "I want to buy a bottle of wine. = Quiero comprar una botella de vino.\n",
            "What you wrote doesn't have any grammatical errors. = Lo que usted escribi\n",
            "\n",
            "[18100 | 360.61] loss=0.94 avg=0.88\n",
            "[18101 | 363.67] loss=0.92 avg=0.88\n",
            "[18102 | 366.73] loss=0.90 avg=0.88\n",
            "[18103 | 369.79] loss=0.87 avg=0.88\n",
            "[18104 | 372.85] loss=0.72 avg=0.88\n",
            "[18105 | 375.89] loss=0.98 avg=0.88\n",
            "[18106 | 378.96] loss=0.90 avg=0.88\n",
            "[18107 | 382.01] loss=0.81 avg=0.88\n",
            "[18108 | 385.08] loss=0.81 avg=0.88\n",
            "[18109 | 388.14] loss=0.83 avg=0.88\n",
            "[18110 | 391.21] loss=0.82 avg=0.88\n",
            "[18111 | 394.28] loss=0.88 avg=0.88\n",
            "[18112 | 397.34] loss=0.92 avg=0.88\n",
            "[18113 | 400.39] loss=0.93 avg=0.88\n",
            "[18114 | 403.45] loss=0.81 avg=0.88\n",
            "[18115 | 406.51] loss=0.88 avg=0.88\n",
            "[18116 | 409.58] loss=0.95 avg=0.88\n",
            "[18117 | 412.64] loss=0.84 avg=0.88\n",
            "[18118 | 415.70] loss=0.99 avg=0.88\n",
            "[18119 | 418.76] loss=0.98 avg=0.88\n",
            "[18120 | 421.83] loss=0.84 avg=0.88\n",
            "[18121 | 424.88] loss=0.88 avg=0.88\n",
            "[18122 | 427.95] loss=0.88 avg=0.88\n",
            "[18123 | 431.01] loss=0.85 avg=0.88\n",
            "[18124 | 434.08] loss=0.97 avg=0.88\n",
            "[18125 | 437.16] loss=0.93 avg=0.88\n",
            "[18126 | 440.24] loss=0.81 avg=0.88\n",
            "[18127 | 443.31] loss=0.91 avg=0.88\n",
            "[18128 | 446.40] loss=0.87 avg=0.88\n",
            "[18129 | 449.47] loss=0.91 avg=0.88\n",
            "[18130 | 452.56] loss=0.88 avg=0.88\n",
            "[18131 | 455.65] loss=0.90 avg=0.88\n",
            "[18132 | 458.72] loss=0.88 avg=0.88\n",
            "[18133 | 461.79] loss=0.84 avg=0.88\n",
            "[18134 | 464.87] loss=0.96 avg=0.88\n",
            "[18135 | 467.95] loss=0.91 avg=0.88\n",
            "[18136 | 471.04] loss=0.87 avg=0.88\n",
            "[18137 | 474.12] loss=0.93 avg=0.88\n",
            "[18138 | 477.19] loss=0.93 avg=0.88\n",
            "[18139 | 480.25] loss=0.92 avg=0.88\n",
            "[18140 | 483.32] loss=0.83 avg=0.88\n",
            "[18141 | 486.41] loss=0.95 avg=0.88\n",
            "[18142 | 489.49] loss=0.98 avg=0.89\n",
            "[18143 | 492.58] loss=0.86 avg=0.89\n",
            "[18144 | 495.65] loss=0.87 avg=0.89\n",
            "[18145 | 498.73] loss=0.91 avg=0.89\n",
            "[18146 | 501.81] loss=0.86 avg=0.89\n",
            "[18147 | 504.89] loss=0.86 avg=0.88\n",
            "[18148 | 507.98] loss=0.91 avg=0.89\n",
            "[18149 | 511.05] loss=0.92 avg=0.89\n",
            "[18150 | 514.11] loss=0.90 avg=0.89\n",
            "[18151 | 517.20] loss=0.80 avg=0.88\n",
            "[18152 | 520.28] loss=0.94 avg=0.89\n",
            "[18153 | 523.38] loss=0.85 avg=0.88\n",
            "[18154 | 526.46] loss=0.88 avg=0.88\n",
            "[18155 | 529.55] loss=0.89 avg=0.88\n",
            "[18156 | 532.64] loss=0.87 avg=0.88\n",
            "[18157 | 535.73] loss=0.95 avg=0.89\n",
            "[18158 | 538.80] loss=0.91 avg=0.89\n",
            "[18159 | 541.89] loss=0.78 avg=0.88\n",
            "[18160 | 544.98] loss=0.81 avg=0.88\n",
            "[18161 | 548.06] loss=0.72 avg=0.88\n",
            "[18162 | 551.14] loss=0.95 avg=0.88\n",
            "[18163 | 554.24] loss=0.91 avg=0.88\n",
            "[18164 | 557.33] loss=0.73 avg=0.88\n",
            "[18165 | 560.43] loss=0.90 avg=0.88\n",
            "[18166 | 563.54] loss=0.78 avg=0.88\n",
            "[18167 | 566.62] loss=0.92 avg=0.88\n",
            "[18168 | 569.72] loss=0.71 avg=0.88\n",
            "[18169 | 572.79] loss=0.90 avg=0.88\n",
            "[18170 | 575.89] loss=0.85 avg=0.88\n",
            "[18171 | 578.96] loss=0.82 avg=0.88\n",
            "[18172 | 582.05] loss=0.93 avg=0.88\n",
            "[18173 | 585.13] loss=0.92 avg=0.88\n",
            "[18174 | 588.19] loss=0.82 avg=0.88\n",
            "[18175 | 591.25] loss=0.65 avg=0.88\n",
            "[18176 | 594.35] loss=0.83 avg=0.87\n",
            "[18177 | 597.43] loss=0.90 avg=0.87\n",
            "[18178 | 600.52] loss=0.93 avg=0.88\n",
            "[18179 | 603.61] loss=0.73 avg=0.87\n",
            "[18180 | 606.71] loss=0.86 avg=0.87\n",
            "[18181 | 609.81] loss=0.94 avg=0.87\n",
            "[18182 | 612.90] loss=0.93 avg=0.88\n",
            "[18183 | 615.95] loss=0.80 avg=0.87\n",
            "[18184 | 619.00] loss=0.86 avg=0.87\n",
            "[18185 | 622.08] loss=0.91 avg=0.87\n",
            "[18186 | 625.16] loss=0.87 avg=0.87\n",
            "[18187 | 628.23] loss=0.82 avg=0.87\n",
            "[18188 | 631.33] loss=0.88 avg=0.87\n",
            "[18189 | 634.42] loss=0.99 avg=0.88\n",
            "[18190 | 637.50] loss=0.95 avg=0.88\n",
            "[18191 | 640.60] loss=0.79 avg=0.87\n",
            "[18192 | 643.69] loss=0.88 avg=0.88\n",
            "[18193 | 646.78] loss=0.78 avg=0.87\n",
            "[18194 | 649.88] loss=0.80 avg=0.87\n",
            "[18195 | 652.94] loss=0.89 avg=0.87\n",
            "[18196 | 656.01] loss=0.90 avg=0.87\n",
            "[18197 | 659.08] loss=0.88 avg=0.87\n",
            "[18198 | 662.18] loss=1.00 avg=0.88\n",
            "[18199 | 665.27] loss=0.85 avg=0.87\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " me here. = Ella vuelve tarde.\n",
            "What do you know about my family? = ¿Qué sabes acerca de mi familia?\n",
            "The train is running twenty minutes late. = El tren va corriente veinte minutos.\n",
            "It is cold in this room. = Hace frío en esta habitación.\n",
            "Some days I don't think about him. = Other¹ siempre pienso en él.\n",
            "There is no need for formal education. = Es posible de la educación de estudiar estudiar fuerte.\n",
            "It didn't rain at all. = No nieva en absoluto.\n",
            "Tom went back to Boston. = Tom volvió a Boston.\n",
            "Tom is looking out for himself. = Tom está mirando por sí mismo.\n",
            "Tom thinks he's good at tennis. = Tom cree que se le dan bien el tenis.\n",
            "This is so stupid. = Esto es tan estúpido.\n",
            "Tom didn't want to go out in the rain. = Tom no quería salir en la lluvia.\n",
            "All of a sudden, they began to laugh. = De repente, se pusieron a reírse.\n",
            "I can't stay long. = No puedo quedarme un rato tiempo.\n",
            "Tom has to go to the bookstore today. = Tom tiene que ir al libro hoy en día.\n",
            "The dog was biting the boy. = El perro estaba mordiendo al niño.\n",
            "Who do you plan to spend Christmas with? = ¿Con quién piensas pasar la navidad?\n",
            "I'm going to buy a new saxophone. = Voy a comprarme un saxo nuevo.\n",
            "I will do my homework after I watch television. = Haré mi estudio después de ver la tele.\n",
            "I know that you'll be able to do it. = Sé que usted será lo que podrá hacer.\n",
            "You're not good enough. = No eres lo suficientemente bueno.\n",
            "No one came. = Nadie vino.\n",
            "It is not my place to tell you what you need to do. = No es mi lugar para decirte lo que hay que hacerse.\n",
            "I wish that I had been there. = Ojalá hubiera estado allí.\n",
            "I'll tell you what that means. = Te diré lo que eso significa.\n",
            "I had my brother carry this bag. = Hice que mi hermano llevara esta maleta.\n",
            "What time is the train arriving? = ¿A qué hora llega el trajeta?\n",
            "We need more time. = Necesitamos más tiempo.\n",
            "A large crowd gathered outside the hotel. = Una gran multitud se reunió afuera del hotel.\n",
            "I love sports. = Me encantan los deportes.\n",
            "I've never forgotten you. = Nunca te he olvidado.\n",
            "They didn't want to do it. = No quiso hacerlo.\n",
            "I'm really not in a mood to talk right now. = Ahora mismo no estoy de humor para hablar.\n",
            "I don't want anybody to hear my dirty work. = No quiero que nadie se oye mi trabajo deportivo.\n",
            "I know this feeling. = Lo sé esta sensación.\n",
            "I'd like a hamburger. = Quisiera una hamburguesa.\n",
            "Tom is my only son. = Tom es mi único hijo.\n",
            "The dog wants some water. = El perro quiere algo de agua.\n",
            "I have to clean the bathroom. = Tengo que limpiar el cuarto de la cuartada.\n",
            "She looked as if she had been ill. = Ella parecía como si hubiera estado enferma.\n",
            "The two countries reached an agreement yesterday. = La dos países llegaron un acuerdo ayer.\n",
            "Would you take care of our dog? = ¿Cuidarías a nuestro perro?\n",
            "I don't know if I can do this. = No sé si puedo hacer esto.\n",
            "Tom asked me if I knew where Mary lived. = Tom me preguntó si acaso sabía dónde vivía María.\n",
            "Is that enough for you? = ¿Es eso suficiente para tú?\n",
            "It could happen to you. = Podría ocurriros a vosot\n",
            "\n",
            "[18200 | 705.88] loss=0.86 avg=0.87\n",
            "[18201 | 708.97] loss=0.95 avg=0.88\n",
            "[18202 | 712.06] loss=0.90 avg=0.88\n",
            "[18203 | 715.14] loss=0.97 avg=0.88\n",
            "[18204 | 718.21] loss=0.89 avg=0.88\n",
            "[18205 | 721.28] loss=0.79 avg=0.88\n",
            "[18206 | 724.38] loss=0.92 avg=0.88\n",
            "[18207 | 727.46] loss=0.74 avg=0.87\n",
            "[18208 | 730.55] loss=0.73 avg=0.87\n",
            "[18209 | 733.62] loss=0.91 avg=0.87\n",
            "[18210 | 736.69] loss=0.91 avg=0.87\n",
            "[18211 | 739.78] loss=0.90 avg=0.87\n",
            "[18212 | 742.87] loss=0.86 avg=0.87\n",
            "[18213 | 745.95] loss=0.87 avg=0.87\n",
            "[18214 | 749.03] loss=0.95 avg=0.88\n",
            "[18215 | 752.12] loss=0.88 avg=0.88\n",
            "[18216 | 755.21] loss=0.86 avg=0.87\n",
            "[18217 | 758.30] loss=0.88 avg=0.88\n",
            "[18218 | 761.38] loss=0.90 avg=0.88\n",
            "[18219 | 764.46] loss=0.86 avg=0.88\n",
            "[18220 | 767.54] loss=0.87 avg=0.88\n",
            "[18221 | 770.62] loss=0.84 avg=0.87\n",
            "[18222 | 773.71] loss=0.85 avg=0.87\n",
            "[18223 | 776.80] loss=0.87 avg=0.87\n",
            "[18224 | 779.88] loss=0.70 avg=0.87\n",
            "[18225 | 782.97] loss=0.99 avg=0.87\n",
            "[18226 | 786.04] loss=0.94 avg=0.87\n",
            "[18227 | 789.12] loss=0.84 avg=0.87\n",
            "[18228 | 792.21] loss=0.91 avg=0.87\n",
            "[18229 | 795.30] loss=0.92 avg=0.88\n",
            "[18230 | 798.39] loss=0.81 avg=0.87\n",
            "[18231 | 801.48] loss=0.89 avg=0.87\n",
            "[18232 | 804.57] loss=0.91 avg=0.88\n",
            "[18233 | 807.66] loss=0.92 avg=0.88\n",
            "[18234 | 810.74] loss=0.95 avg=0.88\n",
            "[18235 | 813.80] loss=0.95 avg=0.88\n",
            "[18236 | 816.86] loss=0.98 avg=0.88\n",
            "[18237 | 819.95] loss=0.93 avg=0.88\n",
            "[18238 | 823.02] loss=0.84 avg=0.88\n",
            "[18239 | 826.10] loss=0.93 avg=0.88\n",
            "[18240 | 829.20] loss=0.81 avg=0.88\n",
            "[18241 | 832.28] loss=0.90 avg=0.88\n",
            "[18242 | 835.37] loss=0.78 avg=0.88\n",
            "[18243 | 838.46] loss=0.91 avg=0.88\n",
            "[18244 | 841.55] loss=0.88 avg=0.88\n",
            "[18245 | 844.63] loss=0.89 avg=0.88\n",
            "[18246 | 847.71] loss=0.94 avg=0.88\n",
            "[18247 | 850.79] loss=0.90 avg=0.88\n",
            "[18248 | 853.88] loss=0.82 avg=0.88\n",
            "[18249 | 856.95] loss=0.89 avg=0.88\n",
            "[18250 | 860.03] loss=0.87 avg=0.88\n",
            "[18251 | 863.11] loss=0.91 avg=0.88\n",
            "[18252 | 866.18] loss=0.82 avg=0.88\n",
            "[18253 | 869.26] loss=0.86 avg=0.88\n",
            "[18254 | 872.35] loss=0.91 avg=0.88\n",
            "[18255 | 875.43] loss=0.85 avg=0.88\n",
            "[18256 | 878.51] loss=0.88 avg=0.88\n",
            "[18257 | 881.60] loss=0.87 avg=0.88\n",
            "[18258 | 884.68] loss=0.88 avg=0.88\n",
            "[18259 | 887.76] loss=0.91 avg=0.88\n",
            "[18260 | 890.83] loss=0.94 avg=0.88\n",
            "[18261 | 893.91] loss=0.87 avg=0.88\n",
            "[18262 | 896.99] loss=0.84 avg=0.88\n",
            "[18263 | 900.08] loss=0.92 avg=0.88\n",
            "[18264 | 903.16] loss=0.88 avg=0.88\n",
            "[18265 | 906.24] loss=0.87 avg=0.88\n",
            "[18266 | 909.33] loss=0.89 avg=0.88\n",
            "[18267 | 912.42] loss=0.96 avg=0.88\n",
            "[18268 | 915.51] loss=0.84 avg=0.88\n",
            "[18269 | 918.60] loss=0.97 avg=0.88\n",
            "[18270 | 921.68] loss=0.87 avg=0.88\n",
            "[18271 | 924.78] loss=0.86 avg=0.88\n",
            "[18272 | 927.87] loss=0.99 avg=0.88\n",
            "[18273 | 930.95] loss=0.87 avg=0.88\n",
            "[18274 | 934.04] loss=0.86 avg=0.88\n",
            "[18275 | 937.13] loss=0.90 avg=0.88\n",
            "[18276 | 940.22] loss=0.82 avg=0.88\n",
            "[18277 | 943.32] loss=0.91 avg=0.88\n",
            "[18278 | 946.41] loss=0.88 avg=0.88\n",
            "[18279 | 949.49] loss=0.86 avg=0.88\n",
            "[18280 | 952.58] loss=1.02 avg=0.88\n",
            "[18281 | 955.66] loss=0.86 avg=0.88\n",
            "[18282 | 958.76] loss=0.90 avg=0.88\n",
            "[18283 | 961.85] loss=0.82 avg=0.88\n",
            "[18284 | 964.94] loss=0.80 avg=0.88\n",
            "[18285 | 968.02] loss=0.91 avg=0.88\n",
            "[18286 | 971.12] loss=0.82 avg=0.88\n",
            "[18287 | 974.20] loss=0.89 avg=0.88\n",
            "[18288 | 977.28] loss=0.88 avg=0.88\n",
            "[18289 | 980.37] loss=0.94 avg=0.88\n",
            "[18290 | 983.46] loss=0.90 avg=0.88\n",
            "[18291 | 986.55] loss=0.77 avg=0.88\n",
            "[18292 | 989.64] loss=0.92 avg=0.88\n",
            "[18293 | 992.73] loss=0.93 avg=0.88\n",
            "[18294 | 995.81] loss=0.88 avg=0.88\n",
            "[18295 | 998.90] loss=0.90 avg=0.88\n",
            "[18296 | 1001.98] loss=0.97 avg=0.88\n",
            "[18297 | 1005.06] loss=0.92 avg=0.88\n",
            "[18298 | 1008.15] loss=0.83 avg=0.88\n",
            "[18299 | 1011.24] loss=1.05 avg=0.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "íblico todos al día.\n",
            "We must not leave the port until tomorrow. = No debemos salir de la porte hasta mañana.\n",
            "He's always complaining about his health. = Él siempre está quejando de su salud.\n",
            "I didn't understand anything Tom said. = No entendí nada de lo que dijo Tom.\n",
            "The teacher warned us at the beginning that there wouldn't be much study. = El profesor nos advirtió a principio que no saliera mucho estudiar.\n",
            "You're as human as any other human being. = Eres tan humano como cualquiera humano.\n",
            "Mary gave her doll a pink dress. = Mary le dio a su vieja ala un traje rosa.\n",
            "She was wearing a black hat. = Ella llevaba puesto un sombrero negro.\n",
            "My aunt has died. = Mi tía ha muerto.\n",
            "Tom isn't sure where to go. = Tom no está seguro de adónde ir.\n",
            "Tom was wearing a white jacket. = Tom llevaba puesto una campera blanca.\n",
            "Why didn't somebody tell me? = ¿Por qué no me dijo nadie?\n",
            "Tom is on his own. = Tom está sola.\n",
            "Tom was taken hostage. = Tom fue tomado entradnada.\n",
            "That isn't what I bought. = Eso no es lo que compré.\n",
            "Tom got lost. = Tom se perdió.\n",
            "Is it warm out? = ¿Está caliente hacia sacar?\n",
            "I was told that I could learn to play the trombone. = Se dijeron que yo podía aprender al tocionero.\n",
            "Tom was my first boyfriend. = Tom fue mi primer novio.\n",
            "He didn't deny that he had stolen the money. = Él no negó que él había robado el dinero.\n",
            "Could you come back later? = ¿Podrías volver más tarde?\n",
            "Tom wants to see you, Mary. = Tom quiere verte, Mary.\n",
            "This box has a very tight fit. = Esta caja tiene un cajón sullido con gusto.\n",
            "I will keep my word. = Mantendré mi palabra.\n",
            "The car has been standing still since the taxi stopped. = El auto está quedando quedado quedado allá.\n",
            "He was the last one to leave. = Él fue el último en irse.\n",
            "Tom isn't sure how long he'll be staying in Boston. = Tom no está seguro de cuánto te permaneá en Boston.\n",
            "I don't have any choice. = No tengo elección.\n",
            "Tom didn't really want to go. = Tom en realidad no quería ir.\n",
            "There is no point in doing that. = No tiene caso en hacerlo.\n",
            "Do you have any idea who wrote this book? = ¿Tiene idea de quién escribió ese libro?\n",
            "It's so hot in this room! = ¡Hala calor en esta habitación!\n",
            "That was a good question. = Eso fue un buen pregunta.\n",
            "The water came up to my knees. = El agua me llegaba hasta las rodillas.\n",
            "I have no regrets. = No me arrepiento nada.\n",
            "They want me. = Me quieren a mí.\n",
            "They are always talking about Tom. = Siempre están hablando acerca de Tom.\n",
            "Tom knows when he's being watched. = Tom sabe cuando está siendo vigilado.\n",
            "Tom is the person who caught Mary. = Tom es la persona que atrapó a Mary.\n",
            "Tom is an alcoholic. = Tom es un alcohólico.\n",
            "Would you put the dishes in the sink? = ¿Pones los platos en el fregadero?\n",
            "We were on the train for three hours. = Hicimos tres horas en el tren.\n",
            "Tom was my friend when I was younger. = Tom era mi amigo cuando yo era más joven.\n",
            "Tom has lost sight of his kids. = Tom perdió de vista a sus niños.\n",
            "My brother is an only child. = Mi hermano es hijo único.\n",
            "I love my girlfriend. = Amo a mi novia.\n",
            "That was the only thing we had to do. = Era lo único que teníamos\n",
            "\n",
            "[18300 | 1051.79] loss=0.99 avg=0.88\n",
            "[18301 | 1054.87] loss=0.87 avg=0.88\n",
            "[18302 | 1057.95] loss=0.89 avg=0.88\n",
            "[18303 | 1061.03] loss=0.89 avg=0.88\n",
            "[18304 | 1064.11] loss=0.83 avg=0.88\n",
            "[18305 | 1067.21] loss=0.90 avg=0.88\n",
            "[18306 | 1070.28] loss=0.72 avg=0.88\n",
            "[18307 | 1073.36] loss=0.67 avg=0.88\n",
            "[18308 | 1076.44] loss=0.90 avg=0.88\n",
            "[18309 | 1079.52] loss=0.91 avg=0.88\n",
            "[18310 | 1082.58] loss=0.96 avg=0.88\n",
            "[18311 | 1085.67] loss=0.92 avg=0.88\n",
            "[18312 | 1088.74] loss=0.89 avg=0.88\n",
            "[18313 | 1091.83] loss=0.91 avg=0.88\n",
            "[18314 | 1094.90] loss=0.89 avg=0.88\n",
            "[18315 | 1097.97] loss=0.88 avg=0.88\n",
            "[18316 | 1101.04] loss=0.82 avg=0.88\n",
            "[18317 | 1104.11] loss=0.85 avg=0.88\n",
            "[18318 | 1107.19] loss=0.93 avg=0.88\n",
            "[18319 | 1110.28] loss=0.99 avg=0.88\n",
            "[18320 | 1113.36] loss=0.93 avg=0.88\n",
            "[18321 | 1116.45] loss=0.99 avg=0.88\n",
            "[18322 | 1119.55] loss=0.89 avg=0.88\n",
            "[18323 | 1122.62] loss=0.91 avg=0.89\n",
            "[18324 | 1125.69] loss=0.86 avg=0.89\n",
            "[18325 | 1128.78] loss=0.89 avg=0.89\n",
            "[18326 | 1131.86] loss=0.88 avg=0.89\n",
            "[18327 | 1134.95] loss=0.92 avg=0.89\n",
            "[18328 | 1138.03] loss=0.77 avg=0.88\n",
            "[18329 | 1141.11] loss=0.93 avg=0.88\n",
            "[18330 | 1144.18] loss=0.93 avg=0.89\n",
            "[18331 | 1147.26] loss=0.83 avg=0.88\n",
            "[18332 | 1150.34] loss=0.94 avg=0.89\n",
            "[18333 | 1153.41] loss=0.86 avg=0.88\n",
            "[18334 | 1156.49] loss=0.84 avg=0.88\n",
            "[18335 | 1159.58] loss=0.84 avg=0.88\n",
            "[18336 | 1162.66] loss=0.85 avg=0.88\n",
            "[18337 | 1165.75] loss=0.84 avg=0.88\n",
            "[18338 | 1168.85] loss=0.96 avg=0.88\n",
            "[18339 | 1171.94] loss=0.90 avg=0.88\n",
            "[18340 | 1175.03] loss=0.88 avg=0.88\n",
            "[18341 | 1178.11] loss=1.02 avg=0.89\n",
            "[18342 | 1181.19] loss=0.69 avg=0.88\n",
            "[18343 | 1184.28] loss=0.90 avg=0.88\n",
            "[18344 | 1187.37] loss=0.87 avg=0.88\n",
            "[18345 | 1190.44] loss=0.84 avg=0.88\n",
            "[18346 | 1193.52] loss=0.86 avg=0.88\n",
            "[18347 | 1196.62] loss=0.79 avg=0.88\n",
            "[18348 | 1199.69] loss=0.90 avg=0.88\n",
            "[18349 | 1202.77] loss=0.76 avg=0.88\n",
            "[18350 | 1205.86] loss=0.91 avg=0.88\n",
            "[18351 | 1208.94] loss=0.75 avg=0.88\n",
            "[18352 | 1212.00] loss=0.94 avg=0.88\n",
            "[18353 | 1215.05] loss=0.79 avg=0.88\n",
            "[18354 | 1218.08] loss=0.88 avg=0.88\n",
            "[18355 | 1221.17] loss=0.85 avg=0.88\n",
            "[18356 | 1224.25] loss=0.82 avg=0.88\n",
            "[18357 | 1227.34] loss=0.78 avg=0.88\n",
            "[18358 | 1230.43] loss=0.91 avg=0.88\n",
            "[18359 | 1233.51] loss=0.85 avg=0.88\n",
            "[18360 | 1236.59] loss=0.96 avg=0.88\n",
            "[18361 | 1239.69] loss=0.84 avg=0.88\n",
            "[18362 | 1242.76] loss=0.92 avg=0.88\n",
            "[18363 | 1245.84] loss=0.84 avg=0.88\n",
            "[18364 | 1248.92] loss=0.90 avg=0.88\n",
            "[18365 | 1252.00] loss=0.86 avg=0.88\n",
            "[18366 | 1255.10] loss=0.79 avg=0.88\n",
            "[18367 | 1258.19] loss=0.86 avg=0.88\n",
            "[18368 | 1261.28] loss=0.85 avg=0.88\n",
            "[18369 | 1264.36] loss=0.92 avg=0.88\n",
            "[18370 | 1267.45] loss=0.98 avg=0.88\n",
            "[18371 | 1270.53] loss=0.85 avg=0.88\n",
            "[18372 | 1273.62] loss=0.85 avg=0.88\n",
            "[18373 | 1276.71] loss=0.75 avg=0.88\n",
            "[18374 | 1279.79] loss=0.91 avg=0.88\n",
            "[18375 | 1282.87] loss=0.82 avg=0.88\n",
            "[18376 | 1285.96] loss=0.98 avg=0.88\n",
            "[18377 | 1289.06] loss=0.86 avg=0.88\n",
            "[18378 | 1292.15] loss=0.82 avg=0.88\n",
            "[18379 | 1295.24] loss=0.89 avg=0.88\n",
            "[18380 | 1298.33] loss=0.93 avg=0.88\n",
            "[18381 | 1301.41] loss=0.91 avg=0.88\n",
            "[18382 | 1304.50] loss=0.96 avg=0.88\n",
            "[18383 | 1307.60] loss=0.92 avg=0.88\n",
            "[18384 | 1310.70] loss=0.85 avg=0.88\n",
            "[18385 | 1313.78] loss=0.86 avg=0.88\n",
            "[18386 | 1316.87] loss=0.94 avg=0.88\n",
            "[18387 | 1319.95] loss=0.85 avg=0.88\n",
            "[18388 | 1323.04] loss=0.88 avg=0.88\n",
            "[18389 | 1326.13] loss=0.92 avg=0.88\n",
            "[18390 | 1329.23] loss=0.86 avg=0.88\n",
            "[18391 | 1332.31] loss=0.95 avg=0.88\n",
            "[18392 | 1335.39] loss=0.89 avg=0.88\n",
            "[18393 | 1338.47] loss=0.97 avg=0.88\n",
            "[18394 | 1341.55] loss=0.81 avg=0.88\n",
            "[18395 | 1344.64] loss=0.87 avg=0.88\n",
            "[18396 | 1347.73] loss=0.84 avg=0.88\n",
            "[18397 | 1350.81] loss=0.96 avg=0.88\n",
            "[18398 | 1353.89] loss=0.80 avg=0.88\n",
            "[18399 | 1356.97] loss=0.96 avg=0.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". = No aceptará.\n",
            "He is a good liar. = Él es un buen mentiroso.\n",
            "I've been to England several times. = He estado en Inglaterra varias veces.\n",
            "I will be glad when she gives birth to a daughter. = A daré encantarme cuando daña a una niña.\n",
            "My mother has been sick for a week. = Mi mamá está enferma durante una semana.\n",
            "Tom asked Mary if she was all right. = Tom le preguntó a Mary si estaba bien.\n",
            "Would you like to dance? = ¿Te gustaría bailar?\n",
            "She looks very pretty today. = Ella se ve muy guapa hoy.\n",
            "That book seems useful. = Ese libro parece siendo útil.\n",
            "Do not open the windows. = No abras la ventana.\n",
            "Why don't you tell me what happened to Tom? = ¿Por qué no me contás qué le sucedió a Tom?\n",
            "What's the best way to keep sharks at bay? = ¿Cuál es la mejor forma de evitar que a las sharkas se engañen?\n",
            "Mary asked me if I really needed an abortion. = Mary me preguntó si solo necesitaba una abortion.\n",
            "I can't tell you what Tom said. = No puedo contarte lo que dijo Tomás.\n",
            "Somebody must've left the door open. = Debe de haber dejado la puerta abierta.\n",
            "It is very difficult. = Es muy difícil.\n",
            "I don't want to give up. = No quiero rendirme.\n",
            "He loves to read. = A él le encanta leer.\n",
            "Did you find out something? = ¿Has descubierto algo?\n",
            "I will stay here until it stops raining. = Me quedaré aquí hasta que parara.\n",
            "Do you want to eat dinner? = ¿Querés cenar?\n",
            "Tom has three times as much money as Mary does. = Tom tienes tres veces más dinero que Mary.\n",
            "Tom is a man of few words. = Tom es un hombre de pocos palabras.\n",
            "Is his story true? = ¿Es ciertamente ciertamente ciertamente?\n",
            "It's not a good place to start a relationship. = No es un buen lugar para empezar una reláfera.\n",
            "He is very patient. = Él es muy paciente.\n",
            "Do you want to talk after class? = ¿Quieres hablar después de la clase?\n",
            "I couldn't have written this story. = No podría haberlo escrito este escrito.\n",
            "I had an engagement, so I had to take the day off. = Tenía un compromiso, así que tuviera tomar el día libre.\n",
            "It has never happened to me before. = Nunca me ha pasado eso antes.\n",
            "Tom didn't go there today. = Tom no fue allá hoy.\n",
            "I can see a castle from my bedroom window. = Puedo ver casa desde la ventana de mi habitación.\n",
            "What happened at the party last night? = ¿Qué pasó anoche en la fiesta?\n",
            "His behavior made my father very angry. = Su conducta lo hace muy enégal de mi papá.\n",
            "I'm so busy I can't help you with anything. = Estoy tan ocupado que no puedo ayudarte con nada.\n",
            "Let's go into the jungle together. = Entramos juntos a la jungla.\n",
            "This is a very special day. = Este es un día muy especial.\n",
            "She took good care of him after his death. = Ella le ocupó bien de él después de su muerte.\n",
            "Do you speak Filipino? = ¿Habláis filipino?\n",
            "It wasn't necessary to take such a risk. = No era necesario que tomar riesgos así.\n",
            "He knows who I am and what I'm trying to do. = Él sabe quién soy y lo que intento hacer.\n",
            "Tom didn't say how many times he had been there. = Tom no dijo cuántas veces había estado allí.\n",
            "What did you learn? = ¿Qué aprendiste?\n",
            "The situation is really scary. = La\n",
            "\n",
            "[18400 | 1397.51] loss=0.93 avg=0.88\n",
            "[18401 | 1400.59] loss=0.84 avg=0.88\n",
            "[18402 | 1403.67] loss=1.00 avg=0.88\n",
            "[18403 | 1406.75] loss=0.93 avg=0.88\n",
            "[18404 | 1409.84] loss=0.90 avg=0.88\n",
            "[18405 | 1412.92] loss=0.93 avg=0.88\n",
            "[18406 | 1415.99] loss=0.79 avg=0.88\n",
            "[18407 | 1419.07] loss=0.89 avg=0.88\n",
            "[18408 | 1422.16] loss=0.82 avg=0.88\n",
            "[18409 | 1425.24] loss=0.83 avg=0.88\n",
            "[18410 | 1428.33] loss=0.91 avg=0.88\n",
            "[18411 | 1431.41] loss=0.85 avg=0.88\n",
            "[18412 | 1434.49] loss=0.82 avg=0.88\n",
            "[18413 | 1437.57] loss=0.85 avg=0.88\n",
            "[18414 | 1440.66] loss=0.96 avg=0.88\n",
            "[18415 | 1443.74] loss=0.86 avg=0.88\n",
            "[18416 | 1446.83] loss=0.82 avg=0.88\n",
            "[18417 | 1449.92] loss=0.94 avg=0.88\n",
            "[18418 | 1453.01] loss=0.91 avg=0.88\n",
            "[18419 | 1456.09] loss=0.90 avg=0.88\n",
            "[18420 | 1459.17] loss=0.87 avg=0.88\n",
            "[18421 | 1462.26] loss=0.92 avg=0.88\n",
            "[18422 | 1465.35] loss=0.91 avg=0.88\n",
            "[18423 | 1468.44] loss=0.82 avg=0.88\n",
            "[18424 | 1471.53] loss=0.90 avg=0.88\n",
            "[18425 | 1474.61] loss=0.83 avg=0.88\n",
            "[18426 | 1477.70] loss=0.86 avg=0.88\n",
            "[18427 | 1480.78] loss=0.81 avg=0.88\n",
            "[18428 | 1483.88] loss=0.90 avg=0.88\n",
            "[18429 | 1486.97] loss=0.77 avg=0.88\n",
            "[18430 | 1490.05] loss=0.85 avg=0.88\n",
            "[18431 | 1493.13] loss=0.69 avg=0.88\n",
            "[18432 | 1496.21] loss=0.84 avg=0.88\n",
            "[18433 | 1499.30] loss=0.89 avg=0.88\n",
            "[18434 | 1502.40] loss=0.87 avg=0.88\n",
            "[18435 | 1505.49] loss=0.91 avg=0.88\n",
            "[18436 | 1508.57] loss=0.76 avg=0.88\n",
            "[18437 | 1511.65] loss=0.94 avg=0.88\n",
            "[18438 | 1514.74] loss=0.73 avg=0.87\n",
            "[18439 | 1517.82] loss=0.94 avg=0.88\n",
            "[18440 | 1520.91] loss=0.74 avg=0.87\n",
            "[18441 | 1524.01] loss=0.87 avg=0.87\n",
            "[18442 | 1527.09] loss=0.88 avg=0.87\n",
            "[18443 | 1530.16] loss=0.90 avg=0.87\n",
            "[18444 | 1533.25] loss=0.88 avg=0.87\n",
            "[18445 | 1536.34] loss=0.96 avg=0.87\n",
            "[18446 | 1539.43] loss=0.85 avg=0.87\n",
            "[18447 | 1542.52] loss=0.80 avg=0.87\n",
            "[18448 | 1545.60] loss=0.84 avg=0.87\n",
            "[18449 | 1548.69] loss=0.92 avg=0.87\n",
            "[18450 | 1551.78] loss=0.89 avg=0.87\n",
            "[18451 | 1554.87] loss=0.97 avg=0.88\n",
            "[18452 | 1557.95] loss=0.79 avg=0.87\n",
            "[18453 | 1561.02] loss=0.89 avg=0.87\n",
            "[18454 | 1564.09] loss=0.82 avg=0.87\n",
            "[18455 | 1567.18] loss=0.92 avg=0.87\n",
            "[18456 | 1570.26] loss=0.84 avg=0.87\n",
            "[18457 | 1573.35] loss=0.92 avg=0.87\n",
            "[18458 | 1576.42] loss=0.88 avg=0.87\n",
            "[18459 | 1579.50] loss=0.92 avg=0.88\n",
            "[18460 | 1582.60] loss=0.92 avg=0.88\n",
            "[18461 | 1585.67] loss=0.92 avg=0.88\n",
            "[18462 | 1588.76] loss=0.82 avg=0.88\n",
            "[18463 | 1591.84] loss=0.63 avg=0.87\n",
            "[18464 | 1594.93] loss=0.93 avg=0.87\n",
            "[18465 | 1598.02] loss=0.98 avg=0.87\n",
            "[18466 | 1601.11] loss=0.81 avg=0.87\n",
            "[18467 | 1604.19] loss=0.87 avg=0.87\n",
            "[18468 | 1607.28] loss=0.91 avg=0.87\n",
            "[18469 | 1610.36] loss=1.00 avg=0.88\n",
            "[18470 | 1613.45] loss=0.93 avg=0.88\n",
            "[18471 | 1616.54] loss=0.82 avg=0.88\n",
            "[18472 | 1619.64] loss=0.89 avg=0.88\n",
            "[18473 | 1622.72] loss=0.85 avg=0.88\n",
            "[18474 | 1625.81] loss=0.94 avg=0.88\n",
            "[18475 | 1628.90] loss=0.81 avg=0.88\n",
            "[18476 | 1631.98] loss=0.86 avg=0.88\n",
            "[18477 | 1635.07] loss=0.91 avg=0.88\n",
            "[18478 | 1638.15] loss=0.80 avg=0.87\n",
            "[18479 | 1641.23] loss=0.96 avg=0.88\n",
            "[18480 | 1644.31] loss=0.88 avg=0.88\n",
            "[18481 | 1647.40] loss=0.88 avg=0.88\n",
            "[18482 | 1650.48] loss=0.86 avg=0.88\n",
            "[18483 | 1653.56] loss=0.94 avg=0.88\n",
            "[18484 | 1656.65] loss=0.95 avg=0.88\n",
            "[18485 | 1659.74] loss=0.88 avg=0.88\n",
            "[18486 | 1662.84] loss=0.84 avg=0.88\n",
            "[18487 | 1665.93] loss=0.92 avg=0.88\n",
            "[18488 | 1669.02] loss=0.85 avg=0.88\n",
            "[18489 | 1672.09] loss=0.88 avg=0.88\n",
            "[18490 | 1675.17] loss=0.91 avg=0.88\n",
            "[18491 | 1678.25] loss=0.94 avg=0.88\n",
            "[18492 | 1681.34] loss=0.84 avg=0.88\n",
            "[18493 | 1684.43] loss=0.90 avg=0.88\n",
            "[18494 | 1687.52] loss=0.75 avg=0.88\n",
            "[18495 | 1690.60] loss=0.93 avg=0.88\n",
            "[18496 | 1693.68] loss=0.92 avg=0.88\n",
            "[18497 | 1696.77] loss=0.87 avg=0.88\n",
            "[18498 | 1699.86] loss=0.84 avg=0.88\n",
            "[18499 | 1702.94] loss=0.93 avg=0.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " el uno.\n",
            "I've never liked him. = Nunca me ha gustado.\n",
            "Where's Tom? = ¿Dónde está Tom?\n",
            "A person like Tom, who gets up late every day, is odd. = Una persona como Tom, que se levanta todos los días se levantado, es raro.\n",
            "He's a man you can rely on. = Él es un hombre en el que puedes confiar.\n",
            "Tom isn't a scientist, but a painter. = Tom no es científico, sino pintaduro.\n",
            "I'm only trying to protect you. = Solo estoy tratando de protegerte.\n",
            "He went to see her four times a week. = Fue a ver caminar a vuestras veces a la semana.\n",
            "Do you think I should go alone? = ¿Tú crees que debería ir solo?\n",
            "I'll call you when I'm free. = Os llamo cuando tengo libre.\n",
            "I know that you are in love with me. = Sé que tú está enamorado de mí.\n",
            "I wish I had understood it then. = Lo deseo tener explicado entonces.\n",
            "The story was a best seller. = La historia era un best seller.\n",
            "It's not good to be a woman. = No es bueno ser una mujer.\n",
            "Tom is a man we can rely on. = Tom es un hombre en el que podemos confiar.\n",
            "Tom was lying on the beach, enjoying the fresh air. = Tom estaba tumbado encima de la playa, disfrutando el aire fresco.\n",
            "I'm an optimist. = Soy optimista.\n",
            "She had a very difficult examination. = Ella tuvo un examen muy difícil.\n",
            "There's no sense in going out today. = Hace falta si salimos hoy.\n",
            "Tom needs us. = Tom nos necesita.\n",
            "Are you coming in or not? = ¿Insensate y no pasa?\n",
            "Tom is waiting. = Tom está esperando.\n",
            "I didn't know that happened here. = No sabía que pasó aquí.\n",
            "Tom was in the middle of a long discussion with Mary. = Tom estaba en medio de unatuelas con Mary.\n",
            "This is so exciting. = Esto es tan emocionante.\n",
            "It looks like you're having a good time. = Parece que estás pasando bien.\n",
            "I don't know why you don't want to talk to me. = No sé por qué no quieres hablar conmigo.\n",
            "Who's that woman standing over there? = ¿Quién es esa mujer parada por allá?\n",
            "Please don't touch me with your foot. = No me toquen usted, por favor.\n",
            "I need to be here. = Necesito estar aquí.\n",
            "I like reading. = Me gusta leer.\n",
            "We'll stay here for a while. = Nos vamos a permanecer un rato tiempo.\n",
            "Tom asked the waitress for a menu. = Tom le pidió a la pequeña con un menú.\n",
            "I was very tired today. = Hoy estaba muy cansada.\n",
            "Can we get started before midnight? = ¿Podemos comenzar a principios antes de la medianoche?\n",
            "I don't know for certain yet. = Aún no sé saber con seguridad.\n",
            "I want to learn to speak Japanese, too. = Yo también quiero aprender a hablar japonés.\n",
            "The police are not going to solve this mystery. = La policía no va a resolver esta misteriosa.\n",
            "When will they arrive? = ¿Cuándo llegarán?\n",
            "She was too afraid to enter his room. = Ella tenía demasiado miedo para entrar a su habitación.\n",
            "Don't call me up after ten o'clock. = No me llame después de las diez.\n",
            "She said that she loved him. = Dijo que lo amaba.\n",
            "Tom ate the whole pizza. = Tom se comió todo el pizza.\n",
            "You can't be too careful when driving. = No puedes ser muy cuidadoso cuando conduces.\n",
            "No one laughed at Tom. = Nadie se reía de Tom.\n",
            "He asked a question that I had not heard him answer. = Le hizo una pre\n",
            "\n",
            "[18500 | 1744.42] loss=1.05 avg=0.88\n",
            "[18501 | 1747.50] loss=0.93 avg=0.88\n",
            "[18502 | 1750.58] loss=0.85 avg=0.88\n",
            "[18503 | 1753.66] loss=0.89 avg=0.88\n",
            "[18504 | 1756.76] loss=0.94 avg=0.88\n",
            "[18505 | 1759.83] loss=0.82 avg=0.88\n",
            "[18506 | 1762.92] loss=0.91 avg=0.88\n",
            "[18507 | 1766.01] loss=0.76 avg=0.88\n",
            "[18508 | 1769.08] loss=0.96 avg=0.88\n",
            "[18509 | 1772.16] loss=0.93 avg=0.88\n",
            "[18510 | 1775.25] loss=0.81 avg=0.88\n",
            "[18511 | 1778.34] loss=0.91 avg=0.88\n",
            "[18512 | 1781.41] loss=0.87 avg=0.88\n",
            "[18513 | 1784.49] loss=1.03 avg=0.88\n",
            "[18514 | 1787.57] loss=0.85 avg=0.88\n",
            "[18515 | 1790.65] loss=0.85 avg=0.88\n",
            "[18516 | 1793.73] loss=0.95 avg=0.88\n",
            "[18517 | 1796.81] loss=0.67 avg=0.88\n",
            "[18518 | 1799.90] loss=0.91 avg=0.88\n",
            "[18519 | 1802.97] loss=0.94 avg=0.88\n",
            "[18520 | 1806.06] loss=0.82 avg=0.88\n",
            "[18521 | 1809.14] loss=0.86 avg=0.88\n",
            "[18522 | 1812.22] loss=0.83 avg=0.88\n",
            "[18523 | 1815.28] loss=1.00 avg=0.88\n",
            "[18524 | 1818.33] loss=0.88 avg=0.88\n",
            "[18525 | 1821.42] loss=0.93 avg=0.88\n",
            "[18526 | 1824.50] loss=0.83 avg=0.88\n",
            "[18527 | 1827.58] loss=0.93 avg=0.88\n",
            "[18528 | 1830.67] loss=0.88 avg=0.88\n",
            "[18529 | 1833.76] loss=0.83 avg=0.88\n",
            "[18530 | 1836.84] loss=0.85 avg=0.88\n",
            "[18531 | 1839.93] loss=0.95 avg=0.88\n",
            "[18532 | 1842.99] loss=0.85 avg=0.88\n",
            "[18533 | 1846.07] loss=0.81 avg=0.88\n",
            "[18534 | 1849.16] loss=0.91 avg=0.88\n",
            "[18535 | 1852.23] loss=0.92 avg=0.88\n",
            "[18536 | 1855.34] loss=0.90 avg=0.88\n",
            "[18537 | 1858.42] loss=0.87 avg=0.88\n",
            "[18538 | 1861.51] loss=0.90 avg=0.88\n",
            "[18539 | 1864.57] loss=0.95 avg=0.88\n",
            "[18540 | 1867.67] loss=0.78 avg=0.88\n",
            "[18541 | 1870.78] loss=0.91 avg=0.88\n",
            "[18542 | 1873.86] loss=0.92 avg=0.88\n",
            "[18543 | 1876.95] loss=0.78 avg=0.88\n",
            "[18544 | 1880.03] loss=0.61 avg=0.88\n",
            "[18545 | 1883.13] loss=0.85 avg=0.88\n",
            "[18546 | 1886.22] loss=0.90 avg=0.88\n",
            "[18547 | 1889.31] loss=0.94 avg=0.88\n",
            "[18548 | 1892.40] loss=0.85 avg=0.88\n",
            "[18549 | 1895.48] loss=0.79 avg=0.88\n",
            "[18550 | 1898.57] loss=0.71 avg=0.87\n",
            "[18551 | 1901.66] loss=0.89 avg=0.87\n",
            "[18552 | 1904.73] loss=0.91 avg=0.87\n",
            "[18553 | 1907.83] loss=0.93 avg=0.88\n",
            "[18554 | 1910.91] loss=0.90 avg=0.88\n",
            "[18555 | 1913.99] loss=0.81 avg=0.88\n",
            "[18556 | 1917.08] loss=0.88 avg=0.88\n",
            "[18557 | 1920.16] loss=0.86 avg=0.87\n",
            "[18558 | 1923.25] loss=0.84 avg=0.87\n",
            "[18559 | 1926.34] loss=0.65 avg=0.87\n",
            "[18560 | 1929.41] loss=0.89 avg=0.87\n",
            "[18561 | 1932.50] loss=0.77 avg=0.87\n",
            "[18562 | 1935.57] loss=0.81 avg=0.87\n",
            "[18563 | 1938.66] loss=0.92 avg=0.87\n",
            "[18564 | 1941.76] loss=0.94 avg=0.87\n",
            "[18565 | 1944.86] loss=0.98 avg=0.87\n",
            "[18566 | 1947.94] loss=0.89 avg=0.87\n",
            "[18567 | 1951.04] loss=0.87 avg=0.87\n",
            "[18568 | 1954.13] loss=0.87 avg=0.87\n",
            "[18569 | 1957.22] loss=0.88 avg=0.87\n",
            "[18570 | 1960.32] loss=0.94 avg=0.87\n",
            "[18571 | 1963.42] loss=0.94 avg=0.87\n",
            "[18572 | 1966.50] loss=0.91 avg=0.88\n",
            "[18573 | 1969.59] loss=0.93 avg=0.88\n",
            "[18574 | 1972.68] loss=0.88 avg=0.88\n",
            "[18575 | 1975.78] loss=0.91 avg=0.88\n",
            "[18576 | 1978.88] loss=0.90 avg=0.88\n",
            "[18577 | 1981.98] loss=0.97 avg=0.88\n",
            "[18578 | 1985.06] loss=0.93 avg=0.88\n",
            "[18579 | 1988.16] loss=0.91 avg=0.88\n",
            "[18580 | 1991.25] loss=0.89 avg=0.88\n",
            "[18581 | 1994.33] loss=0.89 avg=0.88\n",
            "[18582 | 1997.43] loss=0.86 avg=0.88\n",
            "[18583 | 2000.50] loss=0.87 avg=0.88\n",
            "[18584 | 2003.60] loss=0.87 avg=0.88\n",
            "[18585 | 2006.68] loss=0.87 avg=0.88\n",
            "[18586 | 2009.78] loss=0.93 avg=0.88\n",
            "[18587 | 2012.88] loss=0.87 avg=0.88\n",
            "[18588 | 2015.97] loss=0.89 avg=0.88\n",
            "[18589 | 2019.05] loss=0.80 avg=0.88\n",
            "[18590 | 2022.14] loss=0.79 avg=0.88\n",
            "[18591 | 2025.22] loss=0.83 avg=0.88\n",
            "[18592 | 2028.29] loss=0.74 avg=0.87\n",
            "[18593 | 2031.39] loss=0.85 avg=0.87\n",
            "[18594 | 2034.47] loss=0.98 avg=0.88\n",
            "[18595 | 2037.54] loss=0.72 avg=0.87\n",
            "[18596 | 2040.62] loss=0.90 avg=0.87\n",
            "[18597 | 2043.70] loss=0.96 avg=0.88\n",
            "[18598 | 2046.78] loss=0.92 avg=0.88\n",
            "[18599 | 2049.87] loss=0.84 avg=0.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". = Una manera la puede contar con más tímo del tiempo.\n",
            "His house is near the park. = Su casa queda cerca del parque.\n",
            "We're tired. = Estamos cansados.\n",
            "They went swimming. = Ellos fueron a nadar.\n",
            "They are working hard to get their reward. = Están trabajando duro para conseguir su recompensa.\n",
            "You need to talk to me. = Necesitas hablar conmigo.\n",
            "You should've listened to me. = Deberías haberme escuchado.\n",
            "You can't change your username. = No puedes cambiar tu nombre de usuaria.\n",
            "He has a long nose. = Él tiene una nariz barata.\n",
            "I don't want to do this when I'm with you. = No quiero hacer esto cuando estoy contigo.\n",
            "What an interesting book this is! = ¡Qué libro más interesante!\n",
            "You can't take it with you when you die. = No te puedes llevar con ella cuando te mueras.\n",
            "That is all I know about him. = Eso es todo lo que sé sobre él.\n",
            "He is kind to her. = Él es amable con ella.\n",
            "I've got nothing to do with the case. = No tengo nada que ver con el caso.\n",
            "I need you to look at this. = Necesito que mirara esto.\n",
            "My father's plane landed on my birthday. = El avión de mi padre acolló en mi cumpleaños.\n",
            "We found an old man's pocket book in the chest. = Encontramos el libro de un anciano del tarjeta debajo del erecto.\n",
            "You can do it. You can! = ¡Usted puede hacerlo! ¡Sí!\n",
            "He was elected by the students. = Él fue elegido por los estudiantes.\n",
            "This coffee is too strong for me. = Este café está demasiado fuerte para mí.\n",
            "It's a good opportunity. = Es un buen ocasión.\n",
            "Tom doesn't know a thing about cooking. = Tom no sabe nada acocar sobre cocina.\n",
            "What does Tom buy for Mary on his birthday? = ¿Qué compra Tom para Mary de su cumpleaños?\n",
            "That would be embarrassing. = Eso sería avergonzado.\n",
            "Please don't argue. = Por favor, no discutan.\n",
            "Can you tell Tom apart from his brother? = ¿Usted puede distinguir a Tom de su hermano?\n",
            "What a pleasant surprise! = ¡Qué sorpresa agradable!\n",
            "I don't like the heat wave. = No me gusta el televisor centenario.\n",
            "Tom is very sensitive to cold. = Tom es muy sensible al frío.\n",
            "You're very brave, aren't you? = Eres muy valiente, ¿verdad?\n",
            "He had never had a bad experience. = Él nunca tuvo una mala experiencia.\n",
            "The situation is very complicated. = La situación es muy complicada.\n",
            "Tom doesn't have a car. = Tom no tiene coche.\n",
            "I need to know what I can do for you. = Necesito saber qué puedo hacer por ti.\n",
            "I'm not going to get up. = No voy a levantarme.\n",
            "The plane is bound for Chicago. = El avión va con destino a Chicago.\n",
            "What's the most annoying sound you've ever heard? = ¿Qué son las sinceras son que has oído alguna vez?\n",
            "Tom is very busy, isn't he? = Tom está muy ocupado, ¿no?\n",
            "I know nothing about her. = No sé nada acerca de ella.\n",
            "Tom didn't like Mary at first. = A Tom no le gustaba Mary al principio.\n",
            "Tom likes me. = Él me quiere.\n",
            "I love my family. = Amo a mi familia.\n",
            "Don't be rude. = No seas cortés.\n",
            "Tom didn't say how many times he'd been injured, but he had several thousand more than Mary. = Tom no dijo cuántas veces éste había sido herido, pero tuvo pasados más de más que Mary.\n",
            "Tom\n",
            "\n",
            "[18600 | 2090.43] loss=0.90 avg=0.88\n",
            "[18601 | 2093.52] loss=0.85 avg=0.88\n",
            "[18602 | 2096.61] loss=0.73 avg=0.87\n",
            "[18603 | 2099.70] loss=0.87 avg=0.87\n",
            "[18604 | 2102.78] loss=0.87 avg=0.87\n",
            "[18605 | 2105.85] loss=0.83 avg=0.87\n",
            "[18606 | 2108.94] loss=0.93 avg=0.87\n",
            "[18607 | 2112.03] loss=0.87 avg=0.87\n",
            "[18608 | 2115.10] loss=0.94 avg=0.87\n",
            "[18609 | 2118.20] loss=0.88 avg=0.87\n",
            "[18610 | 2121.29] loss=0.90 avg=0.87\n",
            "[18611 | 2124.38] loss=0.82 avg=0.87\n",
            "[18612 | 2127.46] loss=0.94 avg=0.87\n",
            "[18613 | 2130.55] loss=0.85 avg=0.87\n",
            "[18614 | 2133.65] loss=0.87 avg=0.87\n",
            "[18615 | 2136.74] loss=0.95 avg=0.88\n",
            "[18616 | 2139.82] loss=0.76 avg=0.87\n",
            "[18617 | 2142.89] loss=0.82 avg=0.87\n",
            "[18618 | 2145.99] loss=0.89 avg=0.87\n",
            "[18619 | 2149.07] loss=0.93 avg=0.87\n",
            "[18620 | 2152.16] loss=0.91 avg=0.87\n",
            "[18621 | 2155.25] loss=0.96 avg=0.88\n",
            "[18622 | 2158.33] loss=0.87 avg=0.88\n",
            "[18623 | 2161.39] loss=0.88 avg=0.88\n",
            "[18624 | 2164.47] loss=0.90 avg=0.88\n",
            "[18625 | 2167.57] loss=0.88 avg=0.88\n",
            "[18626 | 2170.66] loss=0.86 avg=0.88\n",
            "[18627 | 2173.75] loss=0.91 avg=0.88\n",
            "[18628 | 2176.82] loss=0.89 avg=0.88\n",
            "[18629 | 2179.90] loss=0.82 avg=0.88\n",
            "[18630 | 2182.98] loss=0.85 avg=0.88\n",
            "[18631 | 2186.08] loss=0.93 avg=0.88\n",
            "[18632 | 2189.17] loss=0.93 avg=0.88\n",
            "[18633 | 2192.26] loss=0.78 avg=0.88\n",
            "[18634 | 2195.34] loss=1.00 avg=0.88\n",
            "[18635 | 2198.42] loss=0.74 avg=0.88\n",
            "[18636 | 2201.52] loss=0.77 avg=0.87\n",
            "[18637 | 2204.61] loss=0.88 avg=0.87\n",
            "[18638 | 2207.69] loss=0.90 avg=0.87\n",
            "[18639 | 2210.79] loss=0.90 avg=0.87\n",
            "[18640 | 2213.86] loss=0.67 avg=0.87\n",
            "[18641 | 2216.95] loss=0.80 avg=0.87\n",
            "[18642 | 2220.02] loss=0.67 avg=0.87\n",
            "[18643 | 2223.12] loss=0.97 avg=0.87\n",
            "[18644 | 2226.20] loss=0.84 avg=0.87\n",
            "[18645 | 2229.30] loss=0.92 avg=0.87\n",
            "[18646 | 2232.37] loss=0.70 avg=0.87\n",
            "[18647 | 2235.44] loss=0.86 avg=0.87\n",
            "[18648 | 2238.53] loss=0.87 avg=0.87\n",
            "[18649 | 2241.61] loss=0.80 avg=0.87\n",
            "[18650 | 2244.71] loss=0.95 avg=0.87\n",
            "[18651 | 2247.80] loss=1.00 avg=0.87\n",
            "[18652 | 2250.90] loss=0.85 avg=0.87\n",
            "[18653 | 2253.97] loss=0.89 avg=0.87\n",
            "[18654 | 2257.06] loss=0.82 avg=0.87\n",
            "[18655 | 2260.15] loss=0.89 avg=0.87\n",
            "[18656 | 2263.24] loss=0.97 avg=0.87\n",
            "[18657 | 2266.33] loss=0.83 avg=0.87\n",
            "[18658 | 2269.41] loss=0.74 avg=0.87\n",
            "[18659 | 2272.49] loss=0.91 avg=0.87\n",
            "[18660 | 2275.58] loss=0.83 avg=0.87\n",
            "[18661 | 2278.67] loss=0.86 avg=0.87\n",
            "[18662 | 2281.76] loss=0.80 avg=0.87\n",
            "[18663 | 2284.85] loss=0.91 avg=0.87\n",
            "[18664 | 2287.93] loss=0.86 avg=0.87\n",
            "[18665 | 2291.01] loss=0.91 avg=0.87\n",
            "[18666 | 2294.09] loss=0.90 avg=0.87\n",
            "[18667 | 2297.16] loss=0.76 avg=0.87\n",
            "[18668 | 2300.26] loss=0.69 avg=0.87\n",
            "[18669 | 2303.36] loss=0.88 avg=0.87\n",
            "[18670 | 2306.44] loss=0.96 avg=0.87\n",
            "[18671 | 2309.53] loss=0.78 avg=0.87\n",
            "[18672 | 2312.61] loss=0.79 avg=0.87\n",
            "[18673 | 2315.69] loss=0.71 avg=0.86\n",
            "[18674 | 2318.78] loss=0.93 avg=0.87\n",
            "[18675 | 2321.88] loss=0.86 avg=0.87\n",
            "[18676 | 2324.97] loss=0.91 avg=0.87\n",
            "[18677 | 2328.06] loss=0.69 avg=0.86\n",
            "[18678 | 2331.14] loss=0.84 avg=0.86\n",
            "[18679 | 2334.23] loss=0.91 avg=0.86\n",
            "[18680 | 2337.31] loss=0.92 avg=0.86\n",
            "[18681 | 2340.41] loss=0.93 avg=0.87\n",
            "[18682 | 2343.49] loss=0.98 avg=0.87\n",
            "[18683 | 2346.58] loss=0.90 avg=0.87\n",
            "[18684 | 2349.65] loss=0.96 avg=0.87\n",
            "[18685 | 2352.73] loss=0.85 avg=0.87\n",
            "[18686 | 2355.81] loss=0.79 avg=0.87\n",
            "[18687 | 2358.90] loss=0.91 avg=0.87\n",
            "[18688 | 2361.98] loss=0.85 avg=0.87\n",
            "[18689 | 2365.06] loss=0.84 avg=0.87\n",
            "[18690 | 2368.16] loss=0.69 avg=0.87\n",
            "[18691 | 2371.23] loss=0.86 avg=0.87\n",
            "[18692 | 2374.32] loss=0.99 avg=0.87\n",
            "[18693 | 2377.41] loss=0.87 avg=0.87\n",
            "[18694 | 2380.49] loss=0.85 avg=0.87\n",
            "[18695 | 2383.58] loss=0.83 avg=0.87\n",
            "[18696 | 2386.68] loss=0.82 avg=0.87\n",
            "[18697 | 2389.75] loss=0.88 avg=0.87\n",
            "[18698 | 2392.83] loss=0.86 avg=0.87\n",
            "[18699 | 2395.92] loss=0.76 avg=0.86\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "From them? = ¿Los informes?\n",
            "This tie goes well with your shirt. = Esta corbata va bien con tu camisa.\n",
            "Tom was sentenced to four months in jail. = A Tom le condenaron a cuatro meses en prisión.\n",
            "What's the date today? = ¿Qué fecha es hoy?\n",
            "Tom has a daughter whose name is Mary, but she is not his. = Tom tiene una hija que no es María pero no es su nombre.\n",
            "It would be nice to see this movie again. = Estaría bien ver la película otra vez.\n",
            "Is it true that you came here alone? = ¿Es verdad que viniste aquí solo?\n",
            "He gave up smoking three years ago. = Él dejó el tabaco hace tres años.\n",
            "Tom isn't old enough to drink. = Tom no tiene edad suficiente para beber.\n",
            "Let me speak to Tom about it. = Déjame hablar a Tom acerca de eso.\n",
            "He was the first man to cross the Pacific Ocean. = Él fue el primer hombre en cruzar el Océano Pacífico.\n",
            "Tom is out of town and cannot be reached. = Tom está fuera de la ciudad, no puede llegar por ella.\n",
            "My brother looks just like me. = Mi hermano se parece exactamente a mí.\n",
            "I need you here in case something happens. = Te necesito aquí por si pasa algo.\n",
            "He was wounded in a burglary. = Él fue herido en un robo.\n",
            "Did you find anything? = ¿Encontraste algo?\n",
            "I didn't want to be discovered. = No me quería ser vigilado.\n",
            "Does Tom like to be alone? = ¿A Tom le gusta estar solo?\n",
            "Tom says he doesn't have enough money. = Tom dice que no tiene suficiente dinero.\n",
            "I'm sure Tom misses you. = Estoy seguro que Tom te extraña.\n",
            "A little work will do. = Un poco de trabajo lleva a un poco.\n",
            "He took the book to the station. = Llevó el libro a la estación.\n",
            "I thought you came here yesterday. = Pensé que llegaste ayer aquí.\n",
            "I went to visit my father for the first time in ten years. = Avisé a mi padre por primera vez en diez años.\n",
            "I think he'll like this. = Creo que a él le gustará esto.\n",
            "You're on the wrong train. = Estás en el tren equivocado.\n",
            "I think this is the most interesting book that I've ever read. = Creo que esta es la verdad libro que he leía hasta ahora.\n",
            "I'm just a boy who makes mistakes. = Sólo soy un niño que comete errores.\n",
            "He was angry at everyone. = Él estaba satisfecho con todo el mundo.\n",
            "Let Tom finish. = Deja que Tom termina.\n",
            "I want a picture of my family. = Desea una fotografía de mi familia.\n",
            "You're the new guy. = Sos el nuevo chico.\n",
            "He often speaks to his son from behind. = Voy a menudo con su padre a desviar tras un grupo.\n",
            "I'll have two hot dogs with spicy sauce for dinner. = Trata dos panchos con salsa secundaria para la cena.\n",
            "I'm very tired. = Estoy muy cansado.\n",
            "It's too early to say. = Es demasiado temprano para decirlo.\n",
            "This book was written in French. = Este libro fue escrito en francés.\n",
            "The police would not give us more information. = La Policía no nos darían más información.\n",
            "She told him that she knew the secret. = Ella le dijo que sabía el secreto.\n",
            "Tom is not a very good kisser. = A Tom no le cantara muy bien besar.\n",
            "There's a lot to be learned about this town. = Hay mucho que esto aprender sobre esta ciudad.\n",
            "He said that he had written the letter himself. = Él dijo que él se había escrito la carta.\n",
            "What made you want to come to my party? = ¿Qué te hizo querer para ven\n",
            "\n",
            "[18700 | 2436.38] loss=0.82 avg=0.86\n",
            "[18701 | 2439.47] loss=0.89 avg=0.86\n",
            "[18702 | 2442.56] loss=0.68 avg=0.86\n",
            "[18703 | 2445.65] loss=0.81 avg=0.86\n",
            "[18704 | 2448.72] loss=0.94 avg=0.86\n",
            "[18705 | 2451.81] loss=0.70 avg=0.86\n",
            "[18706 | 2454.88] loss=0.99 avg=0.86\n",
            "[18707 | 2457.96] loss=0.92 avg=0.86\n",
            "[18708 | 2461.05] loss=0.85 avg=0.86\n",
            "[18709 | 2464.12] loss=0.77 avg=0.86\n",
            "[18710 | 2467.20] loss=0.89 avg=0.86\n",
            "[18711 | 2470.28] loss=0.78 avg=0.86\n",
            "[18712 | 2473.36] loss=0.82 avg=0.86\n",
            "[18713 | 2476.44] loss=0.98 avg=0.86\n",
            "[18714 | 2479.52] loss=0.87 avg=0.86\n",
            "[18715 | 2482.61] loss=0.88 avg=0.86\n",
            "[18716 | 2485.69] loss=0.75 avg=0.86\n",
            "[18717 | 2488.77] loss=0.73 avg=0.86\n",
            "[18718 | 2491.85] loss=0.89 avg=0.86\n",
            "[18719 | 2494.93] loss=0.92 avg=0.86\n",
            "[18720 | 2498.02] loss=0.85 avg=0.86\n",
            "[18721 | 2501.11] loss=0.97 avg=0.86\n",
            "[18722 | 2504.20] loss=0.85 avg=0.86\n",
            "[18723 | 2507.27] loss=0.86 avg=0.86\n",
            "[18724 | 2510.37] loss=0.91 avg=0.86\n",
            "[18725 | 2513.45] loss=0.81 avg=0.86\n",
            "[18726 | 2516.55] loss=0.92 avg=0.86\n",
            "[18727 | 2519.65] loss=0.86 avg=0.86\n",
            "[18728 | 2522.72] loss=0.87 avg=0.86\n",
            "[18729 | 2525.79] loss=0.86 avg=0.86\n",
            "[18730 | 2528.88] loss=0.78 avg=0.86\n",
            "[18731 | 2531.95] loss=0.90 avg=0.86\n",
            "[18732 | 2535.03] loss=0.82 avg=0.86\n",
            "[18733 | 2538.09] loss=0.90 avg=0.86\n",
            "[18734 | 2541.18] loss=0.92 avg=0.86\n",
            "[18735 | 2544.27] loss=0.88 avg=0.86\n",
            "[18736 | 2547.35] loss=0.94 avg=0.86\n",
            "[18737 | 2550.43] loss=0.88 avg=0.86\n",
            "[18738 | 2553.53] loss=0.94 avg=0.86\n",
            "[18739 | 2556.62] loss=0.90 avg=0.86\n",
            "[18740 | 2559.71] loss=0.93 avg=0.86\n",
            "[18741 | 2562.78] loss=0.83 avg=0.86\n",
            "[18742 | 2565.86] loss=0.77 avg=0.86\n",
            "[18743 | 2568.95] loss=0.85 avg=0.86\n",
            "[18744 | 2572.04] loss=0.83 avg=0.86\n",
            "[18745 | 2575.13] loss=0.80 avg=0.86\n",
            "[18746 | 2578.22] loss=0.81 avg=0.86\n",
            "[18747 | 2581.31] loss=0.98 avg=0.86\n",
            "[18748 | 2584.40] loss=0.89 avg=0.86\n",
            "[18749 | 2587.50] loss=0.88 avg=0.86\n",
            "[18750 | 2590.57] loss=0.76 avg=0.86\n",
            "[18751 | 2593.65] loss=0.84 avg=0.86\n",
            "[18752 | 2596.75] loss=0.86 avg=0.86\n",
            "[18753 | 2599.84] loss=0.96 avg=0.86\n",
            "[18754 | 2602.92] loss=1.01 avg=0.86\n",
            "[18755 | 2606.02] loss=0.93 avg=0.87\n",
            "[18756 | 2609.10] loss=0.87 avg=0.87\n",
            "[18757 | 2612.18] loss=0.84 avg=0.87\n",
            "[18758 | 2615.26] loss=0.88 avg=0.87\n",
            "[18759 | 2618.34] loss=0.90 avg=0.87\n",
            "[18760 | 2621.42] loss=0.95 avg=0.87\n",
            "[18761 | 2624.50] loss=0.91 avg=0.87\n",
            "[18762 | 2627.59] loss=1.01 avg=0.87\n",
            "[18763 | 2630.68] loss=0.85 avg=0.87\n",
            "[18764 | 2633.77] loss=0.91 avg=0.87\n",
            "[18765 | 2636.85] loss=0.90 avg=0.87\n",
            "[18766 | 2639.94] loss=0.84 avg=0.87\n",
            "[18767 | 2643.02] loss=0.87 avg=0.87\n",
            "[18768 | 2646.11] loss=0.74 avg=0.87\n",
            "[18769 | 2649.18] loss=0.73 avg=0.87\n",
            "[18770 | 2652.29] loss=0.81 avg=0.87\n",
            "[18771 | 2655.36] loss=0.88 avg=0.87\n",
            "[18772 | 2658.45] loss=0.92 avg=0.87\n",
            "[18773 | 2661.53] loss=0.92 avg=0.87\n",
            "[18774 | 2664.62] loss=0.70 avg=0.87\n",
            "[18775 | 2667.72] loss=0.93 avg=0.87\n",
            "[18776 | 2670.83] loss=0.68 avg=0.86\n",
            "[18777 | 2673.92] loss=0.78 avg=0.86\n",
            "[18778 | 2677.01] loss=0.87 avg=0.86\n",
            "[18779 | 2680.10] loss=0.87 avg=0.86\n",
            "[18780 | 2683.19] loss=0.81 avg=0.86\n",
            "[18781 | 2686.27] loss=0.83 avg=0.86\n",
            "[18782 | 2689.38] loss=0.96 avg=0.86\n",
            "[18783 | 2692.47] loss=0.72 avg=0.86\n",
            "[18784 | 2695.56] loss=0.93 avg=0.86\n",
            "[18785 | 2698.65] loss=0.87 avg=0.86\n",
            "[18786 | 2701.74] loss=0.93 avg=0.86\n",
            "[18787 | 2704.83] loss=0.83 avg=0.86\n",
            "[18788 | 2707.91] loss=0.91 avg=0.86\n",
            "[18789 | 2711.00] loss=0.97 avg=0.86\n",
            "[18790 | 2714.08] loss=0.95 avg=0.87\n",
            "[18791 | 2717.17] loss=0.84 avg=0.87\n",
            "[18792 | 2720.26] loss=0.87 avg=0.87\n",
            "[18793 | 2723.35] loss=0.94 avg=0.87\n",
            "[18794 | 2726.43] loss=0.83 avg=0.87\n",
            "[18795 | 2729.51] loss=0.92 avg=0.87\n",
            "[18796 | 2732.58] loss=0.87 avg=0.87\n",
            "[18797 | 2735.67] loss=0.86 avg=0.87\n",
            "[18798 | 2738.76] loss=0.90 avg=0.87\n",
            "[18799 | 2741.85] loss=0.89 avg=0.87\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " = Tom cómodo la cabeza.\n",
            "I would like to watch TV tonight. = Me gustaría ver la televisión esta noche.\n",
            "How many hours a day does he work? = ¿Cuántas horas trabaja?\n",
            "She is on the verge of starvation. = Ella está al bote de hambre.\n",
            "I'd say no if I were you. = Si yo fuera tú, no dicho.\n",
            "Do you want me to be there or not? = ¿Quieres que sea allí o no?\n",
            "Is that a gift for me? = ¿Es esa un regalo para mí?\n",
            "I need the key. = Necesito la llave.\n",
            "Tom is an honest boy. = Tom es un chico honesto.\n",
            "There is no hurry. = No hay apuro.\n",
            "Do you know how far Australia is from New Zealand? = ¿Sabes a qué distancia está Australia de Nueva Zelanda?\n",
            "I think I'd like to live in the country. = Creo que me gustaría vivir en el campo.\n",
            "They're too young to marry. = Ellos tienen demasiado chiquillos para casarse.\n",
            "Do you understand what I just said to you? = ¿Has entendido lo que acabo de decirte?\n",
            "The weather has gotten better recently. = El tiempo mejorá hace pasado.\n",
            "Tom has been arrested three times. = Tom han sido arrestado tres veces.\n",
            "Tom is a member of a secret society. = Tom es un miembro de una sociedad secreta.\n",
            "I'm the same age. = Soy tan misma años.\n",
            "Can you see Tom? = ¿Puedes ver a Tomás?\n",
            "I am going to bed. = Voy a acostar.\n",
            "Can I borrow a tape recorder? = ¿Puedo sacar una carta de rellenas?\n",
            "He was sent to an orphanage. = Él fue enviado a un orfanato.\n",
            "I don't even have your driver's license number. = Ni siquiera tengo tu número de línea.\n",
            "Tom doesn't know when Mary will leave Boston. = Tom no sabe cuándo Mary sale Mary Boston.\n",
            "The boy seemed to have been sick during the week. = El chico parecía haber estado enfermo durante la semana.\n",
            "How deep is Lake Biwa? = ¿Qué profundidad tiene el lago Biwa?\n",
            "They want to sell it. = Quieren vendrá.\n",
            "I'm not a bird. = Yo no soy un pájaro.\n",
            "She married him because she wanted to. = Se casó con él porque quería.\n",
            "He is a man of his word. = Se le da un hombre de palabra.\n",
            "You must be more polite. = Tiéntas que ser más respetuosas.\n",
            "He is getting old. = Siendo viejo.\n",
            "Tom has a secret admirer. = Tom tiene un admirer de secreto.\n",
            "Give me a break. = Dame un descanso.\n",
            "We have a lot of eggs in the fridge. = Tenemos muchos huevos en la nevera.\n",
            "He went home after dark. = Él se fue a casa después de que oscureciera la oscuridad.\n",
            "I'll call the police. = Llamaré a la policía.\n",
            "He told me his age. = Me dijo su edad.\n",
            "He's a good man. = Es un buen hombre.\n",
            "The children will be taking a walk when you arrive. = Las niñas se tomaron a dar un paseo cuando llegues.\n",
            "Please let me know if any change is needed. = Por favor, avísale si hay algún cambio.\n",
            "I think you ought to apologize to Tom. = Creo que deberías pedirle perdón a Tom.\n",
            "I saw him playing baseball. = Lo vi jugar al béisbol.\n",
            "You've been acting strange. = Habéis estado xicando.\n",
            "Tom has been trying to avoid Mary. = Tom ha estado intentando evitar a Mary.\n",
            "There are many islands in Greece. = Hay muchas islas en Grecia.\n",
            "You can't live without water. = No podés vivir sin agua.\n",
            "She was advised by him to spend more time reading. = Le aconsejaron\n",
            "\n",
            "[18800 | 2782.18] loss=0.88 avg=0.87\n",
            "[18801 | 2785.26] loss=0.71 avg=0.87\n",
            "[18802 | 2788.33] loss=0.94 avg=0.87\n",
            "[18803 | 2791.38] loss=0.79 avg=0.87\n",
            "[18804 | 2794.44] loss=0.74 avg=0.86\n",
            "[18805 | 2797.51] loss=0.88 avg=0.86\n",
            "[18806 | 2800.58] loss=0.92 avg=0.86\n",
            "[18807 | 2803.64] loss=0.80 avg=0.86\n",
            "[18808 | 2806.72] loss=0.81 avg=0.86\n",
            "[18809 | 2809.81] loss=0.87 avg=0.86\n",
            "[18810 | 2812.90] loss=0.79 avg=0.86\n",
            "[18811 | 2816.00] loss=0.92 avg=0.86\n",
            "[18812 | 2819.08] loss=0.82 avg=0.86\n",
            "[18813 | 2822.16] loss=0.68 avg=0.86\n",
            "[18814 | 2825.24] loss=0.75 avg=0.86\n",
            "[18815 | 2828.32] loss=0.93 avg=0.86\n",
            "[18816 | 2831.41] loss=0.69 avg=0.86\n",
            "[18817 | 2834.50] loss=0.79 avg=0.86\n",
            "[18818 | 2837.59] loss=0.76 avg=0.86\n",
            "[18819 | 2840.68] loss=0.95 avg=0.86\n",
            "[18820 | 2843.77] loss=0.74 avg=0.86\n",
            "[18821 | 2846.86] loss=0.74 avg=0.86\n",
            "[18822 | 2849.95] loss=0.80 avg=0.86\n",
            "[18823 | 2853.03] loss=0.68 avg=0.85\n",
            "[18824 | 2856.13] loss=0.87 avg=0.85\n",
            "[18825 | 2859.20] loss=0.85 avg=0.85\n",
            "[18826 | 2862.28] loss=1.03 avg=0.86\n",
            "[18827 | 2865.37] loss=0.87 avg=0.86\n",
            "[18828 | 2868.47] loss=0.85 avg=0.86\n",
            "[18829 | 2871.54] loss=0.93 avg=0.86\n",
            "[18830 | 2874.62] loss=0.90 avg=0.86\n",
            "[18831 | 2877.68] loss=1.09 avg=0.86\n",
            "[18832 | 2880.73] loss=0.92 avg=0.86\n",
            "[18833 | 2883.79] loss=0.73 avg=0.86\n",
            "[18834 | 2886.87] loss=0.86 avg=0.86\n",
            "[18835 | 2889.95] loss=0.90 avg=0.86\n",
            "[18836 | 2893.05] loss=0.85 avg=0.86\n",
            "[18837 | 2896.10] loss=0.95 avg=0.86\n",
            "[18838 | 2899.17] loss=0.90 avg=0.86\n",
            "[18839 | 2902.24] loss=0.89 avg=0.86\n",
            "[18840 | 2905.31] loss=0.73 avg=0.86\n",
            "[18841 | 2908.38] loss=0.89 avg=0.86\n",
            "[18842 | 2911.45] loss=0.93 avg=0.86\n",
            "[18843 | 2914.51] loss=0.84 avg=0.86\n",
            "[18844 | 2917.56] loss=0.91 avg=0.86\n",
            "[18845 | 2920.62] loss=0.92 avg=0.86\n",
            "[18846 | 2923.69] loss=0.94 avg=0.86\n",
            "[18847 | 2926.74] loss=0.72 avg=0.86\n",
            "[18848 | 2929.81] loss=0.94 avg=0.86\n",
            "[18849 | 2932.88] loss=0.69 avg=0.86\n",
            "[18850 | 2935.93] loss=0.77 avg=0.86\n",
            "[18851 | 2939.00] loss=0.95 avg=0.86\n",
            "[18852 | 2942.07] loss=0.88 avg=0.86\n",
            "[18853 | 2945.14] loss=0.93 avg=0.86\n",
            "[18854 | 2948.21] loss=0.91 avg=0.86\n",
            "[18855 | 2951.28] loss=0.77 avg=0.86\n",
            "[18856 | 2954.36] loss=0.82 avg=0.86\n",
            "[18857 | 2957.45] loss=0.94 avg=0.86\n",
            "[18858 | 2960.53] loss=0.88 avg=0.86\n",
            "[18859 | 2963.61] loss=0.90 avg=0.86\n",
            "[18860 | 2966.67] loss=0.83 avg=0.86\n",
            "[18861 | 2969.75] loss=0.91 avg=0.86\n",
            "[18862 | 2972.84] loss=0.83 avg=0.86\n",
            "[18863 | 2975.93] loss=0.73 avg=0.86\n",
            "[18864 | 2979.02] loss=0.78 avg=0.86\n",
            "[18865 | 2982.10] loss=0.89 avg=0.86\n",
            "[18866 | 2985.19] loss=0.95 avg=0.86\n",
            "[18867 | 2988.29] loss=0.78 avg=0.86\n",
            "[18868 | 2991.37] loss=0.88 avg=0.86\n",
            "[18869 | 2994.47] loss=0.76 avg=0.86\n",
            "[18870 | 2997.56] loss=0.75 avg=0.86\n",
            "[18871 | 3000.66] loss=0.93 avg=0.86\n",
            "[18872 | 3003.75] loss=0.76 avg=0.86\n",
            "[18873 | 3006.86] loss=0.77 avg=0.86\n",
            "[18874 | 3009.95] loss=0.95 avg=0.86\n",
            "[18875 | 3013.05] loss=0.98 avg=0.86\n",
            "[18876 | 3016.10] loss=0.94 avg=0.86\n",
            "[18877 | 3019.17] loss=0.79 avg=0.86\n",
            "[18878 | 3022.24] loss=0.92 avg=0.86\n",
            "[18879 | 3025.33] loss=0.92 avg=0.86\n",
            "[18880 | 3028.41] loss=0.83 avg=0.86\n",
            "[18881 | 3031.50] loss=0.96 avg=0.86\n",
            "[18882 | 3034.59] loss=0.81 avg=0.86\n",
            "[18883 | 3037.67] loss=0.85 avg=0.86\n",
            "[18884 | 3040.75] loss=0.82 avg=0.86\n",
            "[18885 | 3043.82] loss=0.88 avg=0.86\n",
            "[18886 | 3046.92] loss=0.96 avg=0.86\n",
            "[18887 | 3050.00] loss=0.95 avg=0.86\n",
            "[18888 | 3053.07] loss=0.68 avg=0.86\n",
            "[18889 | 3056.13] loss=0.89 avg=0.86\n",
            "[18890 | 3059.20] loss=0.79 avg=0.86\n",
            "[18891 | 3062.30] loss=0.85 avg=0.86\n",
            "[18892 | 3065.39] loss=0.94 avg=0.86\n",
            "[18893 | 3068.47] loss=0.95 avg=0.86\n",
            "[18894 | 3071.53] loss=0.73 avg=0.86\n",
            "[18895 | 3074.60] loss=0.94 avg=0.86\n",
            "[18896 | 3077.68] loss=0.91 avg=0.86\n",
            "[18897 | 3080.76] loss=0.89 avg=0.86\n",
            "[18898 | 3083.83] loss=0.86 avg=0.86\n",
            "[18899 | 3086.92] loss=0.98 avg=0.86\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "? = ¿Eres de ello?\n",
            "It would be good to take a taxi. = Sería bueno tomar un taxi.\n",
            "He is a man of culture. = Él es un hombre de la cultura.\n",
            "There's a possibility that the accident may have been prevented. = Está la posibilidad de que ese accidente se puede haber sido prevenido.\n",
            "Tom doesn't know what to do next. = Tom no sabe qué hacer después.\n",
            "I'm not going to let you do this. = No te voy a permitir que hagas esto.\n",
            "I don't have time for that now. = Ahora no tengo tiempo para esto.\n",
            "I need it right now. = Lo necesito ahora.\n",
            "I hope you have fun. = Ojalá te diviertas.\n",
            "Tom wanted to know what the problem was. = Tom quería saber cuál era el problema.\n",
            "I love the beach. = Me gusta el playa.\n",
            "Why do you want to hurt them? = ¿Por qué buscas los asustos?\n",
            "I have no choice. = No tengo otra opción.\n",
            "You guys have to do it. = Lo tienen que hacer lo que usted haya pedido.\n",
            "He was tired when he got home. = Él estaba cansado cuando llegó a casa.\n",
            "Did she come by train or by bus? = ¿Ella vino en tren o autobús?\n",
            "Tom is wearing a fake identity card. = Tom lleva puesto una nadie de falso falso.\n",
            "How much is this handkerchief? = ¿Cuánto cuesta este pañuelo?\n",
            "I know exactly what you did. = Sé exactamente lo que hiciste.\n",
            "Why don't you stay a bit longer? = ¿Por qué no te quedas un ratito más?\n",
            "I have to read these documents. = Tengo que leer estas documentas.\n",
            "Tom was surprised that Mary would want to go fishing. = Tom estaba sorprendidos de que Mary querría ir a pescar.\n",
            "I think Tom is lying. = Creo que Tom está mintiendo.\n",
            "He can't swim. = Él no nado.\n",
            "A girl appeared before him. = La chica apareció ante él.\n",
            "He's not busy now. = No está ocupado ahora.\n",
            "This room is very cold. = En este cuarto esta habitación es muy fría.\n",
            "I don't want to go to the movies tonight. = No quiero ir al cine esta noche.\n",
            "I'll call you at eight. = Te llamaré a las ocho a las ocho.\n",
            "Tom is in a bad mood this morning. = Esta mañana, Tom está de mal humor.\n",
            "Can I get you a coffee? = ¿Puedo traerte un café?\n",
            "Tom doesn't get along with his neighbors anymore. = Tom ya no se lleva a un lado de sus vecinos.\n",
            "Tom is not as old as you. = Tomás no es tan viejo como tú.\n",
            "The boy is always complaining. = El chico siempre se está quejando.\n",
            "Tom said that he wanted to be left alone. = Tom dijo que quería que lo dejaría solo.\n",
            "Please tell Tom I said hi. = Por favor, dile a Tom que yo dijera hola.\n",
            "Tom had to stay home because of the storm. = Tom tuvo que quedarse en casa debido al corto.\n",
            "Do you want a glass of wine? = ¿Quieres una copa de vino?\n",
            "Where can we park? = ¿Dónde podemos aparcar?\n",
            "You have to do it. = Tienes que hacerlo.\n",
            "Who are you? = ¿Quiénes eres?\n",
            "There were at least ten people in the room. = Había al menos diez personas en la habitación.\n",
            "He was given a gold watch. = Se le dieron un reloj de oro.\n",
            "I don't want to drink tea anymore. = Ya no te voy a tomar té.\n",
            "Tom didn't really like Mary much. = A Tom no le gustaba demasiado Mary.\n",
            "How old is your boyfriend? = ¿Cuántos años tiene tu novio?\n",
            "When can\n",
            "\n",
            "[18900 | 3126.97] loss=0.79 avg=0.86\n",
            "[18901 | 3130.06] loss=0.85 avg=0.86\n",
            "[18902 | 3133.14] loss=0.73 avg=0.86\n",
            "[18903 | 3136.19] loss=0.89 avg=0.86\n",
            "[18904 | 3139.25] loss=0.86 avg=0.86\n",
            "[18905 | 3142.32] loss=0.94 avg=0.86\n",
            "[18906 | 3145.39] loss=0.76 avg=0.86\n",
            "[18907 | 3148.45] loss=0.81 avg=0.86\n",
            "[18908 | 3151.51] loss=0.71 avg=0.86\n",
            "[18909 | 3154.58] loss=0.91 avg=0.86\n",
            "[18910 | 3157.64] loss=0.90 avg=0.86\n",
            "[18911 | 3160.71] loss=0.82 avg=0.86\n",
            "[18912 | 3163.79] loss=0.86 avg=0.86\n",
            "[18913 | 3166.85] loss=0.83 avg=0.86\n",
            "[18914 | 3169.92] loss=0.97 avg=0.86\n",
            "[18915 | 3172.99] loss=0.79 avg=0.86\n",
            "[18916 | 3176.04] loss=0.91 avg=0.86\n",
            "[18917 | 3179.11] loss=0.93 avg=0.86\n",
            "[18918 | 3182.18] loss=0.92 avg=0.86\n",
            "[18919 | 3185.25] loss=0.92 avg=0.86\n",
            "[18920 | 3188.30] loss=0.98 avg=0.86\n",
            "[18921 | 3191.37] loss=0.76 avg=0.86\n",
            "[18922 | 3194.44] loss=0.96 avg=0.86\n",
            "[18923 | 3197.52] loss=0.85 avg=0.86\n",
            "[18924 | 3200.59] loss=0.86 avg=0.86\n",
            "[18925 | 3203.66] loss=0.81 avg=0.86\n",
            "[18926 | 3206.75] loss=0.80 avg=0.86\n",
            "[18927 | 3209.82] loss=0.93 avg=0.86\n",
            "[18928 | 3212.89] loss=0.80 avg=0.86\n",
            "[18929 | 3215.96] loss=0.73 avg=0.86\n",
            "[18930 | 3219.05] loss=0.90 avg=0.86\n",
            "[18931 | 3222.12] loss=0.87 avg=0.86\n",
            "[18932 | 3225.18] loss=0.80 avg=0.86\n",
            "[18933 | 3228.26] loss=0.83 avg=0.86\n",
            "[18934 | 3231.32] loss=0.93 avg=0.86\n",
            "[18935 | 3234.39] loss=0.76 avg=0.86\n",
            "[18936 | 3237.47] loss=0.91 avg=0.86\n",
            "[18937 | 3240.54] loss=0.82 avg=0.86\n",
            "[18938 | 3243.60] loss=0.75 avg=0.86\n",
            "[18939 | 3246.67] loss=0.86 avg=0.86\n",
            "[18940 | 3249.75] loss=0.88 avg=0.86\n",
            "[18941 | 3252.84] loss=0.89 avg=0.86\n",
            "[18942 | 3255.90] loss=0.71 avg=0.86\n",
            "[18943 | 3258.96] loss=0.74 avg=0.86\n",
            "[18944 | 3262.02] loss=0.94 avg=0.86\n",
            "[18945 | 3265.09] loss=0.84 avg=0.86\n",
            "[18946 | 3268.12] loss=0.81 avg=0.86\n",
            "[18947 | 3271.17] loss=0.90 avg=0.86\n",
            "[18948 | 3274.23] loss=0.91 avg=0.86\n",
            "[18949 | 3277.28] loss=0.69 avg=0.86\n",
            "[18950 | 3280.32] loss=0.87 avg=0.86\n",
            "[18951 | 3283.39] loss=0.86 avg=0.86\n",
            "[18952 | 3286.46] loss=0.90 avg=0.86\n",
            "[18953 | 3289.52] loss=1.01 avg=0.86\n",
            "[18954 | 3292.57] loss=0.90 avg=0.86\n",
            "[18955 | 3295.64] loss=0.80 avg=0.86\n",
            "[18956 | 3298.71] loss=0.82 avg=0.86\n",
            "[18957 | 3301.77] loss=0.71 avg=0.86\n",
            "[18958 | 3304.83] loss=0.83 avg=0.86\n",
            "[18959 | 3307.91] loss=0.87 avg=0.86\n",
            "[18960 | 3310.98] loss=0.95 avg=0.86\n",
            "[18961 | 3314.06] loss=0.88 avg=0.86\n",
            "[18962 | 3317.12] loss=0.68 avg=0.86\n",
            "[18963 | 3320.19] loss=0.77 avg=0.85\n",
            "[18964 | 3323.27] loss=1.00 avg=0.86\n",
            "[18965 | 3326.33] loss=0.93 avg=0.86\n",
            "[18966 | 3329.39] loss=0.81 avg=0.86\n",
            "[18967 | 3332.45] loss=0.78 avg=0.86\n",
            "[18968 | 3335.51] loss=0.84 avg=0.86\n",
            "[18969 | 3338.57] loss=0.90 avg=0.86\n",
            "[18970 | 3341.63] loss=0.73 avg=0.85\n",
            "[18971 | 3344.70] loss=0.81 avg=0.85\n",
            "[18972 | 3347.77] loss=0.89 avg=0.85\n",
            "[18973 | 3350.84] loss=0.79 avg=0.85\n",
            "[18974 | 3353.90] loss=0.83 avg=0.85\n",
            "[18975 | 3356.97] loss=0.96 avg=0.85\n",
            "[18976 | 3360.04] loss=0.75 avg=0.85\n",
            "[18977 | 3363.12] loss=0.79 avg=0.85\n",
            "[18978 | 3366.19] loss=0.66 avg=0.85\n",
            "[18979 | 3369.26] loss=0.92 avg=0.85\n",
            "[18980 | 3372.33] loss=0.62 avg=0.85\n",
            "[18981 | 3375.39] loss=0.92 avg=0.85\n",
            "[18982 | 3378.44] loss=0.73 avg=0.85\n",
            "[18983 | 3381.52] loss=0.91 avg=0.85\n",
            "[18984 | 3384.59] loss=0.85 avg=0.85\n",
            "[18985 | 3387.68] loss=0.87 avg=0.85\n",
            "[18986 | 3390.75] loss=0.97 avg=0.85\n",
            "[18987 | 3393.83] loss=0.98 avg=0.85\n",
            "[18988 | 3396.90] loss=0.75 avg=0.85\n",
            "[18989 | 3399.96] loss=0.82 avg=0.85\n",
            "[18990 | 3403.03] loss=0.66 avg=0.85\n",
            "[18991 | 3406.09] loss=0.94 avg=0.85\n",
            "[18992 | 3409.16] loss=0.95 avg=0.85\n",
            "[18993 | 3412.25] loss=0.86 avg=0.85\n",
            "[18994 | 3415.32] loss=0.90 avg=0.85\n",
            "[18995 | 3418.39] loss=0.82 avg=0.85\n",
            "[18996 | 3421.46] loss=1.06 avg=0.85\n",
            "[18997 | 3424.52] loss=0.81 avg=0.85\n",
            "[18998 | 3427.60] loss=0.93 avg=0.85\n",
            "[18999 | 3430.68] loss=0.87 avg=0.85\n",
            "Saving /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-19000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "Just tell me the truth! = Solo diga la verdad.\n",
            "What's the biggest city in Europe? = ¿Cuál es la ciudad más grande de Europa?\n",
            "My cousin is one of the three girls that I take to the club every year. = Mi primo es una de las tres niñas que me llevan a la escuela cada año.\n",
            "I'm not a very good musician. = No soy un músico muy bueno.\n",
            "Don't go so fast, will you please stop? = No vaya tan gira, ¿quieres de marcho?\n",
            "The man who was shot was never to see another day in his life. = El hombre que fue disparado fue nunca verno nunca a ver a un día.\n",
            "It's your only chance. = Es tu única oportunidad.\n",
            "I love the smell of coffee. = Me gusta el olor de café.\n",
            "His plan sounds good to me. = Su plan me parece interesante.\n",
            "I'd rather go with Tom. = Preferiría ir con Tom.\n",
            "He's the one who feeds our dog. = Él es el que alimenta a nuestro perro.\n",
            "I'll give you some advice. = Te diré algunos consejos.\n",
            "The only reason Tom gave for not going was that he didn't have enough money. = Lo único razón por la que Tomás le di paisaje por no quedarse suficiente dinero.\n",
            "You don't need to hide your feelings. = No tienes que ocultar tus estribas.\n",
            "I need to do this right now. = Necesito hacer esto ahora mismo.\n",
            "What did Tom do then? = ¿Qué hizo Tom entonces?\n",
            "Tom knows that Mary is in love with John. = Tom sabe que Mary está enamorada de John.\n",
            "I don't give favors to people because I expect them to do my work for me. = No dices fuerzas a la gente porque esperaba que hagas mi trabajo meza a mí.\n",
            "Tom wants to know what you did. = Tom quiere saber qué hiciste.\n",
            "I'd try to understand. = Yo trataría a entender.\n",
            "Tom isn't coming back. = Tom no va a volver.\n",
            "Tom's car got caught in a mudslide and he lost control. = El carro de Tom se ha capturado en una escalola y se perdió con el control.\n",
            "This is all Tom's fault. = Este es culpa de Tom.\n",
            "He was not there. = No estaba allí.\n",
            "I am studying law at the moment. = Yo estoy estudiando la ley en este momento.\n",
            "Are you seriously thinking about selling this on eBay? = ¿Estás pensando seriamente en vender esto en eBay?\n",
            "That's all. = Es todo.\n",
            "It makes little difference. = No tiene ninguna dificultad.\n",
            "I'm looking for my car keys. = Busco las llaves de mi coche.\n",
            "He did the work by himself. = Él hizo por sí solo.\n",
            "I think she's a famous actress. = Creo que ella es una actriz famosa.\n",
            "You won't get there on time, and I won't either. = No llegarás ahí a tiempo, y yo tampoco.\n",
            "I don't want any chocolate ice cream. = No quiero helado de chocolate.\n",
            "She had a strange dream last night. = Ella tuvo un sueño extraño anoche.\n",
            "It's too short. = Es demasiado corto.\n",
            "I don't think it's something that would happen in this world. = No pienso que es algo que suceda en este mundo.\n",
            "All the players bowed in respect. = Todos los jugadores inclinaron bien.\n",
            "I want to go with Tom. = Quiero ir con Tom.\n",
            "Tom told Mary that he would explain the matter to her father. = Tom le dijo a Mary que les explicaría el asunto a su padre.\n",
            "It's difficult to be a native speaker in that town. = Es difícil ser hablar un hablante nativo en esa ciudad.\n",
            "We will never understand. = No lo lamentamos nunca.\n",
            "I'll wait until 2:30. = Esperaré hasta las dos y\n",
            "\n",
            "[19000 | 3487.97] loss=0.92 avg=0.85\n",
            "[19001 | 3491.04] loss=0.99 avg=0.86\n",
            "[19002 | 3494.09] loss=0.82 avg=0.85\n",
            "[19003 | 3497.15] loss=0.86 avg=0.85\n",
            "[19004 | 3500.21] loss=0.94 avg=0.86\n",
            "[19005 | 3503.27] loss=0.96 avg=0.86\n",
            "[19006 | 3506.34] loss=0.93 avg=0.86\n",
            "[19007 | 3509.40] loss=0.85 avg=0.86\n",
            "[19008 | 3512.47] loss=0.85 avg=0.86\n",
            "[19009 | 3515.53] loss=0.82 avg=0.86\n",
            "[19010 | 3518.59] loss=0.87 avg=0.86\n",
            "[19011 | 3521.66] loss=0.86 avg=0.86\n",
            "[19012 | 3524.74] loss=0.85 avg=0.86\n",
            "[19013 | 3527.81] loss=0.89 avg=0.86\n",
            "[19014 | 3530.88] loss=0.84 avg=0.86\n",
            "[19015 | 3533.95] loss=0.93 avg=0.86\n",
            "[19016 | 3537.03] loss=0.83 avg=0.86\n",
            "[19017 | 3540.08] loss=0.96 avg=0.86\n",
            "[19018 | 3543.16] loss=0.75 avg=0.86\n",
            "[19019 | 3546.22] loss=0.87 avg=0.86\n",
            "[19020 | 3549.30] loss=0.86 avg=0.86\n",
            "[19021 | 3552.35] loss=0.69 avg=0.86\n",
            "[19022 | 3555.43] loss=0.90 avg=0.86\n",
            "[19023 | 3558.52] loss=0.83 avg=0.86\n",
            "[19024 | 3561.59] loss=0.88 avg=0.86\n",
            "[19025 | 3564.66] loss=0.72 avg=0.86\n",
            "[19026 | 3567.72] loss=0.81 avg=0.85\n",
            "[19027 | 3570.80] loss=0.94 avg=0.86\n",
            "[19028 | 3573.84] loss=0.88 avg=0.86\n",
            "[19029 | 3576.90] loss=0.89 avg=0.86\n",
            "[19030 | 3579.97] loss=0.86 avg=0.86\n",
            "[19031 | 3583.02] loss=0.93 avg=0.86\n",
            "[19032 | 3586.09] loss=0.80 avg=0.86\n",
            "[19033 | 3589.13] loss=0.89 avg=0.86\n",
            "[19034 | 3592.19] loss=0.82 avg=0.86\n",
            "[19035 | 3595.26] loss=0.83 avg=0.86\n",
            "[19036 | 3598.33] loss=0.92 avg=0.86\n",
            "[19037 | 3601.40] loss=0.97 avg=0.86\n",
            "[19038 | 3604.48] loss=0.79 avg=0.86\n",
            "[19039 | 3607.55] loss=0.73 avg=0.86\n",
            "[19040 | 3610.62] loss=0.95 avg=0.86\n",
            "[19041 | 3613.69] loss=0.88 avg=0.86\n",
            "[19042 | 3616.73] loss=0.80 avg=0.86\n",
            "[19043 | 3619.78] loss=0.82 avg=0.86\n",
            "[19044 | 3622.86] loss=0.92 avg=0.86\n",
            "[19045 | 3625.92] loss=0.89 avg=0.86\n",
            "[19046 | 3628.98] loss=0.91 avg=0.86\n",
            "[19047 | 3632.04] loss=0.96 avg=0.86\n",
            "[19048 | 3635.10] loss=0.88 avg=0.86\n",
            "[19049 | 3638.15] loss=1.01 avg=0.86\n",
            "[19050 | 3641.20] loss=0.88 avg=0.86\n",
            "[19051 | 3644.26] loss=0.91 avg=0.86\n",
            "[19052 | 3647.31] loss=0.85 avg=0.86\n",
            "[19053 | 3650.37] loss=0.84 avg=0.86\n",
            "[19054 | 3653.42] loss=0.84 avg=0.86\n",
            "[19055 | 3656.50] loss=0.73 avg=0.86\n",
            "[19056 | 3659.55] loss=0.86 avg=0.86\n",
            "[19057 | 3662.62] loss=0.88 avg=0.86\n",
            "[19058 | 3665.68] loss=0.76 avg=0.86\n",
            "[19059 | 3668.76] loss=0.88 avg=0.86\n",
            "[19060 | 3671.83] loss=0.89 avg=0.86\n",
            "[19061 | 3674.90] loss=0.91 avg=0.86\n",
            "[19062 | 3677.98] loss=0.93 avg=0.86\n",
            "[19063 | 3681.05] loss=0.90 avg=0.86\n",
            "[19064 | 3684.13] loss=0.99 avg=0.86\n",
            "[19065 | 3687.20] loss=0.94 avg=0.86\n",
            "[19066 | 3690.26] loss=0.86 avg=0.86\n",
            "[19067 | 3693.34] loss=0.90 avg=0.86\n",
            "[19068 | 3696.42] loss=0.94 avg=0.86\n",
            "[19069 | 3699.49] loss=0.70 avg=0.86\n",
            "[19070 | 3702.57] loss=0.94 avg=0.86\n",
            "[19071 | 3705.63] loss=0.87 avg=0.86\n",
            "[19072 | 3708.71] loss=0.80 avg=0.86\n",
            "[19073 | 3711.79] loss=0.99 avg=0.86\n",
            "[19074 | 3714.86] loss=0.81 avg=0.86\n",
            "[19075 | 3717.92] loss=0.85 avg=0.86\n",
            "[19076 | 3721.00] loss=0.77 avg=0.86\n",
            "[19077 | 3724.06] loss=0.91 avg=0.86\n",
            "[19078 | 3727.13] loss=0.89 avg=0.86\n",
            "[19079 | 3730.21] loss=0.84 avg=0.86\n",
            "[19080 | 3733.28] loss=0.81 avg=0.86\n",
            "[19081 | 3736.34] loss=0.80 avg=0.86\n",
            "[19082 | 3739.40] loss=0.90 avg=0.86\n",
            "[19083 | 3742.46] loss=0.67 avg=0.86\n",
            "[19084 | 3745.53] loss=0.80 avg=0.86\n",
            "[19085 | 3748.59] loss=0.76 avg=0.86\n",
            "[19086 | 3751.66] loss=0.90 avg=0.86\n",
            "[19087 | 3754.73] loss=0.91 avg=0.86\n",
            "[19088 | 3757.79] loss=0.81 avg=0.86\n",
            "[19089 | 3760.86] loss=0.90 avg=0.86\n",
            "[19090 | 3763.94] loss=0.90 avg=0.86\n",
            "[19091 | 3767.01] loss=1.00 avg=0.86\n",
            "[19092 | 3770.08] loss=0.84 avg=0.86\n",
            "[19093 | 3773.14] loss=0.80 avg=0.86\n",
            "[19094 | 3776.21] loss=1.05 avg=0.86\n",
            "[19095 | 3779.28] loss=0.87 avg=0.86\n",
            "[19096 | 3782.37] loss=0.84 avg=0.86\n",
            "[19097 | 3785.44] loss=0.95 avg=0.86\n",
            "[19098 | 3788.52] loss=0.92 avg=0.86\n",
            "[19099 | 3791.59] loss=0.90 avg=0.86\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " obja.\n",
            "This is too much. = Esto es demasiado.\n",
            "Tom hasn't paid any attention. = Tom no ha pagado ninguna atención.\n",
            "Tom wants to visit England. = Tom quiere visitar Inglaterra.\n",
            "Tom is a talented musician. = Tom es un músico talentoso.\n",
            "He did it to spite me. = Lo hizo para molestarme.\n",
            "What time does the train for New York depart? = ¿A qué hora parte el tren a Nueva York?\n",
            "Tom and Mary are very happy together. = Tom y Mary están muy felices juntos.\n",
            "Tom should've accepted the offer. = Tom debería haber habido la oferta.\n",
            "You have only to call them. = Solo tienes que llamarla.\n",
            "What time do you go to bed? = ¿A qué hora os van a dormir?\n",
            "She loves him. = Ella lo ama.\n",
            "They are going to Paris next Sunday. = Ellos van a París el domingo que viene.\n",
            "This is not a joke. = Esto no es broma.\n",
            "I have to take the entrance examination today. = Debo tomar el examen de ingreso hoy.\n",
            "We didn't know where to go. = No sabíamos adentro.\n",
            "The situation escalated quickly. = La situación acusa veloz.\n",
            "I was very scared. = Estaba muy asustada.\n",
            "The dog was run over by a truck. = Un carro atropelló al perro.\n",
            "Why are you so tired today? = ¿Por qué estás tan cansada hoy?\n",
            "I know you're upset. = Sé que está molesto.\n",
            "Do you want to eat? = ¿Quieres comer?\n",
            "Tom put the package on the table. = Tom puso el paquete en la mesa.\n",
            "I had to get a band-aid on one of the cuts. = Tuve que traer una bala en uno de las círculas.\n",
            "This medicine has cured three times of rheumatism. = Este medicamento cura tres veces de agenería.\n",
            "Somebody's in the bathroom. = Sabía que alguien está en el baño.\n",
            "You're smarter than that. = Eres más listo que eso.\n",
            "I'm tired of all this noise. = Estoy cansando de todo este ruido.\n",
            "It will rain soon. = Pronto lloverá.\n",
            "That's the problem. = Eso es el problema.\n",
            "It's a real possibility. = Es un auténtico real.\n",
            "She used to work as a maid at a castle. = Ella solía trabajar como asesina en un castle.\n",
            "Your watch is less than one meter. = Tu reloj de poco es menos de una mesa.\n",
            "I was just here last Tuesday. = Estuve aquí apenado el martes pasado.\n",
            "Tom's mother is a very good cook. = La madre de Tom es una muy buena cocinera.\n",
            "Do not go into the jungle alone. = No vayas a la jungla solos.\n",
            "I can't believe Tom is still in love with Mary. = No me puedo creer que Tom siga amar a Mary.\n",
            "Can you speak a little louder please? = Hable ya un poco más alto, por favor.\n",
            "My name's Tom. = Soy Tom.\n",
            "I thought the old man told me when he was going to die. = Pensé que el anciano me contó cuando iba a morir.\n",
            "You need more practice. = Tienes que más práctica.\n",
            "They have nothing against terrorism. = No tienen nada en contra de el terrorisma.\n",
            "I'm not a Canadian. = No soy canadiense.\n",
            "Where's the best place to eat Chinese food? = ¿Dónde está el mejor lugar para comer comida china?\n",
            "I wish you a pleasant journey. = Te deseo un bien viaje.\n",
            "You don't seem particularly pleased. = Tú no pareces especialmente complacido.\n",
            "We were disappointed in that. = Nos decepcionamos en eso.\n",
            "I can't say anything about that. = No puedo decir nada acerca de eso.\n",
            "I'm not in a hurry. = No tengo prisa.\n",
            "I am ashamed of you. = Me\n",
            "\n",
            "[19100 | 3831.66] loss=0.98 avg=0.87\n",
            "[19101 | 3834.73] loss=0.91 avg=0.87\n",
            "[19102 | 3837.78] loss=0.82 avg=0.87\n",
            "[19103 | 3840.84] loss=0.86 avg=0.87\n",
            "[19104 | 3843.90] loss=0.92 avg=0.87\n",
            "[19105 | 3846.95] loss=0.70 avg=0.86\n",
            "[19106 | 3850.01] loss=0.91 avg=0.86\n",
            "[19107 | 3853.06] loss=0.88 avg=0.86\n",
            "[19108 | 3856.11] loss=0.81 avg=0.86\n",
            "[19109 | 3859.15] loss=0.78 avg=0.86\n",
            "[19110 | 3862.21] loss=0.97 avg=0.86\n",
            "[19111 | 3865.26] loss=0.89 avg=0.86\n",
            "[19112 | 3868.32] loss=0.91 avg=0.87\n",
            "[19113 | 3871.38] loss=0.65 avg=0.86\n",
            "[19114 | 3874.44] loss=0.82 avg=0.86\n",
            "[19115 | 3877.49] loss=0.72 avg=0.86\n",
            "[19116 | 3880.54] loss=0.93 avg=0.86\n",
            "[19117 | 3883.61] loss=0.82 avg=0.86\n",
            "[19118 | 3886.67] loss=0.75 avg=0.86\n",
            "[19119 | 3889.74] loss=1.01 avg=0.86\n",
            "[19120 | 3892.80] loss=0.74 avg=0.86\n",
            "[19121 | 3895.86] loss=0.77 avg=0.86\n",
            "[19122 | 3898.90] loss=0.88 avg=0.86\n",
            "[19123 | 3901.95] loss=0.97 avg=0.86\n",
            "[19124 | 3905.00] loss=0.80 avg=0.86\n",
            "[19125 | 3908.07] loss=0.85 avg=0.86\n",
            "[19126 | 3911.14] loss=0.80 avg=0.86\n",
            "[19127 | 3914.21] loss=0.91 avg=0.86\n",
            "[19128 | 3917.28] loss=0.83 avg=0.86\n",
            "[19129 | 3920.33] loss=0.71 avg=0.86\n",
            "[19130 | 3923.40] loss=0.84 avg=0.86\n",
            "[19131 | 3926.47] loss=0.73 avg=0.86\n",
            "[19132 | 3929.52] loss=0.76 avg=0.86\n",
            "[19133 | 3932.58] loss=0.94 avg=0.86\n",
            "[19134 | 3935.64] loss=0.79 avg=0.86\n",
            "[19135 | 3938.70] loss=0.85 avg=0.86\n",
            "[19136 | 3941.77] loss=0.81 avg=0.86\n",
            "[19137 | 3944.83] loss=0.93 avg=0.86\n",
            "[19138 | 3947.90] loss=0.80 avg=0.86\n",
            "[19139 | 3950.96] loss=0.89 avg=0.86\n",
            "[19140 | 3954.02] loss=0.92 avg=0.86\n",
            "[19141 | 3957.08] loss=0.89 avg=0.86\n",
            "[19142 | 3960.14] loss=0.87 avg=0.86\n",
            "[19143 | 3963.21] loss=0.90 avg=0.86\n",
            "[19144 | 3966.28] loss=0.88 avg=0.86\n",
            "[19145 | 3969.37] loss=0.77 avg=0.86\n",
            "[19146 | 3972.47] loss=0.91 avg=0.86\n",
            "[19147 | 3975.56] loss=0.90 avg=0.86\n",
            "[19148 | 3978.64] loss=0.94 avg=0.86\n",
            "[19149 | 3981.73] loss=0.79 avg=0.86\n",
            "[19150 | 3984.84] loss=0.90 avg=0.86\n",
            "[19151 | 3987.93] loss=0.85 avg=0.86\n",
            "[19152 | 3991.03] loss=0.86 avg=0.86\n",
            "[19153 | 3994.13] loss=0.81 avg=0.86\n",
            "[19154 | 3997.22] loss=0.85 avg=0.86\n",
            "[19155 | 4000.30] loss=0.72 avg=0.86\n",
            "[19156 | 4003.39] loss=1.05 avg=0.86\n",
            "[19157 | 4006.48] loss=0.85 avg=0.86\n",
            "[19158 | 4009.59] loss=0.84 avg=0.86\n",
            "[19159 | 4012.68] loss=0.89 avg=0.86\n",
            "[19160 | 4015.76] loss=0.79 avg=0.86\n",
            "[19161 | 4018.85] loss=0.89 avg=0.86\n",
            "[19162 | 4021.94] loss=0.90 avg=0.86\n",
            "[19163 | 4024.99] loss=0.68 avg=0.86\n",
            "[19164 | 4028.08] loss=0.85 avg=0.86\n",
            "[19165 | 4031.17] loss=0.90 avg=0.86\n",
            "[19166 | 4034.27] loss=0.85 avg=0.86\n",
            "[19167 | 4037.37] loss=0.73 avg=0.86\n",
            "[19168 | 4040.46] loss=0.90 avg=0.86\n",
            "[19169 | 4043.57] loss=0.74 avg=0.86\n",
            "[19170 | 4046.68] loss=0.89 avg=0.86\n",
            "[19171 | 4049.77] loss=0.87 avg=0.86\n",
            "[19172 | 4052.86] loss=0.82 avg=0.86\n",
            "[19173 | 4055.96] loss=0.94 avg=0.86\n",
            "[19174 | 4059.06] loss=0.84 avg=0.86\n",
            "[19175 | 4062.16] loss=0.93 avg=0.86\n",
            "[19176 | 4065.26] loss=0.90 avg=0.86\n",
            "[19177 | 4068.34] loss=0.64 avg=0.85\n",
            "[19178 | 4071.42] loss=0.81 avg=0.85\n",
            "[19179 | 4074.52] loss=0.75 avg=0.85\n",
            "[19180 | 4077.63] loss=0.75 avg=0.85\n",
            "[19181 | 4080.73] loss=0.88 avg=0.85\n",
            "[19182 | 4083.83] loss=0.76 avg=0.85\n",
            "[19183 | 4086.93] loss=0.91 avg=0.85\n",
            "[19184 | 4090.02] loss=0.81 avg=0.85\n",
            "[19185 | 4093.10] loss=0.82 avg=0.85\n",
            "[19186 | 4096.18] loss=0.83 avg=0.85\n",
            "[19187 | 4099.27] loss=0.74 avg=0.85\n",
            "[19188 | 4102.36] loss=0.75 avg=0.85\n",
            "[19189 | 4105.45] loss=0.76 avg=0.85\n",
            "[19190 | 4108.55] loss=0.73 avg=0.85\n",
            "[19191 | 4111.64] loss=0.76 avg=0.85\n",
            "[19192 | 4114.73] loss=0.89 avg=0.85\n",
            "[19193 | 4117.83] loss=0.68 avg=0.85\n",
            "[19194 | 4120.92] loss=0.94 avg=0.85\n",
            "[19195 | 4124.02] loss=0.89 avg=0.85\n",
            "[19196 | 4127.11] loss=0.82 avg=0.85\n",
            "[19197 | 4130.20] loss=0.80 avg=0.85\n",
            "[19198 | 4133.29] loss=0.89 avg=0.85\n",
            "[19199 | 4136.38] loss=0.93 avg=0.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " de año.\n",
            "The weatherman was silent during the bad weather. = El hombre hizo callado durante el mal clima.\n",
            "I've finished reading the book. = Terminé de leer el libro.\n",
            "The plane made an emergency landing. = El avión hizo un aterrizaje a emergencias.\n",
            "Do you think he likes being rich? = ¿Usted cree que le gusta ser rica?\n",
            "He's taller than me. = Él es más alto que yo.\n",
            "You look beautiful tonight. = Esta noche te ves bien.\n",
            "Tom wants to go with you. = Tom quiere ir contigo.\n",
            "Tom and Mary were talking to each other when the phone rang. = Tom y Mary estaban conversando al conferencia cuando sonó el teléfono.\n",
            "Tom told Mary what John did. = Tom le dijo a Mary lo que John hizo John.\n",
            "We need to take care of ourselves. = Tenemos que cuidarnos.\n",
            "Let's have another beer. = Tomemos otra cerveza.\n",
            "I thought we knew each other. = Pensaba que nos conocíamos.\n",
            "The sky was as black as pitch. = El cielo estaba tan blanco como el pulpo.\n",
            "I will get Tom to listen to me. = Me voy a hacer que Tom me escuche.\n",
            "I got up a few minutes ago. = Me levanté hace unos minutos.\n",
            "Tom and Mary have different philosophies of life. = Tom y Mary tienen diferentes filosofías de vida.\n",
            "They're my friends. = Es mis amigos.\n",
            "I didn't have many friends. = No tenía muchos amigos.\n",
            "It will not be long before we can go on a vacation in the United States. = No tardará mucho para que podemos ir de vacaciones de Estados Unidos.\n",
            "What happened? = ¿Qué ha pasado?\n",
            "I'm a little crazy. = Estoy un poco loca.\n",
            "Do you want to see me again? = ¿Quieres verme de nuevo?\n",
            "I'm happy to have this homework. = Estoy contento de tener este tarea.\n",
            "This is the reason he came here. = Esa es la razón por la que vine aquí.\n",
            "He was not here when you called. = Él no estaba aquí cuando llamaste.\n",
            "He was too shy to talk to girls. = Él era demasiado tímido para hablarle a las niñas.\n",
            "Tom should've called. = Tom debería haber llamado.\n",
            "I know you. = Estos yo.\n",
            "You're the person I've been looking for. = Sos la persona que estaba buscando.\n",
            "The dog is always barking. = El perro siempre ladra.\n",
            "He had a terrible hangover. = Tuvo una resaca terrible.\n",
            "I bought this hat yesterday. = Compré esta coroba ayer.\n",
            "Let's put these flowers on the desk. = Paras necesitándoles en el escritorio.\n",
            "I think I've lost them. = Creo que me los dejé.\n",
            "This is how I wash dishes. = Así es como me lavo la cámara.\n",
            "I wish you wouldn't speak ill of others. = Desearía que no hables de otros.\n",
            "My husband is in his forties. = Mi marido tiene tres deits.\n",
            "A thousand yen will cover the cost of this trip. = Mil yenes socués a través de esta viaje.\n",
            "It's almost three. = Son casi tres.\n",
            "She's good at gardening. = Ella es buena agricole.\n",
            "I can't find him anywhere. = No puedo encontrarlos en ningún sitio.\n",
            "We need your key. = Necesitamos su tarjeta.\n",
            "We had a short holiday. = Pasamos unos cortos cortos.\n",
            "My father is a man of few words. = Mi padre es un hombre de pocas palabras.\n",
            "He had to pay for the books. = Él tuvo que pagar los libros.\n",
            "We have no reason for his arrest. = No hay motivo para su arresto.\n",
            "The teacher told the boy that he should not open the book. = El profesor le dijo a al niño que no debería abrir el libro.\n",
            "\n",
            "\n",
            "[19200 | 4177.00] loss=0.85 avg=0.85\n",
            "[19201 | 4180.10] loss=0.86 avg=0.85\n",
            "[19202 | 4183.20] loss=0.74 avg=0.85\n",
            "[19203 | 4186.29] loss=0.91 avg=0.85\n",
            "[19204 | 4189.39] loss=0.89 avg=0.85\n",
            "[19205 | 4192.49] loss=0.69 avg=0.85\n",
            "[19206 | 4195.57] loss=0.87 avg=0.85\n",
            "[19207 | 4198.67] loss=0.77 avg=0.84\n",
            "[19208 | 4201.78] loss=0.86 avg=0.85\n",
            "[19209 | 4204.84] loss=1.02 avg=0.85\n",
            "[19210 | 4207.91] loss=0.87 avg=0.85\n",
            "[19211 | 4210.98] loss=0.90 avg=0.85\n",
            "[19212 | 4214.05] loss=0.80 avg=0.85\n",
            "[19213 | 4217.09] loss=0.88 avg=0.85\n",
            "[19214 | 4220.18] loss=0.72 avg=0.85\n",
            "[19215 | 4223.28] loss=0.93 avg=0.85\n",
            "[19216 | 4226.38] loss=1.06 avg=0.85\n",
            "[19217 | 4229.48] loss=0.97 avg=0.85\n",
            "[19218 | 4232.58] loss=0.91 avg=0.85\n",
            "[19219 | 4235.67] loss=0.97 avg=0.85\n",
            "[19220 | 4238.74] loss=0.82 avg=0.85\n",
            "[19221 | 4241.84] loss=0.88 avg=0.85\n",
            "[19222 | 4244.93] loss=0.86 avg=0.85\n",
            "[19223 | 4248.03] loss=0.87 avg=0.85\n",
            "[19224 | 4251.11] loss=0.72 avg=0.85\n",
            "[19225 | 4254.21] loss=0.74 avg=0.85\n",
            "[19226 | 4257.29] loss=0.95 avg=0.85\n",
            "[19227 | 4260.39] loss=0.78 avg=0.85\n",
            "[19228 | 4263.47] loss=0.92 avg=0.85\n",
            "[19229 | 4266.57] loss=0.80 avg=0.85\n",
            "[19230 | 4269.66] loss=0.88 avg=0.85\n",
            "[19231 | 4272.76] loss=0.84 avg=0.85\n",
            "[19232 | 4275.84] loss=0.85 avg=0.85\n",
            "[19233 | 4278.93] loss=0.93 avg=0.85\n",
            "[19234 | 4282.02] loss=0.86 avg=0.85\n",
            "[19235 | 4285.12] loss=0.82 avg=0.85\n",
            "[19236 | 4288.21] loss=0.72 avg=0.85\n",
            "[19237 | 4291.30] loss=0.86 avg=0.85\n",
            "[19238 | 4294.40] loss=0.90 avg=0.85\n",
            "[19239 | 4297.48] loss=0.92 avg=0.85\n",
            "[19240 | 4300.58] loss=0.71 avg=0.85\n",
            "[19241 | 4303.69] loss=0.81 avg=0.85\n",
            "[19242 | 4306.79] loss=0.61 avg=0.85\n",
            "[19243 | 4309.88] loss=0.71 avg=0.85\n",
            "[19244 | 4312.98] loss=0.80 avg=0.84\n",
            "[19245 | 4316.07] loss=1.07 avg=0.85\n",
            "[19246 | 4319.15] loss=0.86 avg=0.85\n",
            "[19247 | 4322.25] loss=0.91 avg=0.85\n",
            "[19248 | 4325.34] loss=1.06 avg=0.85\n",
            "[19249 | 4328.44] loss=0.97 avg=0.85\n",
            "[19250 | 4331.54] loss=0.77 avg=0.85\n",
            "[19251 | 4334.62] loss=0.86 avg=0.85\n",
            "[19252 | 4337.68] loss=0.83 avg=0.85\n",
            "[19253 | 4340.75] loss=0.93 avg=0.85\n",
            "[19254 | 4343.82] loss=0.78 avg=0.85\n",
            "[19255 | 4346.89] loss=0.78 avg=0.85\n",
            "[19256 | 4349.95] loss=0.71 avg=0.85\n",
            "[19257 | 4353.03] loss=0.92 avg=0.85\n",
            "[19258 | 4356.11] loss=0.87 avg=0.85\n",
            "[19259 | 4359.16] loss=0.83 avg=0.85\n",
            "[19260 | 4362.22] loss=0.81 avg=0.85\n",
            "[19261 | 4365.28] loss=0.71 avg=0.85\n",
            "[19262 | 4368.35] loss=0.74 avg=0.85\n",
            "[19263 | 4371.42] loss=0.83 avg=0.85\n",
            "[19264 | 4374.50] loss=0.85 avg=0.85\n",
            "[19265 | 4377.56] loss=0.87 avg=0.85\n",
            "[19266 | 4380.63] loss=0.85 avg=0.85\n",
            "[19267 | 4383.69] loss=0.88 avg=0.85\n",
            "[19268 | 4386.77] loss=0.92 avg=0.85\n",
            "[19269 | 4389.86] loss=0.97 avg=0.85\n",
            "[19270 | 4392.93] loss=0.83 avg=0.85\n",
            "[19271 | 4395.99] loss=0.93 avg=0.85\n",
            "[19272 | 4399.07] loss=0.81 avg=0.85\n",
            "[19273 | 4402.12] loss=0.82 avg=0.85\n",
            "[19274 | 4405.19] loss=0.87 avg=0.85\n",
            "[19275 | 4408.27] loss=0.74 avg=0.85\n",
            "[19276 | 4411.36] loss=0.81 avg=0.85\n",
            "[19277 | 4414.43] loss=0.67 avg=0.85\n",
            "[19278 | 4417.50] loss=0.90 avg=0.85\n",
            "[19279 | 4420.58] loss=0.89 avg=0.85\n",
            "[19280 | 4423.65] loss=0.89 avg=0.85\n",
            "[19281 | 4426.73] loss=0.91 avg=0.85\n",
            "[19282 | 4429.80] loss=0.86 avg=0.85\n",
            "[19283 | 4432.87] loss=0.89 avg=0.85\n",
            "[19284 | 4435.93] loss=0.87 avg=0.85\n",
            "[19285 | 4439.00] loss=0.88 avg=0.85\n",
            "[19286 | 4442.07] loss=0.94 avg=0.85\n",
            "[19287 | 4445.14] loss=0.85 avg=0.85\n",
            "[19288 | 4448.21] loss=0.95 avg=0.85\n",
            "[19289 | 4451.28] loss=0.89 avg=0.85\n",
            "[19290 | 4454.35] loss=0.86 avg=0.85\n",
            "[19291 | 4457.42] loss=0.75 avg=0.85\n",
            "[19292 | 4460.48] loss=0.94 avg=0.85\n",
            "[19293 | 4463.55] loss=0.98 avg=0.85\n",
            "[19294 | 4466.61] loss=0.87 avg=0.85\n",
            "[19295 | 4469.67] loss=0.85 avg=0.85\n",
            "[19296 | 4472.74] loss=0.75 avg=0.85\n",
            "[19297 | 4475.82] loss=0.90 avg=0.85\n",
            "[19298 | 4478.90] loss=0.78 avg=0.85\n",
            "[19299 | 4481.95] loss=0.90 avg=0.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "I saw him yesterday for the very first time. = Lo vi ayer por una vez por veces.\n",
            "What are these for? = ¿Para qué son éstos?\n",
            "She turned off the television. = Ella apagó la televisión.\n",
            "It's on the table. = Está en la mesa.\n",
            "Tom was the best player on the team. = Tom era el mejor playero del equipo.\n",
            "You'll understand soon. = Entenderás pronto.\n",
            "I'm happy you could come. = Estoy contento de que entres.\n",
            "Why are you so worried? = ¿Por qué estás tan preocupada?\n",
            "You'd better go. = Mejor te vayas.\n",
            "I'm not in a rush. = No estoy de prisa.\n",
            "She is as big as I am. = Ella es tan hubo como yo.\n",
            "He is not that kind of guy. = Él no es esa clase de tipo.\n",
            "A stranger approached them. = Les se acercó un extraño.\n",
            "You're not the only one who's worried. = No sos el único que está preocupado.\n",
            "I really wish I could figure out how to get more people to visit my website. = Realmente desearía saber adelante que pueda averiguar cómo hacer más personas visitar mi sitio web.\n",
            "Who killed Tom? = ¿Quién mató a Tom?\n",
            "Don't go in that building. There's a fire. = No entres en ese edificio, hay un incendio.\n",
            "I don't have anything to give Tom. = No tengo nada que darle a Tom.\n",
            "I don't know your real purpose in life. = No sé su verdadero propósito en la vida.\n",
            "When he heard the whistle, he knew instantly that somebody had been shot. = Cuando oyó el guto, él sabió de un instante que alguien hubiera sufrido.\n",
            "Tom can sing like a baritone. = Tom puede cantar como un barco.\n",
            "I need a new one. = Necesito una nueva.\n",
            "He is the last man I expected to meet. = Es la última persona que esperaba encontrártelo.\n",
            "Do that while wearing your socks. = Haz eso mientras llevas los calcetines.\n",
            "Tom doesn't want anyone to know he was here. = Tom no quiere que nadie supiera que yo estaba aquí.\n",
            "Please take care of this dog. = Por favor, cuida de este perro.\n",
            "Tom isn't much older than Mary, but they're both really shy. = Tom no es mucho mayor que Mary, pero son dos muy tímidos.\n",
            "I have to find out where Tom lives. = Tengo que averiguar dónde vive Tom.\n",
            "I'll meet you upstairs. = Te veré arriba.\n",
            "I've been to Boston for four days. = Nehe como a Boston por cuatro días.\n",
            "The doctor told Tom that he had a cold. = El médico le dijo a Tom que tenía un resfriado.\n",
            "I really like Italian food. = Me gusta mucho la comida italiana.\n",
            "You're not allowed to use mine. = Usted no consume el cabo.\n",
            "I like eating Chinese food. = Me gusta comer comida china.\n",
            "Don't you like him? = ¿No te gusta?\n",
            "Can we talk? = ¿Podemos hablar?\n",
            "You can't put a price on that. = No le puedes poner un precio a eso.\n",
            "You were stupid. = Eras estúpido.\n",
            "I'm sure she'll succeed. = Estoy seguro de que ella triunfará.\n",
            "He looked his age. = Él menó de su edad.\n",
            "Where did you buy that dress? = ¿Dónde compraste ese vestido?\n",
            "Tom went to see Mary. = Tom fue a ver a Mary.\n",
            "We are going back home. = Vamos a volver a casa.\n",
            "Don't trust him. = No se fío en él.\n",
            "Tom didn't feel like telling Mary the bad news. = Tom no tenía ganas de denunciarle la mala noticia.\n",
            "My grandmother always sings in the car. = Mi abuela siempre canta\n",
            "\n",
            "[19300 | 4522.73] loss=0.79 avg=0.85\n",
            "[19301 | 4525.79] loss=0.79 avg=0.85\n",
            "[19302 | 4528.85] loss=0.95 avg=0.85\n",
            "[19303 | 4531.90] loss=0.94 avg=0.85\n",
            "[19304 | 4534.95] loss=0.81 avg=0.85\n",
            "[19305 | 4537.99] loss=0.91 avg=0.85\n",
            "[19306 | 4541.05] loss=0.98 avg=0.85\n",
            "[19307 | 4544.11] loss=0.72 avg=0.85\n",
            "[19308 | 4547.18] loss=0.87 avg=0.85\n",
            "[19309 | 4550.24] loss=0.81 avg=0.85\n",
            "[19310 | 4553.29] loss=0.77 avg=0.85\n",
            "[19311 | 4556.36] loss=0.76 avg=0.85\n",
            "[19312 | 4559.43] loss=0.90 avg=0.85\n",
            "[19313 | 4562.50] loss=0.72 avg=0.85\n",
            "[19314 | 4565.57] loss=0.85 avg=0.85\n",
            "[19315 | 4568.65] loss=0.88 avg=0.85\n",
            "[19316 | 4571.72] loss=0.97 avg=0.85\n",
            "[19317 | 4574.79] loss=0.94 avg=0.85\n",
            "[19318 | 4577.85] loss=0.79 avg=0.85\n",
            "[19319 | 4580.92] loss=0.89 avg=0.85\n",
            "[19320 | 4583.98] loss=0.62 avg=0.85\n",
            "[19321 | 4587.04] loss=0.95 avg=0.85\n",
            "[19322 | 4590.11] loss=0.92 avg=0.85\n",
            "[19323 | 4593.17] loss=0.69 avg=0.85\n",
            "[19324 | 4596.25] loss=1.00 avg=0.85\n",
            "[19325 | 4599.33] loss=0.94 avg=0.85\n",
            "[19326 | 4602.39] loss=0.91 avg=0.85\n",
            "[19327 | 4605.46] loss=0.79 avg=0.85\n",
            "[19328 | 4608.54] loss=0.77 avg=0.85\n",
            "[19329 | 4611.60] loss=0.73 avg=0.85\n",
            "[19330 | 4614.67] loss=0.69 avg=0.85\n",
            "[19331 | 4617.75] loss=0.82 avg=0.85\n",
            "[19332 | 4620.81] loss=0.90 avg=0.85\n",
            "[19333 | 4623.89] loss=0.77 avg=0.85\n",
            "[19334 | 4626.96] loss=0.76 avg=0.85\n",
            "[19335 | 4630.03] loss=0.88 avg=0.85\n",
            "[19336 | 4633.11] loss=0.88 avg=0.85\n",
            "[19337 | 4636.18] loss=0.80 avg=0.85\n",
            "[19338 | 4639.28] loss=0.77 avg=0.85\n",
            "[19339 | 4642.37] loss=0.75 avg=0.85\n",
            "[19340 | 4645.46] loss=0.82 avg=0.85\n",
            "[19341 | 4648.54] loss=0.67 avg=0.84\n",
            "[19342 | 4651.63] loss=0.83 avg=0.84\n",
            "[19343 | 4654.74] loss=0.91 avg=0.84\n",
            "[19344 | 4657.83] loss=0.85 avg=0.84\n",
            "[19345 | 4660.91] loss=0.80 avg=0.84\n",
            "[19346 | 4664.00] loss=0.91 avg=0.84\n",
            "[19347 | 4667.09] loss=0.89 avg=0.84\n",
            "[19348 | 4670.18] loss=0.86 avg=0.84\n",
            "[19349 | 4673.27] loss=0.79 avg=0.84\n",
            "[19350 | 4676.34] loss=0.87 avg=0.84\n",
            "[19351 | 4679.43] loss=0.94 avg=0.85\n",
            "[19352 | 4682.51] loss=0.80 avg=0.85\n",
            "[19353 | 4685.60] loss=0.86 avg=0.85\n",
            "[19354 | 4688.69] loss=0.74 avg=0.84\n",
            "[19355 | 4691.78] loss=0.89 avg=0.84\n",
            "[19356 | 4694.88] loss=0.81 avg=0.84\n",
            "[19357 | 4697.95] loss=0.97 avg=0.85\n",
            "[19358 | 4701.04] loss=0.91 avg=0.85\n",
            "[19359 | 4704.13] loss=0.84 avg=0.85\n",
            "[19360 | 4707.20] loss=1.06 avg=0.85\n",
            "[19361 | 4710.27] loss=1.07 avg=0.85\n",
            "[19362 | 4713.34] loss=0.76 avg=0.85\n",
            "[19363 | 4716.43] loss=0.97 avg=0.85\n",
            "[19364 | 4719.52] loss=0.77 avg=0.85\n",
            "[19365 | 4722.60] loss=0.92 avg=0.85\n",
            "[19366 | 4725.68] loss=0.89 avg=0.85\n",
            "[19367 | 4728.75] loss=0.74 avg=0.85\n",
            "[19368 | 4731.82] loss=0.86 avg=0.85\n",
            "[19369 | 4734.91] loss=0.97 avg=0.85\n",
            "[19370 | 4737.99] loss=0.93 avg=0.85\n",
            "[19371 | 4741.07] loss=0.90 avg=0.85\n",
            "[19372 | 4744.15] loss=1.05 avg=0.85\n",
            "[19373 | 4747.23] loss=0.86 avg=0.85\n",
            "[19374 | 4750.32] loss=0.94 avg=0.86\n",
            "[19375 | 4753.40] loss=0.78 avg=0.85\n",
            "[19376 | 4756.49] loss=0.83 avg=0.85\n",
            "[19377 | 4759.57] loss=0.71 avg=0.85\n",
            "[19378 | 4762.66] loss=0.97 avg=0.85\n",
            "[19379 | 4765.74] loss=0.88 avg=0.85\n",
            "[19380 | 4768.81] loss=0.71 avg=0.85\n",
            "[19381 | 4771.90] loss=0.76 avg=0.85\n",
            "[19382 | 4774.99] loss=0.92 avg=0.85\n",
            "[19383 | 4778.08] loss=0.85 avg=0.85\n",
            "[19384 | 4781.17] loss=0.92 avg=0.85\n",
            "[19385 | 4784.24] loss=0.90 avg=0.85\n",
            "[19386 | 4787.32] loss=0.77 avg=0.85\n",
            "[19387 | 4790.41] loss=0.75 avg=0.85\n",
            "[19388 | 4793.48] loss=0.92 avg=0.85\n",
            "[19389 | 4796.55] loss=0.93 avg=0.85\n",
            "[19390 | 4799.64] loss=0.82 avg=0.85\n",
            "[19391 | 4802.72] loss=0.86 avg=0.85\n",
            "[19392 | 4805.80] loss=0.83 avg=0.85\n",
            "[19393 | 4808.88] loss=0.91 avg=0.85\n",
            "[19394 | 4811.97] loss=0.93 avg=0.85\n",
            "[19395 | 4815.04] loss=0.82 avg=0.85\n",
            "[19396 | 4818.08] loss=0.86 avg=0.85\n",
            "[19397 | 4821.16] loss=0.83 avg=0.85\n",
            "[19398 | 4824.24] loss=0.69 avg=0.85\n",
            "[19399 | 4827.29] loss=0.61 avg=0.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "es.\n",
            "My name is Tom. = Mi nombre es Tom.\n",
            "You have no friends to talk with. = No tienes amigos con los que charlar.\n",
            "You should see this film if you get the opportunity. = Deberías ver una película si tuviese la ocasión.\n",
            "If I had known her history would have changed. = Si hubiera sabido sabía la historia.\n",
            "It's raining again. = Vuelve llover.\n",
            "I do like it. = Me gusta.\n",
            "I am now as tall as she is. = Ahora soy tan alto como ella.\n",
            "Do you think Tom was in the meeting? = ¿Pensás que Tom estaba en la reunión?\n",
            "Tom never gave up. = Tom nunca se rindió.\n",
            "I feel fine. = Yo me siento bien.\n",
            "The door won't budge. = La puerta no se moverá.\n",
            "I'll take care of your dog while you're away. = Yo cuidaré de tu perro cuando estés fuera.\n",
            "Tom had nothing to eat. = Tom no tenía nada de comer.\n",
            "Tom doesn't have the energy to keep trying. = Tom no tiene energy para seguir intentando.\n",
            "Tom and Mary met on their school days. = Tom y Mary se conocieron en sus días de escuela.\n",
            "Come right in. = Entre.\n",
            "It doesn't really matter, does it? = En realidad no le importa, ¿o vo?\n",
            "I'll make you happy. = Te haré feliz.\n",
            "I need to know something. = Necesito saber algo.\n",
            "Do you want to talk about it? = ¿Querés hablar al respecto?\n",
            "I want you to help us. = Quiero que lo ayucan.\n",
            "I've been waiting a long time for this. = Estaba esperando mucho tiempo por esto.\n",
            "When it rained, my grandfather used to bathe in the bathroom. = Cuando duó la niebla, mi abuelo solía bañar en el baño.\n",
            "Tom should start right away. = Tom debería empezar inmediatamente.\n",
            "I'm not good at math. = No soy buena en matemáticas.\n",
            "It doesn't surprise me. = Yo no me sorprende.\n",
            "He's a man you can rely on. = Él es un hombre en el que puedes confiar.\n",
            "It's already Friday. = Ya es viernes.\n",
            "When we came across his house, we shook hands with him. = Cuando se nos conocía su casa échamos la mano, dieron la mano con él.\n",
            "All things taken into consideration, my father is the best piano teacher I've ever had. = Yo todos lo toques invitaron a considerar a mi padre, a su madre es la mejor mojado que he había tenido nunca.\n",
            "Tom has a beautiful wife. = Tom tiene una hermosa cheza.\n",
            "Tom could tell Mary was nervous because she could hear birds flying in the distance. = Tom podría decir que María estaba nerviosa porque podía oír a los pájaros cerca del lejan.\n",
            "If I'd had time to do that, I would've. = Si hubiera teniado tiempo para hacerlo, lo habría hecho.\n",
            "I'll give it to you. = Os lo daré.\n",
            "She didn't go to the office. = Ella no fue a la oficina.\n",
            "Can we go? = ¿Podemos marcharnos?\n",
            "Is this a hoax? = ¿Esto es un engaño?\n",
            "I am tired from working too much. = Estoy cansado por trabajar demasiado.\n",
            "I didn't expect it to be so easy to do. = No esperaba que fuera tan fácil de hacer.\n",
            "I'll start taking notes in August. = Regresaré cuando tomarme aprenderme por verano.\n",
            "The cat frightened us badly. = El gato nos asustó.\n",
            "I thought this job was easy, but it was actually quite the work. = Pensé que ese trabajo era fácil, pero era fue un trabajo muy fácil.\n",
            "I've never met her. = Nunca la he conocido a ella.\n",
            "He is in a\n",
            "\n",
            "[19400 | 4868.38] loss=0.98 avg=0.85\n",
            "[19401 | 4871.49] loss=0.86 avg=0.85\n",
            "[19402 | 4874.58] loss=0.89 avg=0.85\n",
            "[19403 | 4877.66] loss=0.74 avg=0.85\n",
            "[19404 | 4880.73] loss=0.73 avg=0.85\n",
            "[19405 | 4883.80] loss=0.83 avg=0.85\n",
            "[19406 | 4886.86] loss=0.79 avg=0.85\n",
            "[19407 | 4889.94] loss=0.82 avg=0.85\n",
            "[19408 | 4893.02] loss=0.92 avg=0.85\n",
            "[19409 | 4896.09] loss=0.86 avg=0.85\n",
            "[19410 | 4899.17] loss=0.95 avg=0.85\n",
            "[19411 | 4902.23] loss=0.86 avg=0.85\n",
            "[19412 | 4905.31] loss=0.54 avg=0.85\n",
            "[19413 | 4908.39] loss=0.74 avg=0.85\n",
            "[19414 | 4911.46] loss=0.79 avg=0.84\n",
            "[19415 | 4914.52] loss=0.70 avg=0.84\n",
            "[19416 | 4917.58] loss=0.99 avg=0.84\n",
            "[19417 | 4920.64] loss=0.87 avg=0.85\n",
            "[19418 | 4923.70] loss=0.77 avg=0.84\n",
            "[19419 | 4926.77] loss=0.91 avg=0.85\n",
            "[19420 | 4929.87] loss=0.90 avg=0.85\n",
            "[19421 | 4932.98] loss=0.82 avg=0.85\n",
            "[19422 | 4936.05] loss=0.92 avg=0.85\n",
            "[19423 | 4939.14] loss=0.85 avg=0.85\n",
            "[19424 | 4942.23] loss=0.84 avg=0.85\n",
            "[19425 | 4945.31] loss=0.72 avg=0.84\n",
            "[19426 | 4948.39] loss=0.78 avg=0.84\n",
            "[19427 | 4951.46] loss=0.94 avg=0.84\n",
            "[19428 | 4954.56] loss=0.87 avg=0.85\n",
            "[19429 | 4957.65] loss=0.84 avg=0.85\n",
            "[19430 | 4960.74] loss=0.83 avg=0.84\n",
            "[19431 | 4963.84] loss=0.82 avg=0.84\n",
            "[19432 | 4966.93] loss=0.79 avg=0.84\n",
            "[19433 | 4970.01] loss=0.85 avg=0.84\n",
            "[19434 | 4973.08] loss=0.99 avg=0.85\n",
            "[19435 | 4976.15] loss=0.88 avg=0.85\n",
            "[19436 | 4979.22] loss=0.84 avg=0.85\n",
            "[19437 | 4982.30] loss=0.84 avg=0.85\n",
            "[19438 | 4985.38] loss=0.94 avg=0.85\n",
            "[19439 | 4988.45] loss=0.74 avg=0.85\n",
            "[19440 | 4991.52] loss=0.92 avg=0.85\n",
            "[19441 | 4994.60] loss=0.89 avg=0.85\n",
            "[19442 | 4997.68] loss=0.82 avg=0.85\n",
            "[19443 | 5000.77] loss=0.80 avg=0.85\n",
            "[19444 | 5003.87] loss=0.76 avg=0.85\n",
            "[19445 | 5006.95] loss=0.92 avg=0.85\n",
            "[19446 | 5010.03] loss=0.86 avg=0.85\n",
            "[19447 | 5013.12] loss=0.93 avg=0.85\n",
            "[19448 | 5016.20] loss=0.88 avg=0.85\n",
            "[19449 | 5019.29] loss=0.88 avg=0.85\n",
            "[19450 | 5022.36] loss=0.85 avg=0.85\n",
            "[19451 | 5025.43] loss=0.90 avg=0.85\n",
            "[19452 | 5028.50] loss=0.75 avg=0.85\n",
            "[19453 | 5031.59] loss=0.80 avg=0.85\n",
            "[19454 | 5034.66] loss=0.88 avg=0.85\n",
            "[19455 | 5037.72] loss=0.80 avg=0.85\n",
            "[19456 | 5040.80] loss=0.94 avg=0.85\n",
            "[19457 | 5043.90] loss=0.76 avg=0.85\n",
            "[19458 | 5046.98] loss=0.95 avg=0.85\n",
            "[19459 | 5050.06] loss=0.74 avg=0.85\n",
            "[19460 | 5053.16] loss=0.78 avg=0.85\n",
            "[19461 | 5056.24] loss=0.97 avg=0.85\n",
            "[19462 | 5059.33] loss=0.86 avg=0.85\n",
            "[19463 | 5062.42] loss=0.75 avg=0.85\n",
            "[19464 | 5065.51] loss=0.67 avg=0.84\n",
            "[19465 | 5068.58] loss=0.84 avg=0.84\n",
            "[19466 | 5071.65] loss=0.67 avg=0.84\n",
            "[19467 | 5074.73] loss=0.88 avg=0.84\n",
            "[19468 | 5077.82] loss=0.77 avg=0.84\n",
            "[19469 | 5080.90] loss=0.68 avg=0.84\n",
            "[19470 | 5083.99] loss=0.75 avg=0.84\n",
            "[19471 | 5087.09] loss=0.87 avg=0.84\n",
            "[19472 | 5090.16] loss=0.78 avg=0.84\n",
            "[19473 | 5093.25] loss=0.62 avg=0.84\n",
            "[19474 | 5096.35] loss=0.81 avg=0.84\n",
            "[19475 | 5099.44] loss=0.73 avg=0.84\n",
            "[19476 | 5102.51] loss=0.74 avg=0.84\n",
            "[19477 | 5105.59] loss=0.85 avg=0.84\n",
            "[19478 | 5108.67] loss=0.90 avg=0.84\n",
            "[19479 | 5111.73] loss=0.89 avg=0.84\n",
            "[19480 | 5114.81] loss=1.19 avg=0.84\n",
            "[19481 | 5117.89] loss=0.91 avg=0.84\n",
            "[19482 | 5120.96] loss=1.03 avg=0.84\n",
            "[19483 | 5124.04] loss=0.84 avg=0.84\n",
            "[19484 | 5127.11] loss=0.73 avg=0.84\n",
            "[19485 | 5130.20] loss=0.79 avg=0.84\n",
            "[19486 | 5133.29] loss=0.92 avg=0.84\n",
            "[19487 | 5136.37] loss=0.79 avg=0.84\n",
            "[19488 | 5139.45] loss=0.73 avg=0.84\n",
            "[19489 | 5142.51] loss=0.68 avg=0.84\n",
            "[19490 | 5145.59] loss=0.90 avg=0.84\n",
            "[19491 | 5148.68] loss=0.74 avg=0.84\n",
            "[19492 | 5151.76] loss=0.78 avg=0.84\n",
            "[19493 | 5154.85] loss=0.90 avg=0.84\n",
            "[19494 | 5157.93] loss=0.95 avg=0.84\n",
            "[19495 | 5160.99] loss=0.78 avg=0.84\n",
            "[19496 | 5164.04] loss=0.93 avg=0.84\n",
            "[19497 | 5167.11] loss=0.78 avg=0.84\n",
            "[19498 | 5170.18] loss=0.73 avg=0.84\n",
            "[19499 | 5173.23] loss=0.93 avg=0.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " pisado?\n",
            "Can you remember anything? = ¿Puedes recordar algo?\n",
            "Tom told his children they didn't need to call their father. = Tom les dijo a sus hijos que no necesitaban llamar a su padre.\n",
            "It's not my fault. = No es culpa mía.\n",
            "Tom and Mary looked at each other, puzzled, then at each other. = Tom y María se miraron el uno al otro, desorientados, entre el otro o por el otro.\n",
            "What happened to my book? = ¿Qué le pasó a mi libro?\n",
            "I'll tell you what I did today. = Le diré lo que hice hoy.\n",
            "There is a long line at every cash register. = Hay una larga cola en cada una de las cuenta registradas.\n",
            "Tom didn't want to read this book. = Tom no quería leer este libro.\n",
            "Tom got back into Mary's truck. = Tom regresó en el camión de Mary.\n",
            "Tom wants it back. = Tom lo quiere de vuelta.\n",
            "Mary is very pretty, but she's also very cruel. = Mary es muy guapa, pero también es muy cruel.\n",
            "My grandmother lived to be ninety-five years old. = Mi abuela vivió hasta los noventa y cinco.\n",
            "I don't care. = No importa.\n",
            "Don't trust Tom. = No confíes en Tom.\n",
            "I'd like to be a reporter. = Quisiera ser un periodista.\n",
            "Mary looked up at the sky. = Mary echó la cabeza al cielo.\n",
            "Do you want me to be cruel or to be cruel? = ¿Quieres que sea cruel o está cruel?\n",
            "There are some apples in the basket. = Hay algunas manzanas en la canasta.\n",
            "I went to school with an \"A\". = Fui al mismo \"A\".\n",
            "Tom and Mary are sitting on the couch. = Tom y Mary están sentados en el sofá.\n",
            "It is very hot in the summer in Japan. = Hace mucho calor en el verano en Japón.\n",
            "Is English spoken in Australia? = ¿Es acento inglés en Australia?\n",
            "My son asked me how Tom did it. = Mi hijo me preguntó está cómo lo hizo Tom.\n",
            "He's my stepfather. = Él es mi padrastro.\n",
            "You shouldn't have made me wait that long. = No deberías haberme hincé por eso.\n",
            "She wants to go. = Ella quiere ir.\n",
            "She always reads without studying. = Ella empeora sin estudiar.\n",
            "The boy caught a cricket. = El chico cogió un alballo.\n",
            "Tom didn't expect to be greeted that way. = Tom no esperaba que se llamaran de esa manera.\n",
            "They are always complaining. = Siempre se están quejando.\n",
            "I'm coming for you. = Vengo por tú.\n",
            "No matter how old he is, he never falls in love with a girl over thirty. = No importa cuántos años vaya a amar a una niña más religión, él nunca cayó enamorado de una niña más treinta.\n",
            "The police are interrogating him. = La policía lo está interrogando.\n",
            "The sky will soon clear up. = El cielo se aclarará pronto.\n",
            "It looked like the dog had been bitten by a bee. = Parecía que el perro había recibido una abeja.\n",
            "I have been in Boston for more than six months. = Yo he estado en Boston por más seis meses.\n",
            "This was Tom's idea. = Esto fue idea de Tom.\n",
            "Let's move on. = Espera adelante.\n",
            "It's possible that Tom saw what you did. = Puede que Tom ha visto lo que tú has visto.\n",
            "We had a great time. = Lo pasamos bien.\n",
            "Don't eat too much cake. = No coma tanto torta.\n",
            "She went without shoes. = Ella se fue sin zapatos.\n",
            "I didn't want to do that. = No quería hacerlo.\n",
            "He has two cars. = Tiene dos coches.\n",
            "It's a beautiful day, isn't it? = Ha pasado un hermoso día\n",
            "\n",
            "[19500 | 5213.26] loss=0.83 avg=0.84\n",
            "[19501 | 5216.35] loss=0.87 avg=0.84\n",
            "[19502 | 5219.42] loss=0.72 avg=0.84\n",
            "[19503 | 5222.48] loss=0.89 avg=0.84\n",
            "[19504 | 5225.54] loss=0.77 avg=0.84\n",
            "[19505 | 5228.62] loss=0.97 avg=0.84\n",
            "[19506 | 5231.68] loss=0.89 avg=0.84\n",
            "[19507 | 5234.74] loss=0.96 avg=0.84\n",
            "[19508 | 5237.80] loss=0.91 avg=0.84\n",
            "[19509 | 5240.86] loss=0.83 avg=0.84\n",
            "[19510 | 5243.94] loss=0.83 avg=0.84\n",
            "[19511 | 5247.01] loss=0.95 avg=0.84\n",
            "[19512 | 5250.09] loss=0.82 avg=0.84\n",
            "[19513 | 5253.15] loss=0.82 avg=0.84\n",
            "[19514 | 5256.24] loss=0.81 avg=0.84\n",
            "[19515 | 5259.32] loss=0.66 avg=0.84\n",
            "[19516 | 5262.40] loss=0.93 avg=0.84\n",
            "[19517 | 5265.49] loss=0.86 avg=0.84\n",
            "[19518 | 5268.57] loss=0.88 avg=0.84\n",
            "[19519 | 5271.67] loss=0.77 avg=0.84\n",
            "[19520 | 5274.77] loss=0.87 avg=0.84\n",
            "[19521 | 5277.85] loss=1.02 avg=0.84\n",
            "[19522 | 5280.94] loss=0.83 avg=0.84\n",
            "[19523 | 5284.04] loss=0.76 avg=0.84\n",
            "[19524 | 5287.12] loss=0.92 avg=0.84\n",
            "[19525 | 5290.21] loss=0.77 avg=0.84\n",
            "[19526 | 5293.28] loss=0.93 avg=0.84\n",
            "[19527 | 5296.36] loss=0.63 avg=0.84\n",
            "[19528 | 5299.45] loss=0.78 avg=0.84\n",
            "[19529 | 5302.55] loss=0.84 avg=0.84\n",
            "[19530 | 5305.63] loss=0.91 avg=0.84\n",
            "[19531 | 5308.74] loss=0.74 avg=0.84\n",
            "[19532 | 5311.82] loss=0.85 avg=0.84\n",
            "[19533 | 5314.91] loss=0.80 avg=0.84\n",
            "[19534 | 5318.00] loss=0.72 avg=0.84\n",
            "[19535 | 5321.09] loss=0.85 avg=0.84\n",
            "[19536 | 5324.16] loss=0.73 avg=0.84\n",
            "[19537 | 5327.23] loss=1.04 avg=0.84\n",
            "[19538 | 5330.30] loss=0.92 avg=0.84\n",
            "[19539 | 5333.37] loss=0.74 avg=0.84\n",
            "[19540 | 5336.42] loss=0.91 avg=0.84\n",
            "[19541 | 5339.51] loss=0.87 avg=0.84\n",
            "[19542 | 5342.58] loss=0.86 avg=0.84\n",
            "[19543 | 5345.65] loss=0.78 avg=0.84\n",
            "[19544 | 5348.73] loss=0.76 avg=0.84\n",
            "[19545 | 5351.80] loss=0.63 avg=0.84\n",
            "[19546 | 5354.88] loss=0.95 avg=0.84\n",
            "[19547 | 5357.94] loss=0.89 avg=0.84\n",
            "[19548 | 5361.02] loss=0.81 avg=0.84\n",
            "[19549 | 5364.09] loss=0.73 avg=0.84\n",
            "[19550 | 5367.17] loss=0.87 avg=0.84\n",
            "[19551 | 5370.23] loss=0.76 avg=0.84\n",
            "[19552 | 5373.32] loss=0.66 avg=0.83\n",
            "[19553 | 5376.42] loss=0.85 avg=0.84\n",
            "[19554 | 5379.50] loss=0.85 avg=0.84\n",
            "[19555 | 5382.58] loss=0.86 avg=0.84\n",
            "[19556 | 5385.65] loss=0.82 avg=0.84\n",
            "[19557 | 5388.73] loss=0.95 avg=0.84\n",
            "[19558 | 5391.81] loss=0.84 avg=0.84\n",
            "[19559 | 5394.89] loss=0.85 avg=0.84\n",
            "[19560 | 5397.98] loss=0.78 avg=0.84\n",
            "[19561 | 5401.06] loss=0.85 avg=0.84\n",
            "[19562 | 5404.15] loss=0.90 avg=0.84\n",
            "[19563 | 5407.22] loss=0.69 avg=0.84\n",
            "[19564 | 5410.32] loss=0.85 avg=0.84\n",
            "[19565 | 5413.41] loss=0.90 avg=0.84\n",
            "[19566 | 5416.46] loss=0.85 avg=0.84\n",
            "[19567 | 5419.53] loss=1.00 avg=0.84\n",
            "[19568 | 5422.61] loss=0.83 avg=0.84\n",
            "[19569 | 5425.71] loss=0.67 avg=0.84\n",
            "[19570 | 5428.81] loss=0.82 avg=0.84\n",
            "[19571 | 5431.91] loss=0.84 avg=0.84\n",
            "[19572 | 5435.01] loss=0.90 avg=0.84\n",
            "[19573 | 5438.12] loss=0.62 avg=0.83\n",
            "[19574 | 5441.21] loss=0.76 avg=0.83\n",
            "[19575 | 5444.30] loss=0.93 avg=0.83\n",
            "[19576 | 5447.41] loss=0.88 avg=0.84\n",
            "[19577 | 5450.50] loss=0.83 avg=0.84\n",
            "[19578 | 5453.58] loss=0.75 avg=0.83\n",
            "[19579 | 5456.67] loss=0.73 avg=0.83\n",
            "[19580 | 5459.78] loss=0.86 avg=0.83\n",
            "[19581 | 5462.89] loss=0.76 avg=0.83\n",
            "[19582 | 5466.00] loss=0.73 avg=0.83\n",
            "[19583 | 5469.10] loss=0.85 avg=0.83\n",
            "[19584 | 5472.21] loss=0.70 avg=0.83\n",
            "[19585 | 5475.31] loss=0.97 avg=0.83\n",
            "[19586 | 5478.42] loss=0.79 avg=0.83\n",
            "[19587 | 5481.50] loss=0.87 avg=0.83\n",
            "[19588 | 5484.59] loss=0.66 avg=0.83\n",
            "[19589 | 5487.67] loss=0.91 avg=0.83\n",
            "[19590 | 5490.76] loss=0.73 avg=0.83\n",
            "[19591 | 5493.84] loss=0.74 avg=0.83\n",
            "[19592 | 5496.92] loss=0.98 avg=0.83\n",
            "[19593 | 5500.00] loss=0.83 avg=0.83\n",
            "[19594 | 5503.08] loss=0.74 avg=0.83\n",
            "[19595 | 5506.17] loss=0.81 avg=0.83\n",
            "[19596 | 5509.24] loss=0.96 avg=0.83\n",
            "[19597 | 5512.33] loss=0.77 avg=0.83\n",
            "[19598 | 5515.41] loss=0.90 avg=0.83\n",
            "[19599 | 5518.47] loss=0.86 avg=0.83\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " you have a problem with. = Tengo problemas con un problema.\n",
            "I am trying to find out a secret to surviving as a horse rider. = Estoy tratando de encontrar un secreto para sobrevivir como saddleo de caballo.\n",
            "Why would Tom want to help Mary? = ¿Por qué Tom querría ayudar a María?\n",
            "I'm afraid you have to go tomorrow. = Me preocupas que tienes que marchir mañana.\n",
            "How many times a month do you write your diary? = ¿Cuántas veces al mes escribes tu diario?\n",
            "The two of them got into a fight. = Los dos se pusieron una pelea.\n",
            "The fire burned for three hours. = El fuego cerró durante tres horas.\n",
            "My room has only one window. = Mi cuarto tiene una sola ventana.\n",
            "The rain ended. = La lluvia se acabó.\n",
            "There is a little girl playing on the street. = Hay una niña juega en la calle.\n",
            "I can't stand noisy children. = No soporto los chicos son unsos.\n",
            "We did it. = Lo hicimos.\n",
            "I know your parents very well. = Conozco muy bien a tus padres.\n",
            "Tom is the man you should really date. = Tom es el hombre conocemos muy interesante.\n",
            "I wanted to speak with you about this. = Quería hablar contigo sobre esto.\n",
            "I'm not interested in her advice. = No estoy interesada en su consejo.\n",
            "Don't talk so loud. = No hables tan fuerte.\n",
            "We must take care of the dog once and for all. = Tenemos que cuidar de pero de nosotros una vidas.\n",
            "The car he's driving is not his. = El auto que él conduce no es suyo.\n",
            "It seemed that I couldn't play the piano because I'm not a singer. = Parecía que no piscía tocar el piano porque no soy cantante.\n",
            "Tom could've been murdered if he knew the truth. = Tom podría haber sido asesinado si sabía la verdad.\n",
            "Why is it important? = ¿Por qué es importante?\n",
            "Some of these are mine. = Algunas de éstas son mías.\n",
            "My brother took my camera. = Mi hermano cogió mi cámara.\n",
            "That's a lot of information. = Eso es mucho información.\n",
            "He wants to work. = Quiere trabajar.\n",
            "Tom did it to impress Mary. = Tom lo hizo para impresionar a Mary.\n",
            "I don't mind the heat. = No me molesta el calor.\n",
            "Can you make the cuter ones with less red? = ¿Puedes hacer los más chasquillas con menos rojo?\n",
            "There must have been a third party involved in this terrible crime. = Tiene que haber fuegiado una tercera crimen espantoso.\n",
            "Tom has told me plenty. = Tom ha dicho mucho.\n",
            "She is not his age. = Ella no es su edad.\n",
            "We're here to serve you. = Estamos aquí para servirles.\n",
            "She was wearing a hat. = Ella llevaba un sombrero.\n",
            "The girl left the classroom in order to smoke a cigarette. = La chica dejó salir de clases para fumar un cigarrillo.\n",
            "How long are you going? = ¿Cuánto tiempo vas a gustar?\n",
            "The sun shines in this picture. = El sol brilla en esta fotografía.\n",
            "I have a stomachache. = Me duele la barriga.\n",
            "You don't know what it's like to lose your entire family in a war. = No sabís lo que es perder a la familia entera en una guerra.\n",
            "Is that the reason you're not here today? = ¿Es ésa la razón por esa lugar no esta hoy?\n",
            "The doctor suggested that she return to her country. = El médico sugirió que regresara a su país.\n",
            "Tom knew this was going to happen. = Tom sabía que iba a pasar esto.\n",
            "What time do you get up every day? = ¿A qué hora te levantas\n",
            "\n",
            "[19600 | 5558.90] loss=0.92 avg=0.83\n",
            "[19601 | 5561.98] loss=0.86 avg=0.83\n",
            "[19602 | 5565.07] loss=0.83 avg=0.83\n",
            "[19603 | 5568.14] loss=0.81 avg=0.83\n",
            "[19604 | 5571.22] loss=0.84 avg=0.83\n",
            "[19605 | 5574.32] loss=0.87 avg=0.83\n",
            "[19606 | 5577.40] loss=0.73 avg=0.83\n",
            "[19607 | 5580.48] loss=0.85 avg=0.83\n",
            "[19608 | 5583.57] loss=0.91 avg=0.83\n",
            "[19609 | 5586.66] loss=0.78 avg=0.83\n",
            "[19610 | 5589.75] loss=0.86 avg=0.83\n",
            "[19611 | 5592.86] loss=0.88 avg=0.83\n",
            "[19612 | 5595.95] loss=0.77 avg=0.83\n",
            "[19613 | 5599.04] loss=0.95 avg=0.83\n",
            "[19614 | 5602.12] loss=0.80 avg=0.83\n",
            "[19615 | 5605.20] loss=0.82 avg=0.83\n",
            "[19616 | 5608.30] loss=0.86 avg=0.83\n",
            "[19617 | 5611.37] loss=0.82 avg=0.83\n",
            "[19618 | 5614.46] loss=0.83 avg=0.83\n",
            "[19619 | 5617.54] loss=0.71 avg=0.83\n",
            "[19620 | 5620.64] loss=0.75 avg=0.83\n",
            "[19621 | 5623.71] loss=0.76 avg=0.83\n",
            "[19622 | 5626.82] loss=0.88 avg=0.83\n",
            "[19623 | 5629.90] loss=0.90 avg=0.83\n",
            "[19624 | 5632.99] loss=0.94 avg=0.83\n",
            "[19625 | 5636.07] loss=0.64 avg=0.83\n",
            "[19626 | 5639.16] loss=0.97 avg=0.83\n",
            "[19627 | 5642.25] loss=0.75 avg=0.83\n",
            "[19628 | 5645.34] loss=0.81 avg=0.83\n",
            "[19629 | 5648.42] loss=0.88 avg=0.83\n",
            "[19630 | 5651.51] loss=0.89 avg=0.83\n",
            "[19631 | 5654.59] loss=0.77 avg=0.83\n",
            "[19632 | 5657.63] loss=0.95 avg=0.83\n",
            "[19633 | 5660.70] loss=0.92 avg=0.83\n",
            "[19634 | 5663.77] loss=0.85 avg=0.83\n",
            "[19635 | 5666.82] loss=0.94 avg=0.83\n",
            "[19636 | 5669.88] loss=0.72 avg=0.83\n",
            "[19637 | 5672.94] loss=0.91 avg=0.83\n",
            "[19638 | 5676.00] loss=0.84 avg=0.83\n",
            "[19639 | 5679.06] loss=0.98 avg=0.84\n",
            "[19640 | 5682.12] loss=0.76 avg=0.83\n",
            "[19641 | 5685.19] loss=0.95 avg=0.84\n",
            "[19642 | 5688.25] loss=0.91 avg=0.84\n",
            "[19643 | 5691.32] loss=0.82 avg=0.84\n",
            "[19644 | 5694.40] loss=0.76 avg=0.84\n",
            "[19645 | 5697.48] loss=0.75 avg=0.83\n",
            "[19646 | 5700.55] loss=0.98 avg=0.84\n",
            "[19647 | 5703.62] loss=0.78 avg=0.84\n",
            "[19648 | 5706.70] loss=0.83 avg=0.84\n",
            "[19649 | 5709.77] loss=0.87 avg=0.84\n",
            "[19650 | 5712.83] loss=0.76 avg=0.84\n",
            "[19651 | 5715.91] loss=0.79 avg=0.83\n",
            "[19652 | 5718.98] loss=0.83 avg=0.83\n",
            "[19653 | 5722.06] loss=0.77 avg=0.83\n",
            "[19654 | 5725.14] loss=0.89 avg=0.83\n",
            "[19655 | 5728.20] loss=0.84 avg=0.83\n",
            "[19656 | 5731.26] loss=1.02 avg=0.84\n",
            "[19657 | 5734.34] loss=0.91 avg=0.84\n",
            "[19658 | 5737.42] loss=0.79 avg=0.84\n",
            "[19659 | 5740.52] loss=0.75 avg=0.84\n",
            "[19660 | 5743.62] loss=0.98 avg=0.84\n",
            "[19661 | 5746.70] loss=0.84 avg=0.84\n",
            "[19662 | 5749.77] loss=0.76 avg=0.84\n",
            "[19663 | 5752.84] loss=0.89 avg=0.84\n",
            "[19664 | 5755.90] loss=0.67 avg=0.84\n",
            "[19665 | 5758.97] loss=0.86 avg=0.84\n",
            "[19666 | 5762.03] loss=0.95 avg=0.84\n",
            "[19667 | 5765.08] loss=0.86 avg=0.84\n",
            "[19668 | 5768.13] loss=0.73 avg=0.84\n",
            "[19669 | 5771.20] loss=0.94 avg=0.84\n",
            "[19670 | 5774.30] loss=0.73 avg=0.84\n",
            "[19671 | 5777.37] loss=0.85 avg=0.84\n",
            "[19672 | 5780.46] loss=0.91 avg=0.84\n",
            "[19673 | 5783.57] loss=0.92 avg=0.84\n",
            "[19674 | 5786.66] loss=0.88 avg=0.84\n",
            "[19675 | 5789.74] loss=0.90 avg=0.84\n",
            "[19676 | 5792.81] loss=0.76 avg=0.84\n",
            "[19677 | 5795.88] loss=0.85 avg=0.84\n",
            "[19678 | 5798.95] loss=0.96 avg=0.84\n",
            "[19679 | 5802.03] loss=0.70 avg=0.84\n",
            "[19680 | 5805.11] loss=0.92 avg=0.84\n",
            "[19681 | 5808.17] loss=0.73 avg=0.84\n",
            "[19682 | 5811.24] loss=0.80 avg=0.84\n",
            "[19683 | 5814.31] loss=0.65 avg=0.84\n",
            "[19684 | 5817.38] loss=0.83 avg=0.84\n",
            "[19685 | 5820.47] loss=0.76 avg=0.84\n",
            "[19686 | 5823.53] loss=0.92 avg=0.84\n",
            "[19687 | 5826.60] loss=0.85 avg=0.84\n",
            "[19688 | 5829.67] loss=0.89 avg=0.84\n",
            "[19689 | 5832.74] loss=0.88 avg=0.84\n",
            "[19690 | 5835.84] loss=0.92 avg=0.84\n",
            "[19691 | 5838.92] loss=0.80 avg=0.84\n",
            "[19692 | 5841.99] loss=0.75 avg=0.84\n",
            "[19693 | 5845.07] loss=0.85 avg=0.84\n",
            "[19694 | 5848.14] loss=0.98 avg=0.84\n",
            "[19695 | 5851.20] loss=0.88 avg=0.84\n",
            "[19696 | 5854.29] loss=0.75 avg=0.84\n",
            "[19697 | 5857.37] loss=0.86 avg=0.84\n",
            "[19698 | 5860.46] loss=0.88 avg=0.84\n",
            "[19699 | 5863.53] loss=0.78 avg=0.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". = Lo trancado muy bien que te parece?\n",
            "This is the best soup in the world. = Esta es la mejor sopa del mundo.\n",
            "Do you want this shirt? = ¿Querés esta camisa?\n",
            "Let's get out of here. = Vayámonos de aquí.\n",
            "I'm tired of living this kind of life. = Estoy cansado de vivir esta vida.\n",
            "I don't get it either. = Ni a cualquiera.\n",
            "She turned him down. = Ella lo rechazó.\n",
            "You should not have done it without my permission. = No deberías haberlo hecho sin márquez.\n",
            "You don't have to talk so much. = No tenés que hablar tanto.\n",
            "You don't need to come so early. = No necesitas venir tan temprano.\n",
            "I don't see any stars. = No veo nada estrellas.\n",
            "He's very interested in the weather. = Le interesa el tiempo.\n",
            "What's in this box? = ¿Qué hay en esta caja?\n",
            "If I had more time, I would learn the answer. = Si tuviera más tiempo, aprendería la respuesta.\n",
            "Do you know how to tell brothers apart? = ¿Acaso sabe decir dichos en ellos?\n",
            "I don't like you. = No me gustas.\n",
            "My mom bought me this book at a 20 percent discount. = Mi mamá me compró este libro a un 20% descuento.\n",
            "Tom has trouble concentrating. = Tom tiene problemas para concentrarse.\n",
            "I thought that we had a deal. = Creía que teníamos un trato.\n",
            "Are you sure you don't want to eat some pizza? = ¿Estás seguro de que no quieres comer un pizza?\n",
            "You're the only one I know has money. = Eres la única persona que conozco con el dinero.\n",
            "Tom had a stroke. = Tom tuvo un golpe.\n",
            "The bus was made out of wood. = En ese bicicleta el bus estaba hecha de madera.\n",
            "We only want to help you. = Solo queremos ayudarte.\n",
            "I saw my father on the stairs. = Yo vi a mi padre en las escaleras.\n",
            "Can I leave a message? = ¿Puedo dejar un mensaje?\n",
            "I don't mind doing whatever you tell me to do. = No me importa hacer lo que me digas que haga.\n",
            "I do not understand their plan very well. = No comprendo bien su planeo.\n",
            "He had an accident on his way to school. = Tuvo un accidente de camino al colegio.\n",
            "Tom won't have to do that. = Tom no tendrá que hacer eso.\n",
            "We have a cat and two dogs. = Tenemos un gato y dos perros.\n",
            "He had his collar around his neck. = Él llevaba su cuello alrededor de su cuello.\n",
            "That's a great theory. = Esa es una gran teoría.\n",
            "We should be free to choose whenever we like. = Deberíamos ser libres de elegir cualquier cosa!\n",
            "The old house is being torn down. = Los viejos casas están abajo.\n",
            "Your name sounds familiar to me. = Tu nombre me resulta prefreso.\n",
            "This is the girl my mother gave me for my wedding. = Esta es la chica que mi madre me regaló para mi boda.\n",
            "Would you like to drink some tea or something? = ¿Quieren tomar algo de té arroz o corazonable?\n",
            "I do not know whether I shall learn French next year. = No sé si voy a aprender francés el año que viene.\n",
            "They're always complaining. = Ellos siempre se están quejando.\n",
            "Tom is still young. = Tomás todavía es joven.\n",
            "The doctor says to be careful. = El médico dice ser cuidadoso.\n",
            "Tom has no idea what Mary is planning. = Tom no tiene idea de lo que Mary está planeando.\n",
            "I don't want this house to fall into disrepair. = No quiero que este casa se caerse en disfrutación.\n",
            "Do you have any tickets left? = ¿Le quedáis\n",
            "\n",
            "[19700 | 5903.92] loss=0.81 avg=0.84\n",
            "[19701 | 5906.99] loss=0.68 avg=0.84\n",
            "[19702 | 5910.08] loss=0.89 avg=0.84\n",
            "[19703 | 5913.16] loss=0.67 avg=0.83\n",
            "[19704 | 5916.23] loss=0.87 avg=0.84\n",
            "[19705 | 5919.32] loss=0.96 avg=0.84\n",
            "[19706 | 5922.41] loss=0.73 avg=0.84\n",
            "[19707 | 5925.51] loss=0.87 avg=0.84\n",
            "[19708 | 5928.58] loss=0.78 avg=0.84\n",
            "[19709 | 5931.67] loss=0.99 avg=0.84\n",
            "[19710 | 5934.78] loss=0.99 avg=0.84\n",
            "[19711 | 5937.86] loss=0.81 avg=0.84\n",
            "[19712 | 5940.98] loss=0.89 avg=0.84\n",
            "[19713 | 5944.07] loss=0.94 avg=0.84\n",
            "[19714 | 5947.17] loss=0.67 avg=0.84\n",
            "[19715 | 5950.26] loss=0.81 avg=0.84\n",
            "[19716 | 5953.35] loss=0.79 avg=0.84\n",
            "[19717 | 5956.44] loss=0.95 avg=0.84\n",
            "[19718 | 5959.53] loss=0.83 avg=0.84\n",
            "[19719 | 5962.63] loss=0.80 avg=0.84\n",
            "[19720 | 5965.72] loss=0.77 avg=0.84\n",
            "[19721 | 5968.81] loss=0.73 avg=0.84\n",
            "[19722 | 5971.91] loss=0.67 avg=0.83\n",
            "[19723 | 5974.99] loss=0.81 avg=0.83\n",
            "[19724 | 5978.08] loss=0.79 avg=0.83\n",
            "[19725 | 5981.18] loss=0.86 avg=0.83\n",
            "[19726 | 5984.27] loss=0.79 avg=0.83\n",
            "[19727 | 5987.37] loss=0.84 avg=0.83\n",
            "[19728 | 5990.46] loss=0.96 avg=0.83\n",
            "[19729 | 5993.55] loss=1.02 avg=0.84\n",
            "[19730 | 5996.62] loss=0.78 avg=0.84\n",
            "[19731 | 5999.68] loss=0.92 avg=0.84\n",
            "[19732 | 6002.73] loss=0.78 avg=0.84\n",
            "[19733 | 6005.79] loss=0.76 avg=0.84\n",
            "[19734 | 6008.86] loss=0.95 avg=0.84\n",
            "[19735 | 6011.94] loss=0.93 avg=0.84\n",
            "[19736 | 6014.99] loss=0.85 avg=0.84\n",
            "[19737 | 6018.04] loss=0.88 avg=0.84\n",
            "[19738 | 6021.10] loss=0.71 avg=0.84\n",
            "[19739 | 6024.16] loss=0.79 avg=0.84\n",
            "[19740 | 6027.22] loss=0.77 avg=0.84\n",
            "[19741 | 6030.28] loss=0.79 avg=0.84\n",
            "[19742 | 6033.34] loss=0.72 avg=0.83\n",
            "[19743 | 6036.39] loss=0.80 avg=0.83\n",
            "[19744 | 6039.47] loss=0.90 avg=0.83\n",
            "[19745 | 6042.53] loss=0.89 avg=0.83\n",
            "[19746 | 6045.60] loss=0.85 avg=0.83\n",
            "[19747 | 6048.66] loss=0.85 avg=0.84\n",
            "[19748 | 6051.73] loss=0.82 avg=0.83\n",
            "[19749 | 6054.81] loss=0.74 avg=0.83\n",
            "[19750 | 6057.87] loss=0.93 avg=0.83\n",
            "[19751 | 6060.94] loss=0.94 avg=0.84\n",
            "[19752 | 6064.01] loss=0.83 avg=0.84\n",
            "[19753 | 6067.07] loss=0.85 avg=0.84\n",
            "[19754 | 6070.13] loss=0.80 avg=0.84\n",
            "[19755 | 6073.20] loss=0.83 avg=0.84\n",
            "[19756 | 6076.26] loss=0.84 avg=0.84\n",
            "[19757 | 6079.35] loss=0.92 avg=0.84\n",
            "[19758 | 6082.44] loss=0.87 avg=0.84\n",
            "[19759 | 6085.53] loss=0.88 avg=0.84\n",
            "[19760 | 6088.60] loss=0.82 avg=0.84\n",
            "[19761 | 6091.68] loss=0.79 avg=0.84\n",
            "[19762 | 6094.77] loss=0.93 avg=0.84\n",
            "[19763 | 6097.85] loss=0.88 avg=0.84\n",
            "[19764 | 6100.93] loss=0.80 avg=0.84\n",
            "[19765 | 6104.01] loss=0.77 avg=0.84\n",
            "[19766 | 6107.08] loss=0.75 avg=0.84\n",
            "[19767 | 6110.15] loss=0.81 avg=0.84\n",
            "[19768 | 6113.25] loss=0.73 avg=0.83\n",
            "[19769 | 6116.33] loss=0.93 avg=0.84\n",
            "[19770 | 6119.40] loss=0.78 avg=0.84\n",
            "[19771 | 6122.49] loss=0.82 avg=0.83\n",
            "[19772 | 6125.56] loss=0.73 avg=0.83\n",
            "[19773 | 6128.63] loss=0.89 avg=0.83\n",
            "[19774 | 6131.71] loss=0.75 avg=0.83\n",
            "[19775 | 6134.77] loss=0.84 avg=0.83\n",
            "[19776 | 6137.81] loss=0.84 avg=0.83\n",
            "[19777 | 6140.88] loss=0.83 avg=0.83\n",
            "[19778 | 6143.95] loss=0.86 avg=0.83\n",
            "[19779 | 6147.03] loss=0.84 avg=0.83\n",
            "[19780 | 6150.12] loss=0.94 avg=0.84\n",
            "[19781 | 6153.21] loss=0.98 avg=0.84\n",
            "[19782 | 6156.28] loss=0.93 avg=0.84\n",
            "[19783 | 6159.36] loss=0.92 avg=0.84\n",
            "[19784 | 6162.43] loss=0.83 avg=0.84\n",
            "[19785 | 6165.51] loss=0.74 avg=0.84\n",
            "[19786 | 6168.59] loss=0.76 avg=0.84\n",
            "[19787 | 6171.68] loss=0.88 avg=0.84\n",
            "[19788 | 6174.78] loss=0.83 avg=0.84\n",
            "[19789 | 6177.86] loss=0.76 avg=0.84\n",
            "[19790 | 6180.96] loss=0.93 avg=0.84\n",
            "[19791 | 6184.04] loss=0.82 avg=0.84\n",
            "[19792 | 6187.13] loss=0.83 avg=0.84\n",
            "[19793 | 6190.22] loss=0.88 avg=0.84\n",
            "[19794 | 6193.31] loss=0.92 avg=0.84\n",
            "[19795 | 6196.40] loss=0.86 avg=0.84\n",
            "[19796 | 6199.49] loss=0.70 avg=0.84\n",
            "[19797 | 6202.59] loss=0.75 avg=0.84\n",
            "[19798 | 6205.69] loss=0.91 avg=0.84\n",
            "[19799 | 6208.77] loss=0.87 avg=0.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "But it won't benefit you. = No te para que te beneficiará.\n",
            "Tom thought his cat was still asleep. = Tom pensó que hoy cometía su gato.\n",
            "Your shirt isn't clean. = Tu camisa está limpia.\n",
            "The first step is realizing that someone else has already done something. = El primer paso está en darse cuenta de que alguien ya había already hizo la ley.\n",
            "I'll get you to the airport as soon as I can. = Te consigüé hasta el aeropuerto tan pronto como pueda.\n",
            "I'm not the one who needs to be reminded. = No soy yo el que necesita recordarle.\n",
            "He'll be angry at me, too. = Él también se enfadará conmigo.\n",
            "Tom asked for a receipt, but he wasn't able to get one. = Todo Tom pidió recibo, pero no podía conseguirse suerte.\n",
            "Tom knows nothing about her. = Tom no sabe nada acerca de ella.\n",
            "Tom will be back soon. = Tom volverá en seguida.\n",
            "She used to study English when she was in high school. = Solía estudiando inglés cuando estaba en la prepa.\n",
            "We know that Tom knows. = Sabemos que Tom sabe.\n",
            "The police will catch you sooner or later. = La policía te captura tarde o temprano.\n",
            "I need a couple more people. = Necesito un par de personas más.\n",
            "You can't buy a ticket online. = No puedes comprar una entrada en e-bay.\n",
            "I have no money in my pocket. = No tengo dinero en el bolsillo.\n",
            "I think you'll want to talk to the doctor. = Creo que quieras hablar con el doctor.\n",
            "I want to make sure that you are who you say you are. = Quiero asegurarme de que eres clase.\n",
            "He's in a good mood today. = Hoy hace buen humor.\n",
            "Did you really just call up an hour ago and ask if we could have a party? = ¿Acabas de llamar tiempo y preguntaste si podíamos hacer una fiesta?\n",
            "I'm going to keep this one. = Me quedo este.\n",
            "Do I need to buy a new dress? = ¿Tengo que comprar un nuevo vestido?\n",
            "I am very grateful for your cooperation. = Estoy muy agradecido por tu cooperación.\n",
            "I don't know what to suggest. = No sé que sugerir.\n",
            "Have a croissant. = Tome un cruasán.\n",
            "Tom can't understand why Mary got so angry. = Tom no puede entender por qué se puso tan furiosa.\n",
            "You needn't have taken a taxi. = No necesitaban haber tomado un taxi.\n",
            "I was not at home. = Yo no estaba en casa.\n",
            "What's the meaning of this? = ¿Qué significa esto?\n",
            "His advice would be very useful to you. = Su consejo les resultaría muy útil a ustedes.\n",
            "This has to be done tomorrow. = Esto tiene que estar hacer mañana.\n",
            "Can he play the trumpet? = ¿Él sabe tocar la trompeta?\n",
            "Please be reasonable. = Por favor sé razonable.\n",
            "Please listen to me. = Por favor, escuche.\n",
            "I just wanted to give you a couple of days. = Solo quería darte un par de días.\n",
            "I'm in the truck. = Estoy en el camión.\n",
            "No one believes Tom anymore. = Ya a nadie le creerá de Tom.\n",
            "The police have caught him. = Ha llevado la policía.\n",
            "These plants can be eaten raw. = Estas plantas pueden comer crudas.\n",
            "I hope we can start taking better care of ourselves. = Espero que podamos empezar a cuidar usuario.\n",
            "He is an energetic person and really excels at sports. = Él es una persona energista y verdaderamente el acostumbrado.\n",
            "Please wait here. = Sujeta aquí, por favor.\n",
            "You need to do more than that. = Tienes que hacer más que eso.\n",
            "I wanted to ask you something. = Quería preg\n",
            "\n",
            "[19800 | 6249.39] loss=0.55 avg=0.83\n",
            "[19801 | 6252.49] loss=0.92 avg=0.84\n",
            "[19802 | 6255.56] loss=0.93 avg=0.84\n",
            "[19803 | 6258.63] loss=0.86 avg=0.84\n",
            "[19804 | 6261.71] loss=0.92 avg=0.84\n",
            "[19805 | 6264.79] loss=0.92 avg=0.84\n",
            "[19806 | 6267.89] loss=0.85 avg=0.84\n",
            "[19807 | 6270.98] loss=0.72 avg=0.84\n",
            "[19808 | 6274.07] loss=0.85 avg=0.84\n",
            "[19809 | 6277.16] loss=0.82 avg=0.84\n",
            "[19810 | 6280.24] loss=0.96 avg=0.84\n",
            "[19811 | 6283.34] loss=0.86 avg=0.84\n",
            "[19812 | 6286.42] loss=0.83 avg=0.84\n",
            "[19813 | 6289.51] loss=0.81 avg=0.84\n",
            "[19814 | 6292.59] loss=0.77 avg=0.84\n",
            "[19815 | 6295.67] loss=0.90 avg=0.84\n",
            "[19816 | 6298.75] loss=0.74 avg=0.84\n",
            "[19817 | 6301.83] loss=0.78 avg=0.84\n",
            "[19818 | 6304.93] loss=0.97 avg=0.84\n",
            "[19819 | 6308.03] loss=0.88 avg=0.84\n",
            "[19820 | 6311.12] loss=0.86 avg=0.84\n",
            "[19821 | 6314.20] loss=0.90 avg=0.84\n",
            "[19822 | 6317.29] loss=0.86 avg=0.84\n",
            "[19823 | 6320.39] loss=0.73 avg=0.84\n",
            "[19824 | 6323.45] loss=0.91 avg=0.84\n",
            "[19825 | 6326.53] loss=0.78 avg=0.84\n",
            "[19826 | 6329.61] loss=0.91 avg=0.84\n",
            "[19827 | 6332.69] loss=0.72 avg=0.84\n",
            "[19828 | 6335.79] loss=0.93 avg=0.84\n",
            "[19829 | 6338.88] loss=0.82 avg=0.84\n",
            "[19830 | 6341.97] loss=0.75 avg=0.84\n",
            "[19831 | 6345.03] loss=0.82 avg=0.84\n",
            "[19832 | 6348.12] loss=0.71 avg=0.84\n",
            "[19833 | 6351.20] loss=0.89 avg=0.84\n",
            "[19834 | 6354.28] loss=0.68 avg=0.83\n",
            "[19835 | 6357.37] loss=0.83 avg=0.83\n",
            "[19836 | 6360.46] loss=0.96 avg=0.84\n",
            "[19837 | 6363.54] loss=0.75 avg=0.84\n",
            "[19838 | 6366.61] loss=0.96 avg=0.84\n",
            "[19839 | 6369.67] loss=0.86 avg=0.84\n",
            "[19840 | 6372.75] loss=0.92 avg=0.84\n",
            "[19841 | 6375.83] loss=0.92 avg=0.84\n",
            "[19842 | 6378.92] loss=0.87 avg=0.84\n",
            "[19843 | 6382.01] loss=0.66 avg=0.84\n",
            "[19844 | 6385.08] loss=1.04 avg=0.84\n",
            "[19845 | 6388.15] loss=1.03 avg=0.84\n",
            "[19846 | 6391.23] loss=0.83 avg=0.84\n",
            "[19847 | 6394.32] loss=0.71 avg=0.84\n",
            "[19848 | 6397.38] loss=0.79 avg=0.84\n",
            "[19849 | 6400.48] loss=0.70 avg=0.84\n",
            "[19850 | 6403.57] loss=0.91 avg=0.84\n",
            "[19851 | 6406.65] loss=0.74 avg=0.84\n",
            "[19852 | 6409.73] loss=0.85 avg=0.84\n",
            "[19853 | 6412.81] loss=0.78 avg=0.84\n",
            "[19854 | 6415.90] loss=0.67 avg=0.83\n",
            "[19855 | 6418.99] loss=0.94 avg=0.84\n",
            "[19856 | 6422.08] loss=0.90 avg=0.84\n",
            "[19857 | 6425.17] loss=0.87 avg=0.84\n",
            "[19858 | 6428.27] loss=1.06 avg=0.84\n",
            "[19859 | 6431.37] loss=1.00 avg=0.84\n",
            "[19860 | 6434.46] loss=0.80 avg=0.84\n",
            "[19861 | 6437.56] loss=0.71 avg=0.84\n",
            "[19862 | 6440.65] loss=0.87 avg=0.84\n",
            "[19863 | 6443.75] loss=0.67 avg=0.84\n",
            "[19864 | 6446.84] loss=1.01 avg=0.84\n",
            "[19865 | 6449.94] loss=0.84 avg=0.84\n",
            "[19866 | 6453.02] loss=0.90 avg=0.84\n",
            "[19867 | 6456.10] loss=0.95 avg=0.84\n",
            "[19868 | 6459.20] loss=0.96 avg=0.84\n",
            "[19869 | 6462.30] loss=0.96 avg=0.84\n",
            "[19870 | 6465.38] loss=0.72 avg=0.84\n",
            "[19871 | 6468.47] loss=0.88 avg=0.84\n",
            "[19872 | 6471.56] loss=0.94 avg=0.84\n",
            "[19873 | 6474.66] loss=0.84 avg=0.84\n",
            "[19874 | 6477.76] loss=0.86 avg=0.84\n",
            "[19875 | 6480.85] loss=0.91 avg=0.84\n",
            "[19876 | 6483.95] loss=1.01 avg=0.85\n",
            "[19877 | 6487.05] loss=0.68 avg=0.84\n",
            "[19878 | 6490.14] loss=0.81 avg=0.84\n",
            "[19879 | 6493.23] loss=0.87 avg=0.84\n",
            "[19880 | 6496.31] loss=0.93 avg=0.85\n",
            "[19881 | 6499.39] loss=0.87 avg=0.85\n",
            "[19882 | 6502.48] loss=0.70 avg=0.84\n",
            "[19883 | 6505.57] loss=0.93 avg=0.84\n",
            "[19884 | 6508.66] loss=0.92 avg=0.85\n",
            "[19885 | 6511.74] loss=0.90 avg=0.85\n",
            "[19886 | 6514.84] loss=0.72 avg=0.84\n",
            "[19887 | 6517.95] loss=0.85 avg=0.84\n",
            "[19888 | 6521.05] loss=0.70 avg=0.84\n",
            "[19889 | 6524.14] loss=0.77 avg=0.84\n",
            "[19890 | 6527.23] loss=0.83 avg=0.84\n",
            "[19891 | 6530.31] loss=0.84 avg=0.84\n",
            "[19892 | 6533.40] loss=0.83 avg=0.84\n",
            "[19893 | 6536.49] loss=0.87 avg=0.84\n",
            "[19894 | 6539.59] loss=0.84 avg=0.84\n",
            "[19895 | 6542.68] loss=0.80 avg=0.84\n",
            "[19896 | 6545.77] loss=0.80 avg=0.84\n",
            "[19897 | 6548.85] loss=0.70 avg=0.84\n",
            "[19898 | 6551.94] loss=0.73 avg=0.84\n",
            "[19899 | 6555.05] loss=0.76 avg=0.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". ¿Está bienvenido de los gatos?\n",
            "It's been a long time since I've seen you smile. = Ha pasado mucho tiempo desde que te he visto sonreír.\n",
            "Tom and his father stayed in touch by email. = Tom y su padre se permanecieron en contacto por correo electrónico.\n",
            "Do you want to eat something? = ¿Querés comer algo?\n",
            "How do you explain that? = ¿Cómo explica eso?\n",
            "He was about to call off the marriage when the phone rang. = Estaba a punto de suspendir el matrimonio cuando sonó el teléfono.\n",
            "I don't know who I am. = No sé quién soy.\n",
            "A lot of fish died. = Murieron muchos peces.\n",
            "I wonder what I should do. = Me pregunto quién debería hacer.\n",
            "He is still young. = Él es todavía joven.\n",
            "I just want to be happy. = Solo quiero ser feliz.\n",
            "Tom wanted to speak to Mary. = Tom quería hablar con María.\n",
            "Don't cry. = ¡No tuns!\n",
            "That was a tough choice. = No fue una elección difícil.\n",
            "She didn't bring us gifts. = Ella no nos trajo regalos.\n",
            "This was her first love. = Esta fue su primer amor.\n",
            "I can't help laughing at the girl. = No puedo dejar de reírme de la chica.\n",
            "They're going. = Ellas están viniendo.\n",
            "How many years has she been working here? = ¿Cuántos años lleva trabajándose aquí?\n",
            "Give me that. = Deme eso.\n",
            "He looks just like my sister. = Él se parece a mi hermana.\n",
            "I bought Tom some groceries. = Le compré Tom algunas comestibles.\n",
            "I'd like to speak to one of your representatives in Washington, D.C. = Quisiera hablar con uno de vuestros representados en el Pérdo, D.C.\n",
            "I like watching planes take off. = Me gusta ver las aviones despegando.\n",
            "Tom doesn't have anything to worry about anymore. = Tom ya no tiene nada de qué preocuparnos.\n",
            "Tom will eventually need a job. = A la larga, Tom deben necesitar un trabajo.\n",
            "I have to make a few calls. = Yo debo hacer unas cuantas llamadas.\n",
            "Tom has an ear for music. = Tom tiene hecho para la música.\n",
            "How many people are in this room? = ¿Cuántas personas hay en esta habitación?\n",
            "She has a bicycle. = Ella tiene una bicicleta.\n",
            "My father is a very patient man. = Mi padre es un hombre muy paciente.\n",
            "He must come out of his room. = Él debió ir a su habitación.\n",
            "He's not a fool. = No es un tonto.\n",
            "I have only one cat. = Yo tengo un círculo.\n",
            "Tom was sitting in his car outside the window. = Tom estaba sentado en su auto afuera de la ventana.\n",
            "He was a bad student. = Era un mal estudiante.\n",
            "Tom will be there tomorrow. = Tom estará allá mañana.\n",
            "Would you please explain to me where the market is? = ¿Podrías por favor dado darme la mercancija, ¿por qué es?\n",
            "I have no intention of stopping doing that. = No tengo intención de dejar de hacerlo.\n",
            "Tom should be here within fifteen minutes. = Tom debería estar aquí dentro de quince minutos.\n",
            "Tom was the victim of an attempted crime. = Tom fue víctima de un crimen intentado.\n",
            "Tom is going to teach me his new course. = Tom va a enseñarme su nueva curso.\n",
            "How much is this handkerchief? = ¿Cuánto cuesta este pañuelo?\n",
            "The old man caught the child by the hand. = El anciano capturó al niño por la mano.\n",
            "Tom likes me. = A Tom le gusto.\n",
            "This is my sister's book. = Este es el\n",
            "\n",
            "[19900 | 6595.68] loss=0.91 avg=0.84\n",
            "[19901 | 6598.78] loss=0.77 avg=0.84\n",
            "[19902 | 6601.86] loss=0.76 avg=0.84\n",
            "[19903 | 6604.94] loss=0.88 avg=0.84\n",
            "[19904 | 6608.04] loss=0.75 avg=0.84\n",
            "[19905 | 6611.14] loss=0.84 avg=0.84\n",
            "[19906 | 6614.21] loss=0.95 avg=0.84\n",
            "[19907 | 6617.27] loss=0.75 avg=0.84\n",
            "[19908 | 6620.33] loss=0.66 avg=0.84\n",
            "[19909 | 6623.42] loss=0.76 avg=0.83\n",
            "[19910 | 6626.51] loss=0.89 avg=0.84\n",
            "[19911 | 6629.62] loss=0.87 avg=0.84\n",
            "[19912 | 6632.71] loss=0.81 avg=0.84\n",
            "[19913 | 6635.78] loss=0.67 avg=0.83\n",
            "[19914 | 6638.88] loss=0.65 avg=0.83\n",
            "[19915 | 6641.97] loss=1.02 avg=0.83\n",
            "[19916 | 6645.06] loss=0.91 avg=0.83\n",
            "[19917 | 6648.14] loss=0.80 avg=0.83\n",
            "[19918 | 6651.24] loss=0.85 avg=0.83\n",
            "[19919 | 6654.34] loss=0.90 avg=0.84\n",
            "[19920 | 6657.42] loss=0.86 avg=0.84\n",
            "[19921 | 6660.53] loss=0.99 avg=0.84\n",
            "[19922 | 6663.60] loss=0.84 avg=0.84\n",
            "[19923 | 6666.70] loss=0.84 avg=0.84\n",
            "[19924 | 6669.78] loss=0.95 avg=0.84\n",
            "[19925 | 6672.88] loss=0.87 avg=0.84\n",
            "[19926 | 6675.97] loss=0.90 avg=0.84\n",
            "[19927 | 6679.05] loss=0.93 avg=0.84\n",
            "[19928 | 6682.15] loss=0.76 avg=0.84\n",
            "[19929 | 6685.23] loss=0.81 avg=0.84\n",
            "[19930 | 6688.34] loss=0.79 avg=0.84\n",
            "[19931 | 6691.39] loss=0.85 avg=0.84\n",
            "[19932 | 6694.47] loss=0.90 avg=0.84\n",
            "[19933 | 6697.55] loss=0.83 avg=0.84\n",
            "[19934 | 6700.64] loss=0.79 avg=0.84\n",
            "[19935 | 6703.73] loss=0.73 avg=0.84\n",
            "[19936 | 6706.82] loss=0.78 avg=0.84\n",
            "[19937 | 6709.91] loss=0.91 avg=0.84\n",
            "[19938 | 6712.98] loss=0.70 avg=0.84\n",
            "[19939 | 6716.05] loss=0.90 avg=0.84\n",
            "[19940 | 6719.16] loss=0.78 avg=0.84\n",
            "[19941 | 6722.26] loss=0.66 avg=0.83\n",
            "[19942 | 6725.36] loss=1.07 avg=0.84\n",
            "[19943 | 6728.45] loss=0.96 avg=0.84\n",
            "[19944 | 6731.54] loss=0.70 avg=0.84\n",
            "[19945 | 6734.62] loss=0.78 avg=0.84\n",
            "[19946 | 6737.70] loss=0.74 avg=0.84\n",
            "[19947 | 6740.76] loss=0.88 avg=0.84\n",
            "[19948 | 6743.84] loss=0.64 avg=0.83\n",
            "[19949 | 6746.93] loss=0.85 avg=0.83\n",
            "[19950 | 6750.03] loss=0.87 avg=0.83\n",
            "[19951 | 6753.12] loss=0.72 avg=0.83\n",
            "[19952 | 6756.19] loss=0.77 avg=0.83\n",
            "[19953 | 6759.28] loss=0.76 avg=0.83\n",
            "[19954 | 6762.37] loss=0.70 avg=0.83\n",
            "[19955 | 6765.46] loss=0.84 avg=0.83\n",
            "[19956 | 6768.54] loss=0.89 avg=0.83\n",
            "[19957 | 6771.64] loss=0.98 avg=0.83\n",
            "[19958 | 6774.72] loss=0.74 avg=0.83\n",
            "[19959 | 6777.81] loss=0.83 avg=0.83\n",
            "[19960 | 6780.91] loss=0.85 avg=0.83\n",
            "[19961 | 6784.01] loss=0.69 avg=0.83\n",
            "[19962 | 6787.11] loss=0.91 avg=0.83\n",
            "[19963 | 6790.20] loss=0.72 avg=0.83\n",
            "[19964 | 6793.29] loss=0.89 avg=0.83\n",
            "[19965 | 6796.38] loss=0.94 avg=0.83\n",
            "[19966 | 6799.45] loss=0.82 avg=0.83\n",
            "[19967 | 6802.52] loss=0.90 avg=0.83\n",
            "[19968 | 6805.61] loss=0.88 avg=0.83\n",
            "[19969 | 6808.67] loss=0.75 avg=0.83\n",
            "[19970 | 6811.74] loss=0.87 avg=0.83\n",
            "[19971 | 6814.81] loss=0.90 avg=0.83\n",
            "[19972 | 6817.90] loss=0.93 avg=0.83\n",
            "[19973 | 6820.99] loss=0.89 avg=0.83\n",
            "[19974 | 6824.08] loss=0.84 avg=0.83\n",
            "[19975 | 6827.17] loss=0.85 avg=0.83\n",
            "[19976 | 6830.27] loss=0.80 avg=0.83\n",
            "[19977 | 6833.36] loss=0.91 avg=0.84\n",
            "[19978 | 6836.44] loss=0.96 avg=0.84\n",
            "[19979 | 6839.53] loss=0.96 avg=0.84\n",
            "[19980 | 6842.61] loss=0.89 avg=0.84\n",
            "[19981 | 6845.69] loss=0.98 avg=0.84\n",
            "[19982 | 6848.77] loss=0.80 avg=0.84\n",
            "[19983 | 6851.86] loss=0.88 avg=0.84\n",
            "[19984 | 6854.95] loss=0.84 avg=0.84\n",
            "[19985 | 6858.02] loss=0.82 avg=0.84\n",
            "[19986 | 6861.11] loss=0.99 avg=0.84\n",
            "[19987 | 6864.20] loss=0.91 avg=0.84\n",
            "[19988 | 6867.30] loss=0.75 avg=0.84\n",
            "[19989 | 6870.39] loss=0.84 avg=0.84\n",
            "[19990 | 6873.47] loss=0.83 avg=0.84\n",
            "[19991 | 6876.56] loss=0.90 avg=0.84\n",
            "[19992 | 6879.65] loss=0.79 avg=0.84\n",
            "[19993 | 6882.75] loss=0.76 avg=0.84\n",
            "[19994 | 6885.85] loss=0.90 avg=0.84\n",
            "[19995 | 6888.93] loss=0.72 avg=0.84\n",
            "[19996 | 6892.00] loss=0.79 avg=0.84\n",
            "[19997 | 6895.10] loss=0.94 avg=0.84\n",
            "[19998 | 6898.16] loss=0.96 avg=0.84\n",
            "[19999 | 6901.24] loss=0.84 avg=0.84\n",
            "Saving /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-20000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " la suerte de Tom.\n",
            "It's raining now, isn't it? = Está lloviendo ahora, ¿no?\n",
            "Tom is ready to fight. = Tom está listo para luchar.\n",
            "Your house is ten minutes' walk from the station. = Tu casa queda a diez minutos de pie de la estación.\n",
            "Tom died on November 8, 1829. = Tom murió el 8 de octubre, 1829.\n",
            "She has two daughters. = Ella tiene dos hijas.\n",
            "Tom wants to live in France. = Tom quiere vivir en Francia.\n",
            "Do you know why? = ¿Sabes por qué?\n",
            "I know the man standing behind the curtain. = Conozco al hombre que está parado detrás de la cortina.\n",
            "Tom is the one who pays the rent. = Tom es el que paga el renta.\n",
            "I don't want to end up in court. = No quiero terminar en un juicio.\n",
            "Tom has never met Mary. = Tom no ha conocido nunca a Mary.\n",
            "I don't belong to the club. = Yo no pertenezco al club.\n",
            "We were only joking. = Solo estaban bromeando.\n",
            "I don't want to do it that way. = No quiero hacerlo de esa manera.\n",
            "Tom was at school. = Tom estaba en el colegio.\n",
            "I'm at my parents' place. = Soy donde mis padres.\n",
            "Tell him to call me this afternoon. = Dile que me llame esta tarde.\n",
            "The cat will have to be killed. = El gato tendrá que ser matado.\n",
            "I don't remember you at all. = No te recuerdo para nada.\n",
            "Tom didn't answer. = Tomás no respondió.\n",
            "This isn't fair. = Esto no es justo.\n",
            "I can't tell you what Tom said. = No puedo contarte lo que dijo Tom.\n",
            "Please take a rest. = Por favor descansa un descanso.\n",
            "Who is that man? = ¿Quién es ese hombre?\n",
            "I'll tell Tom you said that. = Le diré a Tom que dijese eso.\n",
            "What does Tom think of Mary? = ¿Qué tal en Tom de Mary?\n",
            "He got hurt when he tripped over the egg. = Tenía años de que resbaló al golpe.\n",
            "It would not have been necessary for me to go there. = No habría sido necesario que yo vaya allí.\n",
            "I had no money left. = No I tenido nada de dinero.\n",
            "I want to play a game. = Quiero jugar un macho.\n",
            "I like my coffee strong. = Me gusta el café fuerte.\n",
            "Tom is not a typical businessman like his father. = Tom no es un típico como un transactivo.\n",
            "All of us look alike. = ¡A todos nosotros son iglesias!\n",
            "Let's hope Tom doesn't walk in. = Esperemos que Tom no entrá.\n",
            "She made him go there himself. = Ella le hizo ir allí en su su y cuando él se fue allá.\n",
            "The boy is very interested in climbing mountains. = El niño se interesan en montaña arrearse.\n",
            "Are they Japanese? = ¿Son japoneses?\n",
            "They can't understand you. = No pueden entenderlo.\n",
            "I love him all of the time. = Lo amo todo el tiempo.\n",
            "He is a good student. = Es un buen estudiante.\n",
            "I think I'll start doing that today. = Me parece a que empezará hoy.\n",
            "You were right, Tom. = Tenías razón, Tom.\n",
            "He is just like you. = Es ni siquiera como tú.\n",
            "My father gets up early in the morning. = Mi padre se levanta temprano por la mañana.\n",
            "Can you solve this problem? = ¿Puede resolver este problema?\n",
            "The old man lives alone. = El hombre vieja vive solo.\n",
            "What exactly did you do there? = ¿Qué hiciste eso exactamente qué hiciste allí?\n",
            "I would much rather be a bird than a fish. = Preferiría ser un pájaro a ser un pez.\n",
            "We are having tea now. = El té est\n",
            "\n",
            "[20000 | 6956.86] loss=0.89 avg=0.84\n",
            "[20001 | 6959.95] loss=0.89 avg=0.84\n",
            "[20002 | 6963.04] loss=0.80 avg=0.84\n",
            "[20003 | 6966.14] loss=0.82 avg=0.84\n",
            "[20004 | 6969.23] loss=0.73 avg=0.84\n",
            "[20005 | 6972.32] loss=0.93 avg=0.84\n",
            "[20006 | 6975.40] loss=0.79 avg=0.84\n",
            "[20007 | 6978.48] loss=0.69 avg=0.84\n",
            "[20008 | 6981.58] loss=0.93 avg=0.84\n",
            "[20009 | 6984.68] loss=0.87 avg=0.84\n",
            "[20010 | 6987.77] loss=0.89 avg=0.84\n",
            "[20011 | 6990.85] loss=0.69 avg=0.84\n",
            "[20012 | 6993.95] loss=0.94 avg=0.84\n",
            "[20013 | 6997.04] loss=0.93 avg=0.84\n",
            "[20014 | 7000.13] loss=0.96 avg=0.84\n",
            "[20015 | 7003.21] loss=0.72 avg=0.84\n",
            "[20016 | 7006.30] loss=0.89 avg=0.84\n",
            "[20017 | 7009.39] loss=0.74 avg=0.84\n",
            "[20018 | 7012.50] loss=0.95 avg=0.84\n",
            "[20019 | 7015.61] loss=0.81 avg=0.84\n",
            "[20020 | 7018.69] loss=0.77 avg=0.84\n",
            "[20021 | 7021.79] loss=0.77 avg=0.84\n",
            "[20022 | 7024.88] loss=0.95 avg=0.84\n",
            "[20023 | 7027.99] loss=0.78 avg=0.84\n",
            "[20024 | 7031.08] loss=0.74 avg=0.84\n",
            "[20025 | 7034.19] loss=0.84 avg=0.84\n",
            "[20026 | 7037.31] loss=0.86 avg=0.84\n",
            "[20027 | 7040.41] loss=0.73 avg=0.84\n",
            "[20028 | 7043.50] loss=0.87 avg=0.84\n",
            "[20029 | 7046.59] loss=0.90 avg=0.84\n",
            "[20030 | 7049.67] loss=0.73 avg=0.84\n",
            "[20031 | 7052.77] loss=0.86 avg=0.84\n",
            "[20032 | 7055.87] loss=0.89 avg=0.84\n",
            "[20033 | 7058.98] loss=0.81 avg=0.84\n",
            "[20034 | 7062.07] loss=0.86 avg=0.84\n",
            "[20035 | 7065.17] loss=0.75 avg=0.84\n",
            "[20036 | 7068.27] loss=0.75 avg=0.84\n",
            "[20037 | 7071.36] loss=0.98 avg=0.84\n",
            "[20038 | 7074.46] loss=0.89 avg=0.84\n",
            "[20039 | 7077.55] loss=0.83 avg=0.84\n",
            "[20040 | 7080.65] loss=0.90 avg=0.84\n",
            "[20041 | 7083.75] loss=0.78 avg=0.84\n",
            "[20042 | 7086.84] loss=0.67 avg=0.84\n",
            "[20043 | 7089.94] loss=0.77 avg=0.84\n",
            "[20044 | 7093.02] loss=0.73 avg=0.84\n",
            "[20045 | 7096.11] loss=0.77 avg=0.83\n",
            "[20046 | 7099.19] loss=0.96 avg=0.84\n",
            "[20047 | 7102.29] loss=0.92 avg=0.84\n",
            "[20048 | 7105.37] loss=0.73 avg=0.84\n",
            "[20049 | 7108.46] loss=0.92 avg=0.84\n",
            "[20050 | 7111.54] loss=0.71 avg=0.84\n",
            "[20051 | 7114.62] loss=0.83 avg=0.84\n",
            "[20052 | 7117.72] loss=0.86 avg=0.84\n",
            "[20053 | 7120.81] loss=0.70 avg=0.83\n",
            "[20054 | 7123.90] loss=0.92 avg=0.83\n",
            "[20055 | 7126.98] loss=0.85 avg=0.84\n",
            "[20056 | 7130.09] loss=0.90 avg=0.84\n",
            "[20057 | 7133.18] loss=0.97 avg=0.84\n",
            "[20058 | 7136.28] loss=0.87 avg=0.84\n",
            "[20059 | 7139.38] loss=0.92 avg=0.84\n",
            "[20060 | 7142.47] loss=0.86 avg=0.84\n",
            "[20061 | 7145.55] loss=0.85 avg=0.84\n",
            "[20062 | 7148.64] loss=1.00 avg=0.84\n",
            "[20063 | 7151.73] loss=0.70 avg=0.84\n",
            "[20064 | 7154.84] loss=0.86 avg=0.84\n",
            "[20065 | 7157.95] loss=0.93 avg=0.84\n",
            "[20066 | 7161.04] loss=0.87 avg=0.84\n",
            "[20067 | 7164.14] loss=0.71 avg=0.84\n",
            "[20068 | 7167.24] loss=0.83 avg=0.84\n",
            "[20069 | 7170.34] loss=0.77 avg=0.84\n",
            "[20070 | 7173.43] loss=0.83 avg=0.84\n",
            "[20071 | 7176.53] loss=0.81 avg=0.84\n",
            "[20072 | 7179.62] loss=0.77 avg=0.84\n",
            "[20073 | 7182.73] loss=0.62 avg=0.83\n",
            "[20074 | 7185.81] loss=0.71 avg=0.83\n",
            "[20075 | 7188.91] loss=0.84 avg=0.83\n",
            "[20076 | 7192.00] loss=0.94 avg=0.83\n",
            "[20077 | 7195.10] loss=0.95 avg=0.84\n",
            "[20078 | 7198.18] loss=0.96 avg=0.84\n",
            "[20079 | 7201.27] loss=0.80 avg=0.84\n",
            "[20080 | 7204.36] loss=0.98 avg=0.84\n",
            "[20081 | 7207.46] loss=0.82 avg=0.84\n",
            "[20082 | 7210.55] loss=0.77 avg=0.84\n",
            "[20083 | 7213.65] loss=0.79 avg=0.84\n",
            "[20084 | 7216.70] loss=1.04 avg=0.84\n",
            "[20085 | 7219.76] loss=0.79 avg=0.84\n",
            "[20086 | 7222.85] loss=0.84 avg=0.84\n",
            "[20087 | 7225.94] loss=0.72 avg=0.84\n",
            "[20088 | 7229.02] loss=0.82 avg=0.84\n",
            "[20089 | 7232.11] loss=0.85 avg=0.84\n",
            "[20090 | 7235.19] loss=0.97 avg=0.84\n",
            "[20091 | 7238.28] loss=0.82 avg=0.84\n",
            "[20092 | 7241.36] loss=0.71 avg=0.84\n",
            "[20093 | 7244.42] loss=0.76 avg=0.84\n",
            "[20094 | 7247.51] loss=0.96 avg=0.84\n",
            "[20095 | 7250.61] loss=0.85 avg=0.84\n",
            "[20096 | 7253.70] loss=0.88 avg=0.84\n",
            "[20097 | 7256.77] loss=0.83 avg=0.84\n",
            "[20098 | 7259.86] loss=0.94 avg=0.84\n",
            "[20099 | 7262.94] loss=0.92 avg=0.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " the same size as mine. = La lengua de Tom es lo mismo de la mía.\n",
            "We've been talking about this topic all day. = Hemos estado hablando de este texto todo el día.\n",
            "You want me to help you, don't you? = Quieres que te ayude, ¿verdad?\n",
            "Tom isn't as talkative as you. = Tom no es tan parlanchín como tú.\n",
            "The house was not destroyed by a flood. = La casa no fue destruida por un gullión.\n",
            "You're the only person I know in Boston that enjoys eating out every night. = Eres la única persona de Boston que conozco a comer todo por esta noche.\n",
            "Is today Friday? = ¿Es viernes hoy?\n",
            "It's possible that he will return to his seat. = Es posible que regrese a su asiento.\n",
            "We went in the dark. = Fuimos en la oscuridad.\n",
            "Tom wasn't very interested in the new movie we showed him. = Tom no estaba muy interesado en la nueva película que le mostele.\n",
            "Can you give me an estimate of how much this costs? = ¿Me puede dar un doble del anteación de comprar esto?\n",
            "We need more space. = Necesitamos más espacio.\n",
            "I told Tom it was my mistake. = Le dije a Tom que fue mi error.\n",
            "I think this could be dangerous. = Creo que podría ser peligroso.\n",
            "I don't know why they do it. = No sé por qué les hacen.\n",
            "I've never seen a rainbow. = Nunca he visto un arco iris.\n",
            "I'm not interested in Italian food. = No estoy interesada en la comida italiana.\n",
            "Please take this medicine twice a day. = Por favor 2 coron este medicamento dos veces al día.\n",
            "I'll be watching you just like you are. = Te estaré observando igual de ti mismo.\n",
            "The weatherman says that it will rain between eight and nine. = El hombre del tiempo dice que tendremos lluvia entre a las ocho y no.\n",
            "It's the least we can do for now. = Es por lo menos que podemos hacer por ahora.\n",
            "He often comes late, often staying a few minutes late. = A menudo llega tarde, por casualidad se quede que se queda un par de minutos tarde.\n",
            "Let's take a taxi. = Tomemos un taxi.\n",
            "Do you really want to wait like this? = ¿De verdad quieres esperar así?\n",
            "My family is not the type to take time off. = Mi familia no es el tipo de tomar tiempo.\n",
            "He's a good artist. = Él es un buen artista.\n",
            "I was just about to go out when it began raining. = Justo estaba a punto de salir cuando empezó a llover.\n",
            "I like watching movies. = Me gusta ver películas.\n",
            "The king and the queen visited the students. = El rey y la reina visitaron a los estudiantes.\n",
            "I'm going out for a walk. = Voy a salir a dar un paseo.\n",
            "I have one question for you. = Tengo una pregunta para vosotros.\n",
            "I didn't think it was that bad. = No pensaba que no era tan malo.\n",
            "You have to be more patient. = Tenéis que ser más pacientes.\n",
            "Tom was looking for the key. = Tom buscaba la llave.\n",
            "I will teach you how to play tennis. = Te enseñaré cómo tocar al tenis.\n",
            "You haven't said a word. = No has dicho ni una palabra.\n",
            "Have you tried the cake we had last night? = ¿Has intentado el pastel que nos había pasado anoche?\n",
            "What are you drinking? = ¿Qué estás tomando?\n",
            "Did you catch anything? = ¿Le cavaron algo?\n",
            "Tom's still sick. = Tom still está enfermo.\n",
            "He likes to learn new things. = A él le gusta aprender cosas nuevas.\n",
            "I don't think he's capable of doing it. = No pienso que él sea capaz de hacerlo.\n",
            "\n",
            "[20100 | 7303.57] loss=0.80 avg=0.84\n",
            "[20101 | 7306.67] loss=0.86 avg=0.84\n",
            "[20102 | 7309.76] loss=0.68 avg=0.84\n",
            "[20103 | 7312.84] loss=0.79 avg=0.84\n",
            "[20104 | 7315.92] loss=0.88 avg=0.84\n",
            "[20105 | 7319.00] loss=0.84 avg=0.84\n",
            "[20106 | 7322.07] loss=0.89 avg=0.84\n",
            "[20107 | 7325.16] loss=0.90 avg=0.84\n",
            "[20108 | 7328.25] loss=0.87 avg=0.84\n",
            "[20109 | 7331.32] loss=0.91 avg=0.84\n",
            "[20110 | 7334.41] loss=0.75 avg=0.84\n",
            "[20111 | 7337.48] loss=0.78 avg=0.84\n",
            "[20112 | 7340.58] loss=0.76 avg=0.84\n",
            "[20113 | 7343.68] loss=0.86 avg=0.84\n",
            "[20114 | 7346.76] loss=0.86 avg=0.84\n",
            "[20115 | 7349.85] loss=0.75 avg=0.84\n",
            "[20116 | 7352.95] loss=0.70 avg=0.84\n",
            "[20117 | 7356.04] loss=0.81 avg=0.84\n",
            "[20118 | 7359.13] loss=0.70 avg=0.83\n",
            "[20119 | 7362.21] loss=0.84 avg=0.83\n",
            "[20120 | 7365.31] loss=0.77 avg=0.83\n",
            "[20121 | 7368.40] loss=0.87 avg=0.83\n",
            "[20122 | 7371.49] loss=0.79 avg=0.83\n",
            "[20123 | 7374.57] loss=0.82 avg=0.83\n",
            "[20124 | 7377.66] loss=0.67 avg=0.83\n",
            "[20125 | 7380.75] loss=0.81 avg=0.83\n",
            "[20126 | 7383.85] loss=0.93 avg=0.83\n",
            "[20127 | 7386.95] loss=0.89 avg=0.83\n",
            "[20128 | 7390.06] loss=0.77 avg=0.83\n",
            "[20129 | 7393.17] loss=0.93 avg=0.83\n",
            "[20130 | 7396.26] loss=0.84 avg=0.83\n",
            "[20131 | 7399.33] loss=0.75 avg=0.83\n",
            "[20132 | 7402.42] loss=0.82 avg=0.83\n",
            "[20133 | 7405.50] loss=0.78 avg=0.83\n",
            "[20134 | 7408.59] loss=0.89 avg=0.83\n",
            "[20135 | 7411.66] loss=0.97 avg=0.83\n",
            "[20136 | 7414.72] loss=0.72 avg=0.83\n",
            "[20137 | 7417.78] loss=0.77 avg=0.83\n",
            "[20138 | 7420.85] loss=0.92 avg=0.83\n",
            "[20139 | 7423.91] loss=0.88 avg=0.83\n",
            "[20140 | 7426.99] loss=0.76 avg=0.83\n",
            "[20141 | 7430.07] loss=1.03 avg=0.83\n",
            "[20142 | 7433.14] loss=0.90 avg=0.84\n",
            "[20143 | 7436.21] loss=0.88 avg=0.84\n",
            "[20144 | 7439.30] loss=0.82 avg=0.84\n",
            "[20145 | 7442.38] loss=0.96 avg=0.84\n",
            "[20146 | 7445.49] loss=0.89 avg=0.84\n",
            "[20147 | 7448.59] loss=0.83 avg=0.84\n",
            "[20148 | 7451.68] loss=0.68 avg=0.84\n",
            "[20149 | 7454.75] loss=0.93 avg=0.84\n",
            "[20150 | 7457.85] loss=0.73 avg=0.84\n",
            "[20151 | 7460.95] loss=0.75 avg=0.83\n",
            "[20152 | 7464.05] loss=0.92 avg=0.84\n",
            "[20153 | 7467.14] loss=0.76 avg=0.83\n",
            "[20154 | 7470.23] loss=0.89 avg=0.84\n",
            "[20155 | 7473.32] loss=0.72 avg=0.83\n",
            "[20156 | 7476.41] loss=0.80 avg=0.83\n",
            "[20157 | 7479.51] loss=0.90 avg=0.83\n",
            "[20158 | 7482.60] loss=0.89 avg=0.84\n",
            "[20159 | 7485.66] loss=0.96 avg=0.84\n",
            "[20160 | 7488.76] loss=0.95 avg=0.84\n",
            "[20161 | 7491.84] loss=0.92 avg=0.84\n",
            "[20162 | 7494.92] loss=0.99 avg=0.84\n",
            "[20163 | 7497.99] loss=1.00 avg=0.84\n",
            "[20164 | 7501.07] loss=0.69 avg=0.84\n",
            "[20165 | 7504.14] loss=0.82 avg=0.84\n",
            "[20166 | 7507.22] loss=0.78 avg=0.84\n",
            "[20167 | 7510.29] loss=0.72 avg=0.84\n",
            "[20168 | 7513.38] loss=0.84 avg=0.84\n",
            "[20169 | 7516.46] loss=0.68 avg=0.84\n",
            "[20170 | 7519.55] loss=0.75 avg=0.84\n",
            "[20171 | 7522.63] loss=0.77 avg=0.83\n",
            "[20172 | 7525.73] loss=0.83 avg=0.83\n",
            "[20173 | 7528.81] loss=0.81 avg=0.83\n",
            "[20174 | 7531.88] loss=0.91 avg=0.84\n",
            "[20175 | 7534.95] loss=0.80 avg=0.83\n",
            "[20176 | 7538.02] loss=0.63 avg=0.83\n",
            "[20177 | 7541.09] loss=0.85 avg=0.83\n",
            "[20178 | 7544.15] loss=0.87 avg=0.83\n",
            "[20179 | 7547.21] loss=0.87 avg=0.83\n",
            "[20180 | 7550.28] loss=0.77 avg=0.83\n",
            "[20181 | 7553.36] loss=0.92 avg=0.83\n",
            "[20182 | 7556.46] loss=0.82 avg=0.83\n",
            "[20183 | 7559.54] loss=0.82 avg=0.83\n",
            "[20184 | 7562.62] loss=0.71 avg=0.83\n",
            "[20185 | 7565.70] loss=0.82 avg=0.83\n",
            "[20186 | 7568.78] loss=0.90 avg=0.83\n",
            "[20187 | 7571.87] loss=0.81 avg=0.83\n",
            "[20188 | 7574.95] loss=0.84 avg=0.83\n",
            "[20189 | 7578.02] loss=0.71 avg=0.83\n",
            "[20190 | 7581.11] loss=0.73 avg=0.83\n",
            "[20191 | 7584.21] loss=0.74 avg=0.83\n",
            "[20192 | 7587.30] loss=0.72 avg=0.83\n",
            "[20193 | 7590.39] loss=0.85 avg=0.83\n",
            "[20194 | 7593.47] loss=0.82 avg=0.83\n",
            "[20195 | 7596.54] loss=0.89 avg=0.83\n",
            "[20196 | 7599.60] loss=0.78 avg=0.83\n",
            "[20197 | 7602.69] loss=0.87 avg=0.83\n",
            "[20198 | 7605.79] loss=0.78 avg=0.83\n",
            "[20199 | 7608.90] loss=0.71 avg=0.83\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " cámara a Tom?\n",
            "Somebody wants to talk to you. = Alguien quiere hablar con usted.\n",
            "He went to see her. = Él fue a verla.\n",
            "There were some people in the hall who did not want to give us a concert. = Había algunas personas en la sala que no nos quería dar una concierto.\n",
            "Tom didn't want to argue with Mary. = Tom no quiso discutir con Mary.\n",
            "Tom had no chance to talk to Mary. = Tom no tuvo oportunidad de hablar con Mary.\n",
            "If I had eaten breakfast this morning, I would not be hungry now. = Si yo hubiera desayunado esta mañana ahora no tendría hambre.\n",
            "Her face is red. = Su cara es rojo.\n",
            "You can't imagine what we went through, yet, still smiling. = No te puedes imaginar lo que hicimos, pero tardas, still sonriendo.\n",
            "I don't know where to open the box. = No sé dónde abre la caja.\n",
            "What did you go to church for? = ¿Para qué fuiste aledor del gitarra?\n",
            "We're going in that direction. = Nos vamos a ir en esa dirección.\n",
            "We are glad you will be coming. = Nos alegramos que venga ustedes.\n",
            "It was necessary to escape from the difficulties. = Había que escapar del desacuerdo.\n",
            "The old man has lived abroad. = El anciano ha vivido en el extranjero.\n",
            "I am sure of getting what I want. = Estoy seguro de recobrarno lo que yo quiero.\n",
            "We'll need a little more time. = Necesitaremos un poco más de tiempo.\n",
            "The old man is sick. = El anciano está enfermo.\n",
            "You are an idiot. = Sos un idiota.\n",
            "This is the first time I've sung in public. = Esta es la primera vez que he cantado en público.\n",
            "My house is somewhere around here. = Mi casa esta alrededor de aquí.\n",
            "I can't stop laughing. = No puedo dejar de reír.\n",
            "I am very tired of it. = Estoy muy cansado de ello.\n",
            "I think I would've liked Tom better if he had come earlier. = Creo que me habría apreciado Tom más si había venido antes.\n",
            "I will keep that knowledge to myself. = Guardaré esa conocimiento para mí mismo.\n",
            "Don't let go of the rope, or you'll catch something sharp. = No sueltes la cuerda, o te te vas a molestar algo acertada.\n",
            "He is about to leave. = Él está a punto de irse.\n",
            "The children were busy. = Los niños estaban ocupados.\n",
            "No other city in Japan is as large as Tokyo. = No hay otra ciudad de Japón más grande como Tōkyoku.\n",
            "Tom gave me a book. = Tom me dio un libro.\n",
            "\"May I have one of these?\" \"You don't have to buy it.\" = \"¿Puede darme uno de éstos?\" \"Tú no tienes que compreslo.\"\n",
            "Don't be afraid of him if you don't have anything to hide from me. = No le tengas miedo si no tengas nada para esconderme.\n",
            "I saw Tom kissing Mary. = Vi a Tom besar a Mary.\n",
            "This story is very hard to understand. = Esta historia es muy difícil de entender.\n",
            "My father often reads magazines on the subway. = Mi padre suele leer revistas por el metro.\n",
            "Tom bought Mary a bottle of vodka. = Tom le compró una botella de fermenta a Mary.\n",
            "Tom is a professional baseball player. = Tomás es un testigo de béisbol profesional.\n",
            "I haven't forgotten you. = No te he olvidado.\n",
            "They don't know how to write their names. = No saben cómo escribir sus nombres.\n",
            "There's a big gap in the floor. = Hay un gran puente en la pieza.\n",
            "I'll have to help Tom. = Yo tendré que ayudar a Tom.\n",
            "A new car will\n",
            "\n",
            "[20200 | 7649.97] loss=0.86 avg=0.83\n",
            "[20201 | 7653.04] loss=0.88 avg=0.83\n",
            "[20202 | 7656.11] loss=0.74 avg=0.83\n",
            "[20203 | 7659.16] loss=0.83 avg=0.83\n",
            "[20204 | 7662.25] loss=0.89 avg=0.83\n",
            "[20205 | 7665.34] loss=0.89 avg=0.83\n",
            "[20206 | 7668.43] loss=0.78 avg=0.83\n",
            "[20207 | 7671.52] loss=0.92 avg=0.83\n",
            "[20208 | 7674.59] loss=0.78 avg=0.83\n",
            "[20209 | 7677.65] loss=0.74 avg=0.83\n",
            "[20210 | 7680.75] loss=0.75 avg=0.83\n",
            "[20211 | 7683.85] loss=0.96 avg=0.83\n",
            "[20212 | 7686.96] loss=0.71 avg=0.83\n",
            "[20213 | 7690.02] loss=0.70 avg=0.83\n",
            "[20214 | 7693.09] loss=0.84 avg=0.83\n",
            "[20215 | 7696.15] loss=0.98 avg=0.83\n",
            "[20216 | 7699.21] loss=0.79 avg=0.83\n",
            "[20217 | 7702.29] loss=0.81 avg=0.83\n",
            "[20218 | 7705.35] loss=0.82 avg=0.83\n",
            "[20219 | 7708.43] loss=0.87 avg=0.83\n",
            "[20220 | 7711.51] loss=0.80 avg=0.83\n",
            "[20221 | 7714.60] loss=0.65 avg=0.83\n",
            "[20222 | 7717.67] loss=0.75 avg=0.83\n",
            "[20223 | 7720.73] loss=0.94 avg=0.83\n",
            "[20224 | 7723.80] loss=0.79 avg=0.83\n",
            "[20225 | 7726.87] loss=0.78 avg=0.83\n",
            "[20226 | 7729.95] loss=0.86 avg=0.83\n",
            "[20227 | 7733.05] loss=0.92 avg=0.83\n",
            "[20228 | 7736.13] loss=0.90 avg=0.83\n",
            "[20229 | 7739.21] loss=0.67 avg=0.83\n",
            "[20230 | 7742.28] loss=0.71 avg=0.82\n",
            "[20231 | 7745.37] loss=1.00 avg=0.83\n",
            "[20232 | 7748.44] loss=0.88 avg=0.83\n",
            "[20233 | 7751.50] loss=0.84 avg=0.83\n",
            "[20234 | 7754.60] loss=0.95 avg=0.83\n",
            "[20235 | 7757.68] loss=0.88 avg=0.83\n",
            "[20236 | 7760.78] loss=0.97 avg=0.83\n",
            "[20237 | 7763.88] loss=0.69 avg=0.83\n",
            "[20238 | 7766.97] loss=0.99 avg=0.83\n",
            "[20239 | 7770.05] loss=0.89 avg=0.83\n",
            "[20240 | 7773.13] loss=0.97 avg=0.83\n",
            "[20241 | 7776.21] loss=0.87 avg=0.83\n",
            "[20242 | 7779.30] loss=0.75 avg=0.83\n",
            "[20243 | 7782.38] loss=0.74 avg=0.83\n",
            "[20244 | 7785.46] loss=0.70 avg=0.83\n",
            "[20245 | 7788.54] loss=0.75 avg=0.83\n",
            "[20246 | 7791.62] loss=0.89 avg=0.83\n",
            "[20247 | 7794.70] loss=0.81 avg=0.83\n",
            "[20248 | 7797.81] loss=0.77 avg=0.83\n",
            "[20249 | 7800.92] loss=0.77 avg=0.83\n",
            "[20250 | 7804.01] loss=0.84 avg=0.83\n",
            "[20251 | 7807.09] loss=0.87 avg=0.83\n",
            "[20252 | 7810.17] loss=0.69 avg=0.83\n",
            "[20253 | 7813.26] loss=0.81 avg=0.83\n",
            "[20254 | 7816.31] loss=0.92 avg=0.83\n",
            "[20255 | 7819.39] loss=0.89 avg=0.83\n",
            "[20256 | 7822.47] loss=0.82 avg=0.83\n",
            "[20257 | 7825.57] loss=0.97 avg=0.83\n",
            "[20258 | 7828.68] loss=0.82 avg=0.83\n",
            "[20259 | 7831.80] loss=0.87 avg=0.83\n",
            "[20260 | 7834.94] loss=0.87 avg=0.83\n",
            "[20261 | 7838.05] loss=0.81 avg=0.83\n",
            "[20262 | 7841.16] loss=0.89 avg=0.83\n",
            "[20263 | 7844.26] loss=0.70 avg=0.83\n",
            "[20264 | 7847.34] loss=0.77 avg=0.83\n",
            "[20265 | 7850.40] loss=0.80 avg=0.83\n",
            "[20266 | 7853.46] loss=0.86 avg=0.83\n",
            "[20267 | 7856.54] loss=0.98 avg=0.83\n",
            "[20268 | 7859.61] loss=0.84 avg=0.83\n",
            "[20269 | 7862.69] loss=0.73 avg=0.83\n",
            "[20270 | 7865.77] loss=0.82 avg=0.83\n",
            "[20271 | 7868.84] loss=0.78 avg=0.83\n",
            "[20272 | 7871.90] loss=0.88 avg=0.83\n",
            "[20273 | 7874.94] loss=0.71 avg=0.83\n",
            "[20274 | 7878.01] loss=0.90 avg=0.83\n",
            "[20275 | 7881.10] loss=0.79 avg=0.83\n",
            "[20276 | 7884.17] loss=0.78 avg=0.83\n",
            "[20277 | 7887.25] loss=0.79 avg=0.83\n",
            "[20278 | 7890.32] loss=0.78 avg=0.83\n",
            "[20279 | 7893.40] loss=0.87 avg=0.83\n",
            "[20280 | 7896.48] loss=0.96 avg=0.83\n",
            "[20281 | 7899.53] loss=1.02 avg=0.83\n",
            "[20282 | 7902.61] loss=0.74 avg=0.83\n",
            "[20283 | 7905.69] loss=0.98 avg=0.83\n",
            "[20284 | 7908.77] loss=0.96 avg=0.83\n",
            "[20285 | 7911.84] loss=0.84 avg=0.83\n",
            "[20286 | 7914.90] loss=0.73 avg=0.83\n",
            "[20287 | 7917.96] loss=0.77 avg=0.83\n",
            "[20288 | 7921.03] loss=0.93 avg=0.83\n",
            "[20289 | 7924.10] loss=0.82 avg=0.83\n",
            "[20290 | 7927.19] loss=0.80 avg=0.83\n",
            "[20291 | 7930.24] loss=0.87 avg=0.83\n",
            "[20292 | 7933.33] loss=0.77 avg=0.83\n",
            "[20293 | 7936.40] loss=0.95 avg=0.83\n",
            "[20294 | 7939.48] loss=0.91 avg=0.83\n",
            "[20295 | 7942.54] loss=0.97 avg=0.84\n",
            "[20296 | 7945.63] loss=0.89 avg=0.84\n",
            "[20297 | 7948.73] loss=0.87 avg=0.84\n",
            "[20298 | 7951.83] loss=0.72 avg=0.83\n",
            "[20299 | 7954.92] loss=0.96 avg=0.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ie.\n",
            "The problem is that money can't buy happiness. = El problema es que el dinero no puede comprar la felicidad.\n",
            "I don't need to do that. = No necesito hacer eso.\n",
            "This book belonged to you. = Este libro le pertenecía.\n",
            "Tom is a musician. = Tom es un músico.\n",
            "Don't let anyone else. = No dejes que alguien más.\n",
            "The police have already surrounded the building. = La Policía ya se llevaron al edificio.\n",
            "This is not a good time for us to be together. = No es un buen momento para estar juntos.\n",
            "He didn't give in to the temptation. = Él no cedió a la tentación.\n",
            "It was not easy to persuade him. = No fue fácil convencerle.\n",
            "I just thought Tom was British. = Simplemente creí que Tom era uno británico.\n",
            "This is my car. Where can I park it? = Este es mi coche. ¿Cuándo puedo aparcar encima?\n",
            "I am at home, and I can't wait till tomorrow. = Estoy en casa y no puedo esperar hasta mañana.\n",
            "Tom asked Mary to sign a new lease of life. = Tom le pidió a Mary que firmara un nuevo leído de vida.\n",
            "There's no point in waiting for Tom. = Esperar hacer eso de nuevo, pero es un vistazo para Tom.\n",
            "Tom wants to see Mary face to face. = Tom quiere ver a Mary cara a cara.\n",
            "I want to give you something as good a present as you could ask for. = Quiero darte algo tan bueno como podrías pedirlo.\n",
            "Tom told us not to be afraid. = Tomás nos dijo que no teníamos miedo.\n",
            "I heard the story from him. = Escuché ese conte de él.\n",
            "His plan will be implemented soon. = Su plan se celebrará pronto.\n",
            "Give me that. = Dame eso.\n",
            "Tom is out of breath. = Tom se está sin aliento.\n",
            "I'm still looking for a job. = Aún estoy buscando trabajo.\n",
            "The door was locked from the outside. = La puerta debió de llave a la interior.\n",
            "Tom is a liar. = Tom es un mentiroso.\n",
            "Tom is not as stupid as he looks. = Tom no es tan estúpido como parece.\n",
            "We're not here to have a good time. = No estamos aquí para alandlos.\n",
            "It's so hard to tell which bank he's referring to. = Es tan difícil distinguir más de qué banco él está referiendo.\n",
            "He never talks about it. = Nunca habla al respecto.\n",
            "I don't like the sun. = No me gusta el sol.\n",
            "It seems that the plane is delayed. = Parece que el avión está retrasado.\n",
            "This is an order. = Esto es una orden.\n",
            "She asked him to leave the baby in the cradle. = Le pidió que dejara la bebé en el cinturón.\n",
            "If he calls again, I don't know what to say. = Si llama otra vez, no sé qué decir.\n",
            "I don't feel like meeting her. = No tengo ganas de reunirme con ella.\n",
            "Tom has a beautiful voice. = Tom tiene una hermosa voz.\n",
            "There are some problems we have to solve. = Hay algunos problemas que tenemos que resolver.\n",
            "I am going to play baseball in the afternoon. = Voy a jugar al béisbol por la tarde.\n",
            "Tom asked Mary how many people had come to see him. = Tom preguntó a Mary cuántas personas habían venido a verle.\n",
            "This problem is too difficult. = Este problema es demasiado difícil.\n",
            "I can't make out what he says. = No puedo entender lo que él dice.\n",
            "There is no excuse for his delay. = Su retraso no admite lugar a torturas.\n",
            "The teacher told the boy not to cross the road. = El maestro denunció al niño que no cruce la calle.\n",
            "Don't get cocky. =\n",
            "\n",
            "[20300 | 7995.64] loss=0.91 avg=0.84\n",
            "[20301 | 7998.73] loss=0.89 avg=0.84\n",
            "[20302 | 8001.80] loss=0.80 avg=0.84\n",
            "[20303 | 8004.88] loss=0.88 avg=0.84\n",
            "[20304 | 8007.98] loss=0.87 avg=0.84\n",
            "[20305 | 8011.07] loss=0.76 avg=0.84\n",
            "[20306 | 8014.15] loss=0.77 avg=0.84\n",
            "[20307 | 8017.24] loss=0.95 avg=0.84\n",
            "[20308 | 8020.31] loss=0.76 avg=0.84\n",
            "[20309 | 8023.38] loss=0.91 avg=0.84\n",
            "[20310 | 8026.47] loss=0.87 avg=0.84\n",
            "[20311 | 8029.55] loss=0.66 avg=0.84\n",
            "[20312 | 8032.63] loss=0.90 avg=0.84\n",
            "[20313 | 8035.71] loss=0.88 avg=0.84\n",
            "[20314 | 8038.81] loss=0.66 avg=0.84\n",
            "[20315 | 8041.91] loss=0.82 avg=0.84\n",
            "[20316 | 8045.01] loss=0.75 avg=0.83\n",
            "[20317 | 8048.12] loss=0.69 avg=0.83\n",
            "[20318 | 8051.22] loss=0.67 avg=0.83\n",
            "[20319 | 8054.31] loss=0.94 avg=0.83\n",
            "[20320 | 8057.40] loss=0.83 avg=0.83\n",
            "[20321 | 8060.49] loss=0.94 avg=0.83\n",
            "[20322 | 8063.59] loss=0.87 avg=0.83\n",
            "[20323 | 8066.69] loss=0.78 avg=0.83\n",
            "[20324 | 8069.78] loss=0.97 avg=0.83\n",
            "[20325 | 8072.89] loss=0.79 avg=0.83\n",
            "[20326 | 8075.98] loss=0.92 avg=0.84\n",
            "[20327 | 8079.08] loss=0.83 avg=0.84\n",
            "[20328 | 8082.17] loss=0.81 avg=0.83\n",
            "[20329 | 8085.27] loss=0.65 avg=0.83\n",
            "[20330 | 8088.38] loss=0.82 avg=0.83\n",
            "[20331 | 8091.47] loss=0.75 avg=0.83\n",
            "[20332 | 8094.57] loss=0.81 avg=0.83\n",
            "[20333 | 8097.66] loss=0.91 avg=0.83\n",
            "[20334 | 8100.75] loss=0.79 avg=0.83\n",
            "[20335 | 8103.85] loss=0.70 avg=0.83\n",
            "[20336 | 8106.95] loss=0.66 avg=0.83\n",
            "[20337 | 8110.05] loss=0.75 avg=0.83\n",
            "[20338 | 8113.15] loss=0.80 avg=0.83\n",
            "[20339 | 8116.25] loss=0.94 avg=0.83\n",
            "[20340 | 8119.33] loss=0.71 avg=0.83\n",
            "[20341 | 8122.43] loss=0.77 avg=0.83\n",
            "[20342 | 8125.52] loss=0.77 avg=0.83\n",
            "[20343 | 8128.61] loss=0.82 avg=0.83\n",
            "[20344 | 8131.70] loss=0.87 avg=0.83\n",
            "[20345 | 8134.81] loss=0.85 avg=0.83\n",
            "[20346 | 8137.90] loss=0.74 avg=0.83\n",
            "[20347 | 8140.99] loss=0.83 avg=0.83\n",
            "[20348 | 8144.08] loss=0.94 avg=0.83\n",
            "[20349 | 8147.17] loss=0.68 avg=0.83\n",
            "[20350 | 8150.28] loss=0.89 avg=0.83\n",
            "[20351 | 8153.37] loss=0.86 avg=0.83\n",
            "[20352 | 8156.45] loss=0.69 avg=0.83\n",
            "[20353 | 8159.53] loss=0.81 avg=0.83\n",
            "[20354 | 8162.61] loss=1.02 avg=0.83\n",
            "[20355 | 8165.71] loss=0.84 avg=0.83\n",
            "[20356 | 8168.81] loss=0.96 avg=0.83\n",
            "[20357 | 8171.91] loss=0.88 avg=0.83\n",
            "[20358 | 8174.99] loss=0.88 avg=0.83\n",
            "[20359 | 8178.07] loss=0.70 avg=0.83\n",
            "[20360 | 8181.16] loss=0.85 avg=0.83\n",
            "[20361 | 8184.23] loss=0.91 avg=0.83\n",
            "[20362 | 8187.30] loss=0.99 avg=0.83\n",
            "[20363 | 8190.38] loss=0.76 avg=0.83\n",
            "[20364 | 8193.45] loss=0.79 avg=0.83\n",
            "[20365 | 8196.55] loss=0.85 avg=0.83\n",
            "[20366 | 8199.63] loss=0.80 avg=0.83\n",
            "[20367 | 8202.72] loss=0.91 avg=0.83\n",
            "[20368 | 8205.79] loss=0.82 avg=0.83\n",
            "[20369 | 8208.83] loss=0.83 avg=0.83\n",
            "[20370 | 8211.92] loss=0.91 avg=0.83\n",
            "[20371 | 8215.00] loss=0.89 avg=0.83\n",
            "[20372 | 8218.10] loss=0.83 avg=0.83\n",
            "[20373 | 8221.19] loss=0.80 avg=0.83\n",
            "[20374 | 8224.27] loss=0.86 avg=0.83\n",
            "[20375 | 8227.34] loss=0.79 avg=0.83\n",
            "[20376 | 8230.42] loss=0.70 avg=0.83\n",
            "[20377 | 8233.48] loss=0.78 avg=0.83\n",
            "[20378 | 8236.57] loss=0.68 avg=0.83\n",
            "[20379 | 8239.63] loss=0.65 avg=0.83\n",
            "[20380 | 8242.69] loss=0.71 avg=0.83\n",
            "[20381 | 8245.76] loss=0.84 avg=0.83\n",
            "[20382 | 8248.82] loss=0.89 avg=0.83\n",
            "[20383 | 8251.87] loss=0.80 avg=0.83\n",
            "[20384 | 8254.95] loss=0.71 avg=0.83\n",
            "[20385 | 8258.04] loss=0.68 avg=0.82\n",
            "[20386 | 8261.13] loss=0.64 avg=0.82\n",
            "[20387 | 8264.21] loss=0.82 avg=0.82\n",
            "[20388 | 8267.30] loss=0.68 avg=0.82\n",
            "[20389 | 8270.39] loss=0.77 avg=0.82\n",
            "[20390 | 8273.49] loss=0.88 avg=0.82\n",
            "[20391 | 8276.58] loss=0.82 avg=0.82\n",
            "[20392 | 8279.67] loss=0.94 avg=0.82\n",
            "[20393 | 8282.77] loss=0.89 avg=0.82\n",
            "[20394 | 8285.86] loss=0.79 avg=0.82\n",
            "[20395 | 8288.95] loss=0.80 avg=0.82\n",
            "[20396 | 8292.03] loss=0.66 avg=0.82\n",
            "[20397 | 8295.12] loss=0.88 avg=0.82\n",
            "[20398 | 8298.20] loss=1.00 avg=0.82\n",
            "[20399 | 8301.30] loss=0.89 avg=0.82\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "a?\n",
            "How many languages do you speak? = ¿Cuántos idiomas hablas tienes?\n",
            "He is kind to me. = Él es amable conmigo.\n",
            "Tom could hear what was being said but couldn't do anything to stop it. = Tom puede oír lo que estaba diciendo pero no pude hacer nada para parar.\n",
            "Tom wasn't able to get in touch with Mary. = Tom no pudo contactarme con Mary.\n",
            "His explanation was too abstract and too strange. = Su explicación era demasiado abstracto y demasiado especial.\n",
            "You're just a kid. = Eres solo un niño.\n",
            "I feel so lost. = Me siento tan perdido.\n",
            "The children are playing in the sandbox. = Los niños están jugando en el arenero.\n",
            "You should know it. = Debes saberlo.\n",
            "It's all yours. = Todo es tuyo.\n",
            "The bus will arrive soon. = El autobús llegará pronto.\n",
            "I'm ready. = Yo estoy listo.\n",
            "I wanted so badly you'd remember. = Quería mucho tratarme en caso de mí.\n",
            "How many books can I borrow? = ¿Cuántos libros puedo tomar?\n",
            "I'm getting sick and tired of your complaining. = Me estoy harto de tus mal tareas.\n",
            "I was just trying to help out. = Solo trato de ayudar.\n",
            "We're going to start on Monday. = Nos vamos a empezar el lunes.\n",
            "Tom should've married Mary. = Tom debió haberse casado Mary.\n",
            "He is kind. = Él es gentil.\n",
            "He is very mean to me. = Él es muy gacho conmigo.\n",
            "He is a kind boy. = Él es un chico gentil.\n",
            "He is a doctor and so am I. = Él es médico, y yo tampoco.\n",
            "Tom isn't good at telling jokes. = A Tom se le da bien les coge chistes.\n",
            "She was advised by him on how to stay healthy. = Él le aconsejó sobre cómo mantenerse sana.\n",
            "Mary is Tom's grandmother. = Mary es la abuela de Tom.\n",
            "I think we should do something funny to commemorate the occasion. = Creo que deberíamos hacer algo divertido para celebrar el evento.\n",
            "I need to know what to say. = Necesito saber qué decir.\n",
            "Tell me which of the two options you prefer. = Decime cuál de los dos alternativas prefieren.\n",
            "Tom wants to know what time Mary gets home. = Tom quiere saber a qué hora Maria sale María.\n",
            "Where can I change the port in my cruise? = ¿En dónde puedo cambiar la vehícula del crucero?\n",
            "I didn't think about it that much. = No pensé tanto en ello.\n",
            "Are you certain that you don't know what you want? = ¿Estás seguro de que sí no sabes qué quieres?\n",
            "When do you eat? = ¿Cuándo comes?\n",
            "I'm ready to go to jail. = Estoy listo para ir a la cárcel.\n",
            "The cat was so shy it didn't want to jump on the bed. = El gato era tan tímido que no quería saltar sobre la cama.\n",
            "You're not jealous because I am, but you are also not. = No están celosos porque yo voy, sino también no.\n",
            "I want you to listen. = Quiero que escuches.\n",
            "I'm not in the mood for now. = No estoy de humor para ahora.\n",
            "Tom went and picked us up. = Tom salió y nos cogieron.\n",
            "I will have him call you at home. = Voy a enfermerte en casa.\n",
            "He looked up at the sky. = Él miró al seto por el cielo.\n",
            "My son sleeps under the couch. = Mi hijo duerme bajo el sofá.\n",
            "She is busy sewing little gifts. = Ella está ocupada cosiendo rellenos pequeños.\n",
            "I didn't feel up for a good fight. = No me apeteció una buena pelea.\n",
            "There's a very good coffee shop downtown\n",
            "\n",
            "[20400 | 8341.90] loss=0.98 avg=0.82\n",
            "[20401 | 8344.99] loss=0.71 avg=0.82\n",
            "[20402 | 8348.09] loss=0.91 avg=0.82\n",
            "[20403 | 8351.20] loss=0.66 avg=0.82\n",
            "[20404 | 8354.30] loss=0.81 avg=0.82\n",
            "[20405 | 8357.41] loss=0.74 avg=0.82\n",
            "[20406 | 8360.52] loss=0.95 avg=0.82\n",
            "[20407 | 8363.61] loss=0.83 avg=0.82\n",
            "[20408 | 8366.70] loss=0.97 avg=0.82\n",
            "[20409 | 8369.79] loss=0.87 avg=0.83\n",
            "[20410 | 8372.88] loss=0.66 avg=0.82\n",
            "[20411 | 8375.97] loss=0.86 avg=0.82\n",
            "[20412 | 8379.07] loss=0.71 avg=0.82\n",
            "[20413 | 8382.15] loss=0.76 avg=0.82\n",
            "[20414 | 8385.23] loss=0.76 avg=0.82\n",
            "[20415 | 8388.34] loss=0.78 avg=0.82\n",
            "[20416 | 8391.42] loss=0.97 avg=0.82\n",
            "[20417 | 8394.51] loss=0.86 avg=0.82\n",
            "[20418 | 8397.60] loss=0.93 avg=0.82\n",
            "[20419 | 8400.70] loss=0.73 avg=0.82\n",
            "[20420 | 8403.79] loss=0.92 avg=0.82\n",
            "[20421 | 8406.90] loss=0.88 avg=0.82\n",
            "[20422 | 8410.00] loss=0.93 avg=0.83\n",
            "[20423 | 8413.07] loss=0.87 avg=0.83\n",
            "[20424 | 8416.10] loss=0.89 avg=0.83\n",
            "[20425 | 8419.18] loss=0.86 avg=0.83\n",
            "[20426 | 8422.26] loss=0.85 avg=0.83\n",
            "[20427 | 8425.37] loss=0.89 avg=0.83\n",
            "[20428 | 8428.48] loss=0.87 avg=0.83\n",
            "[20429 | 8431.58] loss=0.84 avg=0.83\n",
            "[20430 | 8434.69] loss=0.72 avg=0.83\n",
            "[20431 | 8437.80] loss=0.73 avg=0.83\n",
            "[20432 | 8440.90] loss=0.74 avg=0.83\n",
            "[20433 | 8444.01] loss=0.78 avg=0.83\n",
            "[20434 | 8447.08] loss=0.99 avg=0.83\n",
            "[20435 | 8450.17] loss=0.80 avg=0.83\n",
            "[20436 | 8453.27] loss=0.85 avg=0.83\n",
            "[20437 | 8456.38] loss=0.85 avg=0.83\n",
            "[20438 | 8459.47] loss=0.92 avg=0.83\n",
            "[20439 | 8462.56] loss=0.77 avg=0.83\n",
            "[20440 | 8465.64] loss=0.69 avg=0.83\n",
            "[20441 | 8468.72] loss=0.76 avg=0.83\n",
            "[20442 | 8471.81] loss=0.62 avg=0.82\n",
            "[20443 | 8474.90] loss=0.90 avg=0.82\n",
            "[20444 | 8477.98] loss=0.80 avg=0.82\n",
            "[20445 | 8481.04] loss=0.66 avg=0.82\n",
            "[20446 | 8484.11] loss=0.63 avg=0.82\n",
            "[20447 | 8487.18] loss=0.85 avg=0.82\n",
            "[20448 | 8490.26] loss=0.76 avg=0.82\n",
            "[20449 | 8493.33] loss=0.69 avg=0.82\n",
            "[20450 | 8496.40] loss=0.75 avg=0.82\n",
            "[20451 | 8499.46] loss=0.71 avg=0.82\n",
            "[20452 | 8502.53] loss=0.89 avg=0.82\n",
            "[20453 | 8505.63] loss=0.70 avg=0.82\n",
            "[20454 | 8508.70] loss=0.68 avg=0.82\n",
            "[20455 | 8511.78] loss=0.79 avg=0.81\n",
            "[20456 | 8514.87] loss=0.95 avg=0.82\n",
            "[20457 | 8517.96] loss=0.69 avg=0.81\n",
            "[20458 | 8521.04] loss=0.94 avg=0.82\n",
            "[20459 | 8524.12] loss=0.88 avg=0.82\n",
            "[20460 | 8527.20] loss=0.98 avg=0.82\n",
            "[20461 | 8530.26] loss=0.78 avg=0.82\n",
            "[20462 | 8533.33] loss=0.67 avg=0.82\n",
            "[20463 | 8536.38] loss=0.79 avg=0.82\n",
            "[20464 | 8539.46] loss=0.77 avg=0.82\n",
            "[20465 | 8542.52] loss=0.91 avg=0.82\n",
            "[20466 | 8545.59] loss=0.79 avg=0.82\n",
            "[20467 | 8548.65] loss=0.92 avg=0.82\n",
            "[20468 | 8551.71] loss=0.60 avg=0.82\n",
            "[20469 | 8554.76] loss=0.70 avg=0.81\n",
            "[20470 | 8557.81] loss=0.68 avg=0.81\n",
            "[20471 | 8560.84] loss=0.96 avg=0.81\n",
            "[20472 | 8563.91] loss=0.89 avg=0.82\n",
            "[20473 | 8566.98] loss=0.83 avg=0.82\n",
            "[20474 | 8570.05] loss=0.65 avg=0.81\n",
            "[20475 | 8573.14] loss=0.98 avg=0.82\n",
            "[20476 | 8576.21] loss=0.87 avg=0.82\n",
            "[20477 | 8579.29] loss=0.81 avg=0.82\n",
            "[20478 | 8582.35] loss=0.69 avg=0.81\n",
            "[20479 | 8585.42] loss=0.76 avg=0.81\n",
            "[20480 | 8588.50] loss=0.84 avg=0.81\n",
            "[20481 | 8591.57] loss=0.84 avg=0.81\n",
            "[20482 | 8594.63] loss=0.90 avg=0.82\n",
            "[20483 | 8597.71] loss=0.76 avg=0.81\n",
            "[20484 | 8600.80] loss=0.75 avg=0.81\n",
            "[20485 | 8603.89] loss=0.74 avg=0.81\n",
            "[20486 | 8606.96] loss=0.90 avg=0.81\n",
            "[20487 | 8610.05] loss=0.88 avg=0.81\n",
            "[20488 | 8613.14] loss=0.67 avg=0.81\n",
            "[20489 | 8616.20] loss=0.78 avg=0.81\n",
            "[20490 | 8619.27] loss=0.71 avg=0.81\n",
            "[20491 | 8622.36] loss=0.71 avg=0.81\n",
            "[20492 | 8625.45] loss=0.70 avg=0.81\n",
            "[20493 | 8628.55] loss=0.89 avg=0.81\n",
            "[20494 | 8631.63] loss=0.81 avg=0.81\n",
            "[20495 | 8634.73] loss=0.88 avg=0.81\n",
            "[20496 | 8637.83] loss=0.97 avg=0.81\n",
            "[20497 | 8640.92] loss=0.85 avg=0.81\n",
            "[20498 | 8644.00] loss=0.88 avg=0.81\n",
            "[20499 | 8647.10] loss=0.77 avg=0.81\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "?\n",
            "She took care of him wherever she went. = Ella cuidó de él donde se iba.\n",
            "This box is too heavy. = Esta caja es demasiado pesada.\n",
            "The room was cleaned by Tom and Mary. = La habitación fue limpiada por Tom y María.\n",
            "I'll have to explain it to Tom. = Tom tendré que echarle una explicación a que le conteste.\n",
            "We made him work even more. = Le hicimos que trabajase aún más.\n",
            "I have to buy a new saxophone. = Tengo que comprar un saxofón nuevo.\n",
            "This is very bad. = Esto es muy malo.\n",
            "I thought you were Canadian. = Pensé que eras canadiense.\n",
            "Tom is now in prison. = Tom ahora está en prisión.\n",
            "They are playing basketball. = Ellos están jugando al baloncesto.\n",
            "They don't like me. = No les gusto a los autobacs.\n",
            "We have more in common than I expected. = Tenemos más en común de lo que se esperaba.\n",
            "I hate my husband. = Odio a mi marido.\n",
            "Don't get drunk. Just drink some water. = No te cleanses, solo beba de agua.\n",
            "There's an orange on the table. = Hay una naranja sobre el escritorio.\n",
            "He knows a lot about animals. = Él sabe mucho acerca de animales.\n",
            "I have to look for my keys. = Tengo que buscar mis llaves.\n",
            "The dog jumped over the fence. = El perro saltó sobre la cerca.\n",
            "I want another one! = ¡Quiero otro!\n",
            "I'm not sure about it. = Estoy lo wronga.\n",
            "Tom can't cook well. = Tom no puede cocinar bien.\n",
            "I want to ask Tom to sign my book. = Quiero pedirle a Tom que firmaré mi libro.\n",
            "This dog is purebred. = Este perro es clínico.\n",
            "Do you mind if I open the window? = ¿Te molesta si abro la ventana?\n",
            "Who's that woman? = ¿Quién es aquella mujer?\n",
            "That's a stupid rumor. = Ese es un rumor tonta.\n",
            "Let's sing. = Cantemos.\n",
            "Let's not get too excited. = No nos emocionamos demasiado!\n",
            "Tom always puts himself first. = Tom siempre se pone en primer lugar.\n",
            "When do you think he'll come? = ¿Cuándo crees que vendrá?\n",
            "Tom is in his room. = Tom está en su habitación.\n",
            "It was an ugly battle. = Era una lucha deteña.\n",
            "She lives in a small house on the floor. = Ella vive en una casa pequeña en el piso.\n",
            "This is the last time. = Es la última vez.\n",
            "Do you want me to explain it to you. = ¿Desea te explicarle?\n",
            "The children went to school in spite of the heat. = Los niños fueron a clases a pesar de el calor.\n",
            "Tom is not married. = Tom no está casado.\n",
            "Tom doesn't want to talk to Mary anymore. = Tom ya no quiere hablar con Mary.\n",
            "I want you to keep me informed. = Quiero que me shanne enseñando.\n",
            "Did you invite him? = ¿Le has invitado?\n",
            "We want to tell you something important. = Queremos decirte algo importante.\n",
            "You are my teacher. = Tú eres mi profesor.\n",
            "The train is at the station. = El tren está en la estación.\n",
            "You can't hide forever. = ¡No te puedes esconder para otro eterno!\n",
            "He's going to come over tomorrow, won't he? = Él va a pasar a pasar mañana, ¿no?\n",
            "I just want to let you know that I think you're the most beautiful woman alive. = Sólo quiero hacerte saber que que que os pienso pusieras las mujeras viviennes actuales.\n",
            "My wife is a vegetarian. = Mi mujer es vegetariano.\n",
            "My wife used to smoke. = Mi esposa soltó fumar.\n",
            "The only\n",
            "\n",
            "[20500 | 8687.87] loss=0.83 avg=0.81\n",
            "[20501 | 8690.97] loss=0.65 avg=0.81\n",
            "[20502 | 8694.07] loss=0.72 avg=0.81\n",
            "[20503 | 8697.15] loss=0.63 avg=0.81\n",
            "[20504 | 8700.25] loss=0.66 avg=0.81\n",
            "[20505 | 8703.33] loss=0.78 avg=0.81\n",
            "[20506 | 8706.41] loss=0.71 avg=0.81\n",
            "[20507 | 8709.51] loss=0.80 avg=0.81\n",
            "[20508 | 8712.59] loss=1.03 avg=0.81\n",
            "[20509 | 8715.68] loss=0.77 avg=0.81\n",
            "[20510 | 8718.77] loss=0.97 avg=0.81\n",
            "[20511 | 8721.86] loss=0.90 avg=0.81\n",
            "[20512 | 8724.97] loss=0.96 avg=0.81\n",
            "[20513 | 8728.06] loss=0.69 avg=0.81\n",
            "[20514 | 8731.14] loss=0.78 avg=0.81\n",
            "[20515 | 8734.21] loss=0.73 avg=0.81\n",
            "[20516 | 8737.28] loss=0.77 avg=0.81\n",
            "[20517 | 8740.36] loss=0.85 avg=0.81\n",
            "[20518 | 8743.45] loss=0.80 avg=0.81\n",
            "[20519 | 8746.53] loss=0.66 avg=0.81\n",
            "[20520 | 8749.62] loss=0.77 avg=0.81\n",
            "[20521 | 8752.70] loss=0.74 avg=0.81\n",
            "[20522 | 8755.79] loss=0.85 avg=0.81\n",
            "[20523 | 8758.88] loss=0.72 avg=0.81\n",
            "[20524 | 8761.97] loss=0.89 avg=0.81\n",
            "[20525 | 8765.05] loss=0.92 avg=0.81\n",
            "[20526 | 8768.14] loss=0.89 avg=0.81\n",
            "[20527 | 8771.21] loss=0.72 avg=0.81\n",
            "[20528 | 8774.32] loss=0.84 avg=0.81\n",
            "[20529 | 8777.38] loss=0.95 avg=0.81\n",
            "[20530 | 8780.47] loss=0.91 avg=0.81\n",
            "[20531 | 8783.56] loss=0.87 avg=0.81\n",
            "[20532 | 8786.65] loss=0.80 avg=0.81\n",
            "[20533 | 8789.74] loss=0.84 avg=0.81\n",
            "[20534 | 8792.83] loss=0.86 avg=0.81\n",
            "[20535 | 8795.92] loss=0.69 avg=0.81\n",
            "[20536 | 8799.01] loss=0.84 avg=0.81\n",
            "[20537 | 8802.10] loss=0.73 avg=0.81\n",
            "[20538 | 8805.19] loss=0.80 avg=0.81\n",
            "[20539 | 8808.27] loss=0.94 avg=0.81\n",
            "[20540 | 8811.35] loss=0.68 avg=0.81\n",
            "[20541 | 8814.41] loss=0.63 avg=0.81\n",
            "[20542 | 8817.49] loss=0.93 avg=0.81\n",
            "[20543 | 8820.58] loss=0.80 avg=0.81\n",
            "[20544 | 8823.67] loss=0.89 avg=0.81\n",
            "[20545 | 8826.75] loss=0.70 avg=0.81\n",
            "[20546 | 8829.82] loss=0.81 avg=0.81\n",
            "[20547 | 8832.91] loss=0.68 avg=0.81\n",
            "[20548 | 8836.00] loss=0.84 avg=0.81\n",
            "[20549 | 8839.08] loss=0.91 avg=0.81\n",
            "[20550 | 8842.16] loss=0.97 avg=0.81\n",
            "[20551 | 8845.23] loss=0.87 avg=0.81\n",
            "[20552 | 8848.32] loss=0.92 avg=0.81\n",
            "[20553 | 8851.42] loss=0.87 avg=0.81\n",
            "[20554 | 8854.51] loss=0.87 avg=0.81\n",
            "[20555 | 8857.61] loss=0.86 avg=0.81\n",
            "[20556 | 8860.68] loss=0.88 avg=0.82\n",
            "[20557 | 8863.78] loss=0.76 avg=0.81\n",
            "[20558 | 8866.88] loss=0.73 avg=0.81\n",
            "[20559 | 8869.99] loss=0.68 avg=0.81\n",
            "[20560 | 8873.10] loss=0.97 avg=0.81\n",
            "[20561 | 8876.21] loss=0.86 avg=0.81\n",
            "[20562 | 8879.30] loss=0.80 avg=0.81\n",
            "[20563 | 8882.39] loss=0.89 avg=0.81\n",
            "[20564 | 8885.48] loss=0.71 avg=0.81\n",
            "[20565 | 8888.56] loss=0.88 avg=0.81\n",
            "[20566 | 8891.64] loss=0.89 avg=0.82\n",
            "[20567 | 8894.72] loss=0.93 avg=0.82\n",
            "[20568 | 8897.79] loss=0.91 avg=0.82\n",
            "[20569 | 8900.87] loss=0.76 avg=0.82\n",
            "[20570 | 8903.93] loss=1.01 avg=0.82\n",
            "[20571 | 8907.02] loss=0.83 avg=0.82\n",
            "[20572 | 8910.10] loss=0.90 avg=0.82\n",
            "[20573 | 8913.18] loss=0.75 avg=0.82\n",
            "[20574 | 8916.28] loss=0.67 avg=0.82\n",
            "[20575 | 8919.36] loss=0.82 avg=0.82\n",
            "[20576 | 8922.45] loss=0.97 avg=0.82\n",
            "[20577 | 8925.53] loss=0.96 avg=0.82\n",
            "[20578 | 8928.63] loss=0.93 avg=0.82\n",
            "[20579 | 8931.73] loss=0.87 avg=0.82\n",
            "[20580 | 8934.81] loss=0.74 avg=0.82\n",
            "[20581 | 8937.89] loss=0.84 avg=0.82\n",
            "[20582 | 8940.98] loss=1.01 avg=0.82\n",
            "[20583 | 8944.04] loss=0.80 avg=0.82\n",
            "[20584 | 8947.13] loss=0.83 avg=0.82\n",
            "[20585 | 8950.22] loss=0.79 avg=0.82\n",
            "[20586 | 8953.32] loss=0.92 avg=0.82\n",
            "[20587 | 8956.40] loss=0.72 avg=0.82\n",
            "[20588 | 8959.50] loss=0.80 avg=0.82\n",
            "[20589 | 8962.58] loss=0.84 avg=0.82\n",
            "[20590 | 8965.67] loss=0.92 avg=0.82\n",
            "[20591 | 8968.77] loss=0.93 avg=0.82\n",
            "[20592 | 8971.86] loss=0.92 avg=0.83\n",
            "[20593 | 8974.97] loss=0.75 avg=0.82\n",
            "[20594 | 8978.07] loss=0.82 avg=0.82\n",
            "[20595 | 8981.16] loss=0.96 avg=0.83\n",
            "[20596 | 8984.25] loss=0.75 avg=0.83\n",
            "[20597 | 8987.35] loss=0.70 avg=0.82\n",
            "[20598 | 8990.44] loss=0.85 avg=0.82\n",
            "[20599 | 8993.53] loss=0.77 avg=0.82\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "aa en el trabajo.\n",
            "You know a lot about Tom. = Existen mucho de Tom.\n",
            "I didn't go on a diet. = No fui a dieta.\n",
            "I'm sure it wasn't planned. = Estoy seguro de que no era planeado.\n",
            "It took them some time to get used to each other. = Tardaron un poco de tiempo en acostumbrarse el uno al otro.\n",
            "He was a great leader. = Él fue un gran líder.\n",
            "I'm busy at the moment. = En este momento estoy ocupado.\n",
            "We're not that far. = No estamos demasiado lejos.\n",
            "Tom tried to get Mary to join the team. = Tom trató de conseguir que Mary se una una equipa al equipo.\n",
            "He was absent from school yesterday. = Ayer él absenceé a clase.\n",
            "I would like to talk to one of your guests. = Me gustaría hablar con uno de sus invitados.\n",
            "Tom gave Mary a hug and a kiss. = Tom le dio un abrazo a Mary e a besar.\n",
            "Tom hasn't finished yet. = Tom no ha terminado todo.\n",
            "You look fine. = Estás muy bien.\n",
            "The boy started crying. = El chico empezó a llorar.\n",
            "If I told you, you would be stupid. = Si te lo contaras, os fuésemos estúpido.\n",
            "I can't sleep. = No puedo dormir.\n",
            "You're very good at tennis. = Eres muy bueno para el tenis.\n",
            "What can she do? = ¿Qué puede usted hacer?\n",
            "I'm not that kind of guy. = No soy esa clase de tipo.\n",
            "Tom doesn't like it when Mary looks at him. = A Tom no le gustó que Mary lo mira.\n",
            "Do you think so? = ¿Te parezca?\n",
            "Tom is going to come over. = Tom va a pasará.\n",
            "Let me pay for it myself. = Déjame pagarlo mismo.\n",
            "How was school today? = ¿Cómo estuvo una clase hoy?\n",
            "Tom said he didn't have enough money. = Tomás dijo que no tenía dinero suficiente.\n",
            "Tom's car is old. = El carro de Tom es viejo.\n",
            "Can you tell me what this is? = ¿Puede decirme qué es esto?\n",
            "She was crying. = Ella estaba llorando.\n",
            "It is no use asking me for money. = No tiene caso pedirme dinero.\n",
            "Don't even think of trying to hide it. = Ni se te ocurra trataremos.\n",
            "Tom took off his leather jacket. = Tomás se quitó su chaqueta de cuero.\n",
            "It's important. = Es importante.\n",
            "This book is mine. = Este libro es mío.\n",
            "I don't have much time. = No tengo mucho tiempo.\n",
            "I'm not saying it's not possible. = No trato de que no sea posible.\n",
            "Tom's new girlfriend took a shine to him. = La nueva novia de Tom se le lusa de él.\n",
            "Tom is very handsome. = Tom es muy atractivo.\n",
            "Tom doesn't think that Mary was wearing a red blouse. = Tom no piensa que Mary llevaba una blusa roja.\n",
            "It is good to see you again. = Qué bueno verte de nuevo.\n",
            "My mom thinks I'm an alien. = Mi mamá piensa que yo soy un alien.\n",
            "We'll find somebody else. = Hemos encontrarnar a alguien más.\n",
            "She must have known her place. = Debe saber su lugar.\n",
            "I have done that many times. = Lo he hecho muchas veces.\n",
            "I'm already a man. = Ya soy un hombre.\n",
            "You may use my bicycle. = Puedes usar mi bicicleta.\n",
            "They're in danger. = Ellos están en peligro.\n",
            "I would like to visit Spain someday. = Quisiera visitar España algún día.\n",
            "When I was in college, I tried to get a grip on my anger. = Cuando estaba en la universidad, me intenté apretar el enfado.\n",
            "She has no regret about losing the child. = Ell\n",
            "\n",
            "[20600 | 9034.05] loss=0.79 avg=0.82\n",
            "[20601 | 9037.14] loss=0.90 avg=0.82\n",
            "[20602 | 9040.23] loss=0.75 avg=0.82\n",
            "[20603 | 9043.34] loss=0.82 avg=0.82\n",
            "[20604 | 9046.44] loss=0.67 avg=0.82\n",
            "[20605 | 9049.53] loss=0.72 avg=0.82\n",
            "[20606 | 9052.61] loss=0.75 avg=0.82\n",
            "[20607 | 9055.67] loss=0.91 avg=0.82\n",
            "[20608 | 9058.74] loss=0.75 avg=0.82\n",
            "[20609 | 9061.83] loss=0.80 avg=0.82\n",
            "[20610 | 9064.92] loss=0.68 avg=0.82\n",
            "[20611 | 9068.01] loss=0.72 avg=0.82\n",
            "[20612 | 9071.08] loss=0.80 avg=0.82\n",
            "[20613 | 9074.15] loss=0.72 avg=0.82\n",
            "[20614 | 9077.22] loss=0.77 avg=0.82\n",
            "[20615 | 9080.30] loss=0.83 avg=0.82\n",
            "[20616 | 9083.38] loss=0.81 avg=0.82\n",
            "[20617 | 9086.47] loss=0.75 avg=0.82\n",
            "[20618 | 9089.57] loss=0.79 avg=0.82\n",
            "[20619 | 9092.66] loss=0.66 avg=0.81\n",
            "[20620 | 9095.73] loss=0.91 avg=0.81\n",
            "[20621 | 9098.79] loss=0.88 avg=0.82\n",
            "[20622 | 9101.88] loss=0.82 avg=0.82\n",
            "[20623 | 9104.95] loss=0.99 avg=0.82\n",
            "[20624 | 9108.02] loss=0.76 avg=0.82\n",
            "[20625 | 9111.10] loss=0.75 avg=0.82\n",
            "[20626 | 9114.20] loss=0.92 avg=0.82\n",
            "[20627 | 9117.28] loss=0.91 avg=0.82\n",
            "[20628 | 9120.36] loss=0.98 avg=0.82\n",
            "[20629 | 9123.45] loss=0.89 avg=0.82\n",
            "[20630 | 9126.52] loss=0.67 avg=0.82\n",
            "[20631 | 9129.61] loss=0.73 avg=0.82\n",
            "[20632 | 9132.69] loss=0.67 avg=0.82\n",
            "[20633 | 9135.77] loss=0.84 avg=0.82\n",
            "[20634 | 9138.84] loss=0.78 avg=0.82\n",
            "[20635 | 9141.94] loss=0.92 avg=0.82\n",
            "[20636 | 9145.00] loss=0.79 avg=0.82\n",
            "[20637 | 9148.08] loss=0.69 avg=0.82\n",
            "[20638 | 9151.18] loss=0.97 avg=0.82\n",
            "[20639 | 9154.25] loss=0.82 avg=0.82\n",
            "[20640 | 9157.34] loss=0.90 avg=0.82\n",
            "[20641 | 9160.42] loss=0.95 avg=0.82\n",
            "[20642 | 9163.50] loss=0.95 avg=0.82\n",
            "[20643 | 9166.58] loss=0.95 avg=0.82\n",
            "[20644 | 9169.67] loss=0.74 avg=0.82\n",
            "[20645 | 9172.73] loss=0.77 avg=0.82\n",
            "[20646 | 9175.81] loss=0.86 avg=0.82\n",
            "[20647 | 9178.87] loss=0.80 avg=0.82\n",
            "[20648 | 9181.92] loss=0.88 avg=0.82\n",
            "[20649 | 9185.00] loss=0.88 avg=0.82\n",
            "[20650 | 9188.08] loss=0.79 avg=0.82\n",
            "[20651 | 9191.17] loss=0.70 avg=0.82\n",
            "[20652 | 9194.25] loss=0.75 avg=0.82\n",
            "[20653 | 9197.32] loss=0.80 avg=0.82\n",
            "[20654 | 9200.39] loss=0.78 avg=0.82\n",
            "[20655 | 9203.48] loss=0.83 avg=0.82\n",
            "[20656 | 9206.56] loss=0.80 avg=0.82\n",
            "[20657 | 9209.63] loss=0.91 avg=0.82\n",
            "[20658 | 9212.71] loss=0.73 avg=0.82\n",
            "[20659 | 9215.78] loss=0.84 avg=0.82\n",
            "[20660 | 9218.89] loss=0.72 avg=0.82\n",
            "[20661 | 9221.97] loss=0.77 avg=0.82\n",
            "[20662 | 9225.07] loss=0.94 avg=0.82\n",
            "[20663 | 9228.15] loss=0.62 avg=0.82\n",
            "[20664 | 9231.24] loss=0.79 avg=0.82\n",
            "[20665 | 9234.33] loss=0.92 avg=0.82\n",
            "[20666 | 9237.42] loss=0.74 avg=0.82\n",
            "[20667 | 9240.53] loss=0.93 avg=0.82\n",
            "[20668 | 9243.62] loss=0.67 avg=0.82\n",
            "[20669 | 9246.73] loss=0.90 avg=0.82\n",
            "[20670 | 9249.82] loss=0.80 avg=0.82\n",
            "[20671 | 9252.92] loss=0.75 avg=0.82\n",
            "[20672 | 9256.00] loss=0.87 avg=0.82\n",
            "[20673 | 9259.08] loss=0.62 avg=0.82\n",
            "[20674 | 9262.18] loss=0.92 avg=0.82\n",
            "[20675 | 9265.26] loss=0.75 avg=0.82\n",
            "[20676 | 9268.34] loss=0.87 avg=0.82\n",
            "[20677 | 9271.44] loss=0.73 avg=0.82\n",
            "[20678 | 9274.54] loss=0.72 avg=0.81\n",
            "[20679 | 9277.64] loss=0.83 avg=0.81\n",
            "[20680 | 9280.73] loss=0.97 avg=0.82\n",
            "[20681 | 9283.82] loss=0.80 avg=0.82\n",
            "[20682 | 9286.92] loss=0.92 avg=0.82\n",
            "[20683 | 9290.00] loss=0.74 avg=0.82\n",
            "[20684 | 9293.11] loss=0.64 avg=0.81\n",
            "[20685 | 9296.20] loss=0.82 avg=0.81\n",
            "[20686 | 9299.30] loss=1.00 avg=0.82\n",
            "[20687 | 9302.39] loss=0.68 avg=0.82\n",
            "[20688 | 9305.49] loss=0.76 avg=0.81\n",
            "[20689 | 9308.59] loss=0.61 avg=0.81\n",
            "[20690 | 9311.69] loss=0.75 avg=0.81\n",
            "[20691 | 9314.78] loss=0.97 avg=0.81\n",
            "[20692 | 9317.87] loss=0.92 avg=0.81\n",
            "[20693 | 9320.96] loss=0.65 avg=0.81\n",
            "[20694 | 9324.05] loss=0.78 avg=0.81\n",
            "[20695 | 9327.15] loss=0.70 avg=0.81\n",
            "[20696 | 9330.26] loss=0.86 avg=0.81\n",
            "[20697 | 9333.35] loss=0.71 avg=0.81\n",
            "[20698 | 9336.42] loss=0.85 avg=0.81\n",
            "[20699 | 9339.49] loss=0.74 avg=0.81\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Tom.\n",
            "When I told Tom about your accident, he wrote a letter to you. = Al vez que le dijiste a Tom una carta sobre tu accidente, él le escribió una carta.\n",
            "You can't run from the past. = No puedes huir del pasado.\n",
            "You can't blame them. = No pueden culpablele.\n",
            "Tom couldn't get Mary to tell him where John was. = Tom no pudo conseguir que Mary le dijera donde él era John.\n",
            "I'll see you tomorrow. = Te veré mañana.\n",
            "Who do you want to talk to? = ¿Con quién quieres hablar?\n",
            "She is a very shy girl. = Es una niña muy tía.\n",
            "She took over the business after her husband's death. = Ella se hizo cargo del negocio después de la muerte de su esposo.\n",
            "I have little money in my wallet. = Tengo poco dinero en mi monedero.\n",
            "I got it for free. = La conseguí lo libre.\n",
            "I'll be waiting for you. = Te estaré esperándolo.\n",
            "I can't believe it. = No puedo creerlo.\n",
            "That's none of your concern. = Eso no es asunto tuyo.\n",
            "What's there to break? = ¿Qué hay nada que romper?\n",
            "If I were a bird, I would fly to you. = Si yo fuera un pájaro volando hasta ti.\n",
            "When I got to the station, the train was gone. = Cuando llegué a la estación, el tren había desaparecido.\n",
            "Tom is playing with Mary's cat. = Tomás está jugando con el gato de Mary.\n",
            "It's good to see you again. = Que gusto verte de nuevo.\n",
            "We're eating popcorn. = Estamos comiendo pochoclo.\n",
            "How much does he expect to receive in return for his gift? = ¿Cuánto espera que recibir su obsequio?\n",
            "This is too large. = Esto es demasiado grande.\n",
            "He likes to play baseball. = Le gusta jugar al béisbol.\n",
            "You are the tallest man I know. = Tú eres el hombre más alto que conozco.\n",
            "The ship sailed for Hawaii. = El barco navegó a Hawaiʻi.\n",
            "I have a sharp pain here. = Tengo un agudo dolor aquí.\n",
            "Is that all? = ¿Es todo?\n",
            "How long do Christmas trees last? = ¿Cuánto duran los árboles de Navidades?\n",
            "You're always asking me to do your homework. = Tú siempre me estás pidiendo que yo haga tu tarea.\n",
            "I'm an only child. = Soy hijo único.\n",
            "Where do you think Tom would be? = ¿Dónde crees que sería Tom?\n",
            "When does the last train leave? = ¿A dónde sale el último tren?\n",
            "His mother was worried about his bad health. = Su madre estaba inquieta la mala salud de él.\n",
            "I wish I could be in Paris now. = Desearía poder estar en París en este momento.\n",
            "Tom died at the age of ninety. = Tom murió a la edad de 90 años.\n",
            "The more I thought about it, the less I liked it. = Cuanto más consideraba sobre eso, menos la agrada.\n",
            "What time is the bus? = ¿A qué hora es el autobús?\n",
            "They arrived too early. = Ellos llegaron demasiado temprano.\n",
            "I would like to get home by sunset. = Me gustaría volver a casa al sol supimos.\n",
            "I have a cousin. = Tengo una prima.\n",
            "She gave him something cold to drink. = Ella le dio algo frío para beber.\n",
            "He has learned to be patient. = A él se le aprendió a ser paciente.\n",
            "That's a magnificent view. = Esa es una magnífica vista.\n",
            "We should learn from his mistakes. = Deberíamos aprender de sus errores.\n",
            "He is not as tall as his brother. = No es tan\n",
            "\n",
            "[20700 | 9380.14] loss=0.91 avg=0.81\n",
            "[20701 | 9383.23] loss=0.95 avg=0.81\n",
            "[20702 | 9386.32] loss=0.94 avg=0.81\n",
            "[20703 | 9389.40] loss=0.82 avg=0.81\n",
            "[20704 | 9392.48] loss=0.92 avg=0.82\n",
            "[20705 | 9395.57] loss=0.70 avg=0.81\n",
            "[20706 | 9398.64] loss=0.73 avg=0.81\n",
            "[20707 | 9401.73] loss=0.90 avg=0.81\n",
            "[20708 | 9404.83] loss=0.88 avg=0.82\n",
            "[20709 | 9407.92] loss=0.82 avg=0.82\n",
            "[20710 | 9411.00] loss=0.79 avg=0.81\n",
            "[20711 | 9414.09] loss=0.77 avg=0.81\n",
            "[20712 | 9417.16] loss=0.80 avg=0.81\n",
            "[20713 | 9420.25] loss=0.93 avg=0.82\n",
            "[20714 | 9423.35] loss=0.81 avg=0.82\n",
            "[20715 | 9426.44] loss=0.96 avg=0.82\n",
            "[20716 | 9429.53] loss=0.87 avg=0.82\n",
            "[20717 | 9432.61] loss=0.88 avg=0.82\n",
            "[20718 | 9435.72] loss=0.96 avg=0.82\n",
            "[20719 | 9438.80] loss=0.72 avg=0.82\n",
            "[20720 | 9441.89] loss=0.83 avg=0.82\n",
            "[20721 | 9444.98] loss=0.87 avg=0.82\n",
            "[20722 | 9448.07] loss=0.83 avg=0.82\n",
            "[20723 | 9451.17] loss=0.85 avg=0.82\n",
            "[20724 | 9454.25] loss=0.76 avg=0.82\n",
            "[20725 | 9457.36] loss=0.68 avg=0.82\n",
            "[20726 | 9460.45] loss=0.74 avg=0.82\n",
            "[20727 | 9463.53] loss=0.67 avg=0.82\n",
            "[20728 | 9466.63] loss=0.77 avg=0.81\n",
            "[20729 | 9469.72] loss=0.82 avg=0.81\n",
            "[20730 | 9472.82] loss=0.81 avg=0.81\n",
            "[20731 | 9475.90] loss=0.81 avg=0.81\n",
            "[20732 | 9478.98] loss=0.69 avg=0.81\n",
            "[20733 | 9482.07] loss=1.00 avg=0.82\n",
            "[20734 | 9485.15] loss=0.77 avg=0.81\n",
            "[20735 | 9488.24] loss=0.82 avg=0.81\n",
            "[20736 | 9491.32] loss=0.67 avg=0.81\n",
            "[20737 | 9494.40] loss=0.93 avg=0.81\n",
            "[20738 | 9497.46] loss=0.68 avg=0.81\n",
            "[20739 | 9500.55] loss=0.74 avg=0.81\n",
            "[20740 | 9503.65] loss=0.83 avg=0.81\n",
            "[20741 | 9506.74] loss=0.82 avg=0.81\n",
            "[20742 | 9509.85] loss=0.86 avg=0.81\n",
            "[20743 | 9512.95] loss=0.66 avg=0.81\n",
            "[20744 | 9516.03] loss=0.70 avg=0.81\n",
            "[20745 | 9519.12] loss=0.72 avg=0.81\n",
            "[20746 | 9522.22] loss=0.73 avg=0.81\n",
            "[20747 | 9525.31] loss=0.86 avg=0.81\n",
            "[20748 | 9528.41] loss=0.79 avg=0.81\n",
            "[20749 | 9531.50] loss=0.66 avg=0.81\n",
            "[20750 | 9534.58] loss=0.98 avg=0.81\n",
            "[20751 | 9537.67] loss=0.81 avg=0.81\n",
            "[20752 | 9540.77] loss=0.82 avg=0.81\n",
            "[20753 | 9543.87] loss=0.87 avg=0.81\n",
            "[20754 | 9546.95] loss=0.91 avg=0.81\n",
            "[20755 | 9550.04] loss=0.73 avg=0.81\n",
            "[20756 | 9553.14] loss=0.61 avg=0.81\n",
            "[20757 | 9556.23] loss=0.80 avg=0.81\n",
            "[20758 | 9559.34] loss=0.70 avg=0.81\n",
            "[20759 | 9562.43] loss=0.93 avg=0.81\n",
            "[20760 | 9565.53] loss=0.81 avg=0.81\n",
            "[20761 | 9568.62] loss=0.88 avg=0.81\n",
            "[20762 | 9571.72] loss=0.74 avg=0.81\n",
            "[20763 | 9574.80] loss=0.79 avg=0.81\n",
            "[20764 | 9577.91] loss=0.93 avg=0.81\n",
            "[20765 | 9580.99] loss=0.93 avg=0.81\n",
            "[20766 | 9584.09] loss=0.76 avg=0.81\n",
            "[20767 | 9587.20] loss=0.82 avg=0.81\n",
            "[20768 | 9590.29] loss=0.98 avg=0.81\n",
            "[20769 | 9593.41] loss=0.85 avg=0.81\n",
            "[20770 | 9596.51] loss=0.73 avg=0.81\n",
            "[20771 | 9599.57] loss=0.80 avg=0.81\n",
            "[20772 | 9602.66] loss=0.94 avg=0.81\n",
            "[20773 | 9605.76] loss=0.63 avg=0.81\n",
            "[20774 | 9608.85] loss=0.70 avg=0.81\n",
            "[20775 | 9611.94] loss=0.86 avg=0.81\n",
            "[20776 | 9615.01] loss=0.94 avg=0.81\n",
            "[20777 | 9618.05] loss=0.70 avg=0.81\n",
            "[20778 | 9621.14] loss=0.99 avg=0.81\n",
            "[20779 | 9624.23] loss=0.68 avg=0.81\n",
            "[20780 | 9627.34] loss=0.85 avg=0.81\n",
            "[20781 | 9630.44] loss=0.81 avg=0.81\n",
            "[20782 | 9633.53] loss=0.72 avg=0.81\n",
            "[20783 | 9636.61] loss=0.77 avg=0.81\n",
            "[20784 | 9639.70] loss=0.84 avg=0.81\n",
            "[20785 | 9642.79] loss=0.71 avg=0.81\n",
            "[20786 | 9645.90] loss=0.67 avg=0.81\n",
            "[20787 | 9649.01] loss=0.92 avg=0.81\n",
            "[20788 | 9652.11] loss=0.91 avg=0.81\n",
            "[20789 | 9655.21] loss=0.77 avg=0.81\n",
            "[20790 | 9658.29] loss=0.94 avg=0.81\n",
            "[20791 | 9661.39] loss=0.87 avg=0.81\n",
            "[20792 | 9664.48] loss=0.96 avg=0.81\n",
            "[20793 | 9667.59] loss=0.92 avg=0.81\n",
            "[20794 | 9670.68] loss=0.69 avg=0.81\n",
            "[20795 | 9673.75] loss=0.84 avg=0.81\n",
            "[20796 | 9676.83] loss=0.89 avg=0.81\n",
            "[20797 | 9679.94] loss=0.81 avg=0.81\n",
            "[20798 | 9683.04] loss=0.63 avg=0.81\n",
            "[20799 | 9686.14] loss=0.73 avg=0.81\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " going to give away our house? = ¿Nos conseguiremos mi casa?\n",
            "I think that's too much. = Creo que es demasiado.\n",
            "Tom couldn't believe his eyes. = Tom no podía creer lo que miraba.\n",
            "Tom didn't look happy. = Tom no se veía feliz.\n",
            "What's your real goal in life? = ¿Cuál es tu verdadera objetivo en la vida?\n",
            "Your house is nice, but mine is even better. = Tu casa está bien, pero la mía la es todo mejor.\n",
            "This must be Tom's mother. = Esta debe ser la madre de Tom.\n",
            "I can't believe you're actually going to say that. = No puedo creer que realmente vas a decir eso.\n",
            "Don't call me again. = No me llamés de nuevo.\n",
            "It would be easy for her. = Sería fácil para ella.\n",
            "The doctor had to operate because of the patient's illness. = El doctor tuvo que operar por el enfermedad del paciente.\n",
            "She asked me whether she could use the telephone. = Ella me preguntó si podía usar el teléfono.\n",
            "How's the weather in New York? = ¿Cómo es el clima en Nueva York?\n",
            "You're the only person I know who has ever been married. = Sois las únicas persona que conozco que se ha estado casada.\n",
            "Tom and Mary look exactly the same as they were when they were children. = Tom y Mary se ven igual de lo mismo cuando eran niños.\n",
            "Let's take it easy for a while. = Tomémonoslo con poco.\n",
            "Did you do that today? = ¿Has hecho eso hoy?\n",
            "This is the very best way to do it. = Esta es justo la mejor manera de hacerlo.\n",
            "Please give him my best regards. = Tengan saludos a mi mejor salud.\n",
            "Have you ever broken your watch again? = ¿Te has quebrado el reloj otra vez?\n",
            "We're on strike again tomorrow. = Hoy de nuevo.\n",
            "She is a good girl. = Ella es una niña buena.\n",
            "I need to talk to you. = Necesito hablar contigo.\n",
            "It is no exaggeration to call him a genius. = No es una exageración llamarte.\n",
            "Can I have some? = ¿Puedo darme alguna?\n",
            "I don't know what happened, but it didn't go well. = No sé qué pasó, pero no salió bien.\n",
            "A lot of people think I'm crazy. = Mucha gente piensa que estoy loca.\n",
            "I never imagined I'd feel this way about you. = Nunca pensé que me sentiría de esta manera sobre ti.\n",
            "I don't understand what you are trying to say. = No entiendo lo que estás tratando de decir.\n",
            "There are two snakes in the bathroom. = Hay dos tiburones en el baño.\n",
            "What am I supposed to do? = ¿Qué se supone que tengo que hacer?\n",
            "I'm going to miss your cooking. = Voy a echar de menos tus platos.\n",
            "We have to be there Monday. = Tenemos que estar allí el lunes.\n",
            "She is not always honest. = No es siempre honesto.\n",
            "This is the girl I told you about the other day. = Esta es la chica de uno de veces.\n",
            "Tom doesn't understand what you want. = Tom no entiende lo que queréis.\n",
            "The sky is getting dark. = El cielo se está oscureciendo.\n",
            "I can't get on the plane. = No puedo subir al avión.\n",
            "Tom is in there with Mary. = Tom está allí con Mary.\n",
            "I like going on walks. = Me gusta dar las coradas.\n",
            "They said yes. = Dijeron que sí.\n",
            "She is going to start at noon today. = Ella está a partir de mediodía hoy.\n",
            "I didn't want to alarm you. = No desaba alarmarte.\n",
            "That's not the real goal. = Esa no es la verdadera meta.\n",
            "Do it right now. = Hazlo ya mismo.\n",
            "It's been a\n",
            "\n",
            "[20800 | 9727.27] loss=0.78 avg=0.81\n",
            "[20801 | 9730.35] loss=0.79 avg=0.81\n",
            "[20802 | 9733.43] loss=0.89 avg=0.81\n",
            "[20803 | 9736.50] loss=0.83 avg=0.81\n",
            "[20804 | 9739.59] loss=0.89 avg=0.81\n",
            "[20805 | 9742.68] loss=0.65 avg=0.81\n",
            "[20806 | 9745.78] loss=0.85 avg=0.81\n",
            "[20807 | 9748.89] loss=0.71 avg=0.81\n",
            "[20808 | 9752.00] loss=0.95 avg=0.81\n",
            "[20809 | 9755.10] loss=0.85 avg=0.81\n",
            "[20810 | 9758.20] loss=0.97 avg=0.81\n",
            "[20811 | 9761.30] loss=0.79 avg=0.81\n",
            "[20812 | 9764.42] loss=0.87 avg=0.81\n",
            "[20813 | 9767.52] loss=0.89 avg=0.81\n",
            "[20814 | 9770.63] loss=0.82 avg=0.81\n",
            "[20815 | 9773.70] loss=0.81 avg=0.81\n",
            "[20816 | 9776.79] loss=0.83 avg=0.81\n",
            "[20817 | 9779.88] loss=0.80 avg=0.81\n",
            "[20818 | 9782.96] loss=0.86 avg=0.82\n",
            "[20819 | 9786.05] loss=0.68 avg=0.81\n",
            "[20820 | 9789.13] loss=0.79 avg=0.81\n",
            "[20821 | 9792.21] loss=0.71 avg=0.81\n",
            "[20822 | 9795.30] loss=0.63 avg=0.81\n",
            "[20823 | 9798.40] loss=0.71 avg=0.81\n",
            "[20824 | 9801.45] loss=0.69 avg=0.81\n",
            "[20825 | 9804.53] loss=0.88 avg=0.81\n",
            "[20826 | 9807.61] loss=0.88 avg=0.81\n",
            "[20827 | 9810.71] loss=0.86 avg=0.81\n",
            "[20828 | 9813.81] loss=0.74 avg=0.81\n",
            "[20829 | 9816.91] loss=0.71 avg=0.81\n",
            "[20830 | 9820.01] loss=0.74 avg=0.81\n",
            "[20831 | 9823.11] loss=0.86 avg=0.81\n",
            "[20832 | 9826.20] loss=0.83 avg=0.81\n",
            "[20833 | 9829.29] loss=0.73 avg=0.81\n",
            "[20834 | 9832.38] loss=0.64 avg=0.81\n",
            "[20835 | 9835.48] loss=0.73 avg=0.81\n",
            "[20836 | 9838.59] loss=0.72 avg=0.80\n",
            "[20837 | 9841.68] loss=0.54 avg=0.80\n",
            "[20838 | 9844.77] loss=0.68 avg=0.80\n",
            "[20839 | 9847.86] loss=0.86 avg=0.80\n",
            "[20840 | 9850.95] loss=0.72 avg=0.80\n",
            "[20841 | 9854.04] loss=0.95 avg=0.80\n",
            "[20842 | 9857.13] loss=0.79 avg=0.80\n",
            "[20843 | 9860.22] loss=0.65 avg=0.80\n",
            "[20844 | 9863.30] loss=0.73 avg=0.80\n",
            "[20845 | 9866.39] loss=0.65 avg=0.80\n",
            "[20846 | 9869.50] loss=0.89 avg=0.80\n",
            "[20847 | 9872.60] loss=0.95 avg=0.80\n",
            "[20848 | 9875.69] loss=0.83 avg=0.80\n",
            "[20849 | 9878.78] loss=0.93 avg=0.80\n",
            "[20850 | 9881.86] loss=0.85 avg=0.80\n",
            "[20851 | 9884.95] loss=0.78 avg=0.80\n",
            "[20852 | 9888.05] loss=0.71 avg=0.80\n",
            "[20853 | 9891.15] loss=0.76 avg=0.80\n",
            "[20854 | 9894.23] loss=0.82 avg=0.80\n",
            "[20855 | 9897.32] loss=1.03 avg=0.80\n",
            "[20856 | 9900.41] loss=1.04 avg=0.81\n",
            "[20857 | 9903.49] loss=0.82 avg=0.81\n",
            "[20858 | 9906.57] loss=0.99 avg=0.81\n",
            "[20859 | 9909.66] loss=0.79 avg=0.81\n",
            "[20860 | 9912.75] loss=0.85 avg=0.81\n",
            "[20861 | 9915.83] loss=0.86 avg=0.81\n",
            "[20862 | 9918.93] loss=0.69 avg=0.81\n",
            "[20863 | 9922.01] loss=0.79 avg=0.81\n",
            "[20864 | 9925.11] loss=0.85 avg=0.81\n",
            "[20865 | 9928.20] loss=0.92 avg=0.81\n",
            "[20866 | 9931.28] loss=0.77 avg=0.81\n",
            "[20867 | 9934.37] loss=0.71 avg=0.81\n",
            "[20868 | 9937.46] loss=0.87 avg=0.81\n",
            "[20869 | 9940.55] loss=0.86 avg=0.81\n",
            "[20870 | 9943.64] loss=0.75 avg=0.81\n",
            "[20871 | 9946.74] loss=0.76 avg=0.81\n",
            "[20872 | 9949.84] loss=0.73 avg=0.81\n",
            "[20873 | 9952.92] loss=0.83 avg=0.81\n",
            "[20874 | 9956.02] loss=0.68 avg=0.81\n",
            "[20875 | 9959.12] loss=0.74 avg=0.81\n",
            "[20876 | 9962.21] loss=0.60 avg=0.80\n",
            "[20877 | 9965.31] loss=0.72 avg=0.80\n",
            "[20878 | 9968.42] loss=0.87 avg=0.80\n",
            "[20879 | 9971.51] loss=0.91 avg=0.80\n",
            "[20880 | 9974.59] loss=0.64 avg=0.80\n",
            "[20881 | 9977.67] loss=0.74 avg=0.80\n",
            "[20882 | 9980.76] loss=0.88 avg=0.80\n",
            "[20883 | 9983.86] loss=0.75 avg=0.80\n",
            "[20884 | 9986.95] loss=0.76 avg=0.80\n",
            "[20885 | 9990.04] loss=0.71 avg=0.80\n",
            "[20886 | 9993.14] loss=0.74 avg=0.80\n",
            "[20887 | 9996.23] loss=0.77 avg=0.80\n",
            "[20888 | 9999.31] loss=0.92 avg=0.80\n",
            "[20889 | 10002.41] loss=0.68 avg=0.80\n",
            "[20890 | 10005.51] loss=0.72 avg=0.80\n",
            "[20891 | 10008.59] loss=0.90 avg=0.80\n",
            "[20892 | 10011.68] loss=0.70 avg=0.80\n",
            "[20893 | 10014.78] loss=0.77 avg=0.80\n",
            "[20894 | 10017.87] loss=0.73 avg=0.80\n",
            "[20895 | 10020.97] loss=0.86 avg=0.80\n",
            "[20896 | 10024.07] loss=0.69 avg=0.80\n",
            "[20897 | 10027.17] loss=0.69 avg=0.80\n",
            "[20898 | 10030.27] loss=0.94 avg=0.80\n",
            "[20899 | 10033.37] loss=0.67 avg=0.80\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " six of the most dangerous crimes, while the majority of the residents were merely victims and some had committed crimes themselves. = Los cánceranos de alborzo no recibidamente encontraron cada vez más peligrosos de las crimees más peligrosas.\n",
            "Are you trying to steal this? = ¿Estás intentando robar esto?\n",
            "Tom bought two dozen pencils. = Tom compró dos docenas de lápices.\n",
            "I am a student. = Soy un estudiante.\n",
            "We haven't spoken for three hours. = Hace tres horas que no hemos hablado.\n",
            "Tom wanted to find out what Mary wanted him to do. = Tom quería averiguar en qué quería que hiciera Mary.\n",
            "This should be easy. = Esto debería ser fácil.\n",
            "Don't forget your ice. = No olvides tus hielbas.\n",
            "Tom's eyesight isn't as good as it used to be. = Los ojos de Tom no son tan buenos como antes.\n",
            "I was born in Osaka. = Nací en Osaka.\n",
            "She was told that the milk came from China. = Se le dijo que le jugaba la leche de China.\n",
            "You don't realize how lucky you're having it. = No te da cuenta de cuán aparentarlo.\n",
            "I am sorry I did not think of something. = Lamento no haber pensado algo.\n",
            "Please look after the children. = Por favor cuida de atrás.\n",
            "Tom didn't do well on any of the tests. = A Tom no le fue bien en cualquier alguno de las pruebas.\n",
            "Tom asked me not to drive too fast. = Tom me pidió que no conduciera demasiado rápido.\n",
            "I'll tell you what the problem is. = Os diré cuál es el problema.\n",
            "I did it by myself. = Lo hice por mí mismo.\n",
            "If you're busy, I can do it. = Si estás ocupado, puedo hacerlo.\n",
            "I can't tell you why it worked, but I can tell you why it didn't. = No te puedo decir por qué funcionó, pero la podría comprender que no.\n",
            "Tom didn't come. = Tom no vino.\n",
            "Are you interested in politics? = ¿Te interesa la política?\n",
            "You have two choices. = Tienes dos opciones.\n",
            "Tom says that's impossible. = Tomás dice que es imposible.\n",
            "You'll get used to it. = Se acostumbraráis a ello.\n",
            "This mustn't be exposed to the sun. = No hay que exponer esto al sol.\n",
            "It was raining heavily in Osaka. = Llovía muy fuerte en Osaka.\n",
            "I can't imagine her doing that. = No sé que ella haya hecho eso.\n",
            "We've come up against some resistance. = Hemos llegado a nuestro resoluente.\n",
            "Let's finish it right now. = Terminemlo ahora mismo.\n",
            "I have to go get it. = Tengo que ir a cogerlo.\n",
            "They will kill time reading this magazine. = Ellos pasarán tiempo leyendo esta revista.\n",
            "He is at his desk, collecting papers. = Se está en su escritorio, recogiendo papeles.\n",
            "I don't know about you, but I'm starving! = No tengo de usted, pero yo no tengo por vuestra hambre.\n",
            "We should be careful not to make him angry. = Debemos ser cuidadoso no hacerlo animado.\n",
            "Tom doesn't think that's the answer. = Tom no cree que esa es la respuesta.\n",
            "Can I go alone? = ¿Puedo ir solo?\n",
            "Can we just take a minute? = ¿Podemos samos un minuto?\n",
            "I am more nervous than happy. = Estoy más nerviosa que feliz.\n",
            "He is always in a hurry. = Siempre está áspero.\n",
            "Tom wasn't hungry after lunch. = Tom no tenía hambre después de almorzar.\n",
            "I thought you were dead. = Pensé que estabas muerto.\n",
            "I didn't know who else to ask. = No sabía a quién más preguntarle.\n",
            "I don't go out much. = No voy por much\n",
            "\n",
            "[20900 | 10074.06] loss=0.75 avg=0.80\n",
            "[20901 | 10077.16] loss=0.76 avg=0.80\n",
            "[20902 | 10080.24] loss=0.70 avg=0.80\n",
            "[20903 | 10083.33] loss=1.08 avg=0.80\n",
            "[20904 | 10086.43] loss=0.77 avg=0.80\n",
            "[20905 | 10089.52] loss=0.81 avg=0.80\n",
            "[20906 | 10092.62] loss=0.63 avg=0.80\n",
            "[20907 | 10095.67] loss=0.86 avg=0.80\n",
            "[20908 | 10098.76] loss=0.78 avg=0.80\n",
            "[20909 | 10101.85] loss=0.80 avg=0.80\n",
            "[20910 | 10104.94] loss=0.72 avg=0.80\n",
            "[20911 | 10108.02] loss=0.70 avg=0.79\n",
            "[20912 | 10111.09] loss=0.74 avg=0.79\n",
            "[20913 | 10114.15] loss=0.82 avg=0.79\n",
            "[20914 | 10117.25] loss=0.82 avg=0.79\n",
            "[20915 | 10120.32] loss=0.79 avg=0.79\n",
            "[20916 | 10123.42] loss=0.84 avg=0.80\n",
            "[20917 | 10126.51] loss=0.83 avg=0.80\n",
            "[20918 | 10129.60] loss=0.73 avg=0.80\n",
            "[20919 | 10132.68] loss=0.95 avg=0.80\n",
            "[20920 | 10135.77] loss=0.91 avg=0.80\n",
            "[20921 | 10138.82] loss=0.76 avg=0.80\n",
            "[20922 | 10141.91] loss=0.87 avg=0.80\n",
            "[20923 | 10145.01] loss=0.90 avg=0.80\n",
            "[20924 | 10148.13] loss=0.77 avg=0.80\n",
            "[20925 | 10151.22] loss=0.92 avg=0.80\n",
            "[20926 | 10154.31] loss=0.80 avg=0.80\n",
            "[20927 | 10157.40] loss=0.73 avg=0.80\n",
            "[20928 | 10160.49] loss=0.91 avg=0.80\n",
            "[20929 | 10163.59] loss=0.76 avg=0.80\n",
            "[20930 | 10166.69] loss=0.90 avg=0.80\n",
            "[20931 | 10169.78] loss=0.83 avg=0.80\n",
            "[20932 | 10172.87] loss=0.94 avg=0.80\n",
            "[20933 | 10175.97] loss=0.81 avg=0.80\n",
            "[20934 | 10179.05] loss=0.76 avg=0.80\n",
            "[20935 | 10182.16] loss=0.79 avg=0.80\n",
            "[20936 | 10185.25] loss=0.72 avg=0.80\n",
            "[20937 | 10188.35] loss=0.91 avg=0.80\n",
            "[20938 | 10191.44] loss=0.92 avg=0.80\n",
            "[20939 | 10194.53] loss=0.90 avg=0.80\n",
            "[20940 | 10197.64] loss=0.77 avg=0.80\n",
            "[20941 | 10200.72] loss=0.85 avg=0.80\n",
            "[20942 | 10203.82] loss=0.93 avg=0.81\n",
            "[20943 | 10206.91] loss=0.89 avg=0.81\n",
            "[20944 | 10210.00] loss=0.71 avg=0.81\n",
            "[20945 | 10213.11] loss=0.75 avg=0.81\n",
            "[20946 | 10216.16] loss=0.93 avg=0.81\n",
            "[20947 | 10219.24] loss=0.81 avg=0.81\n",
            "[20948 | 10222.33] loss=0.80 avg=0.81\n",
            "[20949 | 10225.43] loss=0.70 avg=0.81\n",
            "[20950 | 10228.54] loss=1.02 avg=0.81\n",
            "[20951 | 10231.64] loss=0.87 avg=0.81\n",
            "[20952 | 10234.73] loss=0.89 avg=0.81\n",
            "[20953 | 10237.81] loss=0.72 avg=0.81\n",
            "[20954 | 10240.90] loss=0.87 avg=0.81\n",
            "[20955 | 10244.00] loss=0.84 avg=0.81\n",
            "[20956 | 10247.10] loss=0.82 avg=0.81\n",
            "[20957 | 10250.19] loss=0.66 avg=0.81\n",
            "[20958 | 10253.28] loss=0.80 avg=0.81\n",
            "[20959 | 10256.38] loss=0.90 avg=0.81\n",
            "[20960 | 10259.49] loss=0.73 avg=0.81\n",
            "[20961 | 10262.59] loss=0.68 avg=0.81\n",
            "[20962 | 10265.67] loss=0.99 avg=0.81\n",
            "[20963 | 10268.77] loss=0.90 avg=0.81\n",
            "[20964 | 10271.88] loss=0.82 avg=0.81\n",
            "[20965 | 10274.97] loss=0.63 avg=0.81\n",
            "[20966 | 10278.08] loss=0.80 avg=0.81\n",
            "[20967 | 10281.16] loss=0.70 avg=0.81\n",
            "[20968 | 10284.27] loss=0.88 avg=0.81\n",
            "[20969 | 10287.36] loss=0.73 avg=0.81\n",
            "[20970 | 10290.46] loss=0.87 avg=0.81\n",
            "[20971 | 10293.56] loss=0.80 avg=0.81\n",
            "[20972 | 10296.66] loss=0.90 avg=0.81\n",
            "[20973 | 10299.75] loss=0.90 avg=0.81\n",
            "[20974 | 10302.83] loss=0.81 avg=0.81\n",
            "[20975 | 10305.90] loss=0.69 avg=0.81\n",
            "[20976 | 10308.97] loss=0.92 avg=0.81\n",
            "[20977 | 10312.04] loss=0.88 avg=0.81\n",
            "[20978 | 10315.14] loss=0.76 avg=0.81\n",
            "[20979 | 10318.23] loss=0.81 avg=0.81\n",
            "[20980 | 10321.32] loss=0.85 avg=0.81\n",
            "[20981 | 10324.41] loss=0.95 avg=0.81\n",
            "[20982 | 10327.51] loss=0.93 avg=0.81\n",
            "[20983 | 10330.61] loss=0.91 avg=0.81\n",
            "[20984 | 10333.70] loss=0.89 avg=0.81\n",
            "[20985 | 10336.81] loss=0.78 avg=0.81\n",
            "[20986 | 10339.90] loss=0.72 avg=0.81\n",
            "[20987 | 10342.99] loss=0.77 avg=0.81\n",
            "[20988 | 10346.08] loss=0.81 avg=0.81\n",
            "[20989 | 10349.17] loss=0.92 avg=0.81\n",
            "[20990 | 10352.26] loss=0.86 avg=0.81\n",
            "[20991 | 10355.34] loss=0.69 avg=0.81\n",
            "[20992 | 10358.43] loss=0.82 avg=0.81\n",
            "[20993 | 10361.53] loss=0.65 avg=0.81\n",
            "[20994 | 10364.62] loss=0.95 avg=0.81\n",
            "[20995 | 10367.71] loss=0.86 avg=0.81\n",
            "[20996 | 10370.79] loss=0.77 avg=0.81\n",
            "[20997 | 10373.87] loss=0.90 avg=0.81\n",
            "[20998 | 10376.95] loss=0.69 avg=0.81\n",
            "[20999 | 10380.03] loss=0.97 avg=0.81\n",
            "Saving /content/drive/My Drive/Colab Notebooks/checkpoints/run1/model-21000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " sus nueces.\n",
            "I think you're wrong. = Yo pienso que estás equivocado.\n",
            "Tom was disappointed in his son. = Tom estaba decepcionado de su hijo.\n",
            "You are very smart. = Eres muy listo.\n",
            "They're so busy. = Ellos están muy ocupados.\n",
            "I didn't quite understand the problem. = No he entendido el problema.\n",
            "Tom was surprised that Mary said that. = Tom se sorprendió de que Mary diga eso.\n",
            "Tom was caught in a lie. = Tom fue atrapado en una mentira.\n",
            "Where's your father now? = ¿Dónde está ahora tu padre?\n",
            "The police found out where to look. = La policía descubrió dónde buscar.\n",
            "I want to take it easy. = Quiero tomarlo bien.\n",
            "This book is mine. = Este libro es mío.\n",
            "You seem like a nice guy. = Te ves como un buen tipo.\n",
            "It's raining again. = Está lloviendo otra vez.\n",
            "I hate to be alone at home on a nice day. = Odio pasarme solo a casa con un bonito día.\n",
            "Tom likes singing. = A Tom le gusta cantar.\n",
            "She made up that story. = Ella se inventó esa historia.\n",
            "I hope we'll never have to do that again. = Espero que nunca tengamos que hacer eso nueve.\n",
            "Tom has a lot of friends in Boston. = Tom tiene muchos amigos en Boston.\n",
            "I want to go home. = Quiero ir a mi casa.\n",
            "I've never met you in person. = No te he encontrado nunca en persona.\n",
            "She likes oranges. = A ella le gustan las naranjas.\n",
            "I can't believe Tom made good use of my time. = No puedo creer que Tom me hiciera buena uso mi tiempo.\n",
            "There is a little boy standing outside crying. = Hay un niño que está parada afuera.\n",
            "We have to change something. = Tenemos que cambiar algo.\n",
            "My grandmother lived to be ninety-five. = Mi abuela vivió hasta los noventa.\n",
            "Tom is good at what he does. = Tom es bueno en lo que hace.\n",
            "Are you having a nice time? = ¿Estás pasando un buen rato?\n",
            "When the telephone rang, the maid ran to answer it. = Cuando sonó el teléfono, la bailazar corrisó al que retribuía.\n",
            "I went to the supermarket and bought a bag to take home. = Fui al supermercado y compré un paquete para llevar a casa.\n",
            "I'd like to sit by the window. = Quisiera sentarme en la ventana.\n",
            "Tom was able to fix the sink this morning. = Tom fue capaz de arreglar la letrina esta mañana.\n",
            "Don't let Tom block your path. = No dejes que Tom seas la raya a tu ruego.\n",
            "Can't we find a compromise? = ¿No podemos encontrar un compromiso?\n",
            "What were you doing? = ¿Qué estabas haciendo?\n",
            "I was there last year. = Yo estuve allá el año pasado.\n",
            "His parents were farmers. = Sus padres eran granjeros.\n",
            "They're here. = Están aquí.\n",
            "Tom couldn't bear the pain any more so he shot himself. = Tom no podía soportar más el dolor, así que se pegó un tiro.\n",
            "She's afraid of cats. = Ella le tiene miedo de gatos.\n",
            "How do I know which car to buy? = ¿Cómo sé cuál es el coche?\n",
            "I have a friend who lives in Japan now. = Tengo un amigo que vive en Japón ahora.\n",
            "Tom and Mary don't know why John decided to stay in Boston all these years. = Tom y Mary no sabe por qué decidiera ante John en que John decidió permanecer en Boston.\n",
            "We both got in. = Obstrebamos a la casa.\n",
            "You were here first. = Tú estuviste aquí primero.\n",
            "That's not what I meant. = Eso no es lo que quise decir.\n",
            "It's not\n",
            "\n",
            "[21000 | 10436.66] loss=0.91 avg=0.81\n",
            "[21001 | 10439.75] loss=0.96 avg=0.82\n",
            "[21002 | 10442.84] loss=0.82 avg=0.82\n",
            "[21003 | 10445.92] loss=0.72 avg=0.81\n",
            "[21004 | 10449.02] loss=1.04 avg=0.82\n",
            "[21005 | 10452.12] loss=0.80 avg=0.82\n",
            "[21006 | 10455.19] loss=0.94 avg=0.82\n",
            "[21007 | 10458.26] loss=0.94 avg=0.82\n",
            "[21008 | 10461.36] loss=0.87 avg=0.82\n",
            "[21009 | 10464.46] loss=0.77 avg=0.82\n",
            "[21010 | 10467.55] loss=0.79 avg=0.82\n",
            "[21011 | 10470.64] loss=1.00 avg=0.82\n",
            "[21012 | 10473.73] loss=0.73 avg=0.82\n",
            "[21013 | 10476.83] loss=0.71 avg=0.82\n",
            "[21014 | 10479.92] loss=0.84 avg=0.82\n",
            "[21015 | 10483.02] loss=0.67 avg=0.82\n",
            "[21016 | 10486.13] loss=0.91 avg=0.82\n",
            "[21017 | 10489.22] loss=0.84 avg=0.82\n",
            "[21018 | 10492.31] loss=0.70 avg=0.82\n",
            "[21019 | 10495.40] loss=0.88 avg=0.82\n",
            "[21020 | 10498.51] loss=0.73 avg=0.82\n",
            "[21021 | 10501.62] loss=0.77 avg=0.82\n",
            "[21022 | 10504.73] loss=0.90 avg=0.82\n",
            "[21023 | 10507.83] loss=0.81 avg=0.82\n",
            "[21024 | 10510.94] loss=0.84 avg=0.82\n",
            "[21025 | 10514.04] loss=0.91 avg=0.82\n",
            "[21026 | 10517.13] loss=0.84 avg=0.82\n",
            "[21027 | 10520.23] loss=0.93 avg=0.82\n",
            "[21028 | 10523.33] loss=0.84 avg=0.82\n",
            "[21029 | 10526.41] loss=0.88 avg=0.82\n",
            "[21030 | 10529.50] loss=0.78 avg=0.82\n",
            "[21031 | 10532.57] loss=0.76 avg=0.82\n",
            "[21032 | 10535.64] loss=0.85 avg=0.82\n",
            "[21033 | 10538.73] loss=0.85 avg=0.82\n",
            "[21034 | 10541.81] loss=1.00 avg=0.82\n",
            "[21035 | 10544.90] loss=0.81 avg=0.82\n",
            "[21036 | 10548.00] loss=0.81 avg=0.82\n",
            "[21037 | 10551.07] loss=0.81 avg=0.82\n",
            "[21038 | 10554.16] loss=0.85 avg=0.82\n",
            "[21039 | 10557.26] loss=0.87 avg=0.82\n",
            "[21040 | 10560.37] loss=0.76 avg=0.82\n",
            "[21041 | 10563.46] loss=0.79 avg=0.82\n",
            "[21042 | 10566.55] loss=0.79 avg=0.82\n",
            "[21043 | 10569.65] loss=0.96 avg=0.82\n",
            "[21044 | 10572.76] loss=0.76 avg=0.82\n",
            "[21045 | 10575.85] loss=0.69 avg=0.82\n",
            "[21046 | 10578.95] loss=0.85 avg=0.82\n",
            "[21047 | 10582.04] loss=0.86 avg=0.82\n",
            "[21048 | 10585.13] loss=0.78 avg=0.82\n",
            "[21049 | 10588.24] loss=0.77 avg=0.82\n",
            "[21050 | 10591.34] loss=0.94 avg=0.82\n",
            "[21051 | 10594.44] loss=0.76 avg=0.82\n",
            "[21052 | 10597.53] loss=0.75 avg=0.82\n",
            "[21053 | 10600.62] loss=0.81 avg=0.82\n",
            "[21054 | 10603.72] loss=0.76 avg=0.82\n",
            "[21055 | 10606.80] loss=0.69 avg=0.82\n",
            "[21056 | 10609.89] loss=0.83 avg=0.82\n",
            "[21057 | 10613.01] loss=0.75 avg=0.82\n",
            "[21058 | 10616.10] loss=0.80 avg=0.82\n",
            "[21059 | 10619.19] loss=0.79 avg=0.82\n",
            "[21060 | 10622.27] loss=0.91 avg=0.82\n",
            "[21061 | 10625.36] loss=0.94 avg=0.82\n",
            "[21062 | 10628.46] loss=0.81 avg=0.82\n",
            "[21063 | 10631.57] loss=0.69 avg=0.82\n",
            "[21064 | 10634.66] loss=0.76 avg=0.82\n",
            "[21065 | 10637.76] loss=0.80 avg=0.82\n",
            "[21066 | 10640.87] loss=0.78 avg=0.82\n",
            "[21067 | 10643.97] loss=0.88 avg=0.82\n",
            "[21068 | 10647.07] loss=0.82 avg=0.82\n",
            "[21069 | 10650.17] loss=0.99 avg=0.82\n",
            "[21070 | 10653.28] loss=0.67 avg=0.82\n",
            "[21071 | 10656.39] loss=0.73 avg=0.82\n",
            "[21072 | 10659.49] loss=0.76 avg=0.82\n",
            "[21073 | 10662.59] loss=0.81 avg=0.82\n",
            "[21074 | 10665.69] loss=0.65 avg=0.81\n",
            "[21075 | 10668.79] loss=0.72 avg=0.81\n",
            "[21076 | 10671.89] loss=0.86 avg=0.81\n",
            "[21077 | 10674.98] loss=0.78 avg=0.81\n",
            "[21078 | 10678.08] loss=0.86 avg=0.81\n",
            "[21079 | 10681.18] loss=0.83 avg=0.81\n",
            "[21080 | 10684.29] loss=0.82 avg=0.81\n",
            "[21081 | 10687.40] loss=0.88 avg=0.82\n",
            "[21082 | 10690.50] loss=0.74 avg=0.81\n",
            "[21083 | 10693.60] loss=0.72 avg=0.81\n",
            "[21084 | 10696.70] loss=0.85 avg=0.81\n",
            "[21085 | 10699.80] loss=0.96 avg=0.82\n",
            "[21086 | 10702.89] loss=0.71 avg=0.81\n",
            "[21087 | 10706.00] loss=0.75 avg=0.81\n",
            "[21088 | 10709.09] loss=0.87 avg=0.81\n",
            "[21089 | 10712.20] loss=0.73 avg=0.81\n",
            "[21090 | 10715.29] loss=0.78 avg=0.81\n",
            "[21091 | 10718.40] loss=0.84 avg=0.81\n",
            "[21092 | 10721.51] loss=0.71 avg=0.81\n",
            "[21093 | 10724.60] loss=0.93 avg=0.81\n",
            "[21094 | 10727.69] loss=0.71 avg=0.81\n",
            "[21095 | 10730.79] loss=0.75 avg=0.81\n",
            "[21096 | 10733.88] loss=0.77 avg=0.81\n",
            "[21097 | 10736.97] loss=0.92 avg=0.81\n",
            "[21098 | 10740.06] loss=0.77 avg=0.81\n",
            "[21099 | 10743.15] loss=0.74 avg=0.81\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " en ajeno.\n",
            "I am going to wait for her. = La esperaré.\n",
            "I am interested in Chinese architecture. = Estoy interesado en la arquitectura china.\n",
            "When was the last time you fed the cows? = ¿Cuándo fue la última vez que alimentaste a las vacas?\n",
            "I just felt like hearing the sound of your voice. = Solo me sentí que estaba escuchando tu voz.\n",
            "I really want to get married. = Realmente quiero casarme.\n",
            "Tom seems lonely without Mary. = Tom parece solo sin Mary.\n",
            "Why do you want me to tell you this? = ¿Por qué quieres que te cuente esto?\n",
            "He was an honest man. = Era un hombre honrado.\n",
            "Do you want me to answer? = ¿Querés que conteste?\n",
            "I know what to do first. = Sé qué hacer primero.\n",
            "That's not a real telephone. = Ese no es un teléfono real.\n",
            "She doesn't give orders. = Ella no da órdenes.\n",
            "Please turn off the light while I'm taking a bath. = Apagá la luz mientras me bañé, por favor.\n",
            "I love that shirt. = Me encanta esa camisa.\n",
            "We've had a lot of snow this year. = Hemos tenido mucha nieve este año.\n",
            "He's very popular among students. = Es muy popular entre los alumútros.\n",
            "That was the least interesting of all the choices. = Era la menor of toda la alternativa.\n",
            "Please make yourself at home. = Por favor, sentí como en casa.\n",
            "I am sure he would be happy to see you. = Estoy seguro de que él iba a gustar verte.\n",
            "The house I remember so well is the one I found today. = La casa que me recuerdo tan bien cuya age que encontré hoy.\n",
            "There is no sense in doing that. = No tiene sentido hacer eso.\n",
            "He lives very alone. = Él vive muy solitude.\n",
            "Tom thought about what Mary had told him. = Tom pensó acerca de lo que Mary les dijo.\n",
            "Tom said he could not get Mary on the phone. = Tom dijo que no pudo a Mary al teléfono.\n",
            "The boy seems to be rich. = Parece que el niño no parece rico.\n",
            "I'd like to think about it. = Quisiera pensármelo.\n",
            "You did it. = Lo hiciste.\n",
            "I have to clean the entire house. = Tengo que limpiar la casa entera.\n",
            "He is a writer. = Es escritor.\n",
            "That was just an enormous bluff. = Eso fue sólo una gran blanca.\n",
            "A woman's beauty depends on her looks. = Las bellecas de las femenas dependen de sus verdes.\n",
            "Are you sure you don't want anything? = ¿Estás segura de que no quieres nada?\n",
            "Mary was wearing a black skirt. = Mary vestía una chaqueta negra de negro.\n",
            "All is still. = Todo está en calma.\n",
            "I have no time to write. Could you send it by email? = No tengo tiempo para escribir. ¿Podría enviarlo por correo electrónico?\n",
            "The dog is very tame. = El perro es muy difícil.\n",
            "What's so special? = ¿Qué es tan especial?\n",
            "There were so many people. = Había nadie de personas.\n",
            "I can't hear you very well. = No te puedo escuchar bien.\n",
            "I was born in 1975. = Nací en 1975.\n",
            "My mother bought me this toy when I was twelve years old. = Mi madre me compró este juguete cuando yo tenía doce años.\n",
            "She was at the station. = Ella estaba en la estación.\n",
            "It is said that the Stone was brought to this place by an angel. = Se dice que la Piscina fue trajido hasta este lugar por un ángel.\n",
            "The boy ran away and ran back to the house. = El chico huyó y se fue corriendo hacia la casa.\n",
            "He tried to solve the problem. = Intentó resolver el problema.\n",
            "I don't\n",
            "\n",
            "[21100 | 10783.88] loss=0.85 avg=0.81\n",
            "[21101 | 10786.97] loss=0.95 avg=0.81\n",
            "[21102 | 10790.07] loss=0.89 avg=0.81\n",
            "[21103 | 10793.17] loss=0.91 avg=0.81\n",
            "[21104 | 10796.28] loss=0.90 avg=0.82\n",
            "[21105 | 10799.38] loss=0.91 avg=0.82\n",
            "[21106 | 10802.49] loss=0.97 avg=0.82\n",
            "[21107 | 10805.59] loss=0.73 avg=0.82\n",
            "[21108 | 10808.68] loss=0.92 avg=0.82\n",
            "[21109 | 10811.78] loss=0.81 avg=0.82\n",
            "[21110 | 10814.87] loss=0.81 avg=0.82\n",
            "[21111 | 10817.92] loss=0.80 avg=0.82\n",
            "[21112 | 10821.01] loss=0.87 avg=0.82\n",
            "[21113 | 10824.11] loss=0.75 avg=0.82\n",
            "[21114 | 10827.21] loss=0.82 avg=0.82\n",
            "[21115 | 10830.31] loss=0.84 avg=0.82\n",
            "[21116 | 10833.42] loss=0.82 avg=0.82\n",
            "[21117 | 10836.52] loss=0.98 avg=0.82\n",
            "[21118 | 10839.62] loss=0.93 avg=0.82\n",
            "[21119 | 10842.72] loss=0.74 avg=0.82\n",
            "[21120 | 10845.82] loss=0.92 avg=0.82\n",
            "[21121 | 10848.92] loss=0.82 avg=0.82\n",
            "[21122 | 10852.01] loss=0.73 avg=0.82\n",
            "[21123 | 10855.11] loss=0.70 avg=0.82\n",
            "[21124 | 10858.21] loss=0.95 avg=0.82\n",
            "[21125 | 10861.31] loss=0.78 avg=0.82\n",
            "[21126 | 10864.42] loss=0.93 avg=0.82\n",
            "[21127 | 10867.51] loss=0.89 avg=0.82\n",
            "[21128 | 10870.61] loss=0.84 avg=0.82\n",
            "[21129 | 10873.70] loss=0.71 avg=0.82\n",
            "[21130 | 10876.78] loss=0.67 avg=0.82\n",
            "[21131 | 10879.89] loss=0.69 avg=0.82\n",
            "[21132 | 10882.98] loss=0.88 avg=0.82\n",
            "[21133 | 10886.07] loss=0.87 avg=0.82\n",
            "[21134 | 10889.18] loss=0.74 avg=0.82\n",
            "[21135 | 10892.28] loss=0.72 avg=0.82\n",
            "[21136 | 10895.38] loss=0.72 avg=0.82\n",
            "[21137 | 10898.47] loss=0.78 avg=0.82\n",
            "[21138 | 10901.57] loss=0.72 avg=0.82\n",
            "[21139 | 10904.67] loss=0.67 avg=0.81\n",
            "[21140 | 10907.78] loss=0.81 avg=0.81\n",
            "[21141 | 10910.89] loss=0.76 avg=0.81\n",
            "[21142 | 10913.99] loss=0.91 avg=0.81\n",
            "[21143 | 10917.11] loss=0.92 avg=0.81\n",
            "[21144 | 10920.21] loss=0.84 avg=0.82\n",
            "[21145 | 10923.32] loss=0.74 avg=0.81\n",
            "[21146 | 10926.42] loss=0.77 avg=0.81\n",
            "[21147 | 10929.52] loss=0.87 avg=0.81\n",
            "[21148 | 10932.63] loss=0.99 avg=0.82\n",
            "[21149 | 10935.72] loss=0.66 avg=0.81\n",
            "[21150 | 10938.82] loss=0.87 avg=0.82\n",
            "[21151 | 10941.92] loss=0.76 avg=0.81\n",
            "[21152 | 10945.03] loss=0.66 avg=0.81\n",
            "[21153 | 10948.13] loss=0.67 avg=0.81\n",
            "[21154 | 10951.25] loss=0.74 avg=0.81\n",
            "[21155 | 10954.35] loss=0.78 avg=0.81\n",
            "[21156 | 10957.46] loss=0.77 avg=0.81\n",
            "[21157 | 10960.55] loss=0.80 avg=0.81\n",
            "[21158 | 10963.65] loss=0.92 avg=0.81\n",
            "[21159 | 10966.75] loss=0.94 avg=0.81\n",
            "[21160 | 10969.86] loss=0.96 avg=0.81\n",
            "[21161 | 10972.96] loss=0.96 avg=0.82\n",
            "[21162 | 10976.06] loss=0.83 avg=0.82\n",
            "[21163 | 10979.15] loss=0.79 avg=0.82\n",
            "[21164 | 10982.26] loss=1.07 avg=0.82\n",
            "[21165 | 10985.35] loss=0.67 avg=0.82\n",
            "[21166 | 10988.45] loss=0.66 avg=0.82\n",
            "[21167 | 10991.54] loss=0.84 avg=0.82\n",
            "[21168 | 10994.63] loss=0.88 avg=0.82\n",
            "[21169 | 10997.72] loss=0.81 avg=0.82\n",
            "[21170 | 11000.83] loss=0.90 avg=0.82\n",
            "[21171 | 11003.92] loss=0.83 avg=0.82\n",
            "[21172 | 11007.02] loss=0.86 avg=0.82\n",
            "[21173 | 11010.11] loss=0.83 avg=0.82\n",
            "[21174 | 11013.18] loss=0.73 avg=0.82\n",
            "[21175 | 11016.29] loss=0.83 avg=0.82\n",
            "[21176 | 11019.38] loss=0.93 avg=0.82\n",
            "[21177 | 11022.46] loss=0.70 avg=0.82\n",
            "[21178 | 11025.53] loss=0.73 avg=0.82\n",
            "[21179 | 11028.61] loss=0.66 avg=0.81\n",
            "[21180 | 11031.68] loss=0.79 avg=0.81\n",
            "[21181 | 11034.77] loss=0.81 avg=0.81\n",
            "[21182 | 11037.87] loss=0.94 avg=0.82\n",
            "[21183 | 11040.95] loss=0.97 avg=0.82\n",
            "[21184 | 11044.03] loss=0.80 avg=0.82\n",
            "[21185 | 11047.09] loss=0.77 avg=0.82\n",
            "[21186 | 11050.15] loss=0.89 avg=0.82\n",
            "[21187 | 11053.21] loss=0.82 avg=0.82\n",
            "[21188 | 11056.28] loss=0.74 avg=0.82\n",
            "[21189 | 11059.34] loss=0.79 avg=0.82\n",
            "[21190 | 11062.43] loss=0.93 avg=0.82\n",
            "[21191 | 11065.53] loss=0.74 avg=0.82\n",
            "[21192 | 11068.61] loss=0.75 avg=0.82\n",
            "[21193 | 11071.70] loss=0.95 avg=0.82\n",
            "[21194 | 11074.76] loss=0.76 avg=0.82\n",
            "[21195 | 11077.85] loss=0.93 avg=0.82\n",
            "[21196 | 11080.93] loss=0.67 avg=0.82\n",
            "[21197 | 11084.02] loss=0.69 avg=0.81\n",
            "[21198 | 11087.10] loss=0.94 avg=0.82\n",
            "[21199 | 11090.17] loss=0.78 avg=0.82\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " lñarse la muerte.\n",
            "He didn't tell me. = Él no me lo dijo.\n",
            "I'm eating lunch, Dad. = Estoy almorzando, papá.\n",
            "He went swimming in the river. = Se fue a nadar al río.\n",
            "Tom always uses the same dictionary on any subject that he researches. = Tom siempre usa el mismo diccionario en cualquier tema del que estudia.\n",
            "Tom should be up soon. = Tom debería estar andado.\n",
            "I have to help my mother. = Tengo que ayudar a mamá.\n",
            "Tom told Mary that he did not love her. = Tom le dijo a Mary que no la amaba.\n",
            "You'd better ask someone else. = Es que buscar a otra persona.\n",
            "Tom is a very good table tennis player. = Tom casi es muy buen jugador de ping-pong.\n",
            "I just got a letter from Tom. = Acabo de recibirle una carta de Tom.\n",
            "It is our duty to help them. = Es nuestro deber ayudarle.\n",
            "My house is here. = Mi casa está aquí.\n",
            "The house is dirty. = La casa está sucia.\n",
            "His old car is parked in front of his house. = Su antiguo vehículo está parado frente a su casa.\n",
            "We're almost out of time. = Muriéramos casi nieto.\n",
            "The woman is wearing a skirt. = La mujer lleva una falda.\n",
            "The dog followed its master tightly. = El perro siguió firmemente con lo suha.\n",
            "Tom wasn't able to finish all his homework quickly enough. = Tom no fue capaz de terminar toda su tarea suficientemente rápido.\n",
            "I'm not going to tell you where it is. = No te voy a decir dónde está.\n",
            "Tom isn't able to do it by himself. = Tom no puede hacerlo por sí solo.\n",
            "Tom bought a red car. = Tom compró un auto rojo.\n",
            "He got up at six every morning. = Él se levantó todos los días convenciendo.\n",
            "She is wearing glasses. = Ella lleva gafas.\n",
            "I have four computers. = Tengo cuatro computadores.\n",
            "He doesn't like him. = Le no le gusta.\n",
            "It's my sister's bicycle. = Es la bicicleta de mi hermana.\n",
            "Tom went into the kitchen looking for Mary. = Tom entró a la cocina a buscar a Mary.\n",
            "All our efforts resulted in nothing. = Todos nuestros intentos resultaron nada.\n",
            "Can we talk? = ¿Podemos hablar?\n",
            "Tom always tells the truth. = Tom siempre dice la verdad.\n",
            "I feel the same way as you do. = Me siento igual que tiene.\n",
            "I'm not sure. = No estoy seguro.\n",
            "I think it's time for me to write my father a letter. = Creo que es hora de que me escriba una carta a mi papá.\n",
            "What do you mean? = ¿Qué quiere decir?\n",
            "He was a good king. = Fue un buen rey.\n",
            "I wanted to make sure that Tom was in his room. = Yo quería asegurarme de que Tom estaba en su habitación.\n",
            "Do you have any advice for me? = ¿Tenéis algún consejo para mí?\n",
            "I'm looking forward to playing tennis with Tom. = Estoy deseando jugar tenis con Tom.\n",
            "I think it's going to be a warm and friendly holiday. = Creo que voy a tener unas flores y amistosa de regoción.\n",
            "This soup is too salty to eat. = Esta sopa está muy salada como para comer.\n",
            "That is not true. = Eso no es verdad.\n",
            "I will stay here for a couple of days. = Me quedaré un par de días a un par de tenerme.\n",
            "That is not true. = Eso no es verdad.\n",
            "I'm trying. = Lo estoy intentando.\n",
            "This is our friend. = Este es nuestro amigo.\n",
            "This dictionary deals with words. = Este diccionario trata a palabras.\n",
            "Why is Tom always so serious? = ¿\n",
            "\n",
            "[21200 | 11130.67] loss=0.70 avg=0.81\n",
            "[21201 | 11133.75] loss=0.84 avg=0.81\n",
            "[21202 | 11136.84] loss=0.82 avg=0.81\n",
            "[21203 | 11139.92] loss=0.89 avg=0.82\n",
            "[21204 | 11143.00] loss=0.77 avg=0.81\n",
            "[21205 | 11146.09] loss=0.71 avg=0.81\n",
            "[21206 | 11149.16] loss=0.83 avg=0.81\n",
            "[21207 | 11152.25] loss=0.73 avg=0.81\n",
            "[21208 | 11155.33] loss=0.92 avg=0.81\n",
            "[21209 | 11158.43] loss=0.84 avg=0.81\n",
            "[21210 | 11161.52] loss=0.79 avg=0.81\n",
            "[21211 | 11164.61] loss=0.94 avg=0.82\n",
            "[21212 | 11167.69] loss=0.94 avg=0.82\n",
            "[21213 | 11170.78] loss=0.74 avg=0.82\n",
            "[21214 | 11173.88] loss=0.69 avg=0.81\n",
            "[21215 | 11176.95] loss=0.73 avg=0.81\n",
            "[21216 | 11180.03] loss=0.69 avg=0.81\n",
            "[21217 | 11183.12] loss=0.74 avg=0.81\n",
            "[21218 | 11186.21] loss=0.79 avg=0.81\n",
            "[21219 | 11189.30] loss=0.69 avg=0.81\n",
            "[21220 | 11192.40] loss=0.78 avg=0.81\n",
            "[21221 | 11195.48] loss=0.89 avg=0.81\n",
            "[21222 | 11198.56] loss=0.62 avg=0.81\n",
            "[21223 | 11201.64] loss=0.79 avg=0.81\n",
            "[21224 | 11204.72] loss=0.91 avg=0.81\n",
            "[21225 | 11207.81] loss=0.70 avg=0.81\n",
            "[21226 | 11210.91] loss=0.65 avg=0.81\n",
            "[21227 | 11214.01] loss=0.81 avg=0.81\n",
            "[21228 | 11217.10] loss=0.63 avg=0.81\n",
            "[21229 | 11220.18] loss=0.75 avg=0.80\n",
            "[21230 | 11223.27] loss=0.77 avg=0.80\n",
            "[21231 | 11226.36] loss=0.81 avg=0.80\n",
            "[21232 | 11229.45] loss=0.94 avg=0.81\n",
            "[21233 | 11232.55] loss=0.71 avg=0.81\n",
            "[21234 | 11235.63] loss=0.93 avg=0.81\n",
            "[21235 | 11238.71] loss=0.95 avg=0.81\n",
            "[21236 | 11241.80] loss=0.81 avg=0.81\n",
            "[21237 | 11244.89] loss=0.90 avg=0.81\n",
            "[21238 | 11248.00] loss=0.78 avg=0.81\n",
            "[21239 | 11251.09] loss=0.62 avg=0.81\n",
            "[21240 | 11254.19] loss=0.65 avg=0.80\n",
            "[21241 | 11257.28] loss=0.86 avg=0.81\n",
            "[21242 | 11260.36] loss=0.74 avg=0.80\n",
            "[21243 | 11263.43] loss=0.79 avg=0.80\n",
            "[21244 | 11266.53] loss=0.77 avg=0.80\n",
            "[21245 | 11269.62] loss=0.85 avg=0.80\n",
            "[21246 | 11272.71] loss=0.92 avg=0.81\n",
            "[21247 | 11275.80] loss=0.87 avg=0.81\n",
            "[21248 | 11278.89] loss=0.89 avg=0.81\n",
            "[21249 | 11282.01] loss=0.92 avg=0.81\n",
            "[21250 | 11285.11] loss=0.92 avg=0.81\n",
            "[21251 | 11288.22] loss=0.90 avg=0.81\n",
            "[21252 | 11291.30] loss=0.66 avg=0.81\n",
            "[21253 | 11294.38] loss=0.82 avg=0.81\n",
            "[21254 | 11297.49] loss=0.85 avg=0.81\n",
            "[21255 | 11300.56] loss=0.76 avg=0.81\n",
            "[21256 | 11303.63] loss=0.73 avg=0.81\n",
            "[21257 | 11306.71] loss=0.78 avg=0.81\n",
            "[21258 | 11309.79] loss=0.93 avg=0.81\n",
            "[21259 | 11312.88] loss=0.75 avg=0.81\n",
            "[21260 | 11315.97] loss=0.83 avg=0.81\n",
            "[21261 | 11319.07] loss=0.62 avg=0.81\n",
            "[21262 | 11322.17] loss=0.99 avg=0.81\n",
            "[21263 | 11325.23] loss=0.80 avg=0.81\n",
            "[21264 | 11328.30] loss=0.89 avg=0.81\n",
            "[21265 | 11331.38] loss=0.71 avg=0.81\n",
            "[21266 | 11334.46] loss=0.71 avg=0.81\n",
            "[21267 | 11337.57] loss=0.94 avg=0.81\n",
            "[21268 | 11340.66] loss=0.87 avg=0.81\n",
            "[21269 | 11343.74] loss=0.75 avg=0.81\n",
            "[21270 | 11346.83] loss=0.83 avg=0.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pENG5_kYWYgt"
      },
      "source": [
        "### Especificar los checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BMA8JAZqWYgw",
        "outputId": "c35c67fa-1c17-44a5-db66-6fc09c4a77cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# modelo tuneado con traducciones de inglés a español\n",
        "checkpoint = checkpoint_dir + '/run1_spa-eng'\n",
        "checkpoint_num = '45000'\n",
        "model_checkpoint_path = 'model_checkpoint_path: \"' + checkpoint + '/model-' + checkpoint_num + '\"'\n",
        "\n",
        "with open('gpt-2/models/345M/checkpoint', \"wt\") as file:\n",
        "    print(model_checkpoint_path)\n",
        "    file.write(model_checkpoint_path)\n",
        "with open('gpt-2/models/345M/counter', \"wt\") as file:\n",
        "    file.write(f'{checkpoint_num}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_checkpoint_path: \"/content/drive/My Drive/Colab Notebooks/checkpoints/run1_spa-eng/model-45000\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "-sSDW9HnAMxT",
        "outputId": "e9796956-30f6-44d0-888e-35be9dbbbde3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "ejemplo = \"Who did you go to the movies with? =\" #@param {type : 'string'}\n",
        "numero_de_muestras = 1 #@param {type : 'number'}\n",
        "temperature=0.1 #@param {type : 'number'}\n",
        "#@markdown La temperatura controla el grado de aleatoriedad (0 = determinista)\n",
        "top_k=40 #@param {type : 'integer'}\n",
        "#@markdown Número de candidatos considerados en el beam search (0 = \"greedy\", funciona bien con 40)\n",
        "top_p=0.1 #@param {type : 'number'}\n",
        "#@markdown Controla la diversidad. (0 = valor por defecto, funciona bien con 0.9)\n",
        "texts = interact_model(prompt=ejemplo,\n",
        "                       model_name='345M',\n",
        "                       nsamples=numero_de_muestras,\n",
        "                       temperature=temperature,\n",
        "                       top_k=top_k,\n",
        "                       top_p=top_p)\n",
        "display(HTML('<p><b><i>' + ejemplo + '</b></i></p>'))\n",
        "display(HTML('<p>' + texts[0][:texts[0].find('.')+1] + '</p>'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p><b><i>Who did you go to the movies with? =</b></i></p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p> ¿Este es mienda con?\n",
              "I'm not going to hurt you.</p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6GbnbEd02Efe",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}