{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img width=\"220px\" src=\"Bertle.gif\"></p>\n",
    "Vamos a hacer un motor de búsqueda semántica con los datos de Wikipedia en español."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T21:10:22.226616Z",
     "start_time": "2019-08-18T21:10:20.768732Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# instalar BERT\n",
    "import sys\n",
    "\n",
    "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
    "if not 'bert_repo' in sys.path:\n",
    "    sys.path += ['bert_repo']\n",
    "\n",
    "# import python modules defined by BERT\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T22:19:43.890614Z",
     "start_time": "2019-08-18T22:19:43.873063Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'IFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e0b94f121b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TFHUB_CACHE_DIR'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./tfhub'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'IFrame'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "os.environ['TFHUB_CACHE_DIR'] = './tfhub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T21:10:22.911318Z",
     "start_time": "2019-08-18T21:10:22.369932Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código para leer el fichero de [Wikipedia](https://es.wikipedia.org/wiki/Wikipedia:Descargas) (en formato ZIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T21:10:22.961868Z",
     "start_time": "2019-08-18T21:10:22.915441Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'robado' de https://github.com/kimbauters/ZIMply/blob/master/zimply/zimply.py\n",
    "\n",
    "import io\n",
    "import lzma\n",
    "import logging\n",
    "from functools import partial, lru_cache\n",
    "from collections import namedtuple\n",
    "from struct import Struct, pack, unpack\n",
    "\n",
    "ZERO = pack(\"B\", 0)  # defined for zero terminated fields\n",
    "Field = namedtuple(\"Field\", [\"format\", \"field_name\"])  # a tuple\n",
    "Article = namedtuple(\"Article\", [\"data\", \"namespace\", \"mimetype\"])  # a triple\n",
    "\n",
    "iso639_3to1 = {\"ara\": \"ar\", \"dan\": \"da\", \"nld\": \"nl\", \"eng\": \"en\",\n",
    "               \"fin\": \"fi\", \"fra\": \"fr\", \"deu\": \"de\", \"hun\": \"hu\",\n",
    "               \"ita\": \"it\", \"nor\": \"no\", \"por\": \"pt\", \"ron\": \"ro\",\n",
    "               \"rus\": \"ru\", \"spa\": \"es\", \"swe\": \"sv\", \"tur\": \"tr\"}\n",
    "\n",
    "\n",
    "def read_zero_terminated(file, encoding):\n",
    "    \"\"\"\n",
    "    Retrieve a ZERO terminated string by reading byte by byte until the ending\n",
    "    ZERO terminated field is encountered.\n",
    "    :param file: the file to read from\n",
    "    :param encoding: the encoding used for the file\n",
    "    :return: the decoded string, up to but not including the ZERO termination\n",
    "    \"\"\"\n",
    "    # read until we find the ZERO termination\n",
    "    buffer = iter(partial(file.read, 1), ZERO)\n",
    "    # join all the bytes together\n",
    "    field = b\"\".join(buffer)\n",
    "    # transform the bytes into a string and return the string\n",
    "    return field.decode(encoding=encoding, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def convert_size(size):\n",
    "    \"\"\"\n",
    "    Convert a given size in bytes to a human-readable string of the file size.\n",
    "    :param size: the size in bytes\n",
    "    :return: a human-readable string of the size\n",
    "    \"\"\"\n",
    "    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "    power = int(floor(log(size, 1024)))\n",
    "    base = pow(1024, power)\n",
    "    size = round(size/base, 2)\n",
    "    return '%s %s' % (size, size_name[power])\n",
    "\n",
    "\n",
    "HEADER = [  # define the HEADER structure of a ZIM file\n",
    "    Field(\"I\", \"magicNumber\"),\n",
    "    Field(\"I\", \"version\"),\n",
    "    Field(\"Q\", \"uuid_low\"),\n",
    "    Field(\"Q\", \"uuid_high\"),\n",
    "    Field(\"I\", \"articleCount\"),\n",
    "    Field(\"I\", \"clusterCount\"),\n",
    "    Field(\"Q\", \"urlPtrPos\"),\n",
    "    Field(\"Q\", \"titlePtrPos\"),\n",
    "    Field(\"Q\", \"clusterPtrPos\"),\n",
    "    Field(\"Q\", \"mimeListPos\"),\n",
    "    Field(\"I\", \"mainPage\"),\n",
    "    Field(\"I\", \"layoutPage\"),\n",
    "    Field(\"Q\", \"checksumPos\")\n",
    "]\n",
    "\n",
    "ARTICLE_ENTRY = [  # define the ARTICLE ENTRY structure of a ZIM file\n",
    "    Field(\"H\", \"mimetype\"),\n",
    "    Field(\"B\", \"parameterLen\"),\n",
    "    Field(\"c\", \"namespace\"),\n",
    "    Field(\"I\", \"revision\"),\n",
    "    Field(\"I\", \"clusterNumber\"),\n",
    "    Field(\"I\", \"blobNumber\")\n",
    "    # zero terminated url of variable length; not a Field\n",
    "    # zero terminated title of variable length; not a Field\n",
    "    # variable length parameter data as per parameterLen; not a Field\n",
    "]\n",
    "\n",
    "REDIRECT_ENTRY = [  # define the REDIRECT ENTRY structure of a ZIM file\n",
    "    Field(\"H\", \"mimetype\"),\n",
    "    Field(\"B\", \"parameterLen\"),\n",
    "    Field(\"c\", \"namespace\"),\n",
    "    Field(\"I\", \"revision\"),\n",
    "    Field(\"I\", \"redirectIndex\")\n",
    "    # zero terminated url of variable length; not a Field\n",
    "    # zero terminated title of variable length; not a Field\n",
    "    # variable length parameter data as per parameterLen; not a Field\n",
    "]\n",
    "\n",
    "CLUSTER = [  # define the CLUSTER structure of a ZIM file\n",
    "    Field(\"B\", \"compressionType\")\n",
    "]\n",
    "\n",
    "\n",
    "class Block:\n",
    "    def __init__(self, structure, encoding):\n",
    "        self._structure = structure\n",
    "        self._encoding = encoding\n",
    "        # Create a new Struct object to correctly read the binary data in this\n",
    "        # block in particular, pass it along that it is a little endian (<),\n",
    "        # along with all expected fields.\n",
    "        self._compiled = Struct(\"<\" + \"\".join(\n",
    "            [field.format for field in self._structure]))\n",
    "        self.size = self._compiled.size\n",
    "\n",
    "    def unpack(self, buffer, offset=0):\n",
    "        # Use the Struct to read the binary data in the buffer\n",
    "        # where this block appears at the given offset.\n",
    "        values = self._compiled.unpack_from(buffer, offset)\n",
    "        # Match up each value with the corresponding field in the block\n",
    "        # and put it in a dictionary for easy reference.\n",
    "        return {field.field_name: value for value, field in\n",
    "                zip(values, self._structure)}\n",
    "\n",
    "    def _unpack_from_file(self, file, offset=None):\n",
    "        if offset is not None:\n",
    "            # move the pointer in the file to the specified offset;\n",
    "            # this is not index 0\n",
    "            file.seek(offset)\n",
    "        # read in the amount of data corresponding to the block size\n",
    "        buffer = file.read(self.size)\n",
    "        # return the values of the fields after unpacking them\n",
    "        return self.unpack(buffer)\n",
    "\n",
    "    def unpack_from_file(self, file, seek=None):\n",
    "        # When more advanced behaviour is needed,\n",
    "        # this method can be overridden by subclassing.\n",
    "        return self._unpack_from_file(file, seek)\n",
    "\n",
    "\n",
    "class HeaderBlock(Block):\n",
    "    def __init__(self, encoding):\n",
    "        super().__init__(HEADER, encoding)\n",
    "\n",
    "\n",
    "class MimeTypeListBlock(Block):\n",
    "    def __init__(self, encoding):\n",
    "        super().__init__(\"\", encoding)\n",
    "\n",
    "    def unpack_from_file(self, file, offset=None):\n",
    "        # move the pointer in the file to the specified offset as\n",
    "        # this is not index 0 when an offset is specified\n",
    "        if offset is not None:\n",
    "            file.seek(offset)\n",
    "        mimetypes = []  # prepare an empty list to store the mimetypes\n",
    "        while True:\n",
    "            # get the next zero terminated field\n",
    "            s = read_zero_terminated(file, self._encoding)\n",
    "            mimetypes.append(s)  # add the newly found mimetype to the list\n",
    "            if s == \"\":  # the last entry must be an empty string\n",
    "                mimetypes.pop()  # pop the last entry\n",
    "                return mimetypes  # return the list of mimetypes we found\n",
    "\n",
    "\n",
    "class ClusterBlock(Block):\n",
    "    def __init__(self, encoding):\n",
    "        super().__init__(CLUSTER, encoding)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=32)  # provide an LRU cache for this object\n",
    "class ClusterData(object):\n",
    "    def __init__(self, file, offset, encoding):\n",
    "        self.file = file  # store the file\n",
    "        self.offset = offset  # store the offset\n",
    "        cluster_info = ClusterBlock(encoding).unpack_from_file(\n",
    "            self.file, self.offset)  # Get the cluster fields.\n",
    "        # Verify whether the cluster has LZMA2 compression\n",
    "        self.compressed = cluster_info['compressionType'] == 4\n",
    "        # at the moment, we don't have any uncompressed data\n",
    "        self.uncompressed = None\n",
    "        self._decompress()  # decompress the contents as needed\n",
    "        # Prepare storage to keep track of the offsets\n",
    "        # of the blobs in the cluster.\n",
    "        self._offsets = []\n",
    "        # proceed to actually read the offsets of the blobs in this cluster\n",
    "        self._read_offsets()\n",
    "\n",
    "    def _decompress(self, chunk_size=32768):\n",
    "        if self.compressed:\n",
    "            # create a bytes stream to store the uncompressed cluster data\n",
    "            self.buffer = io.BytesIO()\n",
    "            decompressor = lzma.LZMADecompressor()  # prepare the decompressor\n",
    "            # move the file pointer to the start of the blobs as long as we\n",
    "            # don't reach the end of the stream.\n",
    "            self.file.seek(self.offset + 1)\n",
    "\n",
    "            while not decompressor.eof:\n",
    "                chunk = self.file.read(chunk_size)  # read in a chunk\n",
    "                data = decompressor.decompress(chunk)  # decompress the chunk\n",
    "                self.buffer.write(data)  # and store it in the buffer area\n",
    "\n",
    "    def _source_buffer(self):\n",
    "        # get the file buffer or the decompressed buffer\n",
    "        buffer = self.buffer if self.compressed else self.file\n",
    "        # move the buffer to the starting position\n",
    "        buffer.seek(0 if self.compressed else self.offset + 1)\n",
    "        return buffer\n",
    "\n",
    "    def _read_offsets(self):\n",
    "        # get the buffer for this cluster\n",
    "        buffer = self._source_buffer()\n",
    "        # read the offset for the first blob\n",
    "        offset0 = unpack(\"<I\", buffer.read(4))[0]\n",
    "        # store this one in the list of offsets\n",
    "        self._offsets.append(offset0)\n",
    "        # calculate the number of blobs by dividing the first blob by 4\n",
    "        number_of_blobs = int(offset0 / 4)\n",
    "        for idx in range(number_of_blobs - 1):\n",
    "            # store the offsets to all other blobs\n",
    "            self._offsets.append(unpack(\"<I\", buffer.read(4))[0])\n",
    "\n",
    "    def read_blob(self, blob_index):\n",
    "        # check if the blob falls within the range\n",
    "        if blob_index >= len(self._offsets) - 1:\n",
    "            raise IOError(\"Blob index exceeds number of blobs available: %s\" %\n",
    "                          blob_index)\n",
    "        buffer = self._source_buffer()  # get the buffer for this cluster\n",
    "        # calculate the size of the blob\n",
    "        blob_size = self._offsets[blob_index+1] - self._offsets[blob_index]\n",
    "        # move to the position of the blob relative to current position\n",
    "        buffer.seek(self._offsets[blob_index], 1)\n",
    "        return buffer.read(blob_size)\n",
    "\n",
    "\n",
    "class DirectoryBlock(Block):\n",
    "    def __init__(self, structure, encoding):\n",
    "        super().__init__(structure, encoding)\n",
    "\n",
    "    def unpack_from_file(self, file, seek=None):\n",
    "        # read the first fields as defined in the ARTICLE_ENTRY structure\n",
    "        field_values = super()._unpack_from_file(file, seek)\n",
    "        # then read in the url, which is a zero terminated field\n",
    "        field_values[\"url\"] = read_zero_terminated(file, self._encoding)\n",
    "        # followed by the title, which is again a zero terminated field\n",
    "        field_values[\"title\"] = read_zero_terminated(file, self._encoding)\n",
    "        field_values[\"namespace\"] = field_values[\"namespace\"].decode(\n",
    "            encoding=self._encoding, errors=\"ignore\")\n",
    "        return field_values\n",
    "\n",
    "\n",
    "class ArticleEntryBlock(DirectoryBlock):\n",
    "    def __init__(self, encoding):\n",
    "        super().__init__(ARTICLE_ENTRY, encoding)\n",
    "\n",
    "\n",
    "class RedirectEntryBlock(DirectoryBlock):\n",
    "    def __init__(self, encoding):\n",
    "        super().__init__(REDIRECT_ENTRY, encoding)        \n",
    "\n",
    "\n",
    "class ZIMFile:\n",
    "    \"\"\"\n",
    "    The main class to access a ZIM file.\n",
    "    Two important public methods are:\n",
    "        get_article_by_url(...)\n",
    "      is used to retrieve an article given its namespace and url.\n",
    "\n",
    "        get_main_page()\n",
    "      is used to retrieve the main page article for the given ZIM file.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename, encoding):\n",
    "        self._enc = encoding\n",
    "        # open the file as a binary file\n",
    "        self.file = open(filename, \"rb\")\n",
    "        # retrieve the header fields\n",
    "        self.header_fields = HeaderBlock(self._enc).unpack_from_file(self.file)\n",
    "        self.mimetype_list = MimeTypeListBlock(self._enc).unpack_from_file(\n",
    "            self.file, self.header_fields[\"mimeListPos\"])\n",
    "        # create the object once for easy access\n",
    "        self.redirectEntryBlock = RedirectEntryBlock(self._enc)\n",
    "\n",
    "        self.articleEntryBlock = ArticleEntryBlock(self._enc)\n",
    "        self.clusterFormat = ClusterBlock(self._enc)\n",
    "\n",
    "    def _read_offset(self, index, field_name, field_format, length):\n",
    "        # move to the desired position in the file\n",
    "        if index != 0xffffffff:\n",
    "            self.file.seek(self.header_fields[field_name] + int(length*index))\n",
    "\n",
    "            # and read and return the particular format\n",
    "            read = self.file.read(length)\n",
    "            # return unpack(\"<\" + field_format, self.file.read(length))[0]\n",
    "            return unpack(\"<\" + field_format, read)[0]\n",
    "        return None\n",
    "\n",
    "    def _read_url_offset(self, index):\n",
    "        return self._read_offset(index, \"urlPtrPos\", \"Q\", 8)\n",
    "\n",
    "    def _read_title_offset(self, index):\n",
    "        return self._read_offset(index, \"titlePtrPos\", \"L\", 4)\n",
    "\n",
    "    def _read_cluster_offset(self, index):\n",
    "        return self._read_offset(index, \"clusterPtrPos\", \"Q\", 8)\n",
    "\n",
    "    def _read_directory_entry(self, offset):\n",
    "        \"\"\"\n",
    "        Read a directory entry using an offset.\n",
    "        :return: a DirectoryBlock - either as Article Entry or Redirect Entry\n",
    "        \"\"\"\n",
    "        logging.debug(\"reading entry with offset \" + str(offset))\n",
    "\n",
    "        self.file.seek(offset)  # move to the desired offset\n",
    "\n",
    "        # retrieve the mimetype to determine the type of block\n",
    "        fields = unpack(\"<H\", self.file.read(2))\n",
    "\n",
    "        # get block class\n",
    "        if fields[0] == 0xffff:\n",
    "            directory_block = self.redirectEntryBlock\n",
    "        else:\n",
    "            directory_block = self.articleEntryBlock\n",
    "        # unpack and return the desired Directory Block\n",
    "        return directory_block.unpack_from_file(self.file, offset)\n",
    "\n",
    "    def read_directory_entry_by_index(self, index):\n",
    "        \"\"\"\n",
    "        Read a directory entry using an index.\n",
    "        :return: a DirectoryBlock - either as Article Entry or Redirect Entry\n",
    "        \"\"\"\n",
    "        # find the offset for the given index\n",
    "        offset = self._read_url_offset(index)\n",
    "        if offset is not None:\n",
    "            # read the entry at that offset\n",
    "            directory_values = self._read_directory_entry(offset)\n",
    "            # set the index in the list of values\n",
    "            directory_values[\"index\"] = index\n",
    "            return directory_values  # and return all these directory values\n",
    "\n",
    "    def _read_blob(self, cluster_index, blob_index):\n",
    "        # get the cluster offset\n",
    "        offset = self._read_cluster_offset(cluster_index)\n",
    "        # get the actual cluster data\n",
    "        cluster_data = ClusterData(self.file, offset, self._enc)\n",
    "        # return the data read from the cluster at the given blob index\n",
    "        return cluster_data.read_blob(blob_index)\n",
    "\n",
    "    def _get_article_by_index(self, index, follow_redirect=True):\n",
    "        # get the info from the DirectoryBlock at the given index\n",
    "        entry = self.read_directory_entry_by_index(index)\n",
    "        if entry is not None:\n",
    "            # check if we have a Redirect Entry\n",
    "            if 'redirectIndex' in entry.keys():\n",
    "                # if we follow up on redirects, return the article it is\n",
    "                # pointing to\n",
    "                if follow_redirect:\n",
    "                    logging.debug(\"redirect to \" + str(entry['redirectIndex']))\n",
    "                    return self._get_article_by_index(entry['redirectIndex'],\n",
    "                                                      follow_redirect)\n",
    "                # otherwise, simply return no data\n",
    "                # and provide the redirect index as the metadata.\n",
    "                else:\n",
    "                    return Article(None, entry['namespace'],\n",
    "                                   entry['redirectIndex'])\n",
    "            else:  # otherwise, we have an Article Entry\n",
    "                # get the data and return the Article\n",
    "                data = self._read_blob(entry['clusterNumber'],\n",
    "                                       entry['blobNumber'])\n",
    "                return Article(data, entry['namespace'],\n",
    "                               self.mimetype_list[entry['mimetype']])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _get_entry_by_url(self, namespace, url, linear=False):\n",
    "        if linear:  # if we are performing a linear search ...\n",
    "            # ... simply iterate over all articles\n",
    "            for idx in range(self.header_fields['articleCount']):\n",
    "                # get the info from the DirectoryBlock at that index\n",
    "                entry = self.read_directory_entry_by_index(idx)\n",
    "                # if we found the article ...\n",
    "                if entry['url'] == url and entry['namespace'] == namespace:\n",
    "                    # return the DirectoryBlock entry and index of the entry\n",
    "                    return entry, idx\n",
    "            # return None, None if we could not find the entry\n",
    "            return None, None\n",
    "        else:\n",
    "            front = middle = 0\n",
    "            end = len(self)\n",
    "            title = full_url(namespace, url)\n",
    "            logging.debug(\"performing binary search with boundaries \" +\n",
    "                          str(front) + \" - \" + str(end))\n",
    "            found = False\n",
    "            # continue as long as the boundaries don't cross and\n",
    "            # we haven't found it\n",
    "            while front <= end and not found:\n",
    "                middle = floor((front + end) / 2)  # determine the middle index\n",
    "                entry = self.read_directory_entry_by_index(middle)\n",
    "                logging.debug(\"checking \" + entry['url'])\n",
    "                found_title = full_url(entry['namespace'], entry['url'])\n",
    "                if found_title == title:\n",
    "                    found = True  # flag it if the item is found\n",
    "                else:\n",
    "                    if found_title < title:  # if the middle is too early ...\n",
    "                        # move the front index to middle\n",
    "                        # (+ 1 to ensure boundaries can be crossed)\n",
    "                        front = middle + 1\n",
    "                    else:  # if the middle falls too late ...\n",
    "                        # move the end index to middle\n",
    "                        # (- 1 to ensure boundaries can be crossed)\n",
    "                        end = middle - 1\n",
    "            if found:\n",
    "                # return the tuple with directory entry and index\n",
    "                # (note the comma before the second argument)\n",
    "                return self.read_directory_entry_by_index(middle), middle\n",
    "            return None, None\n",
    "\n",
    "    def get_article_by_url(self, namespace, url, follow_redirect=True):\n",
    "        entry, idx = self._get_entry_by_url(namespace, url)  # get the entry\n",
    "        if idx:  # we found an index and return the article at that index\n",
    "            return self._get_article_by_index(\n",
    "                idx, follow_redirect=follow_redirect)\n",
    "\n",
    "    def get_main_page(self):\n",
    "        \"\"\"\n",
    "        Get the main page of the ZIM file.\n",
    "        \"\"\"\n",
    "        main_page = self._get_article_by_index(self.header_fields['mainPage'])\n",
    "        if main_page is not None:\n",
    "            return main_page\n",
    "\n",
    "    def metadata(self):\n",
    "        \"\"\"\n",
    "        Retrieve the metadata attached to the ZIM file.\n",
    "        :return: a dict with the entry url as key and the metadata as value\n",
    "        \"\"\"\n",
    "        metadata = {}\n",
    "        # iterate backwards over the entries\n",
    "        for i in range(self.header_fields['articleCount'] - 1, -1, -1):\n",
    "            entry = self.read_directory_entry_by_index(i)  # get the entry\n",
    "            if entry['namespace'] == 'M':  # check that it is still metadata\n",
    "                # turn the key to lowercase as per Kiwix standards\n",
    "                m_name = entry['url'].lower()\n",
    "                # get the data, which is encoded as an article\n",
    "                metadata[m_name] = self._get_article_by_index(i)[0]\n",
    "            else:  # stop as soon as we are no longer looking at metadata\n",
    "                break\n",
    "        return metadata\n",
    "\n",
    "    def __len__(self):  # retrieve the number of articles in the ZIM file\n",
    "        return self.header_fields['articleCount']\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Create an iterator generator to retrieve all articles in the ZIM file.\n",
    "        :return: a yielded entry of an article, containing its full URL,\n",
    "                  its title, and the index of the article\n",
    "        \"\"\"\n",
    "        for idx in range(self.header_fields['articleCount']):\n",
    "            # get the Directory Entry\n",
    "            entry = self.read_directory_entry_by_index(idx)\n",
    "            if entry['namespace'] == \"A\":\n",
    "                # add the full url to the entry\n",
    "                entry['fullUrl'] = full_url(entry['namespace'], entry['url'])\n",
    "                yield entry['fullUrl'], entry['title'], idx\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        \"\"\"\n",
    "        Ensure the ZIM file is properly closed when the object is destroyed.\n",
    "        \"\"\"\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargar el modelo de BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T21:10:25.544587Z",
     "start_time": "2019-08-18T21:10:22.963088Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 23:10:25.301802 139839697622848 deprecation_wrapper.py:119] From bert_repo/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelo_de_bert = 'bert_multi_cased_L-12_H-768_A-12/1'  #@param [\"bert_uncased_L-12_H-768_A-12/1\", \"bert_cased_L-12_H-768_A-12/1\", \"bert_uncased_L-24_H-1024_A-16/1\", \"bert_cased_L-24_H-1024_A-16/1\", \"bert_multi_cased_L-12_H-768_A-12/1\"]\n",
    "bert = hub.Module('https://tfhub.dev/google/' + modelo_de_bert)\n",
    "\n",
    "# instanciar el tokenizador\n",
    "tokenization_info = bert(signature='tokenization_info', as_dict=True)\n",
    "vocab_file, do_lower_case = sess.run([\n",
    "    tokenization_info['vocab_file'],\n",
    "    tokenization_info['do_lower_case'],\n",
    "])\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file,\n",
    "                                       do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T22:28:46.127992Z",
     "start_time": "2019-08-18T22:28:46.120649Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_len = 512\n",
    "\n",
    "input_ids = tf.placeholder(name='input_ids',\n",
    "                           shape=(batch_size, max_len),\n",
    "                           dtype='int32')\n",
    "input_mask = tf.placeholder(name='input_mask',\n",
    "                            shape=(batch_size, max_len),\n",
    "                            dtype='int32')\n",
    "segment_ids = tf.placeholder(name='segment_ids',\n",
    "                             shape=(batch_size, max_len),\n",
    "                             dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T22:28:47.909265Z",
     "start_time": "2019-08-18T22:28:46.130117Z"
    }
   },
   "outputs": [],
   "source": [
    "# instanciar el modelo\n",
    "bert_model = bert(dict(input_ids=input_ids,\n",
    "                       input_mask=input_mask,\n",
    "                       segment_ids=segment_ids),\n",
    "                  signature=\"tokens\",\n",
    "                  as_dict=True)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular los embedding para todos los artículos de Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T22:28:47.917961Z",
     "start_time": "2019-08-18T22:28:47.911430Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_inputs_for_bert(texts):\n",
    "    examples = []\n",
    "    mask = []\n",
    "    segment = []\n",
    "    label = []\n",
    "    for text in texts:\n",
    "        q = tokenizer.tokenize(text)\n",
    "        pad = [0] * (max_len - (len(q) + 2))\n",
    "        examples.append(\n",
    "            tokenizer.convert_tokens_to_ids(['[CLS]'] + q + ['[SEP]'])[:max_len] + pad)\n",
    "        mask.append([1] * min(len(q) + 2, max_len) + pad)\n",
    "        segment.append([0] * max_len)\n",
    "    return (np.array(examples), np.array(mask), np.array(segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T22:28:47.930060Z",
     "start_time": "2019-08-18T22:28:47.919574Z"
    }
   },
   "outputs": [],
   "source": [
    "zim_file = ZIMFile('wikipedia_es_all_nopic_2019-06.zim', 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-18T22:28:47.112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 72640 embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42805dc4de7b4025b7a6a99189eff4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=104271), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_every = 10\n",
    "#embeddings = []\n",
    "embeddings = pickle.load(open('embeddings.p', 'rb'))\n",
    "print(f'Read {len(embeddings * batch_size)} embeddings')\n",
    "\n",
    "for _, i in enumerate(\n",
    "        tqdm_notebook(\n",
    "            range(len(embeddings * batch_size),\n",
    "                  zim_file.header_fields['articleCount'], batch_size))):\n",
    "    texts = []\n",
    "    for j in range(batch_size):\n",
    "        soup = BeautifulSoup(\n",
    "            zim_file._get_article_by_index(i + j).data, \"lxml\")\n",
    "\n",
    "        # kill all script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()  # rip it out\n",
    "\n",
    "        # get text\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines\n",
    "                  for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        texts.append('\\n'.join(chunk for chunk in chunks if chunk))\n",
    "\n",
    "    examples, mask, segment = prepare_inputs_for_bert(texts)\n",
    "    embeddings.append(\n",
    "        sess.run(bert_model['pooled_output'],\n",
    "                 feed_dict={\n",
    "                     input_ids: examples,\n",
    "                     input_mask: mask,\n",
    "                     segment_ids: segment\n",
    "                 }))\n",
    "    if (_ + 1) % save_every == 0:\n",
    "        pickle.dump(embeddings, open('embeddings.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T18:54:49.070469Z",
     "start_time": "2019-08-18T18:54:49.059757Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(embeddings, open('embeddings', 'wb'))\n",
    "zim_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probar el motor de búsqueda semántica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T22:16:02.223230Z",
     "start_time": "2019-08-18T22:16:02.087880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72640, 768)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zim_file = ZIMFile('wikipedia_es_all_nopic_2019-06.zim', 'utf-8')\n",
    "embeddings = pickle.load(open('embeddings.p', 'rb'))\n",
    "embeddings = np.vstack(embeddings)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T22:16:04.880326Z",
     "start_time": "2019-08-18T22:16:02.224629Z"
    }
   },
   "outputs": [],
   "source": [
    "busqueda = \"playas de españa\"  #@param {type: 'integer'}\n",
    "top_n = 10  #@param {type: 'integer'}\n",
    "\n",
    "input_ids, input_mask, segment_ids = prepare_inputs_for_bert([busqueda])\n",
    "bert_model = bert(dict(input_ids=input_ids,\n",
    "                       input_mask=input_mask,\n",
    "                       segment_ids=segment_ids),\n",
    "                  signature=\"tokens\",\n",
    "                  as_dict=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "target_embedding = sess.run(bert_model['pooled_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-18T22:26:53.268897Z",
     "start_time": "2019-08-18T22:26:53.040976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Resultados de la búsqueda <i>playas de españa</i></h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>1. [0.98] <a href=https://es.wikipedia.org/wiki/500s>500s</a></h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://es.wikipedia.org/wiki/500s width=100%></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>2. [0.98] <a href=https://es.wikipedia.org/wiki/1100s>1100s</a></h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://es.wikipedia.org/wiki/1100s width=100%></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>3. [0.98] <a href=https://es.wikipedia.org/wiki//dev/null>/dev/null</a></h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://es.wikipedia.org/wiki//dev/null width=100%></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>4. [0.98] <a href=https://es.wikipedia.org/wiki/914>914</a></h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://es.wikipedia.org/wiki/914 width=100%></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>5. [0.98] <a href=https://es.wikipedia.org/wiki/64_Ozumo>64_Ozumo</a></h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://es.wikipedia.org/wiki/64_Ozumo width=100%></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>6. [0.98] <a href=https://es.wikipedia.org/wiki/64_Ōzumō>64_Ōzumō</a></h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://es.wikipedia.org/wiki/64_Ōzumō width=100%></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>7. [0.98] <a href=https://es.wikipedia.org/wiki/10_petametros>10_petametros</a></h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://es.wikipedia.org/wiki/10_petametros width=100%></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>8. [0.98] <a href=https://es.wikipedia.org/wiki/10_petámetros>10_petámetros</a></h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://es.wikipedia.org/wiki/10_petámetros width=100%></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>9. [0.98] <a href=https://es.wikipedia.org/wiki/1_E+16_m>1_E+16_m</a></h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://es.wikipedia.org/wiki/1_E+16_m width=100%></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>10. [0.98] <a href=https://es.wikipedia.org/wiki/1_E16_m>1_E16_m</a></h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe src=https://es.wikipedia.org/wiki/1_E16_m width=100%></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<h2>Resultados de la búsqueda <i>' + busqueda + '</i></h2>'))\n",
    "cosine_similarities = pd.Series(\n",
    "    cosine_similarity(target_embedding, embeddings).flatten())\n",
    "for _, (i, similarity) in enumerate(\n",
    "        cosine_similarities.nlargest(top_n).iteritems()):\n",
    "    display(\n",
    "        HTML(\n",
    "            f'<h3>{_+1}. [{similarity:.2f}] <a href=https://es.wikipedia.org/wiki/{zim_file.read_directory_entry_by_index(i)[\"url\"]}>{zim_file.read_directory_entry_by_index(i)[\"url\"]}</a></h3>'\n",
    "        ))\n",
    "    display(\n",
    "        HTML('<iframe src=https://es.wikipedia.org/wiki/' +\n",
    "             zim_file.read_directory_entry_by_index(i)[\"url\"] +\n",
    "             ' width=100%></iframe>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store URLs in final embedding pickle so we can remove wikipedia file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
