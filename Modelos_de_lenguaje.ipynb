{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "es"
   },
   "source": [
    "# Modelos de lenguaje\n",
    "\n",
    "Un modelo de lenguaje es una función que estime la probabilidad de la siguiente palabra (o *token*) condicionada al texto que la precede. Aquí vamos a usar el modelo de lenguaje GPT-2 para predecir la continuación de una frase y para llamara a la atención a construcciones poco probables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Language models\n",
    "\n",
    "A language model is a function that estimates the probability of the next word (or *token*) conditioned on the text that precedes it. Here we are going to use the GPT-2 language model to predict the continuation of a sentence and to draw attention to unlikely constructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2Config\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T11:03:01.794494Z",
     "start_time": "2020-01-24T11:03:01.776597Z"
    }
   },
   "outputs": [],
   "source": [
    "# basado en https://github.com/huggingface/transformers/blob/master/examples/run_generation.py\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config,)), ())\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2LMHeadModel, GPT2Tokenizer),\n",
    "}\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(model, tokenizer, length, context, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
    "                    device='cpu'):\n",
    "    text = ''\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(1, 1)\n",
    "    logits = [(tokenizer.decode(context[0, 0].item()), 0)]\n",
    "    with torch.no_grad():\n",
    "        for token in range(context.shape[1] + length - 1):\n",
    "\n",
    "            if token < context.shape[1]:\n",
    "                generated = context[:, :token+1]\n",
    "            inputs = {'input_ids': generated}\n",
    "\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "            if token < context.shape[1]-1:\n",
    "                logits.append((tokenizer.decode(context[0, token+1].item()), next_token_logits[0, context[0, token+1]].item()))\n",
    "\n",
    "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
    "            for i in range(1):\n",
    "                for _ in set(generated[i].tolist()):\n",
    "                    next_token_logits[i, _] /= repetition_penalty\n",
    "                \n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            if temperature == 0: # greedy sampling:\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            \n",
    "            if token >= context.shape[1]-1:\n",
    "                logits.append((tokenizer.decode(generated[0, -1].item()), next_token_logits[0, generated[0, -1]].item()))\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T10:46:46.574348Z",
     "start_time": "2020-01-24T10:46:15.508712Z"
    }
   },
   "outputs": [],
   "source": [
    "model_type = 'gpt2'\n",
    "model_name_or_path = 'gpt2-xl'\n",
    "length = 20\n",
    "temperature = 0\n",
    "repetition_penalty = 1.0\n",
    "top_k = 0\n",
    "top_p = 0.9\n",
    "no_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "model_type = model_type.lower()\n",
    "model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path)\n",
    "model = model_class.from_pretrained(model_name_or_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "if length < 0 and model.config.max_position_embeddings > 0:\n",
    "    length = model.config.max_position_embeddings\n",
    "elif 0 < model.config.max_position_embeddings < length:\n",
    "    length = model.config.max_position_embeddings  # No generation bigger than model size \n",
    "elif length < 0:\n",
    "    length = MAX_LENGTH  # avoid infinite loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T11:30:03.218227Z",
     "start_time": "2020-01-24T11:30:03.194562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad39a575ba464208987737bf7173e22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', layout=Layout(height='100px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3625b27af58847bb9d68b8e8af90694e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = widgets.Textarea(\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"100px\")\n",
    ")\n",
    "output = widgets.HTML()\n",
    "display(text, output)\n",
    "\n",
    "def on_text_changed(b):\n",
    "    if len(b.new) == 0:\n",
    "        output.value = ''\n",
    "        return\n",
    "    context_tokens = tokenizer.encode(b.new, add_special_tokens=False)\n",
    "    predict = 5\n",
    "    logits = sample_sequence(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        context=context_tokens,\n",
    "        length=predict,\n",
    "        temperature=0,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        device=device,\n",
    "    )\n",
    "    html = ''\n",
    "    for _ in range(len(logits) - predict):\n",
    "        prob = np.exp(logits[_][1])/(1 + np.exp(logits[_][1])) if _ > 0 else 1\n",
    "        background = 'rgb(255,' + str(int(255 * prob)) + ',' + str(int(255 * prob)) + ')'\n",
    "        color = 'black' if prob > 0.5 else 'white'\n",
    "        html += '<span style=\"background-color: '+ background + '; color: ' + color + '\">' + logits[_][0] + '</span>'\n",
    "    html += '<span style=\"color: gray\">' + ''.join([logits[_][0] for _ in range(len(logits) - predict, len(logits))]) + '</span>'\n",
    "    output.value = html\n",
    "\n",
    "text.observe(on_text_changed, names=\"value\", type=\"change\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aventuras-con-textos",
   "language": "python",
   "name": "aventuras-con-textos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
