{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "aventuras-con-textos",
      "language": "python",
      "name": "aventuras-con-textos"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "nbTranslate": {
      "displayLangs": [
        "en"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "es",
      "targetLang": "en",
      "useGoogleTranslate": true
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Modelos_de_lenguaje.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "lang": "es",
        "id": "4H0nCXfF2ITd",
        "colab_type": "text"
      },
      "source": [
        "# Modelos de lenguaje\n",
        "\n",
        "Un modelo de lenguaje es una función que estime la probabilidad de la siguiente palabra (o *token*) condicionada al texto que la precede. Aquí vamos a usar el modelo de lenguaje GPT-2 para predecir la continuación de una frase y para llamara a la atención a construcciones poco probables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lang": "en",
        "id": "xdB4dl862ITr",
        "colab_type": "text"
      },
      "source": [
        "# Language models\n",
        "\n",
        "A language model is a function that estimates the probability of the next word (or *token*) conditioned on the text that precedes it. Here we are going to use the GPT-2 language model to predict the continuation of a sentence and to draw attention to unlikely constructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfPtO36w5xVO",
        "colab_type": "text"
      },
      "source": [
        "### Instalar la librería de Transformers de Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHqPNDJ24YTj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "3ddb7b28-7755-4997-e603-f75edfa30d9c"
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 450kB 3.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 80.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 870kB 61.8MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jukPlE3s54Ai",
        "colab_type": "text"
      },
      "source": [
        "### Importar las librerías"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COeclCGU2ITy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "462ed5a7-5d76-46a6-b879-af50eb742cb1"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import sys\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from transformers import GPT2Config\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pViA8Abu57qq",
        "colab_type": "text"
      },
      "source": [
        "### Basado en https://github.com/huggingface/transformers/blob/master/examples/run_generation.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-24T11:03:01.794494Z",
          "start_time": "2020-01-24T11:03:01.776597Z"
        },
        "id": "UCLSM0rr2IUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'gpt2': (GPT2LMHeadModel, GPT2Tokenizer),\n",
        "}\n",
        "\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (batch size x vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(model, tokenizer, length, context, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
        "                    device='cpu'):\n",
        "    text = ''\n",
        "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(1, 1)\n",
        "    logits = [(tokenizer.decode(context[0, 0].item()), 0)]\n",
        "    with torch.no_grad():\n",
        "        for token in range(context.shape[1] + length - 1):\n",
        "\n",
        "            if token < context.shape[1]:\n",
        "                generated = context[:, :token+1]\n",
        "            inputs = {'input_ids': generated}\n",
        "\n",
        "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
        "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
        "            if token < context.shape[1]-1:\n",
        "                logits.append((tokenizer.decode(context[0, token+1].item()), next_token_logits[0, context[0, token+1]].item()))\n",
        "\n",
        "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
        "            for i in range(1):\n",
        "                for _ in set(generated[i].tolist()):\n",
        "                    next_token_logits[i, _] /= repetition_penalty\n",
        "                \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            if temperature == 0: # greedy sampling:\n",
        "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
        "            else:\n",
        "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "            \n",
        "            if token >= context.shape[1]-1:\n",
        "                logits.append((tokenizer.decode(generated[0, -1].item()), next_token_logits[0, generated[0, -1]].item()))\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2g9VqMx6PiU",
        "colab_type": "text"
      },
      "source": [
        "### Descargar los checkpoints e inicializar el modelo (puede tardar bastante)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-24T10:46:46.574348Z",
          "start_time": "2020-01-24T10:46:15.508712Z"
        },
        "id": "hhqhsjxo2IUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_type = 'gpt2'\n",
        "model_name_or_path = 'gpt2-xl'\n",
        "length = 20\n",
        "temperature = 0\n",
        "repetition_penalty = 1.0\n",
        "top_k = 0\n",
        "top_p = 0.9\n",
        "no_cuda = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
        "model_type = model_type.lower()\n",
        "model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
        "tokenizer = tokenizer_class.from_pretrained(model_name_or_path)\n",
        "model = model_class.from_pretrained(model_name_or_path)\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGgPBFb66UFd",
        "colab_type": "text"
      },
      "source": [
        "### Predecir las siguientes palabras y destacar construcciones poco probables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-24T11:30:03.218227Z",
          "start_time": "2020-01-24T11:30:03.194562Z"
        },
        "id": "HcSiyNnU2IUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict = 5\n",
        "text = widgets.Textarea(\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"100px\")\n",
        ")\n",
        "output = widgets.HTML()\n",
        "display(text, output)\n",
        "\n",
        "def on_text_changed(b):\n",
        "    if len(b.new) == 0:\n",
        "        output.value = ''\n",
        "        return\n",
        "    context_tokens = tokenizer.encode(b.new, add_special_tokens=False)\n",
        "    logits = sample_sequence(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        context=context_tokens,\n",
        "        length=predict,\n",
        "        temperature=0,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        device=device,\n",
        "    )\n",
        "    html = ''\n",
        "    for _ in range(len(logits) - predict):\n",
        "        prob = np.exp(logits[_][1])/(1 + np.exp(logits[_][1])) if _ > 0 else 1\n",
        "        background = 'rgb(255,' + str(int(255 * prob)) + ',' + str(int(255 * prob)) + ')'\n",
        "        color = 'black' if prob > 0.5 else 'white'\n",
        "        html += '<span style=\"background-color: '+ background + '; color: ' + color + '\">' + logits[_][0] + '</span>'\n",
        "    html += '<span style=\"color: gray\">' + ''.join([logits[_][0] for _ in range(len(logits) - predict, len(logits))]) + '</span>'\n",
        "    output.value = html\n",
        "\n",
        "text.observe(on_text_changed, names=\"value\", type=\"change\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}